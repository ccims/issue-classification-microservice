[{"labels":[null,"enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\n## Motivation\r\n\r\nThere is a pervasive pattern within the PyTorch community where users have the need to prepare a Module before calling torch.jit.script. An example here might include replacing an instance of an object with an instance of its JIT-able equivalent (e.g. [PyText ScriptVocab](https://github.com/facebookresearch/pytext/blob/master/pytext/torchscript/vocab.py#L12)) for deployment.\r\n\r\n## Pitch\r\n\r\nAdd a method .script to nn.Module, which is called in a fashion similar to nn.Module.eval. \r\n\r\nThe method can be overridden by the user to allow a user-defined preparation of the object for scripting. This might, for example, involve replacing or removing member variables that are not JIT-able.\r\n\r\n## Alternatives\r\n\r\nConcrete user-defined methods such as torchtextâ€™s [to_ivalue](https://github.com/pytorch/text/blob/53e3ae293359bc2a629d27e195e99d1e7f7b85c3/test/data/test_functional.py#L93) or separate wrappers such as [PyText ScriptVocab](https://github.com/facebookresearch/pytext/blob/master/pytext/torchscript/vocab.py#L12)\r\n## Additional context\r\n\r\nThe method should be out of place so the original module input, which the user may continue to use, is not modified. The method should be called within torch.jit.script (if present) so as not to expand the API surface for the end user.\n\ncc @gmagogsfm @albanD @mruberry"},{"labels":["enhancement",null,null],"text":"It would be useful for mode filter that should operate on discrete inputs (for smoothing discrete outptus):\r\n```python\r\nimport torch\r\n\r\ndef mode_pooling1d(tensor, kernel_size, stride, padding):\r\n    return torch.nn.functional.unfold(tensor[:, None, None, :], kernel_size = (1, kernel_size), padding = (0, padding), stride = (1, stride)).mode(1).values\r\n\r\nif __name__ == '__main__':\r\n    x = torch.randint(low = 0, high = 3, size = (2, 128))\r\n    y = mode_pooling1d(x, kernel_size = 7, padding = 7 // 2, stride = 1)\r\n    print(x.shape, y.shape)\r\n```\r\n```\r\n).values\r\n  File \"/miniconda/lib/python3.8/site-packages/torch/nn/functional.py\", line 3793, in unfold\r\n    return torch._C._nn.im2col(input, _pair(kernel_size),\r\nRuntimeError: \"im2col_out_cpu\" not implemented for 'Long'\r\n```"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nThe option for constraint-aware optimization techniques within torch.optim. [Sequential Quadratic Programming](https://en.wikipedia.org/wiki/Sequential_quadratic_programming) is a very common choice, but there are other algorithms out there.  \r\n\r\n## Motivation\r\nSQP (and related techniques) are nice for when parameters need to be within a specified range (or satify some other constraint function), such as when using optimization for solving a kinematics problem under real-world kinematic constraints. Or when optimizing a dynamic trajectory for a robot to follow such that the trajectory doesn't exceed a robot's physical limits.\r\n\r\n## Pitch\r\n\r\nPyTorch's optim package is quite powerful not just for neural networks, but for much more general optimization problems. The autograd functionality and it's ability to use GPU acceleration implicitly are super convenient.  However, none of the current optimization approaches in PyTorch are \"constraint-aware\" in the sense that the parameters you optimize can float wherever they need to in <img src=\"https://render.githubusercontent.com/render/math?math=\\mathbb{R}^n\">, where n in the dimension of whatever parameter space is being optimized, so long as that results in a smaller objective functional.  \r\n\r\n\r\n## Alternatives\r\n\r\nThere exist [python implementations of SQP and other non-linear optimization approaches](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html), but they are cpu-only and lack PyTorch's autograd functionality for computing derivatives/jacobians automatically (read: \"I am too lazy to implement derivatives myself\").\r\n\r\n## Additional context\r\nNone presently identified.\r\n\n\ncc @vincentqb"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nFuse `softmax` and mask calculation in `multi_head_attention_forward`.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nCurrently `torch.nn.functional.multi_head_attention_forward` masks attention and then calls `softmax` as follows:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/caea1adc35404b39fe4f2e3fa75f9a0b9e554bbc/torch/nn/functional.py#L4265-L4281\r\n\r\n(the code in C++ frontend is similar). In our own C++ implementation (not currently publicly available), we found that fusing these operations led to ~5% perfomance improvement in attention. Of course the number can't be expected to be the same in Pytorch, but it is probably a reasonable estimate.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nAdd a `masked_softmax` operation which effectively only calls `softmax` on un-masked values and fills the masked values with 0 without doing extra calculation. Include a fallback implementation for the cases we can't optimize (e.g. because an external library `softmax` is used, CUDA, etc.).\r\n\r\nIf accepted, we'll be happy to work on this.\r\n\r\nThe operation probably shouldn't be part of the public API, unless there are other significant uses for it.\r\n\n\ncc @zhangguanheng66"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nSo we have params and buffers in `nn.Module`, which are awesome.\r\n\r\nWhat can we do if we have a huge param like variable that we don't want to save with the model, e.g. if that variable gets deterministically calculated and it's huge. For example positional embeddings in transformers models https://github.com/huggingface/transformers/issues/7229 - in the presented example those come to 250MB and can be bigger, yet they aren't trained and are calculated.\r\n\r\nSo currently one needs to hack around this by either deleting the var from state_dict before saving, or like fairseq approached it - to make the variable non-parameteric, but use a small dummy buffer hack to move the large weights to the right device during `forward` (I'm still referring to the linked issue). Except the latter approach is problematic for torchscript, it wants all the vars to be on the same device before `forward`.\r\n\r\n## Request\r\n\r\nWhat if we had a third type of variable, which is like a parameter, but of a transient nature and which automatically gets ignored by load/save yet, otherwise behaves like param/buffer for all other purposes?\r\n\r\nPerhaps it has already been suggested/discussed.\r\n\r\nThank you for reading.\n\ncc @albanD @mruberry"},{"labels":["enhancement",null,null],"text":"First of all, thanks for this awesome library\r\n\r\n## ðŸš€ Feature\r\nAdd support of List(List(Tensors)) in onnx export facilities \r\n\r\n## Motivation\r\n\r\n(Note i already ask on pytorch forum but without luck, hope i m in the right place and you have time to have a look)\r\n\r\nI have some kind of transformer models (aka encoder/decoder)\r\nThe input of the decoder have the form List(List(Tensor)) where list have variable size and some dim of tensor is also dynamic.\r\nThe output have the form Tuple(List(Tensor),List(Tensor)) where list have variable size and some dim of tensor is also dynamic.\r\n(Note i m' not producing the model so i can't change the architecture :()\r\nI m able to convert to torchscript using scripting scheme, its great :)\r\nBut i need to translate the model to onnx to target dedicated device\r\nAnd i can't figure out what to set in torch.onnx.export :(\r\n## Pitch\r\n\r\nwhen i look at https://pytorch.org/docs/stable/onnx.html#frequently-asked-questions\r\n\"\"\"Q: Is tensor list exportable to ONNX?\"\"\"\r\nmy case is more tricky\r\n\r\ntorch.onnx.export(\r\n            decoder,\r\n            (output_encoder,),\r\n            \"decoder.onnx\",\r\n            opset_version=12,\r\n            input_names=[\"input\"],\r\n            output_names=[\"output\"],\r\n            dynamic_axes={\"input\": {0: \"sequence\"}, \"output\": {0: \"sequence\"}}, <---- howto expose my use case here ?\r\n        )\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\nbranch 1.6\r\n\n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nSupport for complex-valued `torch.cholesky`.\r\n\r\n## Motivation\r\nPytorch 1.6 supports complex-valued tensors, however, most of the matrix operations cannot be written in terms of their real analogue  (in contrast to e.g. multiplication, which currently also does not support complex-valued data). Cholesky decomposition is important in decorrelating e.g. Monte Carlo data. The MAGMA backend also supports Cholesky decompositions for complex-valued data.\r\n\r\n## Pitch\r\ntorch.cholesky supports complex-valued data.\r\n\r\n## Alternatives\r\nPossible workarounds\r\n\n\ncc @ezyang @gchanan @zou3519 @anjali411 @dylanbespalko @nikitaved"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nSupport NVIDIA UVM and allow to use RAM when there is not enough GPU memory\r\n\r\n## Motivation\r\n\r\nThe NVIDIA UVM technology allows to use RAM when the GPU memory is not enough to allocate, I believe this is helpful for those who want to train heavy model on limited GPU memory. The Tensorflow has already support UVM by setting per_process_gpu_memory_fraction bigger than 1, by the way.\r\n\r\n## Pitch\r\n\r\nAllow user to set a environment variable to use UVM.\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\nSome discussion and documents of Tensorflow to support UVM, these may be helpful.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L36\r\nhttps://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/\r\nhttps://github.com/tensorflow/tensorflow/commit/cd4f5840\r\nhttps://github.com/tensorflow/tensorflow/issues/9720\r\n\r\n\r\n\n\ncc @ngimel"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nAdd argument `layer_norm_eps` to `Transformer` modules.\r\n\r\n## Motivation\r\n\r\nCurrently `Transformer` modules initialize `LayerNorm` with default arguments (`eps=1e-5`), however different model architectures may require different eps values (e.g. BERT uses eps=1e-12). It would be better to make it an argument, so user could change it accordingly.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nAdd argument `layer_norm_eps` to `Transformer` modules, which will be passed to inner `LayerNorm` modules.\r\n\n\ncc @zhangguanheng66"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nI want to be able to determine the version of libtorch in my c++ binary in cases where I don't have access to the python library.\r\n\r\nSomething like:\r\n```\r\n#define TORCH_VERSION_MAJOR=1\r\n#define TORCH_VERSION_MINOR=6\r\n#define TORCH_VERSION_PATCH=0\r\n```\r\n\r\n## Motivation\r\n\r\nThis is useful for \r\n- Turning off torch features depending on the version of torch we are running\r\n- Debugging which version of torch is installed in production where different versions may be running.\r\n\r\n## Pitch\r\n\r\n1) I'm a developer trying to upgrade the API, but a backwards incompatible API change requires me to refactor code before upgrading.  I need to roll this out so I want to only enable the new features if I build my application for the new version of torch.\r\n2) I'm a site reliability engineer and I am rolling out a the server deploy with a new version of libtorch.  There is a bug that is claimed to be fixed in the newer version of torch and I want to verify that the new version was deployed when I validate this issue is resolved.\r\n\r\n## Alternatives\r\n\r\nCurrently I am pulling the version from the strings in libtorch_cpu.so, but this didn't get updated in v1.6.0.  This is an unreliable hack solution that I would like to remove.\r\n```\r\nstrings libtorch_cpu.so | egrep -A1 pytorch_version\r\npytorch_version\r\n1.5.0\r\n```\r\n\r\npytorch has the version number baked into the pip install and in the torch.__version__.  For my purposes I don't have access to either of these as we are not using python in our production environment and the shared libraries are built separate from the ones in the pip wheel.\r\n\r\n## Additional context\r\n\r\nI recently upgraded to libtorch version 1.6.0 to resolve a segfault on destruction of `DeviceThreadHandlePool` which seemed related to: https://github.com/pytorch/pytorch/pull/36416.  After upgrading I am still seeing some segfaults on destruction, but was worried that I might not have deployed the new version of libtorch.   By looking at other strings in that library I believe I am at v1.6.0, but I wasted some time on the red herring of the 1.5.0 version in the strings of libtorch_cpu.so and would like to avoid that in the future.\n\ncc @yf225 @glaringlee"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nSupport parallelized/asynchronous execution of ops on CPU. \r\n\r\nPyTorch currently supports [asynchronous execution on GPU](https://pytorch.org/docs/master/notes/cuda.html#asynchronous-execution), where ops are executed in parallel as long as there are no data dependencies between them. However, currently, there is no straightforward way to do this on CPU, even on machines with a large number of cores. \r\n\r\n## Motivation\r\n\r\nInterested in using PyTorch for Reinforcement Learning ([ML-Agents Toolkit](https://github.com/Unity-Technologies/ml-agents)). Currently (from what I understand) PyTorch parallelizes across CPUs in one of three main ways: \r\n\r\n1. Intra-op using OpenMP and associated optimizations\r\n2. Inter-op using JIT fork and wait\r\n3. Using Python multithreading (needs to contend with GIL) and multiprocessing. \r\n\r\nMethod 1 works well for typical supervised learning tasks, where the network is large and each op is quite expensive. However, none of the three ways makes a lot of sense for Reinforcement Learning workloads, where a) there could be multiple networks (e.g. Policy and Value), b) each network is fairly small (2-3 layers, <200 hidden units), and c) they often share a loss function, making multiprocess very complicated/difficult. \r\n\r\nFor reference, here is the pseudo-code for the update function of the Soft-Actor Crtitic algorithm (full code [here](https://github.com/Unity-Technologies/ml-agents/blob/376168bbb55deac540a572617fb70effffe98cd5/ml-agents/mlagents/trainers/sac/optimizer_torch.py#L438)). \r\n\r\n```\r\n# Sample observations, dones, rewards from experience replay buffer\r\nobservations, next_observations, dones, rewards = sample_batch()\r\n\r\n# Evaluate current policy on sampled observations\r\n(\r\n    sampled_actions,\r\n    log_probs,\r\n    entropies,\r\n    sampled_values,\r\n) = policy.sample_actions_and_values(observations)\r\n\r\n# Evaluate Q networks on observations and actions\r\nq1p_out, q2p_out = value_network(observations, sampled_actions)\r\nq1_out, q2_out = value_network(observations, actions)\r\n\r\n# Evaluate target network on next observations\r\nwith torch.no_grad():\r\n    target_values = target_network(next_observations)\r\n\r\n# Evaluate losses\r\nq1_loss, q2_loss = sac_q_loss(q1_out, q2_out, target_values, dones, rewards)\r\nvalue_loss = sac_value_loss(log_probs, sampled_values, q1p_out, q2p_out)\r\npolicy_loss = sac_policy_loss(log_probs, q1p_out)\r\nentropy_loss = sac_entropy_loss(log_probs)\r\n```\r\nNote that the `policy` contains a policy and a critic network, and each `value_network` consists of two Q networks, so the code above contains a total of 7 forward passes and 3 backwards passes, performed sequentially. As the networks are quite small, intra-op parallelism is not very effective (in fact setting `num_threads` to 1 is most performant).  \r\n\r\nIn fact, even after CPU/environment variable/thread optimization, **the resulting PyTorch code is about 2x slower than the equivalent TensorFlow code** while running on a 6-core CPU, with the approximate time spent 35% doing backprop, 45% during forward pass, and 20% during the optimizer step functions. No such performance gap is observed between Torch and TF on GPU. \r\n\r\nWe've tried alternatives 2 (JIT w/ forking) and 3 (multithreading) but didn't notice much more than around 5% improvement. We believe it's due to the overhead of spawning new threads outweighing the lightweight network evaluations. \r\n\r\n## Pitch\r\n\r\nSupport the exact mechanism on CUDA for CPU, respecting `num_interop_threads`, or provide a straightforward way to implicitly parallelize multiple networks. \r\n\r\n## Alternatives\r\n\r\nUsing JIT fork - Often requires considerable refactors. Furthermore, in our testing did not provide much benefit, possibly due to the overhead of forking new threads for small networks outweighing the benefit. \r\n\r\nUsing multithreading - Ops in networks are so small that the Python GIL is thrashed when trying to parallelize multiple networks. \r\n\r\nUsing multiprocessing - Requires significantly more complex code structure, especially because the loss functions are shared between multiple networks. \r\n\r\nAdditional alternatives/suggestions greatly welcome!\r\n\r\n## Additional context\r\n\r\nCode in question: https://github.com/Unity-Technologies/ml-agents/blob/376168bbb55deac540a572617fb70effffe98cd5/ml-agents/mlagents/trainers/sac/optimizer_torch.py#L438\r\n\r\n\r\ncc @VitalyFedyunin"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nI want to be able to use my system's zstd lib instead of the static library built by pytorch.\r\n\r\n## Motivation\r\n\r\nIt would allow for nicer integration into Linux distributions. Same reason for why there are so many use `USE_SYSTEM_` directives already."},{"labels":[null,"enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nWe are introducing a new flag to guard the behavior of NCCL Async Error Handling. Users can set the environment variable NCCL_ASYNC_ERROR_HANDLING to control this feature (set to 1 if NCCL should crash instead of hanging due to timeouts/errors). A generalized helper function to parse environment variables and set the appropriate flags in `ProcessGroupNCCL` would help make the code more modular, since we already have other flags like NCCL_BLOCKING_WAIT.\r\n\r\nSomething like the following could work (for binary env vars):\r\n\r\n```\r\nvoid parseBinaryEnvVar(std::string varName, bool& flag) {\r\n   char* varString = getenv(varName);\r\n     if (varString != nullptr) {\r\n       auto val = std::stoi(varString);\r\n       if (val == 1) {\r\n         flag = true;\r\n       } else if (val != 0) {\r\n         throw std::runtime_error(\r\n             \"Invalid value for environment variable: \" +\r\n             std::string(varName));\r\n       }\r\n    }\r\n}\r\n```\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nAdd an option to run GPU tests only, and skip all CPU tests\r\n\r\nFor example, we can have an environment variable `PYTORCH_TEST_GPU_ONLY=1`.\r\n\r\n## Motivation\r\n\r\nUsually, when people only want to test GPU functionalities, they can skip CPU tests to save plenty of times. \r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nFor example, some pytorch tests look like (assuming pytorch built with GPU)\r\n```python\r\ndef test_something(self, device):\r\n    # do something\r\n```\r\nAt runtime, two tests `test_something_cpu` and `test_something_cuda` will be generated. The goal is to skip the `test_something_cpu` test and only run `test_something_cuda`. \r\n\r\nFor example, we can do it like this\r\n```bash\r\nPYTORCH_TEST_GPU_ONLY=1 python test/run_test.py\r\n```\r\n\r\n## Alternatives\r\n\r\nN/A\r\n\r\n## How to implement \r\n\r\nIf I understand the code correctly, this feature can be easily implemented like this\r\n\r\n1. Add `TEST_GPU_ONLY = os.getenv('PYTORCH_TEST_GPU_ONLY', '0') == '1' ` here\r\nhttps://github.com/pytorch/pytorch/blob/665feda15bc45d0f50326596ecde6f2d96ac6644/torch/testing/_internal/common_utils.py#L346-L347\r\n\r\n2. Add something like\r\n```python\r\n    if TEST_GPU_ONLY:\r\n        fullname = self.id().lower()\r\n        is_cuda_test = 'gpu' in fullname or 'cuda' in fullname or 'cuda' in torch.tensor([]).device.type\r\n        if not is_cuda_test:\r\n            raise unittest.SkipTest(\"Test is probably not a GPU test. We disabled it with PYTORCH_TEST_GPU_ONLY\")\r\n```\r\nhere in `setup` of `class common_utils.TestCase`\r\nhttps://github.com/pytorch/pytorch/blob/665feda15bc45d0f50326596ecde6f2d96ac6644/torch/testing/_internal/common_utils.py#L825-L833\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\r\nN/A \r\n\r\ncc @mruberry @VitalyFedyunin @ptrblck @ngimel \r\n"},{"labels":["enhancement",null,null,null],"text":"with @lw @beauby \r\n\r\n## API\r\n\r\nThere is no new API, but has a new behavior for sending/receiving CUDA tensors. The TensorPipe RPC agent will maintain a stream pool, and grab streams from the pool to 1) send tensors 2) receive tensors 3) run user functions. \r\n\r\nThe guarantees we offer are:\r\n\r\n1. RPC user functions are launched when all comm ops are enqueued to the current stream for all involved devices, but these is no guarantee any of the comm ops are done. This should be fine if user functions do not switch streams. However, if they do use different stream, they will need to explicitly synchronize. \r\n2. `NCCL_BLOCKING_WAIT` dictates the behavior of `Future.wait()` when the response contains CUDA tensors. If `NCCL_BLOCKING_WAIT=1` at the RPC initialization time, `Future.wait()` means that all ops are done. Otherwise, `Future.wait()` only means all CUDA ops are enqueued to the stream. \r\n\r\n## Design \r\n\r\ntwo principles:\r\n\r\n* No RPC CUDA tensor transfer should block ops in the current stream. \r\n* All ops in a stream must be cleared before it is returned to the pool to avoid pollute next user of this stream.\r\n\r\n### Request on the Caller\r\n\r\n* Determine the set of devices used by the CUDA tensors in the message.\r\n* Get the current streams for all these devices.\r\n* Record an event in each of these current streams (and then we wonâ€™t do anything more on these streams)\r\n* Pick a new fresh stream for each device from the stream pool\r\n* On each of these new streams, enqueue a wait for the above events\r\n* Pass these new streams to TensorPipe in the form of `cudaStream_t` (as TensorPipe cannot depend on PyTorch and hence cannot use PyTorch's Stream type) \r\n* In the on-complete callback of `pipe->write`, either capture the stream in another lambda for `cudaStreamAddCallback` or use a thread to synchronize the stream before destructing it (returning it to the pool).\r\n\r\n\r\n### Request on the Callee\r\n\r\n\r\n* Let `TensorPipe::Tensor.metadata` capture the device information, so that callee can know which device it will use from the descriptor before calling `pipe->read`.\r\n* Grab one stream for each distinct device, and pass those streams to TensorPipe `pipe->read`.\r\n* In `pipe->read`'s on-complete callback, set those streams from the pool as current, and then directly launch user functions without synchronization. All comm ops are already enqueued into the current stream. If user functions need to use different streams, they need to explicitly synchronize the streams. \r\n\r\n\r\n### Response on the callee\r\n\r\n* Once the user function returns, preserve the current streams (they should still be the same ones from pool).\r\n* Do not synchronize on these streams and directly pass them to TensorPipe to send response message.\r\n* After the write callback from TensorPipe `pipe->write` fires, we synchronize on those streams and only return them to their pools once all the enqueued work on them has completed.\r\n\r\n### Response on the caller\r\n    \r\n\r\n* When the response is coming in, pick a stream for each relevant device from the pool.\r\n* Pass these streams to TensorPipe to use for receiving the response tensors\r\n* After the read callback fires, synchronize with the work in these streams before returning them to the pool.\r\n* If `NCCL_BLOCKING_WAIT=1`, mark the `Future` as complete after all streams are synchronized. Otherwise, mark the `Future` as complete when TensorPipe `pipe->read` returns. \r\n\r\n\r\n### Discussion\r\n\r\n1. Do we need a \"power-user\" mode to disable stream pool on the caller? We can remember the current stream when sending the requests and use them to receive responses. This can give users more control. \r\n2. Regarding stream pool, will use `c10::cuda::getStreamFromPool` in the first version, and then will add a new stream pool implement, which instead of using a fixed number of stream, it will create new ones when its queue is depleted. \r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar @jiayisuse @lw @beauby"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nShapeGuard allows you to very succinctly assert the expected shapes of tensors in a dynamic, einsum inspired style.\r\n\r\n## Motivation\r\n\r\nItâ€™s easy to make bugs in ml. One particular rich source of bugs is unintended broadcasting and the flexibility of the operators: a*b works whether a and b are vectors, scalar vector, vector vector, etc, although it might not do what you intended. Since we're doing optimization whatever computation we end up performing, we can probably optimize it to work reasonably, even if it's not doing what we intended. So our algorithm might \"work\" even if we have bugs (just less well). This makes bugs super hard to discover. \r\n\r\nThe best way Iâ€™ve found to avoid bugs is to religiously check the shapes of all my tensors, all the time, so I end up spending a lot of time debugging and writing comments like `# (bs, n_samples, z_size)` all over the place.\r\n\r\n## Pitch\r\n\r\n\r\nWhy not algorithmically check the shapes then? Well it gets ugly fast. \r\n1) You have to add `assert foo.shape == (bs, n_samples, x_size)` everywhere, which essentially doubles your linecount and \r\n2) you have to define all your dimensional sizes (bs, etc.), which might vary across train/test, batches, etc.\r\n\r\nSo I made a small helper that makes it much nicer. I call it `ShapeGuard`. When you import it, It adds a method `sg` to the tensor class, and exposes a static `ShapeGuard` class.\r\n\r\nYou use the `sg` method like an assert: \r\n\r\n```\r\ndef forward(self, x, y):\r\n    x.sg(\"bchw\")\r\n    y.sg(\"by\")\r\n```\r\n\r\nThis will verify that x has 4 dimensions, y has 2 dimensions and that x and y have the same size in the first dimension 'b'. If the assert passes, the tensor is returned. This means you can also use it inline on results of operations: `z = f(x).sg(\"bnz\")`. If the assert fails it produces a nice error message. See screenshots.\r\n\r\nIt works in the following way: the first time `sg` is called for an unseen shape, the size of the vector for that shape is saved in the `ShapeGuard.shapes` global dict. Subsequent calls sees this shape in the shapes dict and asserts that the tensor is the same shape for that dimension. If e.g. your batch size changes between train and test you can call `ShapeGuard.reset(\"b\")` to reset the \"b\" shape. I've found it works well to reset all shapes at the start of my main `nn.Module.forward` by calling `ShapeGuard.reset()`. If you want to verify an exact dimension you can pass a tuple of mixed strings or ints e.g.\r\n\r\n```\r\ndef forward(self, x, y):\r\n    x.sg((\"b\", 1, \"h\", \"w\"))\r\n    y.sg(\"by\")\r\n```\r\n\r\nThe special shape '*' is reserved for shapes that should not be asserted: `x.sg(\"*chw\")` would assert all shapes except the first.\r\n\r\n![image](https://user-images.githubusercontent.com/206013/91993619-e50b6400-ed35-11ea-91ba-5ac6902c797a.png)\r\n![image](https://user-images.githubusercontent.com/206013/91993581-d58c1b00-ed35-11ea-80c6-34c7b283bdbe.png)\r\n\r\n\r\nThe code is quite minimal\r\n\r\n```python\r\nimport torch as t\r\n\r\n\r\nclass ShapeGuard:\r\n    shapes = dict()\r\n\r\n    @staticmethod\r\n    def reset(dims=None):\r\n        if dims is None:\r\n            ShapeGuard.shapes = dict()\r\n        else:\r\n            for d in dims:\r\n                if d in ShapeGuard.shapes:\r\n                    del ShapeGuard.shapes[d]\r\n\r\n    @staticmethod\r\n    def assert_shape(actual_shape: t.Size, expected_shape):\r\n        assert len(actual_shape) == len(expected_shape), f\"expected {len(expected_shape)} dimensions but tensor had {len(tensor.shape)}\"\r\n        for d, e in zip(actual_shape, expected_shape):\r\n            if isinstance(e, int):\r\n                assert e == d, f\"expected {e} but was {d}\"\r\n            elif e == '*':\r\n                continue\r\n            elif e in ShapeGuard.shapes:\r\n                assert ShapeGuard.shapes[e] == d, f\"expected '{e}' to be {ShapeGuard.shapes[e]} but was {d}\"\r\n            else:\r\n                ShapeGuard.shapes[e] = d\r\n\r\n    @staticmethod\r\n    def assert_tensor_shape(tensor: t.Tensor, expected_shape) -> t.Tensor:\r\n        ShapeGuard.assert_shape(tensor.shape, expected_shape)\r\n        return tensor\r\n\r\n    @staticmethod\r\n    def assert_distribution_shape(dist: t.distributions.Distribution, expected_shape) -> t.distributions.Distribution:\r\n        ShapeGuard.assert_shape(dist.batch_shape, expected_shape)\r\n        return dist\r\n\r\n\r\nt.Tensor.sg = ShapeGuard.assert_tensor_shape\r\nt.distributions.Distribution.sg = ShapeGuard.assert_distribution_shape\r\n```\r\n\r\ncc @ezyang"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nA simple feature to allow passing custom keyword arguments for `nn.TransformerEncoderLayer` and `nn.TransformerDecoderLayer`. Ideally, users should be able to flexibly use `custom_encoder` and `custom_decoder` arguments of `nn.Transformer` which can accept arguments of any arbitrary name.\r\n\r\n## Motivation\r\n\r\nCurrently, the `forward` method of any custom encoders and decoders require to follow the exact API of `nn.TransformerEncoder` and `nn.TransformerDecoder` â€” what if custom transformer layers require some extra input tensors? For example, [DETR](https://github.com/facebookresearch/detr) injects sinusoidal positional embedding at every encoder layer. To enable this feature, one needs to reimplement/copy-paste large chunks of the codebase, [like DETR does here](https://github.com/facebookresearch/detr/blob/master/models/transformer.py)\r\n\r\n## Pitch\r\n\r\nLet the `torch.nn.modules.transformer` Transformer modules follow this common API:\r\n\r\n```python\r\nclass Transformer(nn.Module):\r\n    def __init__(self, *existing_pytorch_args):\r\n        # ... existing method body.\r\n\r\n    def forward(self, *existing_args,  **encoder_kwargs, **decoder_kwargs):\r\n        # Separate encoder_kwargs and decoder_kargs by checking \"encoder_\" and \"decoder_\"\r\n        # prefix. We can use `inpect.signature()` for this.\r\n        memory = self.encoder(..., **encoder_kwargs)\r\n        output = self.decoder(..., **decoder_kwargs)\r\n```\r\n\r\nSimilarly, `nn.TransformerEncoder` should allow `**encoder_layer_kwargs` and `nn.TransformerDecoderLayer` should allow `**decoder_layer_kwargs`.\r\n\n\ncc @zhangguanheng66"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nIt would be nice to make `torch.Generator` picklable to checkpoint and restore random number generator states, as well as enabling `copy.deepcopy` of `Module`s and other classes that store the random state.\r\n\r\n## Motivation\r\nI'm currently rewriting https://github.com/Yusufma03/pfrnns, which samples particle candidates from a normal distribution. Rather than fixing the global random seed with `torch.random.manual_seed`, I would like to store the random number generator within the model. However, because `torch.Generator` is currently not picklable, the `Module` cannot be easily copied with `copy.deepcopy`.\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nA `torch.hub.load_local()` function that can load models from any local directory with a `hubconf.py`.\r\n\r\n## Motivation\r\n`torch.hub.load()` currently allows loading model definitions from a GitHub repo that has a `hubconf.py` file. It does this by downloading the repo, and then loading functions from `hubconf.py`.\r\n\r\nSince it only accepts locations of the form `<repo_owner>/<repo_name>`, it is limited to repos hosted on Github.\r\n\r\nThis means it does not support the following functionality/use cases:\r\n- The ability to use `torch.hub` on machines that do not have internet access.\r\n- The ability to archive model definition code locally.\r\n- The ability to load models from local directories without having to push them to Github, especially during development when you want to quickly test out the changes you make to the model definition code.\r\n\r\n## Pitch\r\nIt will cover the above use cases.\r\n\r\n`torch.hub.load_local()` should be trivial to implement. It will have the same signature as `torch.hub.load()`, with the first argument, `github`, replaced by `repo_dir` (which will be a local path). In terms of code, the only difference from `torch.hub.load()` will be the removal of the line that downloads the GitHub repo (https://github.com/pytorch/pytorch/blob/master/torch/hub.py#L343) and the associated `force_reload` argument (https://github.com/pytorch/pytorch/blob/master/torch/hub.py#L338).\r\n\r\nI would probably be implementing this functionality for [azavea/raster-vision#975](https://github.com/azavea/raster-vision/issues/975) and would be happy to open a PR here, if there is interest.\r\n\r\n## Alternatives\r\n\r\nA more general, though harder to implement, solution would be a `torch.hub.load_uri()` function that can fetch code from any URI, local or remote.\r\n\n\ncc @ailzhang"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nAdd an entropy function in PyTorch to compute entropy = -sum(p * log(p), axis=axis), which is analogous to scipy.stats.entropy.\r\n\r\n## Motivation\r\n\r\nIt is a common thing to compute entropy given a distribution p in many use cases. Currently there are several ways to compute entropy in PyTorch listed as below.\r\n\r\n1. Direct implementation which is `entropy = -((p * p.log()).sum(dim=-1))`. This one works in many cases, but not elegant and efficient enough.\r\n2. Using torch.bmm as `entropy = -(torch.bmm(p.view(M, 1, N), p.log().view(M, N, 1))).squeeze()`. This one has better forward pass performance compared to the 1st one on CPU when MKL is enabled. However, the backward pass performance will be worse because of the backward computation of bmm.\r\n3. Using torch.distributions.categorical.Categorical.entropy as `entropy = torch.distributions.categorical.Categorical(p).entropy()`. This one looks the most direct one. However, it is much slower than both of the previous approaches in both forward and backward pass even if we don't need logit at all.\r\n\r\nThe 1st and 2nd approaches are not a direct function to compute entropy as scipy. So we can consider either to add a scipy-like entropy function or optimize the implementation of torch.distributions.categorical.Categorical.entropy.\r\n\r\n\r\n\r\ncc @mruberry @rgommers"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nSupport for Multi-Categorical in torch.distributions\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nAs openai gym supports ``MultiDiscrete`` space, it would be nice if pytorch can support the corresponding distribution, too. This is also a common scenario, where the action consists of multiple sub-actions, each sub-action has different number of choices.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n```python\r\n# I have three sub-actions, each have 3, 5, 4 choices, respectively\r\ndist = multi_categorical_maker([3, 5, 4])\r\n# give it batch of logits with 5 items. \r\n# the first 3 logits corresponds to the first sub-action, \r\n# the next 5 for the second, \r\n# and the last 4 for the last sub-action\r\nd = dist(logits=torch.rand(5, 12)) \r\n\r\n# I get 5 actions, the first sub-action ranges in [0, 3), the second in [0, 5), the last in [0, 4)\r\nd.sample()\r\ntensor([[0, 4, 3],\r\n        [0, 4, 0],\r\n        [0, 3, 1],\r\n        [0, 0, 3],\r\n        [0, 0, 3]])\r\n\r\n# return the log_prob of actions, which are the sum of log_prob of sub-actions\r\nd.log_prob(d.sample())\r\ntensor([-3.8733, -4.3669, -4.6643, -3.5386, -4.1125])\r\n```\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Possible implementation\r\n\r\nThe implementation is very simple and straightforward. Here is just an example:\r\n```python\r\nimport torch\r\nfrom torch.distributions import Categorical, Distribution\r\nfrom typing import List\r\n\r\n\r\nclass MultiCategorical(Distribution):\r\n\r\n    def __init__(self, dists: List[Categorical]):\r\n        super().__init__()\r\n        self.dists = dists\r\n\r\n    def log_prob(self, value):\r\n        ans = []\r\n        for d, v in zip(self.dists, torch.split(value, 1, dim=-1)):\r\n            ans.append(d.log_prob(v.squeeze(-1)))\r\n        return torch.stack(ans, dim=-1).sum(dim=-1)\r\n\r\n    def entropy(self):\r\n        return torch.stack([d.entropy() for d in self.dists], dim=-1).sum(dim=-1)\r\n\r\n    def sample(self, sample_shape=torch.Size()):\r\n        return torch.stack([d.sample(sample_shape) for d in self.dists], dim=-1)\r\n\r\n\r\ndef multi_categorical_maker(nvec):\r\n    def get_multi_categorical(logits):\r\n        start = 0\r\n        ans = []\r\n        for n in nvec:\r\n            ans.append(Categorical(logits=logits[:, start: start + n]))\r\n            start += n\r\n        return MultiCategorical(ans)\r\n    return get_multi_categorical\r\n\r\n```\n\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nIt would be useful to configure PyTorch to use an external memory allocator for its allocations.\r\n\r\n## Motivation\r\n\r\nWhen working on GPUs, memory can be a somewhat limited resources. Particularly when using multiple libraries each handling their own memory. In this case it is possible for libraries to \"compete\" for memory and exhaust it more quickly as a result. In these use cases, it's helpful to have a single memory allocator that can be shared across libraries. This reduces some of the friction of memory allocations between libraries.\r\n\r\n## Pitch\r\n\r\nIt would be great to have some mechanism for users of PyTorch to specify an external memory allocator possibly like [CuPy's]( https://docs.cupy.dev/en/stable/reference/memory.html#changing-memory-pool ), [Numba's]( https://numba.readthedocs.io/en/stable/cuda/external-memory.html ), or similar.\r\n\r\n## Alternatives\r\n\r\nFeel free to suggest ðŸ˜‰ \r\n\r\n## Additional context\r\n\r\nWe frequently receive reports from users of RAPIDS that they can run out of memory when using RAPIDS (say for preprocessing) and pass data to PyTorch for DL. In this context having a more unified memory management story should help users transition smoothly between libraries and reuse memory already freed (though still with the pool) of RAPIDS Memory Manager ([RMM]( https://github.com/rapidsai/rmm )).\n\ncc @ngimel"},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\n[gradcheck.py#L264](https://github.com/pytorch/pytorch/blob/master/torch/autograd/gradcheck.py#L264) explicitly requires its inputs to be of type torch.Tensor and thus doesn't work for Tensor-like objects.\r\n\r\n## Motivation\r\n\r\nTensor-like objects might implement operations with autograd support. gradcheck helps check their correctness. Extending torch.autograd.gradcheck with support for Tensor-like types makes it easier for users to test their code.\r\n\r\n## Pitch\r\n\r\nThis feature requests asks to extend gradcheck to also accept Tensor-like types and proposes using the presence of ```__torch_function__``` as a determinant, as we [usually do](https://github.com/pytorch/pytorch/blob/3d46e02ea1f8951bcef167c33bad0217ec9da980/torch/csrc/utils/python_arg_parser.cpp#L182). The rest of the code should work fine then, assuming the user has reasonable implementations for the used features. Potentially we could guard using methods such as \"is_sparse\" based on whether the given type is Tensor-like and actually has that method defined. This might ease the implementation of Tensor-like objects, because it doesn't require the user to implement those. We could signal this with a warning as well as we do when the user doesn't use float64 as an input dtype, etc.\r\n\r\n## Alternatives\r\n\r\nAn alternative as a user is to copy-paste and modify this code for their own purposes.\r\n\r\n## Additional context\r\n\r\nI'm using this in context of the NestedTensor project. I labeled this as \"module: numpy\" based on the labels used in https://github.com/pytorch/pytorch/issues/24015.\r\n\r\n\r\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @mruberry @rgommers @VitalyFedyunin @hameerabbasi "},{"labels":[null,"enhancement",null,null],"text":"Python does support adding typing information at variable declaration site (even if it's always FloatTensor/LongTensor). It would be good for better code inspection and comprehension\r\n\r\nCompare (pay attention to typing in variable declaration):\r\n```\r\ngraph(%self : __torch__.decoder.transformer.modules.decoder.AcousticTransformerDecoder,\r\n      %tensor.1 : Tensor):\r\n  %20 : bool = prim::Constant[value=0]()\r\n  %14 : int = prim::Constant[value=-1]() #  ...\r\n  %6 : None = prim::Constant()\r\n  %3 : int = prim::Constant[value=0]() #  ...\r\n  %12 : int = prim::Constant[value=1]() #  ...\r\n  %dim.1 : int = aten::size(%tensor.1, %3) #  ...\r\n  %arange.1 : Tensor = aten::arange(%dim.1, %6, %6, %6, %6) #  ...\r\n  %15 : Tensor = aten::unsqueeze(%arange.1, %14) #  ...\r\n  %19 : int[] = prim::ListConstruct(%14, %dim.1)\r\n  %21 : Tensor = aten::expand(%15, %19, %20) # ...\r\n  %24 : Tensor = aten::sub(%arange.1, %12, %12) #... \r\n  %_future_mask.1 : Tensor = aten::gt(%21, %24) #  ...\r\n  %31 : Tensor = aten::to(%_future_mask.1, %tensor.1, %20, %20, %6) #  ...\r\n  %_future_mask.3 : Tensor = aten::log(%31) #  ...\r\n  %36 : Tensor = aten::slice(%_future_mask.3, %3, %3, %dim.1, %12) #  ...\r\n  %39 : Tensor = aten::slice(%36, %12, %3, %dim.1, %12) #  ...\r\n  return (%39)\r\n``` \r\n\r\nand\r\n\r\n```\r\ndef buffered_future_mask(self,\r\n    tensor: Tensor) -> Tensor:\r\n  dim = torch.size(tensor, 0)\r\n  arange = torch.arange(dim, dtype=None, layout=None, device=None, pin_memory=None)\r\n  _0 = torch.expand(torch.unsqueeze(arange, -1), [-1, dim], implicit=False)\r\n  _future_mask = torch.gt(_0, torch.sub(arange, 1, 1))\r\n  _1 = torch.to(_future_mask, tensor, False, False, None)\r\n  _future_mask0 = torch.log(_1)\r\n  _2 = torch.slice(_future_mask0, 0, 0, dim, 1)\r\n  return torch.slice(_2, 1, 0, dim, 1)\r\n```\r\n\r\ncc @suo @gmagogsfm"},{"labels":["enhancement",null,null],"text":"## Background\r\n\r\n\r\n[DeepSpeed](https://github.com/microsoft/DeepSpeed) reduces distributed data parallel training memory footprint by partitioning parameters, gradients, optimizer states, and recomputing activations. See [this link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/) for more details.\r\n\r\nIt takes care of the entire training iteration instead of exposing an `nn.Module`-like API.\r\n\r\n```python\r\nmodel_engine, optimizer, trainloader, _ = deepspeed.initialize(\r\n    args=args, \r\n    model=net, \r\n    model_parameters=parameters, \r\n    training_data=trainset\r\n)\r\n\r\nfor inputs in batches:\r\n    outputs = model_engine(inputs)\r\n    loss = criterion(outputs, labels)\r\n    model_engine.backward(loss)\r\n    model_engine.step()\r\n```\r\n\r\n\r\nDeepSpeed1 provides optimizer states partitioning. DeepSpeed2 provides the full feature set. \r\n\r\n\r\n## API\r\n\r\nTo integrate DeepSpeed into PyTorch, it will be great if we can have a consistent API, i.e., `model(inputs)`, `loss.backward()`, `optimizer.step()`. In this way, we can minimize the surprise to users and can minimize the code change when existing applications would like to adopt this technique. We can try to decompose DeepSpeed into the following two concepts, open to better names :) \r\n\r\n\r\n* `torch.distributed.nn.ShardedDataParallel`: comply with the `torch.nn.Module` API. \r\n* `torch.distributed.optim.ShardedOptimizer`: comply with the `torch.optim.Optimizer` API.\r\n\r\n\r\nAnd training loops can look like the following, which is almost the same as local training.\r\n\r\n\r\n```python\r\nimport torch.optim as optim\r\n\r\ndist.init_process_group(...)\r\nmodel = torch.distributed.optim.ShardedDataParallel(model)\r\nopt = torch.distributed.optim.ShardedOptimizer(optim.Adam, model.parameters())\r\n# model.parameters() only returns the ones owned by the current rank\r\n\r\nloss_fn(model(inputs), labels).backward()\r\nopt.step()\r\n```\r\n\r\n\r\n\r\n## Upstream DeepSpeed to PyTorch\r\n\r\n### `ShardedOptimizer`\r\n\r\n\r\nThe optimizer part is relatively independent, and [fairscale](https://github.com/facebookresearch/fairscale) has already implemented one using PyTorch optimizer API. We can start the upstream effort from there. See [`oss.py`](https://github.com/facebookresearch/fairscale/blob/525e709bcb851db9eeb5d09a41501e6ef478d714/fairscale/optim/oss.py#L92-L98)\r\n\r\n```python\r\nclass ShardedOptimizer(torch.optim.Optimizer):\r\n    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:\r\n        loss = self.optim.step(closure=closure)\r\n        for rank, param_groups in enumerate(self.partition_parameters()):\r\n            for param_group in param_groups:\r\n                for param in param_group[\"params\"]:\r\n                    dist.broadcast(param, rank, group=self.group)\r\n        return loss\r\n```\r\n\r\n\r\n### `ShardedDataParallel`\r\n\r\n\r\nI havenâ€™t got into all details, but we might be able to wrap DeepSpeed algorithm as an `nn.Module` using custom autograd functions. The high-level idea is that we can split the input model into shards and then insert phony layers between shards to load and drop model parameters/grads accordingly. \r\n\r\n\r\n```python\r\n# representing one shard of the model, which expose APIs to load/drop parameters\r\n# and gather gradients\r\nclass ModelShard(nn.Module):\r\n\r\n    def __init__(self, cpu_model_shard, owner_rank, pg):\r\n        ...\r\n        \r\n    def forward_load(self):\r\n        # materialize local GPU parameters, can enhance with bucketing\r\n        futures = [pg.broadcast(p, owner_rank, async_op=True) \r\n                   for p in self.parameters()]\r\n        # NB: this requires consolidating c10d work with torch.futures.Future\r\n        torch.futures.wait_all(futures)\r\n\r\n    ...\r\n    \r\n    def reduce_grads(self):\r\n        futures = [pg.reduce(p, owner_rank, async_op=True)\r\n                   for p in self.parameters()]\r\n        torch.futures.wait_all(futures)\r\n\r\n# The phony layer is a synchronization point between model shards.\r\n# In the forward pass, it drops parameters in the previous shard and\r\n# loads parameters for the next shard. In the backward pass, it does \r\n# the reverse and also gathers gradients to the owner.\r\n# It does not change or create any outputs at all, instead it just\r\n# forward the input as the output.\r\nclass PhonyLayer(Function)\r\n    \r\n    @staticmethod\r\n    def forward(ctx, prev_shard, next_shard, *inputs)\r\n        # check None accordingly\r\n        prev_shard.forward_drop()\r\n        next_shard.forward_load()\r\n        ctx.prev_shard = prev_shard\r\n        ctx.next_shard = next_shard\r\n        return *inputs\r\n        \r\n    @staticmethod\r\n    def backward(ctx, *grad_outputs):\r\n        ctx.next_shard.reduce_grads()\r\n        ctx.next_shard.backward_drop()\r\n        ctx.prev_shard.backward_load()\r\n        return *grad_outputs\r\n```\r\n\r\nWith the above building blocks `ShardedDataParallel` can assemble the `PhonyLayer` and `ModelShard` objects in its forward pass. \r\n\r\n```python\r\nclass ShardedDataParallel(nn.Module):\r\n    def __init__(self, cpu_model: nn.Sequential, pg: ProcessGroup):\r\n        # split the input model into shards based on config/profiling/etc.\r\n        self.model_shards = split(cpu_model, pg)\r\n        \r\n    def forward(self, *inputs):\r\n        for prev, next in zip([None, *shards], [*shards, None]):\r\n            inputs = prev(inputs) if prev else inputs\r\n            inputs = PhonyLayer.apply(prev, next, *inputs)\r\n        return inputs\r\n\r\n```\r\n\r\n## Integrate DeepSpeed into DDP\r\n\r\n\r\nExisting `torch.distributed.DistributedDataParallel` provides gradient bucketing and comm/comp overlapping features through the `Reducer`, which would be useful for DeepSpeed too, especially as DeepSpeed targets large models. Instead of implementing everything from scratch for DeepSpeed, we can try to integrate DeepSpeed into DDP. The DeepSpeed optimizer can stay independent, and the rest of DeepSpeed algorithms can be decomposed to fit into the `Reducer` API. However, todayâ€™s `Reducer` API is not sufficient though, as it would materialize all buckets at construction time, install all autograd hooks at construction time, and always uses `AllReduce` to communicate gradients. We will need the [modularized DDP API](https://github.com/pytorch/pytorch/issues/37002) .\r\n\r\nThe current DDP algorithm can be decomposed and briefly represented using the following Python pseudo-code (although the current implementation is in C++). The four concepts (Grad Reader, Grad Bucketer, Comm Scheduler, and Grad Writer) in #37002 map to four functions below. The `grad_ready` function triggers DDP backward logics. \r\n\r\n```python\r\nclass ReducerBase:\r\n    def __init__(self, model, pg, *bucket_cap_mb* = 25):\r\n        self.model, self.*bucket_cap_mb*, self.pg = model, *bucket_cap_mb*, pg\r\n        self.grad2bucket = self.build_buckets()\r\n        # let param.grad points to bucket offset (view)\r\n        self.register_grad_hooks()\r\n        \r\n    def register_grad_hooks(self):\r\n        for p in self.model.parameters():\r\n            # register self.grad_ready to p's GradAccumulator as callback\r\n            \r\n    # Grad Bucketer\r\n    def build_buckets(self):\r\n        # map self.model.parameters() to a list of GradBucket objects\r\n        \r\n    # Grad Reader\r\n    def grad_ready(self, grad, idx):\r\n        bucket = self.grad2bucket[idx]\r\n        bucket.pending_grad -= 1\r\n        if self.bucket.pending_grad == 0:\r\n            # only launch one bucket when all its preceding ones are \r\n            # launched, so that we won't mismatch allreduce comms\r\n            for i in range(min_pending_bucket, bucket.idx + 1):\r\n                if self.buckets[i].pending_grad == 0:\r\n                    self.bucket_ready(self.buckets[i])\r\n       \r\n    # Comm Scheduler\r\n    def bucket_ready(self, bucket):\r\n        self.futures.append(\r\n            self.pg.allreduce(bucket, async_op=True).then(self.comm_ready)\r\n        )\r\n        self.pending_bucket -= 1\r\n        if self.pending_buckets == 0\r\n            # the last bucket should block to make sure when backward()\r\n            # returns, all grads are indeed ready\r\n            torch.futures.wait_all(self.futures)\r\n        \r\n    # Grad Writer\r\n    def comm_ready(self, comm_future):\r\n        pass\r\n```\r\n\r\n\r\nTo integrate DeepSpeed with DDP, we can override the `build_buckets` and `bucket_ready` functions accordingly to take care of the gradient communication in the backward pass. Besides, as not all parameters will be materialized at construction time, it should leave out the invocation of `register_grad_hooks` in ctor as well.\r\n\r\n\r\n```python\r\nclass ShardedReducer(RecuderBase):\r\n    ...\r\n    def build_buckets(self):\r\n        # map self.model.parameters() to a list of GradBucket objects\r\n        # only materialize the ones this rank owns. \r\n        \r\n    # Comm Scheduler\r\n    def bucket_ready(self, bucket):\r\n        self.futures.append(\r\n            self.pg.reduce(bucket, bucket_owner_map(bucket.idx), async_op=True)\r\n        )\r\n        self.model_shard.pending_bucket -= 1\r\n        if self.model_shard.pending_buckets == 0\r\n            torch.futures.wait_all(self.futures)\r\n```\r\n\r\n\r\n\r\nThe `PhonyLayer` would still be necessary to load and drop model shards. However, instead of gathering grads, it only needs to run `grad_ready` accordingly to trigger `Reducer` code.\r\n\r\n\r\n```python\r\nclass ModelShard(nn.Module):\r\n    def __init__(self, cpu_model_shard, owner_rank, pg, reducer):\r\n        ...\r\n        self.reducer = reducer\r\n        \r\n    def register_grad_reader(self):\r\n        # this shard must have been materialized when reaching here\r\n        for p in self.parameters():\r\n            # register self.reducer.grad_ready to p's GradAccumulator as callback\r\n\r\n# this is the same as the above PhonyLayer except it no longer\r\n# communicates grad on its own, instead it registers grad hooks \r\n# at dynamically loaded parameters in the backward pass.\r\nclass PhonyLayer(Function)\r\n    \r\n    @staticmethod\r\n    def forward(ctx, prev_shard, next_shard, *inputs)\r\n        # check None accordingly\r\n        prev_shard.forward_drop()\r\n        next_shard.forward_load()\r\n        ctx.prev_shard = prev_shard\r\n        ctx.next_shard = next_shard\r\n        return *inputs\r\n        \r\n    @staticmethod\r\n    def backward(ctx, *grad_outputs):\r\n        # ctx.next_shard.reduce_grads()\r\n        # check None accordingly\r\n        ctx.next_shard.backward_drop()\r\n        ctx.prev_shard.backward_load()\r\n        ctx.prev_shard.register_grad_reader()\r\n        return *grad_outputs\r\n```\r\n\r\n\r\nThen, the `SharedDataparallel` `nn.Module` becomes:\r\n\r\n```python\r\nclass ShardedDataParallel(nn.Module):\r\n\r\n    def __init__(self, cpu_model: nn.Sequential, pg: ProcessGroup):\r\n        # Split the input model into shards based on config/profiling/etc.\r\n        # Let each model shard keep a copy of the reducer, so that it can \r\n        # call reducer.grad_ready accordingly\r\n        self.reducer = ShardedReducer(cpu_model, pg)\r\n        self.shards = split(cpu_model, pg, self.redcuer)\r\n        \r\n    def forward(self, *inputs):\r\n        # same as the above one\r\n```\r\n\r\n## Implementation Plan\r\n\r\n* Phase 1: we should start from the `ShardedOptimizer`, as there are no road blockers as of today and fairscale already has an elegant implementation. \r\n* Phase 2: when the composable `Reducer` API is ready, we can move on to add parameter + gradients sharding. \r\n* Phase 3: add activation re-computation. \r\n\r\n\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nIt would be great for the serialization to be much broader than what it is now in libtorch. at least by providing a way for the users to be able to use existing functionality to serialize their objects. \r\n\r\n## Motivation\r\n\r\nI was trying to serialize a `std::vector<std::tuple<std::string, torch::Tensor>>` in libtorch where I found out its simply impossible. there is no way to extend the `InputArchive ` so I can add the needed logic to load from it. \r\n\r\n## Pitch\r\n\r\nAdd support for `STL` containers at least when it comes to serialization. or allow the users to be able to extend existing functionality to support their own custom types.  \r\n\r\n## Alternatives\r\n\r\nI ultimately ended up using Protobuf for serialization \r\n\n\ncc @yf225 @glaringlee"},{"labels":["enhancement",null,null,null],"text":"For two complex numbers the [dot product](https://en.wikipedia.org/wiki/Dot_product#Complex_vectors) is defined as:\r\n\r\n<img width=\"840\" alt=\"Screen Shot 2020-08-07 at 3 57 51 PM\" src=\"https://user-images.githubusercontent.com/20081078/89683624-deb8d200-d8c6-11ea-8ed0-4d8bd870317d.png\">\r\n\r\nThe `torch.vdot(a, b)` function (similar to [numpy.vdot](https://numpy.org/doc/stable/reference/generated/numpy.vdot.html))will treat complex numbers differently than dot(a, b). If the first argument is complex the complex conjugate of the first argument will be used for the calculation of the dot product.\r\n\n\ncc @ezyang @anjali411 @dylanbespalko @mruberry @rgommers"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nIt would be useful to have a way to reinterpret the type of existing data as another type (without copying). For example, one can do this with NumPy (or CuPy) like so...\r\n\r\n```python\r\nIn [1]: import numpy                                                            \r\n\r\nIn [2]: b = b\"\\x00\\x00\\x00<\\x00@\"                                               \r\n\r\nIn [3]: a = numpy.asarray(memoryview(b))                                        \r\n\r\nIn [4]: a                                                                       \r\nOut[4]: array([ 0,  0,  0, 60,  0, 64], dtype=uint8)\r\n\r\nIn [5]: a.view(numpy.float16)                                                   \r\nOut[5]: array([0., 1., 2.], dtype=float16)\r\n```\r\n\r\n## Motivation\r\n\r\nIn some cases users may feed data into PyTorch, which comes from more raw formats (like `bytes` and `bytearray`). So they don't really have the type factored in when provided. In these cases what's needed is a way to add the type after the fact.\r\n\r\n## Pitch\r\n\r\nIdeally `Tensor` would get a new method, which allows users to reinterpret the type of data and returns a `Tensor` viewing the same data, but with a different type.\r\n\r\n## Alternatives\r\n\r\nSuggestions welcome ðŸ™‚\r\n\r\n## Additional context\r\n\r\nNA\r\n\r\nHave searched the issue tracker and the discussion forum, but didn't come across anything that looked related. Though please let me know if I've merely overlooked something.\n\ncc @mruberry @rgommers"},{"labels":["enhancement",null,null,null],"text":"Tracking issue for tasks related to unary \"[universal functions](https://numpy.org/doc/stable/reference/ufuncs.html?highlight=ufunc),\" elementwise functions of a single tensor input.\r\n\r\nPyTorch already has many unary ufuncs, like `torch.sin`, but some other unary ufuncs, like `torch.exp2`, are missing, and several systemic features like type promotion aren't implemented.\r\n\r\n**High Priority Tasks: Implementations**\r\n- [x] `torch.exp2`  (#44184)\r\n- [ ] `torch.frexp`\r\n- [ ] `torch.nan_to_num` (#44592)\r\n- [ ] `torch.sinc` (#44713)\r\n\r\n**Core Tasks**\r\n- [x] Create test_unary_ufuncs.py (stub added in https://github.com/pytorch/pytorch/pull/41662)\r\n- [ ] Enable unary ufuncs to have `out` arguments with different dtypes than their inputs\r\n- [ ] Enable integer -> float unary type promotion for ops like `sin`\r\n- [ ] Consistently support the `dtype` kwarg\r\n\r\n**Missing aliases**\r\n- [x] `torch.clip` (https://github.com/pytorch/pytorch/pull/42770)\r\n- [ ] `torch.degrees` (#42910, blocked on supporting signature-specific aliases)\r\n- [ ] `torch.radians` (blocked on supporting signature-specific aliases)\r\n- [x] `torch.fix` (#43326)\r\n- [x] `torch.negative` (#43400)\r\n\r\n\r\n**Unary ufuncs that need updates**\r\n- [ ] [torch.round](https://pytorch.org/docs/master/generated/torch.round.html?highlight=round#torch.round) needs a `decimal` parameter, analogous to [numpy.around](https://numpy.org/doc/stable/reference/generated/numpy.around.html?highlight=around#numpy.around)\r\n\r\nIn the future we may want to implement more common ufunc features, too, like NumPy's `where` kwarg. \r\n\r\ncc @ezyang @gchanan @zou3519 @mruberry @rgommers "},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nProvide small script that detects hardware system capabilities for running PyTorch and/or different types of payloads.\r\n\r\n## Motivation\r\n\r\nI'm always frustrated when I don't understand if I have sufficient hardware for certain tasks (distributed computing, virtualisation), and what the performance of my hardware would be. It would be very nice to fork/extend tools like `neofetch` to show hardware compatibility stats for PyTorch prior to installing it. For an inspiration.\r\n\r\n![image](https://user-images.githubusercontent.com/8781107/89210036-10c8ec00-d5c8-11ea-98ad-9b11c7512e2c.png)\r\n\r\n## Pitch\r\n\r\n```\r\n          /:-------------:\\          anatoli@blackred (86.57.247.127)\r\n       :-------------------::        ---------------- \r\n     :-----------/shhOHbmp---:\\      OS: Linux x86_64 (Fedora)\r\n   /-----------omMMMNNNMMD  ---:\r\n  :-----------sMMMMNMNMP.    ---:    CPU: Intel i5-4300U (4) @ 2.900GHz\r\n :-----------:MMMdP-------    ---\\   GPU: Intel Haswell-ULT (Not Supported)\r\n,------------:MMMd--------    ---:   Memory: 5522MiB / 7660MiB \r\n:------------:MMMd-------    .---: \r\n:----    oNMMMMMMMMMNho     .----:   Python: 3.8.5\r\n:--     .+shhhMMMmhhy++   .------/   PyTorch: Not Detected (1.6 Available)\r\n:-    -------:MMMd--------------:    CUDA: No Hardware\r\n:-   --------/MMMd-------------;\r\n:-    ------/hMMMy------------:\r\n:-- :dMNdhhdNMMNo------------;       \r\n:---:sdNMMMMNds:------------: \r\n:------:://:-------------::\r\n:---------------------://            \r\n```\r\n\r\n## Alternatives\r\n\r\nInterested to know myself. This might be implemented by some PyTorch installers for some platforms."},{"labels":["enhancement",null],"text":"Want to develop Time series algorithms Holt Winter under pyTorch."},{"labels":["enhancement",null,null,null],"text":"hope build_android can support nn::module nn::Functional nn::Linear\r\n\r\nwant to train in android app\r\n\r\nalso refer to\r\nhttps://github.com/pytorch/pytorch/issues/42355"},{"labels":["enhancement",null,null],"text":"I am working on the IBM POWER9 Series cluster. When compute the fft with torch. I have to transfer it to GPU and then transfer back to CPU. It's real bad experience espcially when realted code called on Dataloader. Then I have to disable the multi-worker to prevent the error.\r\n\r\n```\r\nx = torch.randn(10,10,2)\r\ntorch.fft(x,1)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: fft: ATen not compiled with MKL support\r\n```\r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"torch.distributions.multinomial.Multinomial(total_count=1, probs=None, logits=None, validate_args=None)\r\n\r\nthe total_count is must be int, but not tensor, so I can't use this function in a batch scenario.  It's quite inconvenient.\r\n![image](https://user-images.githubusercontent.com/30493522/89102992-f7e1fe80-d440-11ea-971b-e111e3cb94c4.png)\r\n\n\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw"},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\nVisionDataset  is slow in loading large batches. This happens because of data being loaded and transformed by single item in regular Python cycle. \r\n\r\n## Motivation\r\nI have a bit of frustration when seeing slower Pytorch code vs TF in data loading functions. \r\n\r\n## Pitch\r\n1) fix `__getitem__()` for `VisionDataset()` class so it allows loading and transforming batches at once\r\n2) allow `auto_collation = False` in [_MapDatasetFetcher ](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/fetch.py)\r\n\r\n## Alternatives\r\nThere may be ideas based on how TF does it - works much faster in such situations\r\n\r\n## Additional context\r\nsee question and answer at SO [here](https://stackoverflow.com/questions/63202478/why-pytorch-is-slower-than-tensorflow-in-read-data/63204186#63204186).\r\n\r\n\n\ncc @SsnL @VitalyFedyunin @fmassa @vfdev-5 @ngimel"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nAdd NMS version which takes confidence threshold and prunes out low confidence scores from the entire prediction.\r\n\r\n## Motivation\r\n\r\nMotivation is that if Pytorch NMS requires only boxes which cross confidence threshold, it means that the filtering should be done apriori. This filtering usually results in an ONNX NONZERO operator, and if using GLOW, NONZERO can't be supported, owing to the dynamic shape of the output tensor. ONNX supports NonMaxSuppression node, which does exactly the same thing. If pytorch also supports this, conversion of Pytorch NMS -> ONNX NMS becomes much easier.\r\n\r\n\r\n## Pitch\r\n\r\nAdd an additional input to NMS operator (confidence_threshold). New NMS operator takes entire model prediction as an input, and based on the confidence_threshold, filters out boxes lower than the threshold before moving to IOU.\r\n\r\n## Alternatives\r\n\r\n\r\n## Additional context\r\n\r\n"},{"labels":["enhancement",null,null,null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nTo support float16 (optionally bf16) in `fake_quantize_per_tensor_affine` /  `fake_quantize_per_channel_affine`\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nWhen `amp.auto_cast` is enabled, the fake_quantize may fail.\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @mcarilli @ppwwyyxx @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a @vkuzo"},{"labels":["enhancement",null,null,null],"text":"## ðŸ› Bug\r\n\r\npytorch recently upgraded the nccl version to 2.7.3 https://github.com/pytorch/pytorch/pull/40622, which has support for point-to-point communication such as send and recv. However, I encounter `RuntimeError: ProcessGroupNCCL does not support recv` when using a nccl backend for send and recv. `torch.cuda.nccl.version()` returns `273`.\r\n\r\n## Stack trace\r\n\r\n```\r\n  File \"/private/home/sijun/projects/CrypTen/crypten/mpc/provider/ttp_provider.py\", line 75, in generate_binary_triple\r\n    c = TTPClient.get().ttp_request(\"binary\", device, size0, size1)\r\n  File \"/private/home/sijun/projects/CrypTen/crypten/mpc/provider/ttp_provider.py\", line 203, in ttp_request\r\n    comm.get().recv(result, ttp_rank, self.comm_group)\r\n  File \"/private/home/sijun/projects/CrypTen/crypten/communicator/communicator.py\", line 223, in logging_wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/private/home/sijun/projects/CrypTen/crypten/communicator/distributed_communicator.py\", line 123, in recv\r\n    dist.recv(result.data, src=src, group=group)\r\n  File \"/private/home/sijun/.conda/envs/torch_preview/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 751, in recv\r\n    pg.recv([tensor], group_src_rank, tag).wait()\r\nRuntimeError: ProcessGroupNCCL does not support recv\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): torch-nightly (1.7.0)\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): \r\n - Python version: 3.8.3\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @ngimel @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nA module ```nn.Orthogonal``` similar to ```nn.Linear``` where the weight matrix <img src=\"https://latex.codecogs.com/svg.latex?W\" /> is constrained to be orthogonal, i.e., <img src=\"https://latex.codecogs.com/svg.latex?W^T=W^{-1}\"/>.  \r\n\r\n## Motivation\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nThere has been a growing interest in orthogonal parameterization of neural networks, see, e.g., [1,2,3,4,5].\r\nTo use orthogonal parameterization with PyTorch one has to implement it themselves or use third party code. \r\nIt would be convenient if PyTorch has a built-in module ```nn.Orthogonal```that handles everything automatically. \r\nIn particular, it would be convenient if ```nn.Orthogonal``` support different methods by, e.g.,   ```method={fasth,cayley,exp}```. \r\n\r\n## Pitch\r\n<!-- A clear and concise description of what you want to happen. -->\r\nDuring ICML I was suggested to make a pull request for PyTorch for [fasth](https://invertibleworkshop.github.io/accepted_papers/pdfs/10.pdf) [5] as ```nn.Orthogonal```. \r\nI want to\r\n1. be sure this feature is desired \r\n2. discuss potential ways of interfacing with the user\r\n3. implement the code and submit a pull request. \r\n\r\nI want ```nn.Orthogonal``` to support three methods: [Cayley transform](https://github.com/Lezcano/expRNN/blob/master/trivializations.py), [matrix exponential](https://github.com/Lezcano/expRNN/blob/master/trivializations.py) and  [fasth](https://invertibleworkshop.github.io/accepted_papers/pdfs/10.pdf). \r\n\r\n## Additional context\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nThe [contribution instructions ](url) (see screenshot below) states that, generally, algorithms from recently-published research are not accepted, but it is suggested to open an issue, as I have now done. \r\nFastH is up to 20 times faster than the previous sequential algorithm (see image in bottom of page). \r\nPlease note this is an algorithmic speed-up, it computes the exact same thing as the previous algorithm, just faster. \r\n\r\n![image](https://user-images.githubusercontent.com/8614529/88838933-8a349900-d1da-11ea-803e-d6585f44d247.png)\r\n\r\n\r\n# References \r\n[1] Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections (ICML 2017)\r\n[2] A Simple Parametrization of the Orthogonal and Unitary Group (ICML 2019)\r\n[3] Stabilizing Gradients forDeep Neural Networks via Efficient SVD Parameterization (ICML 2018)\r\n[4] Trivializations for Gradient-Based Optimization on Manifolds (NeurIPS 2019)\r\n[5] Faster Orthogonal Parameterization with Householder Matrices (ICML Workshop 2020)\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/8614529/88841111-c1587980-d1dd-11ea-823a-d073d180aaaf.png)\r\n\r\n\n\ncc @albanD @mruberry"},{"labels":["enhancement",null,null],"text":"Requested in #32867 and #38349 (NumPy rollup).\r\n\r\ncc @mruberry @rgommers "},{"labels":["enhancement",null,null,null,null,null],"text":"Tracking issue for tasks related to the torch.fft namespace, analogous to NumPy's [numpy.fft namespace](https://numpy.org/doc/stable/reference/routines.fft.html?highlight=fft#module-numpy.fft) and SciPy's [scipy.fft namespace](https://docs.scipy.org/doc/scipy/reference/fft.html#module-scipy.fft).\r\n\r\nPyTorch already has fft functions ([fft](https://pytorch.org/docs/master/generated/torch.fft.html?highlight=fft#torch.fft), [ifft](https://pytorch.org/docs/master/generated/torch.ifft.html?highlight=fft#torch.ifft), [rfft](https://pytorch.org/docs/master/generated/torch.rfft.html?highlight=fft#torch.rfft) , [irfft](https://pytorch.org/docs/master/generated/torch.irfft.html?highlight=fft#torch.irfft), [stft](https://pytorch.org/docs/master/generated/torch.stft.html?highlight=stft#torch.stft), [istft](https://pytorch.org/docs/master/generated/torch.istft.html?highlight=stft#torch.istft)), but they're inconsistent with NumPy and don't accept complex tensor inputs. The torch.fft namespace should be consistent with NumPy and SciPy where possible, plus provide a path towards removing PyTorch's existing fft functions in the 1.8 release (deprecating them in 1.7). \r\n\r\nWhile adding the torch.fft namespace infrastructure and deprecating PyTorch's current fft-related functions are the top priorities, PyTorch is also missing many helpful functions, listed below, which should (eventually) be added to the new namespace, too. \r\n\r\n\r\nTracking tasks:\r\n\r\n**Infrastructure:**\r\n- [x] Create test_spectral_ops.py (https://github.com/pytorch/pytorch/pull/42157)\r\n- [x] Create torch.fft namespace (https://github.com/pytorch/pytorch/pull/41911)\r\n\r\n**Top Priorities**\r\n\r\nIt may be easiest to start with forward for these and implement backward as separate follow-ups.\r\n\r\n- [x] Implement torch.fft.fft()\r\n- [x] Implement torch.fft.ifft()\r\n- [x] Implement torch.fft.rfft()\r\n- [x] Implement torch.fft.irfft()\r\n- [x] Update torch.stft() and torch.istft() to handle complex tensors (like librosa does) and document librosa-compat (not SciPy)\r\n\r\n**New Functions:**\r\n\r\n- [ ] Implement torch.fft.fft2()\r\n- [ ] Implement torch.fft.iff2()\r\n- [ ] Implement torch.fft.fftn()\r\n- [ ] Implement torch.fft.ifftn()\r\n- [ ] Implement torch.fft.rfft2()\r\n- [ ] Implement torch.fft.irfft2()\r\n- [ ] Implement torch.fft.fftfreq()\r\n- [ ] Implement torch.fft.rfftfreq()\r\n- [ ] Implement torch.fft.fftshift()\r\n- [ ] Implement torch.fft.ifftshift()\r\n\r\n**Top Issues:**\r\n- [x] https://github.com/pytorch/pytorch/issues/42213, pytorch istft runs slower than torchaudio istft\r\n\r\n**Deprecations:**\r\n- [ ] Deprecate torch.fft()\r\n- [ ] Deprecate torch.ifft()\r\n- [ ] Deprecate torch.rfft()\r\n- [ ] Deprecate torch.irfft()\r\n- [x] Deprecate torch.stft() and torch.istft() returning non-complex tensors mimicking complex tensors\r\n\r\ncc @ezyang @gchanan @zou3519 @anjali411 @dylanbespalko @mruberry @rgommers "},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nIn the PyTorch Python API, it is possible to move a tensor to shared memory via calling the `Tensor.share_memory_()` function. I could not find similar functionality on the  C++ side using `at::Tensor`.\r\n\r\n## Motivation\r\nI have code that reads tensors from network on the C++ side (using multiple threads). These tensors are then pulled into Python land and go through Python multi-processing transforms. I want to avoid copying tensors across process boundaries, so that the multi-processing transforms can be executed efficiently. To do this, I need to move the tensor storage to shared memory. However, for efficiency purposes, I want to move the tensor memory to shared memory on the C++ side, where I have a thread pool for reading tensors from the network. \r\n\r\nAnother potential area where I am planning to use this feature is out-of process execution of PyTorch scripts in C++ land, where again, we can use shared memory to minimize the cost of transferring tensors across processes.\r\n\r\n## Pitch\r\n\r\nWhat I want is a `share_memory()` method on `at::Tensor`, mimicking the one on PyTorch tensors.\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @yf225 @glaringlee"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nA python script to automate the installation process for all the different requirements according to the user's need.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nThese days no one wants to do the hardwork, everyone wants things automated and to work at one click. By automating this process of installation users could simply enter their requirement as to how they want PyTorch on their system and the script would do the rest for them.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nThere are various options for the user as to the way in which he/she wants to install and use PyTorch. For example, if the user wants to install the Docker image, the script will ask if he wants the pre-built images or built a image by himself, and do the further installation accordingly. \r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @malfet"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nin numpy, we can specify the destination (maybe a slice of an large array ) of result,  is there similar way for pytorch to specify the result \"destination\" when copying cuda tensor to cpu?  something like:\r\n\r\n```python\r\ndestination_array = np.empty(shape=(...very_large_n...)\r\ncuda_tesnor.cpu(out=destination_array[some_slice])\r\n```\r\n\r\n## Motivation\r\n\r\nIâ€™v tried\r\n\r\n```python\r\ndestination_array[some_slice] = cuda_tensor.cpu().data.numpy()\r\n```\r\nfound that is used twice time as purely \r\n```\r\ntensor.cpu().data.numpy(), \r\n```\r\nthis feature request aims to avoid the unnecessary copy\r\n\r\n\r\n\n\ncc @mruberry @rgommers"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\n```python\r\nmodel =     # initialize\r\noptimizer = # initialize\r\n\r\nmodel.cuda()\r\n\r\noptimizer.load_state_dict(optimizer_checkpoint)  # if checkpoint is not on gpu memory\r\noptimizer.cuda()                                 # <<<<<< this code can be helpful\r\n\r\n# and training codes...\r\nloss.backward()\r\noptimizer.step()\r\n```\r\n\r\nI think there is no appropriate way to send optimizer states to gpu memory currently.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nWhen training should be saved and resumed, I packed all state dict to one dictionary object like `{\"model\": model.module.state_dict(), \"optimizer\": optimizer.state_dict(), \"lr_scheduler\": scheduler.state_dict()}` and save it. Due to the limit of GPU memory, when resuming training, I always load state dict using `map_location=torch.device('cpu')`.\r\n\r\nAt that situation, I have to cast all optimizer state to gpu manually like below code(From issue https://github.com/pytorch/pytorch/issues/2830).\r\n\r\n```python\r\nfor state in optimizer.state.values():\r\n    for k, v in state.items():\r\n        if torch.is_tensor(v):\r\n            state[k] = v.cuda()\r\n```\r\n\r\n\r\n## Pitch\r\n\r\nImplement `to`, `cpu`, and `cuda` method to `optim.Optimizer` like `nn.Module`.\r\n\r\n## Additional Context\r\n\r\nMaybe related to #2830\n\ncc @vincentqb"},{"labels":["enhancement",null,null],"text":"Some useful reduction modes:\r\nFor general ops: +=, -=, *=, logaddexp\r\nFor binary ops: It would also be cool to have various reduction modes for out-of-place ops: |=, &=, ^= etc. \r\n\r\nCurrently only copy_ is done.\r\n\r\nscatter_ is already getting reduction mode choice: #22378\r\n\r\nThere is also an angle that currently autograd does not support out-variants. There was an issue about fixing it in some limited cases, I think it was by @albanD, but I can't find it now.\r\n\r\nTwo usecases for binary ops:\r\n\r\n1. Current lines from https://gist.github.com/vadimkantorov/30ea6d278bc492abf6ad328c6965613a could be simplified if `<<` and `|=` could be fused:\r\n```python\r\n        compress = sliced_input << (nbits * (packed_size - e - 1))\r\n        torch.bitwise_or(compress, sliced_output, out = sliced_output)\r\n```\r\n\r\n2. Another usecase is fused relu_dropout: https://gist.github.com/vadimkantorov/360ece06de4fd2641fa9ed1085f76d48#file-reludropoutinplace-py-L13 that could be simplified if `<` and `|=` could be fused:\r\n```python\r\nmask = torch.rand_like(x) > p1m\r\nmask |= (x < 0)\r\n``` \r\n\r\nOriginal issue: https://github.com/pytorch/pytorch/issues/41624#issue-660069164\r\n\r\nMaybe JIT script already can handle inplace / out-variants? and fuse them (including cuda)?"},{"labels":["enhancement",null,null,null,null],"text":"I am writing to inquire if the PyTorch team has any interest or plans for implementing locality sensitive hashing based optimizations for enhanced CPU only performance and reduced overall computational resource consumption as detailed in this paper?\r\n\r\nhttps://www.cs.rice.edu/~as143/Papers/SLIDE_MLSys.pdf\r\n\r\nIt would appear that these techniques have reached a level maturity to consider implementation and that this would be a great benefit to reducing the GPU barrier of entry for developers, reduce complexity and expense for both training and deploying large deep learning models, and also reduce waste in terms of not only hardware expense but also energy usage and ecological impact.\r\n\n\ncc @VitalyFedyunin @ngimel"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nAdd a parallel method to compute the diagonal of a Jacobian in parallel without computing the whole Jacobian.\r\n\r\n## Motivation\r\nIf one wants the diagonal elements of a jacobian, e.g., if you want to get d^2 f(x)/dx_i^2, one has to do multiple backward passes. There are several method on how one could achieve this at the moment but either it is memory or time inefficient.\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\nA function like [tf.diag_jacobian](https://www.tensorflow.org/probability/api_docs/python/tfp/math/diag_jacobian) would be great. In tensorflow this function allows to comput the diagonal of the Jacobian in parallel.\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\nThis implementation is space efficient but takes a lot of compute time:\r\n```python\r\nX = x.unbind(-1)\r\nx = torch.stack(x, -1)\r\ny = f(x)\r\nY = y.unbind(-1)\r\ndx = torch.stack([torch.autograd.grad(y_, x_, retain_graph=True) for x_, y_ zip(X, Y)], -1)\r\n```\r\n\r\nIn contrast to that this is very space consuming (it computes the whole jacobian) but very fast:\r\n```python\r\n# assuming x is (batch_size, inp_dim) and f: inp_dim-> inp_dim\r\nx = x.unsqueeze(1).repeat(1, inp_dim, 1)\r\ny = f(x)\r\ndx = torch.diagonal(toch.autograd.grad(torch.diagonal(y, 0, -2, -1), x), 0, -2, -1)\r\n```\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["enhancement",null,null,null],"text":"## ðŸ“š Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nHello, I noticed this older issue https://github.com/pytorch/pytorch/issues/6662 is still open and looked through this PR https://github.com/pytorch/pytorch/pull/24435 about adding `doctest` to jit. If it would be helpful I can work on other parts of the docs to convert code blocks to use `doctest`. Currently it seems there are over 400 code blocks in the docs using the format `Example::` that are not being tested, e.g. a bunch are in this [file](https://github.com/pytorch/pytorch/blob/master/torch/_torch_docs.py). \n\ncc @jlin27 @mruberry @VitalyFedyunin"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\n\r\n## Motivation\r\n\r\n* From https://discuss.pytorch.org/t/gloo-and-infiniband/89303. Some GPUs like RTX2080ti GPUs do not support GPUDirect/RDMA anyway.\r\n* With GPUs w/ GPUDirect/RDMA, in some scenarios we still would like to do CPU RDMA to avoid DtoH memory copy which causes certain synchronicity and breaks the pipelining.\r\n\r\nFor these use cases, GLOO infiniband could help achieve lower latency and higher bandwidth, and remove host/device synchronicity.\r\n\r\n## Pitch\r\n\r\nGLOO has an ibverbs transport in place https://github.com/facebookincubator/gloo/tree/master/gloo/transport/ibverbs. However it was not tested or used with PyTorch. We would like to test and integrate with PyTorch c10d library.\r\n\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski"},{"labels":["enhancement",null,null],"text":"-- with @tierex, @mrshenli , @pritamdamania, @lw, @agolynski , @osalpekar, @zhaojuanmao, @rohan-varma \r\n\r\n## ðŸš€ Feature\r\nConcept (and thus a set of APIs) of a `distributed.application` that comprises of:\r\n1. Higher level torch rpc APIs (role-based rpc APIs)\r\n1. Succinct init methods to for apps using both rpc and process groups with a heterogeneous topology (roles)\r\n1. Ensure all the above works well when launching these apps using [TorchElastic](https://github.com/pytorch/elastic/).\r\n\r\nThese sets of APIs makes it easier and more succinct for the user to express different types of application topologies. The goal is to make things simpler for the user hence we assume that apps will be launched via TorchElastic since using TorchElastic to launch distributed PT scripts makes life simpler for the end user as rank assignment, master selection, etc are done automatically and the user no longer has to specify these manually.\r\n\r\n**Note:** `distributed.application` does not necessarily have to be a new module, it is merely a set of APIs that function on top of the existing rpc and process group APIs that make it simple for the user to write distributed pytorch applications. Hence the proposed methods can be built directly into `torch.distributed` or `torch.distributed.rpc`. However, for the sake of clarity, in this doc, we refer to them as being part of `distributed.app`.\r\n\r\n## Motivation\r\nWhen writing a DDP-style application with distributed torch and launching it with torchelastic the user mainly uses ProcessGroups. The user code looks as follows:\r\n\r\n```\r\ndef main():\r\n    # torchelastic sets the following env vars so that init_method=\"env://\" works:\r\n    #    RANK, WORLD_SIZE, MASTER_ADDR, MASTER_PORT\r\n    torch.distributed.init_process_group(init_method=\"env://\")\r\n    \r\n    # run collective operations using the process group\r\n    torch.distributed.all_reduce(...)\r\n```\r\nThere are a few implications here:\r\n\r\n1. torchelastic performs rank assignment\r\n2. implicitly there is a `TCPStore` that is created on the endpoint - `MASTER_ADDR:MASTER_PORT` \r\n3. the `TCPStore` object (both the server and client) lives in the workerâ€™s process\r\n\r\n### Homogeneous Torch RCP Apps\r\nWhen all workers in the application are homogenous, they will all participate in the same RPC group and the same process group. The diagram below shows this topology on 3 nodes, each running 4 workers. \r\n![image](https://user-images.githubusercontent.com/43595115/87467964-a24acc80-c5cd-11ea-8e32-3c7e52888003.png)\r\n\r\nIn this case, the application continues working well with torchelastic:\r\n```\r\ndef main_rpc_backend_process_group():  \r\n    rank=int(os.environ[\"RANK\"]),\r\n    rpc.init_rpc(\r\n        name=f\"worker{rank}\",\r\n        rank=rank,\r\n        world_size=int(os.environ[\"WORLD_SIZE\"]),\r\n        backend=BackendType.PROCESS_GROUP)\r\n    # default process group is initialized as part of init_rpc()\r\n    dist.all_reduce(...)\r\n```\r\n\r\n### Heterogeneous Torch RPC Apps\r\nUnlike the homogeneous case, in the heterogeneous application, there are sub-groups of nodes that play different roles in the job. The diagram below shows a topology where there is a master node (running a single master process), 2 trainer nodes (each with 4 trainer workers), 3 parameter server nodes (each with 2 parameter server workers).\r\n![image](https://user-images.githubusercontent.com/43595115/87468085-cf977a80-c5cd-11ea-9be9-35cb3c580523.png)\r\n\r\nIn the heterogeneous case, we want all nodes to be able to invoke remote procedures and hold remote references on each other. However we want the collective operations to be done among the workers of the same role. Ideally, in the heterogeneous case we'd like to make group-rpc calls (e.g. call the same function on all trainers). This is currently cumbersome to do since torch rpc APIs are point-to-point and requires the user to book-keep the worker names for a role and for-loop around them when making rpc calls.\r\n\r\n## Pitch\r\nRecalling the heterogeneous case above, ideally we'd like the application code to look like the following:\r\n\r\n```\r\ndef master_main():\r\n    rpc.init_app(role=\"master\", backend=backend, backend_options)\r\n    trainer_rets = rpc.rpc_async(on_role=\"trainer\", func=run_trainer, args=(...))\r\n    ps_rets = rpc.rpc_async(on_role=\"ps\", func=run_ps, args=(...))\r\n    rpc.wait_all(trainer_rets + ps_rets)\r\n\r\ndef trainer_main():\r\n    rpc.init_app(role=\"trainer\", backend=backend, backend_options)\r\n    rpc.shutdown()\r\n\r\ndef run_trainer():\r\n    # run the trainer - invoked by master via rpc  \r\n    all_trainers_pg = rpc.init_process_group()\r\n\r\n\r\n# technically trainer_main and ps_main can be the same (just pass role as arg)\r\ndef ps_main():\r\n    rpc.init_app(role=\"ps\", backend=backend, backend_options)\r\n    rpc.shutdown()\r\n    \r\ndef run_ps():\r\n    # run the ps - invoked by master via rpc\r\n    all_ps_pg = rpc.init_process_group()\r\n```\r\n\r\nThe next sub-sections describe the changes proposed in the sample code above.\r\n\r\n### RPC Init App\r\n`rpc.init_app(role: str, backend, backend_options)` is similar to `rpc.init_rpc()` except that the call pattern is the same for all processes of the same role. Unlike `init_rpc` it does not take rank, world_size or init_method since it expects `RANK`, `WORLD_SIZE`, `MASTER_ADDR`, `MASTER_PORT` to be in the env var. When used with torchelastic these env vars will be set by the elastic agent. Otherwise, the user needs to ensure that the env vars are set before invoking the main method.\r\n\r\nAfter the application is initialized the following hold true:\r\n\r\n1. Every process (regardless of role) can invoke rpc APIs on one another.\r\n1. Every process has a *global rank* (e.g. one that was passed as `RANK` env var) that can be obtained via `dist.get_rank()`\r\n1. Each node is assigned a rank. (this is equivalent to the elastic agentâ€™s rank - a.k.a `GROUP_RANK` in torchelastic)\r\n1. The global rank can be derived from the following formula:\r\n1. ![image](https://user-images.githubusercontent.com/43595115/87468522-a75c4b80-c5ce-11ea-9994-72b39363fd97.png)\r\n1. Where `node_dims[i]` returns the number of local processes on node rank i. \r\n1. `NODE_RANK` (`GROUP_RANK`), `LOCAL_RANK` and node_dims is set by elastic agent.\r\n1. A `roleRank` is assigned - this is a number between `0 - numWorkersInRole` and may be different from global rank. In the example below the trainer process `0` on trainer node `2` gets a global rank of `4` but a role local rank of `3`.\r\n1. The `roleRank` is used in the `worker_name = {role}:{roleRank}`. \r\n![image](https://user-images.githubusercontent.com/43595115/87468640-dbd00780-c5ce-11ea-9ef1-3c040b4194aa.png)\r\n\r\n### Role-based rpc APIs\r\nSince we are working with the concept of roles this is really a syntactic sugar that allows users to perform group rpc calls on all workers belonging to a role rather than having to for-loop around them. It can trivially be implemented by first getting all the worker names for the role (or derive it by using `role_world_size = roles[roleRank].x * roles[roleRank].y`)\r\n\r\n```\r\nret_futures = {}\r\nfor idx in range(0, roles[trainerRoleRank].x * roles[trainerRoleRank].y)):\r\n    name=f\"trainer:{idx}\"\r\n    \r\n    ret_futures[name] = rpc.rpc_async(to=name, func=foobar, args=(...)))\r\n\r\n# versus\r\nret_futures = rpc.rpc_async_role(role=\"trainer\", func=foobar, args=(...))\r\n```\r\n\r\n**Note** The original design also included an `rpc.wait_all()` API for completeness. This has already been implemented in the form of `torch.futures.collect_all` (https://github.com/pytorch/pytorch/blob/master/torch/futures/__init__.py#L88)\r\n\r\n### App Init Process Group\r\n**Note:** this is also discussed in the context of: https://github.com/pytorch/pytorch/issues/33583\r\n\r\nSimilar to `dist.init_process_group()` except that this function creates a process group per `ROLE`. By default it will create the process group for the role that the caller process belongs to. We know the role of the caller since we expect the caller to have called `rpc.init_app(role=\"my_role\", ...)`.\r\n\r\n**Open Question:** should we implement `rpc.new_group(names=[])` allowing the user to create process groups, except that since we are in the context of rpc (hence all processes have names) we provide a way to create process groups using names rather than global rank. This is useful as users will tend to think of processes by their names (e.g. the ones they have assigned) rather than their numerical ranks.\r\n\r\n### App Info Accessors\r\n`rpc.get_role_info()` expands on rpc.get_worker_info(name) by returning a map of worker_name to WorkerInfo. Useful when operating with roles since when performing point-to-point communication users may need to lookup specific worker information given a role. For instance, the length of the returned collection can be used to determine the total number of workers for a particular role to do some type of index striding for simply for-loop around them to make rpc calls.\r\n\r\n## Failure Handling Behaviors\r\n\r\nThis section describes the types of failures and who/how those failures are handled.\r\n\r\nWe focus on worker process(es)  failure since a node failure can be viewed as multiple worker failures. In general TorchElastic views failures as two scaling events: scale-down + scale-up. In TorchElastic the world size is fixed between \"rounds\" of quorum where \"rounds\" is defined as the state of the world between two rendezvous versions. Between rounds ALL existing workers are shut down and restarted, hence TorchElastic follows a \"all-or-nothing\" model when it comes to dealing with faults.\r\n\r\nThere are three types of torch applications that we care about:\r\n\r\n1. RPC only (no process groups)\r\n2. RPC + DDP\r\n3. RPC + DDP + Pipelining\r\n\r\nIn all three cases the following behavior on failures is guaranteed by TorchElastic:\r\n\r\n1. Worker process(es) fail\r\n2. elastic agent detects the failure\r\n3. elastic agent tears down all other workers on the same host\r\n4. elastic agent enters a re-rendezvous\r\n5. other elastic agents are notified of the re-rendezvous event\r\n6. other elastic agents tear down all their respective local workers\r\n7. all elastic agents re-rendezvous (e.g. next round) and get assigned a rank\r\n8. each elastic agent computes the worker ranks based on the agent ranks\r\n9. all elastic agents restart the worker processes\r\n\r\n**Note:** This implies that work between checkpoints are lost.\r\n\r\n**Note on node failure:** TorchElastic is NOT a scheduler hence it cannot deal with replacement of NODES. We rely on the scheduler to replace failed nodes (or containers). Assuming the scheduler replaces the failed node the following behavior is observed on node failures:\r\n\r\n1. Node(s) fail and are replaced by the scheduler\r\n2. Elastic agents are started on the replaced nodes and attempt to join the rendezvous\r\n3. The surviving agents are notified of such event and they all tear down their local workers and re-rendezvous\r\n4. A new version of rendezvous (e.g. round) is created and the worker processes are started again.\r\n\r\n\r\n\r\n\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar @jiayisuse"},{"labels":["enhancement",null,null],"text":"## ðŸ› Bug\r\n\r\n`torch.onnx.export` fails when `nn.Fold` module is present. \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\n\r\nclass Demo(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.fold = nn.Fold(output_size=(4,5), kernel_size=(2,2))\r\n        \r\n    def forward(self, x):\r\n        folded = self.fold(x)\r\n        return folded\r\n    \r\ninput_tensor = torch.randn((1, 3*2*2, 12))\r\ndemo = Demo()\r\nout = demo(input_tensor)\r\n\r\ntorch.onnx.export(\r\n    demo,\r\n    input_tensor,\r\n    \"debug.onnx\",\r\n    verbose=True,\r\n    input_names=[\"data\"],\r\n    opset_version=12,\r\n    dynamic_axes={\"data\": {0: \"batch\"}}\r\n)\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\nThis returns\r\n```\r\nRuntimeError: Exporting the operator col2im to ONNX opset version 12 is not supported. Please open a bug to request ONNX export support for the missing operator.\r\n```\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py) output\r\n```\r\nPyTorch version: 1.5.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce RTX 2080 Ti\r\nNvidia driver version: 440.82\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.5\r\n[pip3] torch==1.5.1\r\n[pip3] torchvision==0.6.0a0+35d732a\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cudatoolkit               10.2.89              hfd86e86_1  \r\n[conda] mkl                       2020.1                      217  \r\n[conda] mkl-service               2.3.0            py36he904b0f_0  \r\n[conda] mkl_fft                   1.1.0            py36h23d657b_0  \r\n[conda] mkl_random                1.1.1            py36h0573a6f_0  \r\n[conda] numpy                     1.18.5           py36ha1c710e_0  \r\n[conda] numpy-base                1.18.5           py36hde5b4d6_0  \r\n[conda] pytorch                   1.5.1           py3.6_cuda10.2.89_cudnn7.6.5_0    pytorch\r\n[conda] torchvision               0.6.1                py36_cu102    pytorch\r\n\r\n```\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof"},{"labels":["enhancement",null,null,null],"text":"## Goal\r\n\r\nAs of v1.6, `torch.distributed.rpc` only accepts CPU tensors. Applications have to first move the tensor to CPU and then pass it to RPC APIs. This is both inconvenient and slow. The original reason for enforcing CPU tensors is to avoid invalid devices on the destination process. The RPC device placement API aims at (partly) addressing this issue by allowing applications to configure the device mapping on caller and callee. Note that this proposal focuses on changes in API and RPC agents. There will be separate efforts in the comm layer and the distributed autograd engine to improve its efficiency. \r\n\r\n## API\r\n\r\nWe can add one `map_location` argument to [`RpcBackendOptions`](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.RpcBackendOptions). The example below shows how to configure this field. \r\n\r\n```python\r\nrpc.init_rpc(\r\n    \"worker0\",\r\n    rank=0,\r\n    world_size=2,\r\n    backend=rpc.BackendType.TENSORPIPE,\r\n    rpc_backend_options=rpc.TensorPipeRpcBackendOptions(\r\n        num_worker_threads=8,\r\n        rpc_timeout=20, # 20 second timeout\r\n        map_location={\r\n        \t\"worker1\": {\r\n        \t\t\"cpu\":     \t\"cuda:0\",\r\n        \t\t\"cuda:0\":  \t\"cuda:1\"\r\n        \t},\r\n        \t\"worker2\": {\r\n        \t\t\"cpu\": \t\t\"cuda:1\",\r\n        \t\t\"cuda:0\":\t\"cuda:2\"\r\n        \t}\r\n        }\r\n    )\r\n)\r\n\r\nx = torch.zeros(2).to(0)\r\ny = torch.ones(2).to(0)\r\n\r\n# x and y will be materialized on \"cuda:1\" in \"worker1\".\r\n# the return value z will be materialized on \"cuda:0\" on \"worker0\",\r\n# which is defined by the inverse of the mapping specified by \"worker0\"\r\nz = rpc.rpc_sync(\"worker1\", torch.add, args=(x, y))\r\n\r\n# x and y will be materialized on \"cuda:2\" in \"worker2\".\r\n# the return value z will be materialized on \"cuda:0\" on \"worker0\".\r\nz = rpc.rpc_sync(\"worker2\", torch.add, args=(x, y))\r\n\r\nrpc.shutdown()\r\n```\r\n\r\nIn this proposal, we only aim to add default `map_location`. If necessary, we can add the per-RPC `map_location` later. \r\n\r\nThis is only intended to be added to the `TensorPipe` backend. `ProcessGroup` backend will throw an exception is this field is specified.\r\n\r\n\r\n### Device Placement for Return Value\r\n\r\nThere are a few options here.\r\n\r\n* Option 1: we can use the inverse of the `map_location` to place tensors in the return value.\r\n* Option 2: we can add another `return_map_location` for this purpose. \r\n* Option 3: use the `map_location` defined on the destination to send return values. \r\n\r\nThe downside of Option 3 is that the caller cannot fully control the behavior of its own RPC. Imagine a Parameter-Server\r\ntraining application with one PS with multiple trainers. Option 3 would requre the PS to define the `map_location` entry\r\nfor all trainers, which is cumbersome. So it might be more convenient for applications to use Option 1 or 2.  \r\n\r\nAs the first version, we can go with Option 1, and when we see requests for Option 2, we can add a `return_map_location` and\r\nuse entries defined there to override the inverse of `map_location`, so that there will be no BC issue.\r\n\r\n\r\n## Implementation\r\n\r\nIn the `init_rpc` call, all processes will use the initialized RPC to communicate `map_location` to make sure that there \r\nare no invalid devices. If there is any in any process, all processes will throw an error. \r\n\r\nThis `map_location` will be stored in the agent and not necessary to modify `Message` structure. \r\n\r\nAs there is no remote device type yet, agent will first use integers to store devices, -1 for CPU. \n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar @jiayisuse @lw @beauby"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nTensorflow.js allows tensorflow users to run models in a browser.  They can share models with their friends, and run realtime inference easily.  While ONNX.js exists, I'm finding it doesn't really have a community or any learning resources really except for one youtube video.  The official recommendation is to convert pytorch to onnx to tensorflow to tensorflow.js\r\n\r\nIt would be nice if we could have just a pytorch.js to convert to.\r\n\r\n## Motivation\r\nI would love to create ML applications that people can use just by navigating to a URL, instead of having to install dependencies.\r\n\r\n## Pitch\r\n\r\nI want to be able to use Pytorch models in the browser\r\n\r\n## Alternatives\r\n\r\nONNX.js, I found it to be too confusing, and I couldn't figure out how to make it work with object detection."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nSupporting more loss functions like `FocalLoss` or `DICE` natively will be a nice addition, especially as their already implementations to them out there.\r\n\r\n## Motivation\r\n\r\nNative support for more loss functions will reduce boilerplate code.\r\n\r\n## Pitch\r\n\r\nReview and addition of loss functions found [Here](https://github.com/JunMa11/SegLoss)\r\n\r\n## Alternatives\r\n\r\nThere is implementations on PyTorch Forums for some of the loss functions mentioned above but because they are not reviewed they sometimes have big flaws.\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸ› Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nFor multi task training with different ranks running different tasks, parameters of inactive tasks are unused locally, even though used globally. By default DDP would allreduce these parameters and update all with the mean of grad across all ranks. Such grad update could trigger the weight decay with some optimizers (such as SGD) even for those inactive tasks on rank. Such weight update would propagate to ranks running active tasks and eventually pollute the grad there. \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Have two ranks R1 and R2. Both of them load the same model.\r\n1. R1 runs task A branch and R2 runs task B branch.\r\n1. After the first iteration, we would see B's grad on R1 and A's grad on R2 get updated.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nB's grad on R1 and A's grad on R2 should never get updated as task B is inactive on R1 and task A is inactive on R2.\r\n\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nSupport torch.solve on the GPU\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nIn https://github.com/pytorch/audio/issues/768 I asked about the complex support in torch for some linear algebra functions and\r\n@anjali411 said, I should post here my feature request.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nIt would be nice, when the following code would not fail in the last line:\r\n```python\r\n>>> import numpy as np, torch\r\n>>> torch.__version__\r\n'1.7.0.dev20200705+cu101'\r\n>>> t = torch.tensor(np.random.randn(2, 2))\r\n>>> _ = torch.solve(t, t)\r\n>>> _ = torch.solve(t.to(0), t.to(0))\r\n>>> t = torch.tensor(1j * np.random.randn(2, 2))\r\n>>> _ = torch.solve(t, t)\r\n>>> _ = torch.solve(t.to(0), t.to(0))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: \"solve_cuda\" not implemented for 'ComplexDouble'\r\n```\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\nCurrently, I am using\r\nhttps://github.com/kamo-naoyuki/pytorch_complex/tree/master/torch_complex\r\nbut a native support would be faster, consume less memory and the code would contain fewer hacks to distinguish real and complex tensors.\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @ezyang @anjali411 @dylanbespalko"},{"labels":["enhancement",null,null,null],"text":"`SyncBatchNorm` provides a classmethod to convert `BatchNorm` layers to `SyncBatchNorm` layers:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/881c1adfcd916b6cd5de91bc343eb86aff88cc80/torch/nn/modules/batchnorm.py#L510-L556\r\n\r\nIt will be helpful to provide an API to do the reverse conversion as well. \r\n\r\ncc @albanD @mruberry @ayush29feb"},{"labels":["enhancement",null,null,null],"text":"Hi,\r\n\r\nI am trying to perform sparse and dense matrix multiplication using half precision tensors in pytorch.\r\n\r\nThe following code:\r\n```\r\nimport torch\r\na = torch.randn(3,2).half().cuda()\r\ni = torch.LongTensor([[0, 1, 1],  [2, 0, 2]]) \r\nv = torch.FloatTensor([3, 4, 5]) \r\nb = torch.sparse.FloatTensor(i, v, torch.Size([2,3])).half().cuda()\r\nc = torch.spmm(b, a)\r\n```\r\nwill produce this error: `RuntimeError: \"addmm_sparse_cuda\" not implemented for 'Half'`\r\n\r\nIs there anyway to solve this?\r\n\r\n## Environment\r\n\r\n-PyTorch version: 1.5.0\r\n-Is debug build: No\r\n-CUDA used to build PyTorch: 10.2\r\n\r\n-OS: Arch Linux\r\n-GCC version: (GCC) 10.1.0\r\n-CMake version: version 3.17.3\r\n\r\n-Python version: 3.8\r\n-Is CUDA available: Yes\r\n-CUDA runtime version: 10.2.89\r\n-GPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\n-Nvidia driver version: 440.100\r\n-cuDNN version: /usr/lib/libcudnn.so.7.6.5\r\n\n\ncc @vincentqb @aocsa"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nEnable IntelÂ® VTuneâ„¢ Profiler's Instrumentation and Tracing Technology APIs (ITT) to PyTorch\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nThis is Intel's counterpart functionality of NVidiaâ€™s NVTX (https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.emit_nvtx).\r\n1.\tThe instrumentation and tracing technology (ITT) APIs of IntelÂ® VTune Profiler enables application to generate and control the collection of trace data during its execution. ITT provides scoped timer and visualizes your program execution in OP/function/sub-function granularity. By applying the API functions on executed ops, we can get performance visualization on individual PyTorch ops. From the following screenshot we can see CPU usage on each op, like the conv2d in this example. This will be helpful for op-level tuning.\r\n![ITT1](https://user-images.githubusercontent.com/25193121/86559027-ed455e00-bf95-11ea-979c-58472fde7789.png)\r\n\r\n2.\tUpon existing op level profiling, users can annotate a scope of multiple ops to have a higher level understanding of performance manually.\r\nIn the following screenshot of ResNet50 profiling result, each epoch, with train step and test step inside, is shown in timeline graph. Also, it is very clear to see how long was spent on train step, how long was spent on test step.\r\n![ITT2](https://user-images.githubusercontent.com/25193121/86559036-f6cec600-bf95-11ea-8046-c00d71323c43.png)\r\n\r\n      We can also expand this timeline graph to see profiling results with more details, in case users need to track how individual ops got invoked in their topologies.\r\n![ITT3](https://user-images.githubusercontent.com/25193121/86559047-fc2c1080-bf95-11ea-9eb2-ac684d8992ce.png)\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n1. Visualize execution of topology as well as performance of metrics of each op.\r\n2. Annotation of a scope of ops to have a higher level understanding of the topology execution.\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @VitalyFedyunin @ngimel"},{"labels":["enhancement",null,null],"text":"See forum discussion: https://discuss.pytorch.org/t/distributed-model-parallel-using-distributed-rpc/87875/3\r\n\r\nThis might be helpful to provide launching script for RPC as we did for DDP. \n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar @jiayisuse"},{"labels":["enhancement",null,null],"text":"As of today, we release the distributed autograd context when exiting the context manager:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/c71ec1c717e5b225f28ef3bacde416c69f3c4d77/torch/distributed/autograd/__init__.py#L32-L37\r\n\r\nAs a result, the following code wouldn't work, as when running the `run_backward` callback function, the distributed autograd context might have been released. \r\n\r\n```python\r\n\r\nclass Model(nn.Module):\r\n    def forward(self, x):\r\n        return ps_rref.rpc_async().foward(x)\r\n\r\nwith dist_autograd.context() as context_id:\r\n    def run_backward(out_fut):\r\n        dist_autograd.backward(context_id, [loss_fn(out_fut.wait(), label)]\r\n\r\n    model(x).then(run_backward)\r\n```\r\n\r\n### Pitch\r\n\r\nInstead of always releasing the distributed autograd context on exit the scope, we can manage its lifetime using a `shared_ptr` or other reference counting methods. Then, we can let the TLS own a copy of the `shared_ptr` to the context. So that callbacks can still access the context if necessary. Another option might be expanding `context_id` into a custom type whose lifetime dictates the distributed autograd context's lifetime. \r\n\r\nThis feature will be useful to parallelize the backward pass in pipeline parallel applications. \r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar @jiayisuse"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nAn incremental version of pca_lowrank - processing a mini-batch of samples in each iteration.\r\nSimilar to sklearn.decomposition.IncrementalPCA(), but in GPU.\r\n\r\n## Motivation\r\n\r\nThe current implementation processes all the data in one go and can run quickly only if all the data fits into the GPU memory.\r\n\r\n## Pitch\r\n\r\nIt is sometimes needed to perform PCA on a large dataset of images. Doing it in GPU using pytorch is great, but currently limited by GPU memory. An incremental version will allow processing large datasets.\r\n\r\n## Alternatives\r\n\r\nDoing PCA in CPU using sklearn :(\r\n\n\ncc @ngimel"},{"labels":["enhancement",null,null,null],"text":"This is inspired by #40690 and other requests we saw before. Applications might want to use c10d operations (e.g., `all_gather`, `all_reduce`) in the forward pass and expect they been linked in the same autograd graph. This would require implementing autograd functions for c10d operations as what's done for `scatter` and `gather` in [nn/parallel/_functions.py](https://github.com/pytorch/pytorch/blob/b35cdc5200af963e410c0a25400fd07f30b89bca/torch/nn/parallel/_functions.py).\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nThe ability to turn off scientific notation (standard form) using the C++ API.\r\n\r\n## Motivation\r\n\r\nIt would be useful to turn off the scientific notation used to display tensors. Sometimes I will often get numbers like: 9.9999e-01 which - in the common tongue - is more frequently known as 0.9999 or 1.0. This notation can be really helpful for very small numbers close to zero but in certain situations it is just confusing.\r\n\r\n## Pitch\r\n\r\nIt would be nice to have an option when creating tensors to turn off scientific notation.\r\n\n\ncc @yf225 @glaringlee"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nas is:\r\n\r\n```python\r\n# 20 workers will be launched and up to 40 items(2 * num_workers) will be prefetched. (in queue)\r\ndataloader = DataLoader(dataset, batch_size=64, num_workers=20)\r\n```\r\n\r\nto be:\r\n\r\n```python\r\n# 20 workers will be launched and up to 80 items(4 * num_workers) will be prefetched. (in queue)\r\ndataloader = DataLoader(dataset, batch_size=64, num_workers=20, max_prefetched_item=80)\r\n# or\r\n# dataloader = DataLoader(dataset, batch_size=64, num_workers=20, worker_factor=4)\r\n```\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nhttps://github.com/pytorch/pytorch/blob/ac79c874cefee2f8bc1605eed9a924d80c0b3542/torch/utils/data/dataloader.py#L754\r\n\r\nI want to preload more data but use only small number of workers.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nParameterize _MultiProcessingDataLoaderIter's multiplier or the number of prefetched items'.\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nA simple and easy way to download custom pytorch models with private URLs (such as JFROG), to a custom path. To provide a consistent API for the same. As an additional feature, it would be great to auto-save to a url.\r\nIf need be, we should be able to implement a custom download/uploader.\r\n\r\n```python\r\nmodel_dict = torch.load_from_url(url, local_path, env_variables = 'JFROG_SERVER=xyz.com', custom_downloader=my_jfrog_downloader)\r\ntorch.save_to_url(model_dict, url, env_variables='', custom_uploader=my_jfrog_uploader)\r\n```\r\n\r\n\r\n\r\n## Motivation\r\n\r\nThere are lots of individuals and organizations that host their models on custom artefactories / google-drive / etc. Each one has to implement their own way to download the weights file, and often is done in an ugly fashion - bash scripts, curl requests, etc. Often, it might not be reproducable\r\n\r\n## Pitch\r\n\r\nWith more and more people training their own custom models, and hosting it somewhere. It is essential to have a consistent api to simplify and standardise these tasks. At the moment, the community chooses to do it in it's own way, and there might be unnecessary dependencies.\r\n\r\n## Alternatives\r\n\r\nBash scripts, python scripts with urllib, requests. \r\n\r\n## Additional context\r\n\r\nNone\n\ncc @ailzhang"},{"labels":["enhancement",null,null],"text":"It could be nice to be able to type arguments e.g. as `torch.Tensor['BCHW']` or `torch.Tensor['*C']` or `torch.LongTensor['?2']` even for user code self-documentation purposes (e.g. like one can specify dimension semantics when exporting to ONNX).\r\n\r\nTyping information may come in different forms: maybe even some generic API for expressing frequent constraints such as \"two tensors should have same first dimensions\", \"these two tensors should have exactly same shapes\" or \"these two two tensors should have broadcastable shapes\".\r\n\r\nI understand that expressing these different constraints may be a difficult task, but maybe this was already tackled somehow in functional programming community (parametrized types) and [\"design by contract\"](https://en.wikipedia.org/wiki/Design_by_contract) paradigm (e.g. Code Contracts integrated in C# language). Maybe some reduced / concise way that is already useful can be found\n\ncc @ezyang @bhosmer @smessmer @ljk53"},{"labels":["enhancement",null,null,null],"text":"RPC uses `_wait_all_workers` as a barrier to do graceful shutdown. This might be a useful feature to applications as well, i.e., we can have `rpc.barrier()` for RPC and `dist.barrier()` for c10d. \r\n\r\nNote that this is different from #40107:\r\n\r\n* The `rpc.barrier()` only makes sure that all workers reach the same point, while there could still be on-going concurrent execution in other RPC threads to satisfy futures created by the main thread. \r\n* The context manager collects and waits for all futures created in scope, but does not communicate with other workers.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/3fb1e73a4e928a0452c3b73ca6815f67127424e9/torch/distributed/rpc/api.py#L137-L198\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null,null,null],"text":"Inspired by this forum post: https://discuss.pytorch.org/t/distributed-pagerank-with-pytorch/78529/17\r\n\r\nCurrently, applications need to manually collect all futures created by RPC and wait for them if necessary. This is a bit cumbersome and might even be hard if some imported libraries uses RPC. \r\n\r\nTo make it easier to use, we could create a context manager to automatically collects all futures created in scope by `RpcAgent::send` and wait for all of them on exit. It might be sufficient to just collect futures created by the current threads and ignore nested RPCs in requests handled in the RPC thread pool.\r\n\r\n## Update\r\n\r\nAs we will be using this as a bootcamp task, let me add more details to this issue. \r\n\r\n### Pitch\r\n\r\n#### API\r\nSuppose the context manager is called `wait_all()`. Open to better names. It creates a thread local list to hold all `Future` objects returned by `RpcAgent::sendWithRetries` and wait for all of them on exit.\r\n\r\n```python\r\nwith rpc.wait_all():\r\n    rpc.rpc_async(...)\r\n    rref = rpc.remote(...)\r\n    some_imported_func_with_rpc(...)\r\n    # block here until all created futures (including the ones for RRefs) are done\r\n```\r\n\r\nNote that this can be used in conjunction with #40166. Applications can perform per-iteration sync using:\r\n\r\n```python\r\nwith rpc.wait_all():\r\n    run_one_iteration()\r\nrpc.barrier()\r\n```\r\n\r\n#### Implementation Phase 1 (Python-Only)\r\n\r\n**Goal**: only wait for `Future` objects created by `rpc_async`. \r\n\r\n\r\n1. Add the context manager, which collects the future returned by the following function\r\n    https://github.com/pytorch/pytorch/blob/591fffc5245e1ce8b00cf2a778d5874e43655d53/torch/distributed/rpc/api.py#L593-L674\r\n\r\n2. Add tests to cover `rpc_async` to [`rpc_test.py`](https://github.com/pytorch/pytorch/blob/591fffc5245e1ce8b00cf2a778d5874e43655d53/torch/testing/_internal/distributed/rpc/rpc_test.py). Note that this might change if [this stack](https://github.com/pytorch/pytorch/pull/40860) lands first.\r\n\r\n#### Implementation Phase 2 (Touches C++ Code)\r\n\r\n1. Create a C++ thread local variable to collect Futures returned by the following function:\r\n    https://github.com/pytorch/pytorch/blob/591fffc5245e1ce8b00cf2a778d5874e43655d53/torch/csrc/distributed/rpc/rpc_agent.cpp#L48-L83\r\n2. Expose that variable (setter/getter) through pybind11. Similar to the code below:\r\n    https://github.com/pytorch/pytorch/blob/591fffc5245e1ce8b00cf2a778d5874e43655d53/torch/csrc/distributed/rpc/init.cpp#L718-L723\r\n3. Remove the Python thread local variable added in phase 1, and port the context manager to use the pybind exported APIs.\r\n4. Add tests to cover `remote` to [`rpc_test.py`](https://github.com/pytorch/pytorch/blob/591fffc5245e1ce8b00cf2a778d5874e43655d53/torch/testing/_internal/distributed/rpc/rpc_test.py). \r\n\r\n\r\n\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null,null],"text":"In OSS, ProcessGroup RPC backend verifies all worker names are distinct and uses ranks as ids. So that wrong name-to-id mapping will be spotted at construction time. However, for other RPC backend without collective communication capabilities, it is not that easy to gather all information from all workers and check their correctness. \r\n\r\nAn alternative is to let `torch.distributed.rpc` provide a backend-agnostic helper to check all worker names using `c10d::Store`, and assign ids using the `c10d::Store`. This should help prevent applications from introducing unintentional mapping errors.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null,null],"text":"Approx variants of gelu are apparently used in google's BERT / GPT-2 (`gelu_new` in [huggingface](https://github.com/huggingface/transformers/blob/master/src/transformers/activations.py#L25) and `gelu_accurate` in [fairseq](https://github.com/pytorch/fairseq/blob/master/fairseq/modules/gelu.py#L16)). The version in fairseq and in huggingface do not have [stable gradients](https://github.com/pytorch/fairseq/issues/2235) at large inputs for fp16). So maybe it's worth adding it to core in C++ for memory saving / fusion and error-avoidance. One way could be an `approx=True` option to F.gelu / nn.GELU.\r\n\r\nThis is commonly used now, but I don't know why (maybe for simplicity of impl). And it can be deemed too case-specific for adding into core (otoh gelu itself is quite specific).\n\ncc @albanD @mruberry"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nLRP-based explanation rules that work during or similar to backwards passes.\r\n\r\n## Motivation\r\nI'm a PhD student working in explainability for Video Rec. models. I much prefer PyTorch to other frameworks, but ended up implementing my own torch-based LRP library to be able to generate explanations. I'm focused on 3D CNNs, but my code works with 1D and 2D models and with the most common layers. I'd like to try my hand at extending it to the LibTorch C++ library, and if it works out well enough, I'd be very proud to add it to the PyTorch codebase.\r\n\r\n## Pitch\r\n\r\nRules defined for each of the torch.nn modules that either work as modified nn.Functions or could be run through a similar process (e.g. .explain() as opposed to .backwards())\r\n\r\n## Alternatives\r\nGranted this has become a little more popular among PyTorch devs lately, but it still seems pretty DIY. Most methods I've seen just iterate forwards and then backwards through layers. But clearly, defining it in a layerwise fashion as the regular backprop rules are, then allows for non-sequential/more complex models that can't feasibly be flattened into a list of layers.\r\n\r\n## Additional context\r\n\r\nAs I've said, I've already written a working prototype for this in PyTorch that just tweaks the models gradient on backprop. It's good enough as of yet for what I've done, but I'd love to be able to clean it up and package it like torchvision : https://github.com/liamhiley/torchexplain\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nsupport mainstream pruning techniques.\r\n\r\n## Motivation\r\nRecently, lots of new pruning algorithms are proposed, but the [current implementation](https://github.com/pytorch/pytorch/blob/4fef3763dd7266195e8ef20c0a5d4dd1219afeb0/torch/nn/utils/prune.py) mainly contains norm-based pruning methods. \r\n\r\nBased on the [discussion here](https://github.com/pytorch/pytorch/issues/38598#issuecomment-641009728), I believe it's meaningful to add some state-of-the-art pruning methods.\n\ncc @albanD @mruberry"},{"labels":["enhancement",null,null],"text":"Cross products are useful in a number of situations (e.g., calculations of quaternion products), and it would be great if we could broadcast with `torch.cross` \n\ncc @mruberry @rgommers"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nCurrently, an `nn.TransformerEncoder` accepts an `nn.TransformerEncoderLayer` instance in its `__init__` method and deep clones it `nlayer` times.\r\nHowever, a better and more intuitive way is to pass a list of `nn.TransformerEncoderLayer` instances into an `nn.TransformerEncoder` as its internal layers.\r\n\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nCurrent implementation has two drawbacks:\r\n1. If someone implements a Transformer model as follows (note the `self.` in curly brackets):\r\n```\r\nclass TransformerModel(nn.Module):\r\n    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\r\n        super(TransformerModel, self).__init__()\r\n        {self.}encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\r\n        self.transformer_encoder = nn.TransformerEncoder({self.}encoder_layers, nlayers)\r\n```\r\nIf there's no `{self.}` (as in [official transformer tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)), the code just works fine; but if someone adds the `{self.}`, the `TransformerModel` instance will treat `self.encoder_layers` as its submodule while not able to find gradients of its parameters during back-propagation (because `nlayers` new `TransformerEncoderLayer` instances will be deep cloned inside `nn.TransformerEncoder` and `self.encoder_layers` itself will NOT participate in the Module's forward calculation) and leads to a failure. This is exactly why I make this feature request here - I spent a lot of time on this and find the deep clone issue.\r\n\r\n2. `nlayer` Transformer layers cannot share parameters (though one can achieve this by calling the same `nn.TransformerEncoderLayer` instance multiple times).\r\n\r\n## Pitch\r\n\r\nThe better abstraction will be: do NOT deep clone the `nn.TransformerEncoderLayer` instance `nlayer` times. Instead, pass a length-`nlayer` `nn.TransformerEncoderLayer` instances and use them directly in the newly created `nn.TransformerEncoder`. This design resembles `RNNCell` and `MultiRNNCell` in TensorFlow.\r\n\r\nTo keep backwards compatibility, one can check the data type of `encoder_layer` in `nn.TransformerEncoder`'s `__init__` method `def __init__(self, encoder_layer, num_layers, norm=None)`. A possible implementation could be:\r\n```\r\n    def __init__(self, encoder_layer, num_layers, norm=None):\r\n        super(TransformerEncoder, self).__init__()\r\n        if isinstance(encoder_layer, list) and all([isinstance(l, TransformerEncoderLayer) for l in encoder_layer]):\r\n            self.layers = ...   # whatever one can do\r\n        else:\r\n            self.layers = _get_clones(encoder_layer, num_layers)  # the original logic\r\n```\r\n\r\nBut one has to be careful about the parameter sharing in this case (inside and outside the `nn.TransformerEncoder`), which I'm not quite familiar with.\r\n\r\n## Alternatives\r\n1. Also, one can provide another `__init__` method for `nn.TransformerEncoder`: pass in hyperparameters (instead of an instance) of each `nn.TransformerEncoderLayer` and construct all `nn.TransformerEncoderLayer`s directly. This can avoid the confusing behavior, too.\r\n2. Do not produce gradient errors for unused parameters in an `nn.Module`. Print warnings instead."},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nExport fake_quantize_per_tensor_affine and fake_quantize_per_channel_affine functions to ONNX.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nTo support deploying QAT network by backend like TensorRT  outside pytorch through ONNX, fake quantization needs to be exported to ONNX operator.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nfake quantization will be broken into a pair of QuantizeLinear and DequantizeLinear ONNX operator.\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nFake quantization is effectively quantize followed dequantize. Have discussed with @raghuramank100 and reached an agreement this is the right way to export. \r\n\r\nSimilar functionality has been added to tensorflow-onnx, https://github.com/onnx/tensorflow-onnx/pull/919. \n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nhttps://github.com/pytorch/pytorch/pull/38590/files is adding support for RRef timeouts, although, this can be improved in the case of `to_here`. Right now, we only check if the rpc.remote call for `to_here` timed out before making a blocking call on waiting on the corresponding future for `to_here`. However, we instead have to_here wait on 2 conditions:\r\n1) [Happy path] the `to_here` future successfully finishes\r\n2) [Timeout]: `isTimedOut()` (added in the mentioned PR) is set to true. We can probably handle these conditions by signalling on a condvar.\r\n\r\nThis will make timeouts in the case of `to_here` more robust, as we will be able to interrupt the blocking call when we get notified that the original rpc.remote() creation has failed.\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nBasic morphological operations (e.g. dilation and erosion)\r\n\r\n## Motivation\r\nMany pytorch applications deal with images. It would therefore be great if pytorch would natively support basic morphological operations, so that the user does not have to resort to other libraries (e.g. scipy) to carry them out, which involves moving the data from the GPU to the CPU and can therefore result in a bottleneck.\r\n\r\n## Pitch\r\nImplement basic morphological operations in pytorch. This should be very straight-forward, since the code can basically be translated from numpy to torch based on other implementations (e.g. scipy). If this feature request is accepted, then I would be glad to help with the implementation.\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nScripting modules with recursive function calls. torch script currently does not support this case.\r\n\r\nCurrent behavior:\r\nDuring scripting the module below, I see an exception:\r\n\r\nRuntimeError:\r\nmethod 'get_value_' is called recursively. Recursive calls are not supported\r\n\r\n## Pitch\r\n\r\nTo enable scripting modules that contain recursive functions.\r\n\r\n## Additional context\r\n\r\nTo Reproduce:\r\n\r\n```python\r\n# Scripting the module below throws the above exception:\r\nimport torch\r\nfrom torch import Tensor\r\nfrom torch.jit.annotations import Dict\r\n\r\n\r\nclass MyModule(torch.nn.Module):\r\n    def __init__(self):\r\n        super(MyModule, self).__init__()\r\n        self.dict = torch.jit.Attribute({}, Dict[Tensor, float])\r\n\r\n    def get_value_(self, y):\r\n        if y in self.dict:\r\n            return self.dict.get(y)\r\n        else:\r\n            return self.get_value_(y[1:])\r\n\r\n    def forward(self, x):\r\n        return self.get_value_(x)\r\n\r\ntorch.jit.script(MyModule())\r\n\r\n```\r\n\n\ncc @suo"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nAbility to convert models with operator torch.var_mean to ONNX format.\r\n\r\n## Motivation\r\n\r\nCurrently it's not possible to convert such models to ONNX and this operator is used in many new neural nets (for example recently open-sourced Google's BiT.\r\nAfter running \"torch.onnx.export\" I'm getting message that \"torch.onnx.symbolic_opset9.var_mean does not exist\". I'm using pytorch 1.5.0\r\n\r\nThank you\r\n\r\n"},{"labels":[null,"enhancement",null,null],"text":"## ðŸš€ Feature/Enhnacement\r\n\r\nDuring RPC initialization, each participating process initializes a singleton `DistAutogradContainer`. During shutdown, we cleanup items such as the RRef context and RPC agent, but do not reset this singleton dist autograd container. This prevents RPC from being reinitialized (i.e. a group of processes cannot call `rpc.shutdown() ; dist.barrier() ; rpc.init_rpc(...)`. It will fail with the following error:\r\n\r\n```\r\n/test/distributed/rpc/faulty_agent/rpc_spawn_faulty#binary,link-tree\r\n/torch/distributed/rpc/__init__.py\", line 85, in init_rpc\r\n>     dist_autograd._init(rank)\r\n> RuntimeError: Container is already initialized! Cannot initialize it twice!\r\n```\r\n\r\n## Motivation\r\nI was looking into seeing if we could reuse spawned subprocesses across our RPC tests, which would decrease the amount of time it takes to run the RPC test suite; however, this would require processes to be able to re-init RPC, which is blocked by this error. Also, I was writing a set of similar tests in https://github.com/pytorch/pytorch/pull/38590, and wanted to use this pattern to reduce code duplication, but couldn't (the tests need to use different initializations of RPC since they test the RRef deletion behavior triggered by the shutdown sequence). \r\n\r\n## Pitch\r\nI don't know if we necessarily need to destroy the container, maybe we could just de-initialize it, and then re-init if RPC is being started back up again.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nLet's design below what an API for differential optimizers would look like. The goal is to facilitate the implementation of differential optimizer for libraries such as [higher](https://github.com/facebookresearch/higher/) (e.g. [section 4](https://arxiv.org/pdf/1910.01727.pdf))\r\n\r\nRelated discussions in #12659 and #32005, and [Internal document](https://fb.quip.com/7fsdAavbPGJ0)\r\n\r\ncc @albanD @mruberry @vincentqb @egrefen"},{"labels":["enhancement",null,null,null],"text":"Please complete torchvision for libtorch c++.\r\nlike torchvision.ops ,torchvision.models.detection\r\nIt is best to provide a compiled torchvision.lib for libtorch.\r\n\n\ncc @yf225 @glaringlee @fmassa"},{"labels":["enhancement",null,null],"text":"current scenario -\r\nthe current nn.Fold module adds all the overlalpping values, would it be a good idea to add `mode` to nn.Fold, for example,\r\n```\r\nx = torch.randn(5, 5)\r\nx = x.unfold(0, 3, 1).unfold(1, 3, 1)\r\nx = x.reshape(1, 9, 9)\r\nx\r\n```\r\ngives\r\n```\r\ntensor([[[-0.6498,  0.0957,  0.0787,  0.8407, -2.1366,  0.4104,  1.2258,\r\n           1.3592,  2.0226],\r\n         [ 0.0957,  0.0787, -0.5516, -2.1366,  0.4104,  0.8823,  1.3592,\r\n           2.0226, -2.2944],\r\n         [ 0.0787, -0.5516,  1.2226,  0.4104,  0.8823,  0.5543,  2.0226,\r\n          -2.2944, -1.3409],\r\n         [ 0.8407, -2.1366,  0.4104,  1.2258,  1.3592,  2.0226, -1.6431,\r\n          -0.3634,  0.8279],\r\n         [-2.1366,  0.4104,  0.8823,  1.3592,  2.0226, -2.2944, -0.3634,\r\n           0.8279, -0.0448],\r\n         [ 0.4104,  0.8823,  0.5543,  2.0226, -2.2944, -1.3409,  0.8279,\r\n          -0.0448, -0.0296],\r\n         [ 1.2258,  1.3592,  2.0226, -1.6431, -0.3634,  0.8279,  0.2688,\r\n          -0.4359, -0.9921],\r\n         [ 1.3592,  2.0226, -2.2944, -0.3634,  0.8279, -0.0448, -0.4359,\r\n          -0.9921,  1.3234],\r\n         [ 2.0226, -2.2944, -1.3409,  0.8279, -0.0448, -0.0296, -0.9921,\r\n           1.3234, -0.6501]]])\r\n```\r\nwhen we do,\r\n```\r\na = nn.Fold((5, 5), (3))\r\na(x)\r\n```\r\nthen it gives,\r\n```\r\ntensor([[[[ -0.6498,   0.1913,   0.2362,  -1.1031,   1.2226],\r\n          [  1.6814,  -8.5464,   2.4621,   3.5293,   1.1086],\r\n          [  3.6775,   8.1553,  18.2035, -13.7661,  -4.0226],\r\n          [ -3.2862,  -1.4536,   4.9672,  -0.1792,  -0.0592],\r\n          [  0.2688,  -0.8718,  -2.9763,   2.6468,  -0.6501]]]])\r\n```\r\noverlapping values got added, like, ```0.0957*2``` gives ```0.1913```, ```0.0787*3``` gives ```0.2362``` and so on\r\n\r\nexpected scenario -\r\nthere is a mode option in nn.Fold, something like,\r\n```\r\na = nn.Fold((5, 5), 3, mode='mul/sum/mean')\r\n```\r\nso that these variations of overlapping values could be obtained, or a custom function option, to manipulate overlapping values, for example, if user wants to not do ```mul/sum/mean``` but instead get overlapping value added by 1, or something like that."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n```py\r\nfuture_list = some_user_funct()\r\n\r\ncollected_fut = rpc.collect_all(futureu_list)\r\n\r\ncollected_fut.wait()\r\n```\r\n\r\n`collected_fut.wait()` should internally wait for all futures in the list.\r\n\r\n## Motivation\r\n\r\nFor sharded embeddings, what we do is\r\n\r\n```py\r\nfuts = []\r\nfor shard in shards:\r\n  fut = rpc.rpc_async(shard.owner(), helper_that_runs_shard_forward, input)\r\n  futs.append(fut)\r\n\r\n# In a later user function\r\nrets = [fut.wait() for fut in futs]\r\ntorch.sum(rets)\r\n```\r\n\r\nA cleaner API would be\r\n\r\n```py\r\nfuts = []\r\nfor shard in shards:\r\n  fut = rpc.rpc_async(shard.owner(), helper_that_runs_shard_forward, input)\r\n  futs.append(fut)\r\n\r\ncollected_fut = rpc.wait_all(futs)\r\n\r\ndef callback(collected_fut):\r\n  return torch.sum(collected_fut.wait())\r\n\r\ncollected_fut._then(callback)\r\n\r\n\r\n# In a later user function\r\ncollected_fut.wait()\r\n```\r\n\r\n## Pitch\r\n\r\nFollow the [Folly Future collectAll API](https://github.com/facebook/folly/blob/master/folly/futures/Future.h),\r\nadd a `future = rpc.collect_all(future_list)` API.\r\n\r\nCould use https://github.com/pytorch/pytorch/pull/37311 as a reference.\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nIt would be easy for `broadcast` and `reduce_add` to support `out=`. Such a functionality would be useful when users want to optimize customize intraprocess communication. For me this comes up when I code a SyncBatchNorm variant for DataParallel.\n\ncc @ngimel @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\n<!-- A clear and concise description of the feature proposal -->\nA way to gather embedding and reduce via methods such as sum or mean when a sparse index tensor is provided.\n\n## Motivation\n\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\n\nIt is frequently used in processing variable length sequence data, such as processing sentences.\n\n## Pitch\n\n<!-- A clear and concise description of what you want to happen. -->\n\nAn equivalent of tf.embedding_lookup_sparse\n\n## Alternatives\n\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\n\nI didn't find an alternative\n\n## Additional context\n\n<!-- Add any other context or screenshots about the feature request here. -->\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nCurrently, only the [ReduceLROnPlateau](https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#ReduceLROnPlateau) scheduler has the verbose parameter. Proposing that this be added to other schedulers (where applicable; example: [StepLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR) is an obvious one)\r\n\r\n## Motivation\r\nWhen viewing train logs, it is often not obvious when the LR is being changed. The ReduceLROnPlateau scheduler's verbose parameter allows this to be logged out whenever the LR is changed. \r\n\r\n## Pitch\r\nIf the LR is changed, and if the `verbose` is `True`, a message is printed to stdout for each update. [Example](https://github.com/pytorch/pytorch/blob/2de4f245c6b1e1c294a8b2a9d7f916d43380af4b/torch/optim/lr_scheduler.py#L629-L631) from ReduceLROnPlateau: \r\n\r\n```\r\n                if self.verbose:\r\n                    print('Epoch {:5d}: reducing learning rate'\r\n                          ' of group {} to {:.4e}.'.format(epoch, i, new_lr))\r\n```\r\n\r\n## Alternatives\r\nn/a\r\n\r\n## Additional context\r\nn/a\n\ncc @vincentqb"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nAdd PlackettLuce and RelaxedPlackettLuce distributions. It is a simple distribution over permutations.\r\n\r\n## Motivation\r\n\r\nFor optimization over categorical/binary variables (i.e. variational inference with discrete variational approximate posteriors, RL and etc) we already can use `RelaxedBernoulli` /`RelaxedOneHotCategorical` and `Bernoulli`/`OneHotCategorical` depending on if the method is relaxation based or not. \r\n\r\nAlso, the same Gumbel-Softmax trick holds for the distribution over the set of permutations - Plackett-Luce distribution. You can see successful applications in https://arxiv.org/abs/1911.10036, https://arxiv.org/abs/1903.08850.\r\n\r\n## Pitch\r\n\r\nTensorflow has tfp.distributions.PlackettLuce. I think it will be very cool to have it in the pytorch.\r\n\r\n## Additional context\r\n\r\nFor the efficient implementation we will need numerically stable implementation of LogCumSumExp function which is almost finished in PR https://github.com/pytorch/pytorch/pull/36308\r\n"},{"labels":["enhancement",null,null,null],"text":"As discussed in [this forum thread](https://discuss.pytorch.org/t/mpi-cuda-stream/80702/4). CUDA stream/event callback can be a useful feature. It might be helpful to add it to the Python API. \r\n\r\nBesides, some recent discussion for TensorPipe RPC backend also considered using CUDA callback to sync streams and handle errors. Hence creating this issue to track and join discussions in one place.\n\ncc @ngimel @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null,null],"text":"We should have efficient implementations for a small subset of operations on the lists of tensors, such as \r\n```\r\n_tensor_list_add.Tensor(Tensor[] self, Tensor[] other, *, Scalar alpha=1)\r\n```\r\n## Motivation\r\n\r\nFor a lot of GPU training workloads, the performance of the optimizer becomes the bottleneck. Currently there are two extreme cases of gpu optimizer performance - very flexible and hackable implementations in torch.optim, and fully fused rigid implementations in apex (github.com/nvidia/apex), available for sgd, Adam, and, as of couple days back, adagrad. People \r\nwho want to experiment with optimizers either have to suffer through bad performance that typically would be seen when implementing optimizer the usual way, iterating over lists of tensors, or, if they are up to it, locally modify apex to have a fast fused kernel. The latter part can be pretty challenging. \r\n\r\nA typical optimizer launches 5-10 kernels per parameter tensor. Models can have hundreds of parameters, so 500-1000 kernels have to be launched. Those kernels are often very small, and the operations become cpu-bound. If we can launch a single kernel that operates on all the parameter tensors at the same time, we can reduce the number of kernels launches by a factor of 100 and relieve the cpu-boundness. \r\n\r\nOther situations where efficient implementation of operations on tensor lists are helpful are `.zero_grad()` function, unscaling gradients and checking for `nan` values in the mixed precision training, computing norm of parameters for `clip_grad_norm`, converting parameters/gradients between low- and high- precision type for mixed precision training. \r\n\r\nNvidia has kernels that can efficiently operate on disjoint list of tensors in apex, but it's impossible to use those kernels outside of apex, and each particular operation has to be implemented (currently those kernels are used for mixed precision gradient scaling and a few fused optimizers). If efficient list operations were available in pytorch core, that would\r\nenable much easier experimentation. \r\n\r\n## Proposal\r\n\r\nExpose experimental tensor list operations for arithmetic operations, `fill_`, `norm` backed by efficient implementations.\r\nInitial list of operations can be discussed, but tentatively, it should be enough to implement operations in the existing optimizers. In particular, we should support inplace variants, and operations between tensor list and scalar. E.g., for `add` we want the \r\nfollowing variants:\r\n```\r\n_add_tensor_list.Tensor(Tensor[] self, Tensor other[], *, Scalar alpha=1) -> Tensor[]\r\n_add_tensor_list.Scalar(Tensor[] self, Scalar other, Scalar alpha=1) -> Tensor[]\r\n_add_tensor_list_.Tensor(Tensor[](a!) self, Tensor[] other, *, Scalar alpha=1) -> Tensor[](a!) #inplace\r\n_add_tensor_list_.Scalar(Tensor[](a!) self, Scalar other, Scalar alpha=1) -> Tensor[](a!) #inplace scalar variant\r\n_add_tensor_list.out(Tensor[] self, Tensor[] other, *, Scalar alpha=1, Tensor[](a!) out) -> Tensor[](a!)\r\n```\r\nType promotion should be supported as for regular operations. \r\nShapes in the list arguments should be the same, error out on mismatch or if broadcasting is required (this is open for debate).\r\nFor backends that don't have efficient implementations or don't need them fall back to implemetations looping over tensors in the list. \r\n\r\nAfter this is done, existing optimizers can be rewritten to use these ops, leading to better performance. Optimizer implementation itself will still remain readable, accessible and hackable.\r\n\r\nBackward formulas can be provided for operations on the list. However, given that primary focus is use in optimizers, backward should not be a blocker. \r\n\r\n## Alternatives\r\n\r\nThis proposal has a significant overlap with NestedTensor proposal, but we understand that it is hard to implement NestedTensors to cover all the relevant scenarios. This proposal is much more limited in scope, and efficient implementations that are put in the core as part of this proposal can be reused for NestedTensor. \r\nSimilarly, once efficient implementations are in the core, TorchScript can have passes that will convert operations that loop over tensors in the list to call into these efficient implementations. \r\n\r\ncc @cpuhrsch @vincentqb @mcarilli @soumith @dzhulgakov @ezyang "},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nIt would be nice to be able to get a tensor instead of a list of tensors when splitting or chunking if the dimensions match. What I propose is to add a flag (that could be called something like `return_tensor`) to split and chunk to do so. Here is an example of implementation.\r\n\r\n```python\r\ndef view_chunk(tensor, chunks, dim=0):\r\n    assert tensor.shape[dim] % chunks == 0\r\n    cur_shape = tensor.shape\r\n    new_shape = cur_shape[:dim] + [tensor.shape[dim] // chunks, chunks] + cur_shape[dim + 1:]\r\n    return tensor.reshape(*new_shape)\r\n\r\ndef view_split(tensor, split_size, dim=0):\r\n    assert tensor.shape[dim] % split_size == 0\r\n    cur_shape = tensor.shape\r\n    new_shape = cur_shape[:dim] + [split_size, tensor.shape[dim] // split_size] + cur_shape[dim + 1:]\r\n    return tensor.reshape(*new_shape)\r\n```\r\n\r\n## Motivation\r\n\r\nEssentially, the goal is to avoid copying memory when subsequently stacking the split afterwards if desirable. For instance, this can be useful when wanting to separate the dimensions of the output of an RNN.\r\n\r\n## Pitch\r\n\r\n```python\r\ntorch.chunk(tensor, chunks, dim=dim) # returns a list of tensors\r\ntorch.chunk(tensor, chunks, dim=dim, return_tensor=True) # returns a tensor with a new dimension\r\n```\n\ncc @ezyang @VitalyFedyunin @ngimel"},{"labels":["enhancement",null,null,null],"text":"The original request is from [this discussion](https://discuss.pytorch.org/t/does-distributedoptimizer-suppor-zero-grad-and-lr-scheduling) on forum. Currently, applications need to use the raw RPC APIs to implement LR scheduling in user code. It will be useful provide a helper for this. \n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null,null],"text":"Problem\r\n=======\r\n\r\nAs datasets become larger and larger, storing training samples as individual files becomes impractical and inefficient. This can be addressed using sequential storage formats and sharding (see \"Alternatives Considered\" for other implementations).  PyTorch lacks such a common storage format right now.\r\n\r\n\r\nProposal\r\n========\r\n\r\n[WebDataset](http://github.com/tmbdev/webdataset) provides an implementation of `IterableDataset` based on sharded tar archives. This format provides efficient I/O for very large datasets, makes migration from file-based I/O easy, and works well locally, with cloud storage, and web servers.  It also provides a simple, standard format in which large datasets can be distributed easily and used directly without unpacking.\r\n\r\nThe implementation is small (1000-1800 LOC) and has no external dependencies.  The proposal is to incorporate the `webdataset.Dataset` class into the PyTorch base distribution. The source repository is here:\r\n\r\nhttp://github.com/tmbdev/webdataset\r\n\r\nMy suggestion would be to incorporate the library with minimal changes into its own subpackage. I can perform the integration and generate a PR once there is general agreement.\r\n\r\n\r\nMore Background\r\n===============\r\n\r\nFor a general introduction to how we handle large scale training with WebDataset, see [this YouTube playlist](https://www.youtube.com/playlist?list=PL0dsKxFNMcX4XcB0w1Wm-pvSfQu-eWM26)\r\n\r\nThe WebDataset library (github.com/tmbdev/webdataset) provides an implementation of `IterableDataset` that uses POSIX tar archives as its native storage format.  The format itself is based on a simple convention:\r\n\r\n- datasets are POSIX tar archives\r\n- each training sample consists of adjacent files with the same basename\r\n- shards are numbered consecutively\r\n\r\nFor example, ImageNet is stored in 147 separate 1 Gbyte shards with names `imagenet-train-0000.tar` to `imagenet-train-0147.tar`; the contents of the first shard are:\r\n\r\n```\r\n-r--r--r-- bigdata/bigdata      3 2020-05-08 21:23 n03991062_24866.cls\r\n-r--r--r-- bigdata/bigdata 108611 2020-05-08 21:23 n03991062_24866.jpg\r\n-r--r--r-- bigdata/bigdata      3 2020-05-08 21:23 n07749582_9506.cls\r\n-r--r--r-- bigdata/bigdata 129044 2020-05-08 21:23 n07749582_9506.jpg\r\n-r--r--r-- bigdata/bigdata      3 2020-05-08 21:23 n03425413_23604.cls\r\n-r--r--r-- bigdata/bigdata 106255 2020-05-08 21:23 n03425413_23604.jpg\r\n-r--r--r-- bigdata/bigdata      3 2020-05-08 21:23 n02795169_27274.cls\r\n```\r\n\r\nDatasets in WebDataset format can be used directly after downloading without unpacking; they can also be mounted as a file system.  Content in WebDataset format can used any file-based compression scheme.  In addition, the tar file itself can also be compressed and WebDataset will transparently decompress it.\r\n\r\nWebDatsets can be used directly from local disk, from web servers (hence the name), from cloud storage, and from object stores, just by changing a URL.\r\n\r\nWebDataset readers/writers are easy to implement (we have Python, Golang, and C++ implementations).\r\n\r\nWebDataset performs shuffling both at the shard level and at the sample level.  Splitting of data across multiple workers is performed at the shard level using a user-provided `shard_selection` function that defaults to a function that splits based on `get_worker_info`. (WebDataset can be combined with the `tensorcom` library to offload decompression/data augmentation and provide RDMA and direct-to-GPU loading; see below.)\r\n\r\nWe are storing and processing petabytes of training data as tar archives and are using the format for unsupervised learning from video, OCR and scene text recognition, and large scale object recognition experiments.  The same code and I/O pipelines work efficiently on the desktop, on a local cluster, or in the cloud.  [Our benchmarks](https://arxiv.org/abs/2001.01858) show scalability and the ability to take advantage of the full I/O bandwidth of each disk drive across very large training jobs and storage clusters.\r\n\r\n\r\nCode Sample\r\n===========\r\n\r\nThis shows how to use WebDataset with ImageNet.\r\n\r\n```Python\r\nimport webdataset as wds\r\nimprot ...\r\n\r\nshardurl = \"/imagenet/imagenet-train-{0000..0147}.tar\"\r\n\r\nnormalize = transforms.Normalize(\r\n    mean=[0.485, 0.456, 0.406],\r\n    std=[0.229, 0.224, 0.225])\r\n\r\npreproc = transforms.Compose([\r\n    transforms.RandomResizedCrop(224),\r\n    transforms.RandomHorizontalFlip(),\r\n    transforms.ToTensor(),\r\n    normalize,\r\n])\r\n\r\ndataset = (\r\n    wds.Dataset(shardurl)\r\n    .shuffle(1000)\r\n    .decode(\"pil\")\r\n    .rename(image=\"jpg;png\", data=\"json\")\r\n    .map_dict(image=preproc)\r\n    .to_tuple(\"image\", \"data\")\r\n)\r\n\r\nloader = torch.utils.data.DataLoader(dataset, batch_size=64, num_workers=8)\r\n\r\nfor inputs, targets in loader:\r\n    ...\r\n```\r\n\r\nWebDataset uses a fluent API for configuration that internally builds up a processing pipeline. Without any added processing stages, WebDataset just iterates through each training sample as a dictionary:\r\n\r\n```Python\r\n# load from a web server using a separate client process\r\nshardurl = \"pipe:curl -s http://server/imagenet/imagenet-train-{0000..0147}.tar\"\r\n\r\ndataset = wds.Dataset(shardurl)\r\n\r\nfor sample in dataset:\r\n    # sample[\"jpg\"] contains the raw image data\r\n    # sample[\"cls\"] contains the class\r\n    ...\r\n```\r\n\r\n\r\nAlternatives Considered\r\n=======================\r\n\r\nKeeping WebDataset as as Separate Library\r\n-----------------------------------------\r\n\r\nWebDataset is perfectly usable as a separate library, so why not keep it that way?\r\n\r\n- Users of deep learning libraries expect an efficient data format that avoids the \"many small file\" problem; Tensorflow provides TFRecord/tf.Example Making WebDataset part of PyTorch itself provides a straightforward solution for most users and reduces dependencies.\r\n- Providing a format reader in the standard library encourages dataset standardization, tool building, and adoption of the same format by other libraries.\r\n- Having a realistic implementation of `IterableDataset` in PyTorch provides a common reference against which to address issues and provide test cases in the `DataLoader` implementation and its use of `IterableDataset`\r\n\r\nMany of the larger datasets distributed in `torchvision` could be distributed easily in WebDataset format. This would allow users to either train directly against web-hosted data, or to train on the datasets immediately after downloading without unpacking. The way data is arranged in WebDataset also allows users to download just a few shards for testing code locally, and then use the entire dataset when running on a cluster. Furthermore, the way `webdataset.Dataset` works in most cases, no special code is needed in order to read them; many training jobs can be retargeted to different datasets simply by using a different URL for the dataset.\r\n\r\n\r\nTFRecord+protobuf, Parquet\r\n----------------------------\r\n\r\nThese formats are suitable for large scale data processing for machine learning and deep learning applications and some datasets exist in this format and more will continue to be generated for the Tensorflow ecosystem. However, they are not good candidates for incorporating into PyTorch as core feature because:\r\n\r\n- TFRecord+protobuf (tf.Example) and Parquet have significant external library dependencies; in contrast, WebDataset is pure Python, using the built-in libraries for tar decoding.\r\n- Protobuf and Parquet represent serialized data structures, while WebDataset uses file-based representations. File based representations make creation of WebDatasets easy, result in bit-identical representations of data in WebDatasets, and allow existing file-based codecs and I/O pipelines to be reused easily.\r\n- Except for their native frameworks, there is not much third party support for these formats; in contrast, POSIX tar archive libraries exist in all major programming languages and are easily processed, either by unpacking shards or using tools like `tarp`.\r\n\r\nNote that WebDataset supports usage scenarios similar to TFRecord+protobuf, since serialized data structures can be incorporated as files, and WebDataset will decode them automatically. For example, OpenImages multi-instance data is simply stored in a `.json` file accompanying each `.jpg` file:\r\n\r\n```\r\n$ curl -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000554.tar | tar tf -\r\n0b946d1d5201c06c.jpg\r\n0b946d1d5201c06c.json\r\n05c9d72ff9e64010.jpg\r\n05c9d72ff9e64010.json\r\n...\r\n$\r\n```\r\n\r\nzip instead of tar\r\n--------------------\r\n\r\nThe zip format is another archival format. Unlike tar format, which is just a sequence of records, zip format stores a file index at the very end of the file, making it unsuitable for streaming. Tar files can be made random access (and, in fact, can be mounted as file systems), but they use a separate index file to support that functionality.\r\n\r\nLMDB, HDF5, Databases\r\n---------------------\r\n\r\nThese formats are not suitable for streaming and require the entire dataset to fit onto local disk. In addition, while they nominally solve the \"many small file problems\", they don't solve the problem that indexing into the dataset still results in expensive seek operations.\r\n\r\nLocal File System Caches\r\n------------------------\r\n\r\nAn approach for extending file-system based I/O pipelines to large distributed storage systems is to use some form of \"pre-caching\" or \"staging\" on a local NVMe drive. Generally, there is little advantage to this. For large datasets, it does not increase throughput. Input pipelines still need to be modified to schedule the pre-caching. And generally, this requires volume plugins or virtual file system support. A similar effect can be achieved with WebDataset by simply unpacking shards to the local file system when direct file access is required.\r\n\r\n\r\nRelated Software\r\n================\r\n\r\n[AIStore](http://github.com/NVIDIA/AIStore) is an open source object store capable of full bandwidth disk-to-GPU data delivery (meaning that if you have 1000 rotational drives with 200 MB/s read speed, AIStore actually delivers an aggregate bandwidth of 200 GB/s to the GPUs). AIStore is fully compatible with WebDataset as a client, and in addition understands the WebDataset format, permitting it to perform shuffling, sorting, ETL, and some map-reduce operations directly in the storage system.\r\n\r\n[tarp](http://github.com/tmbdev/tarp) is a small command line program for splitting, merging, shuffling, and processing tar archives and WebDataset datasets.\r\n\r\n[tensorcom](http://github.com/NVLabs/tensorcom) is a library supporting distributed data augmentation and RDMA to GPU.\r\n\r\n[webdataset-examples](http://github.com/tmbdev/webdataset-examples) contains an example (and soon more examples) of how to use WebDataset in practice.\r\n\r\n[Bigdata 2019 Paper with Benchmarks](https://arxiv.org/abs/2001.01858)\n\ncc @SsnL"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nImplementation or modification of col2im function/method for 4D tensor support in torch.nn.functionnal.fold().\r\n\r\n## Motivation\r\nI came to this \"issue\" when i was working on a custom locally connected decoder.\r\n(the layer behave like a transposed convolution but without the weight sharing)\r\nI initially build a locally connected network for encoding purpose using the torch.nn.functional.unfold() function; I then assume that the torch.nn.functional.fold() work in an inverse manner with 4D tensor, until i find out that it only support 3D tensor which is misleading and not consistent with unfold() and other function.\r\nThis issue halt the development of custom convolution / transposed convolution in my opinion.\r\n\r\nI am open to alternative to build my decoder !\r\n\r\nhere is the current code : \r\n\r\n```\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.nn import init\r\n\r\n\r\n# Main decoder locally connected linear layer\r\nclass LocallyConnected2dTranspose(nn.Module):\r\n    def calculate_spatial_transposed_output_shape(self, input_shape, kernel_size, dilation, input_padding, out_padding, stride):\r\n        return [np.floor((input_shape[index]-1)*stride[index]-2*input_padding[index]+dilation[index]*kernel_size[index]-1+out_padding[index]+1).astype(int) for index in range(len(input_shape))]\r\n    def __init__(self, input_shape, in_channels, out_channels, kernel_size, dilation, input_padding, out_padding, stride):\r\n        super().__init__()\r\n        self.kernel_size = kernel_size\r\n        self.out_channels = out_channels\r\n        self.dilation = dilation\r\n        self.input_padding = input_padding\r\n        self.out_padding = out_padding\r\n        self.stride = stride\r\n        # calculate desired output shape and generate weight/bias matrix\r\n        self.output_height, self.output_width = self.calculate_spatial_transposed_output_shape(input_shape, kernel_size, dilation, input_padding, out_padding, stride)\r\n        # weight and spatial block\r\n        self.weight_tensor_depth = in_channels * kernel_size[0] * kernel_size[1]\r\n        self.spatial_blocks_size = self.output_height * self.output_width\r\n        self.weights = nn.Parameter(torch.empty((1, self.weight_tensor_depth, self.spatial_blocks_size, out_channels),requires_grad=True, dtype=torch.float))\r\n        print(self.weights.shape)\r\n        self.bias = nn.Parameter(torch.empty((1, out_channels, self.output_height, self.output_width),requires_grad=True, dtype=torch.float))\r\n        # init weight and bias\r\n        torch.nn.init.xavier_uniform_(self.weights)\r\n        torch.nn.init.xavier_uniform_(self.bias)\r\n\r\n    def forward(self, input):\r\n        input_f = torch.nn.functional.fold(input=input, output_size=[self.output_height, self.output_width], kernel_size=self.kernel_size, dilation=self.dilation, padding=self.out_padding, stride=self.stride)\r\n        local_conv_f = (input_f.view((*input_f.shape, 1)) * self.weights)\r\n        print(local_conv_f.shape)\r\n        return local_conv_f.sum(dim=1).transpose(2, 1).reshape((-1, self.out_channels, self.output_height, self.output_width)) + self.bias\r\n\r\n\r\nlayer_ = LocallyConnected2dTranspose([64,64], 49, 3, [5,5], (1,1), (2,2), (1,1), (2,2))\r\nht = torch.randn(1,49,64,64)\r\nprint(layer_(ht).shape)\r\n\r\n```\r\n\r\nbest regards ^^\r\n\r\nPS : I am not sure the last 2 lines of my code work...\r\n\r\n"},{"labels":["enhancement",null,null,null,null],"text":"I'd like implement logaddexp for cpu and cuda mentioned in #38349. these functions are perticularly useful when doing long probability accumulation and maintaining good numerical precesion. \r\n\r\ncc @mruberry"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nThis feature is about modifying torch.distributed.rpc.__init__.py (init_rpc) method to accept the store parameter as a part of the **rpc_backend_options** .\r\n\r\n## Motivation\r\nCurrently init_rpc method performs rendezvous to create a common store. This may not always work in the environment that also uses other forms of communication, e.g. process groups: if the rendezvous has already been performed by the **torch process groups**, the users will not be able to invoke init_rpc method. This can happen if both torch process groups and torch rpc use init_method=env.\r\n\r\nMoreover, http://pytorch.org/elastic is a library that provides the ability to perform rendezvous outside of the worker process. The TorchElastic will be compatible with the torch rpc if the torch rpc will be able to accept store as additional argument.\r\n\r\nThe proposed approach is to make rpc_backend_options to accept store as additional parameter. \r\nIn this case, init_rpc will be modified to check whether the store is available or not. If the store is available, the rendezvous will not be performed. The change is not a breaking change, since the definition of the init_rpc method will not be changed.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null,null],"text":"Analogous to TensorFlow's https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss. \r\n\r\nThis was first proposed in #3718.\n\ncc @albanD @mruberry"},{"labels":["enhancement",null,null],"text":"Analogous to TensorFlow's https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/log-uniform-candidate-sampler. \r\n\r\nThis was first proposed in https://github.com/pytorch/pytorch/issues/3718.\n\ncc @albanD @mruberry"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nwith @pritamdamania87 @mrshenli @zhaojuanmao \r\nThis RFC is to summarize the current proposal for supporting uneven inputs across different DDP processes. Related discussion in https://github.com/pytorch/pytorch/issues/33148. An example pain point from a user is on the [PyTorch forums](https://discuss.pytorch.org/t/best-practice-for-uneven-dataset-sizes-with-distributeddataparallel/67308\r\n).\r\n#### Problem\r\n[torch.nn.parallel.DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel) is a commonly used tool for distributed data-parallel training, but currently obliges the user to provide an equal number of inputs across each participating DDP process (or appropriately handle the error otherwise). DDP currently fails when different processes have an unequal number of inputs to process during training. While there are utilities such as [DataLoader](https://pytorch.org/docs/stable/data.html) and [DistributedSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler) that make navigating this assumption in DDP easier by evenly distributing the dataset, we can't expect these to solve all use cases and many users have had use cases where uneven inputs need to be supported.\r\n\r\nThe following script gives a simple example of the error:\r\n\r\n```\r\nimport torch\r\nimport torch.distributed as dist\r\nimport os\r\nimport torch.multiprocessing as mp\r\nimport torch.nn as nn\r\ndef worker(rank):\r\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\r\n    torch.cuda.set_device(rank)\r\n    model = nn.Linear(1, 1, bias=False).to(rank)\r\n    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank], output_device=rank)\r\n    # Create uneven inputs, rank 1 will get one more input than rank 0. This will cause a hang.\r\n    inputs = [torch.tensor([1]).float() for _ in range(10 + rank)]\r\n    for _ in range(5):\r\n        for inp in inputs:\r\n            loss = model(inp).sum()\r\n            loss.backward()\r\n    torch.cuda.synchronize(device=rank)\r\n\r\nif __name__ == '__main__':\r\n    os.environ[\"MASTER_ADDR\"] = \"localhost\" ; os.environ[\"MASTER_PORT\"] = \"29501\"\r\n    mp.spawn(worker, nprocs=2, args=())\r\n```\r\n\r\nWith the NCCL backend (recommended choice when training with multiple GPUs) this will result in a hang, as process 1 will wait for communication (allreduce) from process 0, but process 0 has already exited its training loop. On the other hand, with the gloo backend, this results in a \"Connection reset by peer\" error.\r\n\r\n#### Proposal\r\n\r\nThis was proposed by @pritamdamania87 and is influenced by the approach taken by Horovod to resolve a similar problem (https://github.com/horovod/horovod/issues/832).\r\n1. Provide a context manager such as `with torch.nn.parallel.distributed.join()`. In the `__enter__`, we will set a flag indicating that we will run the below process for managing uneven inputs.\r\n2. The context manager's `__exit__` indicates that the process has depleted its input and is ready to join. When a trainer calls this:\r\n         a. Schedule an allreduce with `torch.tensor(0)`. This allreduce will match the allreduce scheduled by non-joined processes (explained below in point 3)\r\n         b. If the result of the above is zero, this means that all processes have depleted their inputs and we can move to step (d)\r\n         c. Otherwise, schedule an allreduce for all buckets in the backwards pass, with all gradients zeroed out (this is so that joined ranks don't affect gradients of the rest of the training processes). This will match the allreduce done in the backwards pass for currently active trainers. Go back to step a.\r\n    d. If (a) returns all zeros, this means that all ranks have terminated their inputs and we can move on to cleanup. We also need to keep track of the process with the latest model parameters, and broadcast them to all ranks to maintain the fact that DDP ensures all parameters across processes are the same. We can do this via a simple version counter. In this step, we can then allgather this version counter, and have the process with the maximum counter broadcast its parameters to the rest of the processes. Ties can be broken arbitrarily.\r\n\r\n3. If a trainer has not called `__exit__`, then:\r\n    a. Before scheduling allreduces for the backwards pass, schedule an allreduce with `torch.tensor(1)`. This allreduce matches the one scheduled in (2a). We can schedule this allreduce in the forward pass, but we should not await it here for performance reasons; it should be awaited at the end of the backwards pass.\r\n    b. Schedule allreduce ops for all the buckets as typical in the backwards pass for DDP. Processes which have depleted their inputs will match these allreduces as a result of step 2c.  These processes will have zero as the argument for their gradients so they will not contribute to gradient averaging.\r\n    c. Instead of dividing by a static world_size, since we now can have a smaller effective world size (initial_world_size - currently_joined_processes), divide by this instead to ensure that we are still correctly averaging gradients. This can be done by taking the value returned in 3a, which will be interpreted as an int representing the number of currently active processes. \r\n\r\n#### Code sample\r\n\r\n```\r\nmodel = nn.Linear(1, 1, bias=False).to(rank)\r\n    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank], output_device=rank)\r\n    # Create uneven inputs\r\n    inputs = [torch.tensor([1]).float() for _ in range(10 + rank)]\r\n    for _ in range(epochs):\r\n        with torch.nn.parallel.distributed.join():\r\n            for inp in inputs:\r\n                loss = model(inp).sum()\r\n                loss.backward()\r\n```\r\n\r\n#### Alternatives considered\r\nWe considered the alternative of all trainers raising a `StopIteration` once we detect that at least one trainer has depleted its input (via the above method). However, the user would then have to catch this StopIteration and this would also result in all processes stopping their training, whereas the currently proposed method allows training to continue with a smaller effective world size. In the future if we see the need for users to actually stop the training early in these situations, we can provide the appropriate options, although we would like to keep usage of this API as simple as reasonably possible.\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar"},{"labels":["enhancement",null,null],"text":"Currently, there is no convenient ways to save and load RPC models. Applications need to implement this in the user code, e.g., by using RPC to tell all participating works to trigger local save/load. Moreover, we intentionally disabled RRef pickling to avoid messing up reference count, which makes the save/load process even trickier. \r\n\r\n\r\n## Pitch\r\n\r\nWe should add a helper to save/load RPC-based models. \r\n\r\n### Save\r\n\r\nSimilar to distributed optimizer, the helper can potentially:\r\n\r\n1. Take a model and recursively walk through all its attributes and extract all RRefs into a set \r\n2. Save all non-RRef attributes locally\r\n3. For RRef attributes, use RPC to recursively trigger save on the remote side \r\n4. For `OwnerRRef` save its `local_value()` and remember all its `UserRRef`s. This also needs to be done recursively as the `local_value` can contain other RRefs. \r\n\r\n### Load\r\n\r\n1. Load all local parts\r\n2. Use loaded local parts to create `OwnerRRef`s. \r\n3. Use `OwnerRRef` to create all `UserRRef`s. Step 2 and 3 need to be done recursively, as `OwnerRRef.local_value` can contain other RRefs.\r\n\r\n\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nAll the NN modules within the `torch.nn` namespace must accept 0-batch size tensors for forward and backward.\r\n\r\nFollow up from https://github.com/pytorch/pytorch/issues/12013 since the list is long. Recommeneded way of solving this would be to club at least a few of the layers into a single PR.\r\n\r\nNote that this list is work under construction and more layers may crop up as I discover more issues.\r\n\r\n# Survey of various layers\r\n\r\n- [x] Linear:Forward\r\n- [x] Linear:Backward\r\n- [ ] Bilinear:Linear\r\n- [ ] Bilinear:Backward\r\n- [ ] AdaptiveAvgPool2d:Forward\r\n- [ ] AdaptiveAvgPool2d:Backward\r\n- [ ] AdaptiveAvgPool3d:Forward\r\n- [ ] AdaptiveAvgPool3d:Backward\r\n- [x] AlphaDropout:Forward\r\n- [ ] AlphaDropout:Backward\r\n- [ ] AvgPool2d:Forward\r\n- [ ] AvgPool2d:Backward\r\n- [x] CELU:Forward\r\n- [ ] CELU:Backward\r\n- [x] ConstantPad2d:Forward\r\n- [ ] ConstantPad2d:Backward\r\n- [ ] CrossEntropyLoss:Forward\r\n- [ ] CrossEntropyLoss:Backward\r\n- [x] Dropout:Forward\r\n- [ ] Dropout:Backward\r\n- [x] Dropout2d:Forward\r\n- [ ] Dropout2d:Backward\r\n- [x] ELU:Forward\r\n- [ ] ELU:Backward\r\n- [x] FeatureAlphaDropout:Forward\r\n- [ ] FeatureAlphaDropout:Backward\r\n- [ ] Flatten:Forward\r\n- [ ] Flatten:Backward\r\n- [ ] FractionalMaxPool2d:Forward\r\n- [ ] FractionalMaxPool2d:Backward\r\n- [x] GLU:Forward\r\n- [ ] GLU:Backward\r\n- [ ] GRU:Forward\r\n- [ ] GRU:Backward\r\n- [x] Hardshrink:Forward\r\n- [ ] Hardshrink:Backward\r\n- [x] Hardtanh:Forward\r\n- [ ] Hardtanh:Backward\r\n- [x] Identity:Forward\r\n- [ ] Identity:Backward\r\n- [ ] InstanceNorm1d:Forward\r\n- [ ] InstanceNorm1d:Backward\r\n- [ ] InstanceNorm2d:Forward\r\n- [ ] InstanceNorm2d:Backward\r\n- [ ] InstanceNorm3d:Forward\r\n- [ ] InstanceNorm3d:Backward\r\n- [x] L1Loss:Forward\r\n- [ ] L1Loss:Backward\r\n- [x] PoissonNLLLoss:Forward\r\n- [ ] PoissonNLLLoss:Backward\r\n- [ ] RNN:Forward\r\n- [ ] RNN:Backward\r\n- [ ] RNNCell:Forward\r\n- [ ] RNNCell:Backward\r\n- [x] RReLU:Forward\r\n- [ ] RReLU:Backward\r\n- [x] ReLU:Forward\r\n- [ ] ReLU:Backward\r\n- [x] ReLU6:Forward\r\n- [ ] ReLU6:Backward\r\n- [ ] ReflectionPad1d:Forward\r\n- [ ] ReflectionPad1d:Backward\r\n- [ ] ReflectionPad2d:Forward\r\n- [ ] ReflectionPad2d:Backward\r\n- [ ] ReplicationPad1d:Forward\r\n- [ ] ReplicationPad1d:Backward\r\n- [ ] ReplicationPad2d:Forward\r\n- [ ] ReplicationPad2d:Backward\r\n- [ ] ReplicationPad3d:Forward\r\n- [ ] ReplicationPad3d:Backward\r\n- [x] SELU:Forward\r\n- [ ] SELU:Backward\r\n- [x] Sigmoid:Forward\r\n- [ ] Sigmoid:Backward\r\n- [ ] SmoothL1Loss:Forward\r\n- [ ] SmoothL1Loss:Backward\r\n- [ ] SoftMarginLoss:Forward\r\n- [ ] SoftMarginLoss:Backward\r\n- [x] Softmax:Forward\r\n- [ ] Softmax:Backward\r\n- [x] Softmax2d:Forward\r\n- [ ] Softmax2d:Backward\r\n- [x] Softmin:Forward\r\n- [ ] Softmin:Backward\r\n- [x] Softplus:Forward\r\n- [ ] Softplus:Backward\r\n- [x] Softshrink:Forward\r\n- [ ] Softshrink:Backward\r\n- [x] Softsign:Forward\r\n- [ ] Softsign:Backward\r\n- [x] Tanh:Forward\r\n- [ ] Tanh:Backward\r\n- [x] Tanhshrink:Forward\r\n- [ ] Tanhshrink:Backward\r\n- [x] Threshold:Forward\r\n- [ ] Threshold:Backward\r\n- [ ] Transformer:Forward\r\n- [ ] Transformer:Backward\r\n- [ ] TransformerDecoder:Forward\r\n- [ ] TransformerDecoder:Backward\r\n- [ ] TransformerDecoderLayer:Forward\r\n- [ ] TransformerDecoderLayer:Backward\r\n- [ ] TransformerEncoder:Forward\r\n- [ ] TransformerEncoder:Backward\r\n- [ ] TransformerEncoderLayer:Forward\r\n- [ ] TransformerEncoderLayer:Backward\r\n- [ ] Unfold:Forward\r\n- [ ] Unfold:Backward\r\n- [ ] Upsample:Forward\r\n- [ ] Upsample:Backward\r\n- [ ] UpsamplingBilinear2d:Forward\r\n- [ ] UpsamplingBilinear2d:Backward\r\n- [ ] UpsamplingNearest2d:Forward\r\n- [ ] UpsamplingNearest2d:Backward\r\n- [x] ZeroPad2d:Forward\r\n- [ ] ZeroPad2d:Backward\r\n\r\n## Linear\r\n\r\n### Forward\r\n\r\nStatus: works :heavy_check_mark: \r\n\r\n``` python\r\nimport sys\r\nimport torch\r\n\r\ntry:\r\n    z = torch.ones(0, 3, 10, 10)\r\n    w = torch.ones(10,10)\r\n\r\n    lin = torch.nn.Linear(10, 10)\r\n    lin(z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0, 3, 10, 10)\r\n    w = torch.ones(10,10)\r\n\r\n    lin = torch.nn.Linear(10, 10)\r\n    lin_b = lin(z)\r\n    lin_b.sum().backward() # WORKS\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n## Bilinear\r\n\r\n### Linear\r\n\r\nStatus: :negative_squared_cross_mark: \r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0, 3, 10, 10)\r\n    bil = torch.nn.Bilinear(10, 10, 10)\r\n    bil(z, z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0, 3, 10, 10)\r\n    bil = torch.nn.Bilinear(10, 10, 10)\r\n    bil(z, z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n## AdaptiveAvgPool2d\r\n\r\n### Forward\r\n\r\nStatus: :negative_squared_cross_mark: \r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0, 3, 10, 10)\r\n    avg_p2 = torch.nn.AdaptiveAvgPool2d((5,5))\r\n    avg_p2(z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0, 3, 10, 10)\r\n    avg_p2 = torch.nn.AdaptiveAvgPool2d((5,5))\r\n    avg_p2(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## AdaptiveAvgPool3d\r\n\r\n### Forward\r\n\r\nStatus: :negative_squared_cross_mark: \r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0, 3, 10, 10, 10)\r\n    p3 = torch.nn.AdaptiveAvgPool2d((5,5,5))\r\n    p3(z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0, 3, 10, 10, 10)\r\n    p3 = torch.nn.AdaptiveAvgPool2d((5,5,5))\r\n    p3(z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n## AlphaDropout\r\n\r\n### Forward\r\n\r\nStatus: works :heavy_check_mark: \r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.AlphaDropout(p=0.3)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.AlphaDropout(p=0.3)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## AvgPool2d\r\n\r\n### Forward\r\n\r\nStatus: not working :negative_squared_cross_mark: \r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.AvgPool2d((3,3))\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.AvgPool2d((3,3))\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## CELU\r\n\r\n### Forward\r\n\r\nStatus: works\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.CELU()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.CELU()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## ConstantPad2d\r\n\r\n### Forward\r\n\r\nStatus: works\r\n\r\n``` python\r\nimport torch\r\nimport sys \r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ConstantPad2d(2,3.5)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ConstantPad2d(2,3.5)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## CrossEntropyLoss\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    target = torch.LongTensor(0)\r\n    bn = torch.nn.CrossEntropyLoss()\r\n    bn(z, target)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    target = torch.LongTensor(0)\r\n    bn = torch.nn.CrossEntropyLoss()\r\n    bn(z, target).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Dropout\r\n\r\n### Forward\r\n\r\n\r\n``` python\r\nimport torch\r\n\r\nz = torch.ones(0,3,10,10)\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    m = torch.nn.Dropout(p=0.5)\r\n    z = torch.ones(0, 3, 10, 10)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    m = torch.nn.Dropout(p=0.5)\r\n    z = torch.ones(0, 3, 10, 10)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Dropout2d\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Dropout2d(p=0.5)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Dropout2d(p = 0.5)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## ELU\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ELU()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ELU()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n\r\n## FeatureAlphaDropout\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.FeatureAlphaDropout(p=0.5)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.FeatureAlphaDropout(p=0.5)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Flatten\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Sequential(\r\n        torch.nn.BatchNorm2d(5),\r\n        torch.nn.Flatten()\r\n    )\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Sequential(\r\n        torch.nn.BatchNorm2d(5),\r\n        torch.nn.Flatten()\r\n    )\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## FractionalMaxPool2d\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5))\r\n    input = torch.ones(0, 16, 50, 32)\r\n    m(input)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5))\r\n    input = torch.ones(0, 16, 50, 32)\r\n    m(input).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## GLU\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.GLU()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.GLU()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## GRU\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    rnn = nn.GRU(10, 20, 2)\r\n    h0 = torch.randn(2, 3, 20)\r\n    \r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Hardshrink\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Hardshrink()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Hardshrink()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Hardtanh\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Hardtanh(-2, 2)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Hardtanh(-2, 2)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n## Identity\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Identity(40)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Identity(40)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## InstanceNorm1d\r\n\r\n### Forward\r\n\r\nStatus: not working\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.InstanceNorm1d(100)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.InstanceNorm1d(100)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## InstanceNorm2d\r\n\r\n### Forward\r\n\r\nStatus: not working\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.InstanceNorm2d(100)\r\n    m(z)\r\nexpect:\r\n    sys.exit(1)\r\n    \r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.InstanceNorm2d(100)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## InstanceNorm3d\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10, 10)\r\n    m = torch.nn.InstanceNorm3d(100)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10, 10)\r\n    m = torch.nn.InstanceNorm3d(100)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## L1Loss\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    target = torch.ones(0,3,10,10)\r\n    m = torch.nn.L1Loss()\r\n    m(z, target)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    target = torch.ones(0,3,10,10)\r\n    m = torch.nn.L1Loss()\r\n    m(z, target).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## PoissonNLLLoss\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    target = torch.ones(0, 3, 10, 10)\r\n    m = torch.nn.PoissonNLLLoss()\r\n    m(z, target)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    target = torch.ones(0, 3, 10, 10)\r\n    m = torch.nn.PoissonNLLLoss()\r\n    m(z, target).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## RNN\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    rnn = nn.RNN(10, 20, 2)\r\n    input = torch.randn(0, 3, 10)\r\n    h0 = torch.randn(2, 3, 20)\r\n    rnn(input, h0)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    rnn = nn.RNN(10, 20, 2)\r\n    input = torch.randn(0, 3, 10)\r\n    h0 = torch.randn(2, 3, 20)\r\n    rnn(input, h0).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## RNNCell\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.RNNCell(10, 20)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.RNNCell(10, 20)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## RReLU\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.RReLU()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.RReLU()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## ReLU\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ReLU()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ReLU()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## ReLU6\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ReLU6()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ReLU6()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## ReflectionPad1d\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10)\r\n    m = torch.nn.ReflectionPad1d(2)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10)\r\n    m = torch.nn.ReflectionPad1d(2)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## ReflectionPad2d\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ReflectionPad2d(2)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ReflectionPad2d(2)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## ReplicationPad1d\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10)\r\n    m = torch.nn.ReplicationPad1d(3)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10)\r\n    m = torch.nn.ReplicationPad1d(3)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## ReplicationPad2d\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ReplicationPad2d(3)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ReplicationPad2d(3)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## ReplicationPad3d\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10,10)\r\n    m = torch.nn.ReplicationPad3d(3)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10,10)\r\n    m = torch.nn.ReplicationPad3d(3)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## SELU\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.SELU()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.SELU()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Sigmoid\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Sigmoid()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Sigmoid()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## SmoothL1Loss\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.SmoothL1Loss()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.SmoothL1Loss()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## SoftMarginLoss\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.SoftMarginLoss()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.SoftMarginLoss()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Softmax\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Softmax()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Softmax()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Softmax2d\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Softmax2d()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Softmax2d()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Softmin\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Softmin()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Softmin()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Softplus\r\n\r\n### Forward\r\n\r\n``` python\r\n\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Softplus()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Softplus()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Softshrink\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Softshrink()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Softshrink()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Softsign\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Softsign()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Softsign()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Tanh\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Tanh()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Tanh()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Tanhshrink\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Tanhshrink()\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Tanhshrink()\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Threshold\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Threshold(0.1, 20)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Threshold(0.1, 20)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Transformer\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\r\n    src = torch.rand((0, 10, 32, 512))\r\n    tgt = torch.rand((0, 20, 32, 512))\r\n    transformer_model(src, tgt)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\r\n    src = torch.rand((0, 10, 32, 512))\r\n    tgt = torch.rand((0, 20, 32, 512))\r\n    out = transformer_model(src, tgt)\r\n    out.sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## TransformerDecoder\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\r\n    transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\r\n    memory = torch.rand(0, 10, 32, 512)\r\n    tgt = torch.rand(0, 20, 32, 512)\r\n    out = transformer_decoder(tgt, memory)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\r\n    transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\r\n    memory = torch.rand(0, 10, 32, 512)\r\n    tgt = torch.rand(0, 20, 32, 512)\r\n    out = transformer_decoder(tgt, memory)\r\n    out.sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## TransformerDecoderLayer\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\r\n    memory = torch.rand(0, 10, 32, 512)\r\n    tgt = torch.rand(0,20, 32, 512)\r\n    decoder_layer(tgt, memory)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\r\n    memory = torch.rand(0, 10, 32, 512)\r\n    tgt = torch.rand(0,20, 32, 512)\r\n    decoder_layer(tgt, memory).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## TransformerEncoder\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\r\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\r\n    src = torch.rand(0, 10, 32, 512)\r\n    transformer_encoder(src)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\r\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\r\n    src = torch.rand(0, 10, 32, 512)\r\n    transformer_encoder(src).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## TransformerEncoderLayer\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    src = torch.ones(0, 10, 32, 512)\r\n    encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\r\n    encoder_layer(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    src = torch.ones(0, 10, 32, 512)\r\n    encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\r\n    encoder_layer(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Unfold\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    unfold = nn.Unfold(kernel_size=(2, 3))\r\n    unfold(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    unfold = nn.Unfold(kernel_size=(2, 3))\r\n    unfold(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## Upsample\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Upsample(scale_factor=2)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.Upsample(scale_factor=2)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## UpsamplingBilinear2d\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.UpsamplingBilinear2d(scale_factor=2)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.UpsamplingBilinear2d(scale_factor=2)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## UpsamplingNearest2d\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.UpsamplingNearest2d(scale_factor=2)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.UpsamplingNearest2d(scale_factor=2)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n## ZeroPad2d\r\n\r\n### Forward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ZeroPad2d(2)\r\n    m(z)\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n\r\n### Backward\r\n\r\n``` python\r\nimport torch\r\nimport sys\r\n\r\ntry:\r\n    z = torch.ones(0,3,10,10)\r\n    m = torch.nn.ZeroPad2d(2)\r\n    m(z).sum().backward()\r\nexcept:\r\n    sys.exit(1)\r\n\r\nsys.exit(0)\r\n```\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nThis feature request is within regards to the padding size argument of `F.pad`.  From my perspective there are two deficiencies with the current format:\r\n\r\n1. For normal tensors (not named), the current format can be very awkward.  If you're adding padding to the last dimension it's elegant.  But sometimes you're adding padding to the first or second dimension, and things can get awkward.  For instance I have code that adds padding to the second dimension of a tensor.  The unfortunate thing is that the tensor is sometimes 3d and sometimes 4d.  When you combine these situations with the awkward dimension counting from the end, you sometimes end up code like:\r\n```\r\npad_size = [0 for i in range(2 * len(x.shape))]\r\npad_size[2*len(x.shape) - 1 - 2 * pad_dim] = padding\r\nF.pad(x, pad_size, ...)\r\n```\r\n\r\n2. The more pressing issue, from my perspective, is that `F.pad`'s formatting is fundamentally incompatible with named tensors.\r\n\r\n## Motivation\r\n\r\nI'd like to see a more elegant way of padding both named tensors and regular tensors.\r\n\r\n## Pitch\r\n\r\nMy proposal is to increase the expressiveness of the the padding_size argument of `F.pad`.  I propose padding_size can be one of two types:\r\n- (tuple) in which case the current behavior is maintained.  This ensures total backward compatibility.\r\n- (dict) in which case the padding format is `{dim: [padding_left, padding_right]}`.  `dim` could take multiple values\r\n-- If dim is a positive int, it specifies the index of the dimension to be padded.\r\n-- If dim is a negative int, it specifies the index from the back to be padded. `-1` would be the last dimension.\r\n-- If dim is a string, it specifies the name of the dimension to be padded.\r\n\r\n## Alternatives\r\nThe alternative is to continue to use `F.pad` as it's written, convert all named tensors to non-named tensors before padding, and then convert them back to named tensors afterward.\r\n\r\n## Additional context\r\n\r\nI'd be happy to implement this if a proposal is agreed upon.\n\ncc @albanD @mruberry"},{"labels":["enhancement",null,null],"text":"## ðŸ› Bug\r\n\r\nfreeze_module JIT pass does not preserve any other method besides forward at the moment.\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nAs mentioned.\r\n\r\n## To Reproduce\r\n```\r\nimport torch\r\nimport torch.utils.mobile_optimizer\r\nfrom torch.jit._recursive import wrap_cpp_module\r\n\r\nclass MyMod(torch.nn.Module):\r\n    def forward(self, arg):\r\n        return arg\r\n    @torch.jit.export\r\n    def version(self):\r\n        return 1\r\n\r\nsm = torch.jit.script(MyMod())\r\nsm.eval()\r\nprint(\"Original\")\r\nsm._c.dump(True, True, False)\r\nprint(\"==========\")\r\nfrozen = wrap_cpp_module(torch._C._freeze_module(sm_1._c))\r\nprint(\"POST FREEZING\")\r\nfrozen._c.dump(True, True, False)\r\nprint(\"==========\")\r\n```\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nversion method in the above code snippet preserved.\r\n\r\n\n\ncc @suo"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nAs mentioned by @xush6528 in https://github.com/pytorch/pytorch/issues/37557, A future `.then()` API that satisfies the following contract would be useful:\r\n```\r\nfut = rpc.rpc_async(...).then(lambda result: some_other_potentially_async_fn(result)\r\nfut.wait() # will be completed once rpc_async is completed and the above lambda has finished execution\r\n```\r\nThe implementation could look something like:\r\n```\r\ndef then(self, func):\r\n    ret = Future()\r\n    self.addCallback(lambda result: func(result) ; ret.markCompleted())\r\n    return ret\r\n```\r\n\r\n## Motivation\r\nThe original motivation for this is fixing the RPC profiling - currently, the profiling is done through attached callbacks; however, this does not guarantee that the callbacks are ran when the profiler exits. We already enforce that the future corresponding to the RPC must be awaited in order to be correctly profiled - so wrapping the callbacks in the `.then()` as above and then waiting on this future is a viable solution.\r\n\r\nIt may also be useful for async user functions in RPC (https://github.com/pytorch/pytorch/issues/36071), although I'll let @mrshenli clarify whether it would be useful or not.\r\n\r\nNote: `add_done_callback` is also being exposed to the user, see https://github.com/pytorch/pytorch/pull/37311. The user could of course implement the above with `add_done_callback`, but it's worth considering to support this natively.\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nthe reshape method should support Fortran-like order except for C-like order. e.g. wo could use a vecotr to fill a matrix in column order first, not just rows.\r\n\r\nthx~~\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nLog-linear version of cumsum and cumprod, as the current version has quadratic time-complexity.\r\n\r\n## Motivation\r\n\r\nThe current cumsum and cumprod are too slow for long sequences. They need a faster implementation such as the one I wrote here, which I translated from Jax ([here](https://github.com/google/jax/commit/824ac86620572285b86dd09529f8869ef36883ad)) to Pytorch ([here](https://gist.github.com/AranKomat/be50d1bcee38411681f7218d2b81dede)). This one doesn't utilize a custom CUDA kernel, and it's slower if the sequence length is short. Nevertheless, it's substantially faster than the existing one due to the log-linear time-complexity if it's sufficiently long.   \r\n\r\n## Pitch\r\n\r\nIf you like it, maybe you can add it (maybe just copy mine) to PyTorch possibly along with many variants of this, including logcumsumexp. \r\n\n\ncc @VitalyFedyunin @ngimel"},{"labels":["enhancement",null,null],"text":"## ðŸ› Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n![image](https://user-images.githubusercontent.com/44078961/80440384-401dec00-893b-11ea-8a79-7c3838019d23.png)\r\nMany nodes do not have a COMMON name. It is difficault to figure out who it is.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.OH, you can use a forward function with conv2d method and jit.trace it into a graph. And you wiil get it\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI believe that name with number should change into name by word, like bias and weight.\r\n\r\n## Environment\r\n\r\nThat graph is build by the tensorbaord example of pytorch. I think common pytorch is ok.\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n<!-- Add any other context about the problem here. -->\r\n```shell\r\ngraph(%self : ClassType<LSTM>,\r\n      %input : Float(1, 1, 3),\r\n      %6 : (Float(1, 1, 3), Float(1, 1, 3))):\r\n  %1 : Tensor = prim::GetAttr[name=\"weight_ih_l0\"](%self)\r\n  %2 : Tensor = prim::GetAttr[name=\"weight_hh_l0\"](%self)\r\n  %3 : Tensor = prim::GetAttr[name=\"bias_ih_l0\"](%self)\r\n  %4 : Tensor = prim::GetAttr[name=\"bias_hh_l0\"](%self)\r\n  %hx.1 : Float(1, 1, 3), %hx : Float(1, 1, 3) = prim::TupleUnpack(%6)\r\n  %48 : Tensor[] = prim::ListConstruct(%hx.1, %hx), scope: LSTM\r\n  %49 : Tensor[] = prim::ListConstruct(%1, %2, %3, %4), scope: LSTM\r\n...\r\n```\r\nIt is the graph's scirptmodule built by jit.trace. It seems like the node use the number the node's name. Maybe we can use the \"name=\"weight_ih_l0\"\" word to name the node.\r\n\r\ncc @suo"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nA `torch.nn.BufferDict` container that mirrors `ParameterDict`.\r\n\r\n## Motivation\r\nWe sometimes work with a lot of buffers, and it would be nice to have a convenient way of dealing with those, similar to `ParameterDict`, but well, with the values being buffered tensors rather than of type `torch.nn.Parameter`. Currently `ParameterDict` specifically uses `register_parameter`, so it's not straightforward to use that transparently.\r\n\r\n## Pitch\r\nImplement `BufferDict` by mirroring `ParameterDict`. Like so: #37385\r\n\r\n## Alternatives\r\nManually handle toms of buffers. That gets clunky quickly.\r\n\n\ncc @albanD @mruberry"},{"labels":["enhancement",null,null,null],"text":"We now have a few code places where we have special case where we return specific subgradients.\r\nFor example [here](https://github.com/pytorch/pytorch/blob/201ba139115adeecae4f094a9c9790200e53ff99/tools/autograd/templates/Functions.cpp#L115) for the norm function at 0.\r\n\r\nWe now have few functions that perform the same thing.\r\nIt would be interesting to allow users that know that they won't encounter these problematic points a way to disable these operations.\r\n\r\nWe could provide a general flag that disable these operations during the backward pass.\r\n- Either while the flag is set (so the flag should be set for the whole backward pass in general). Unless hooks are used to change it.\r\n- Either during the forward pass and we record it as an argument to the backward function.\r\n\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @VitalyFedyunin @ngimel"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nTorchScript looks like support the work flow. And trace-graph will miss it. I find that we support TorchScript to complie the if-condition. Is it we do not have engouh motivation to level-up the tesnorboardx.?\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\noh I just found that keras's graph have more information than pytorch in tensorboard. Maybe pytorch's graph can bcome better.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nI hope model can create a better graph. \r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\noh ,of course it look difficult just because the 'node constructer' looks complicate. But I believe the \r\njit supporter can fix it easily\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @suo"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nA data caching module that can be instantiated and passed into a dataloader, similar to the way a sampler or dataset can currently. This module would accept a cache path, epoch to cache on, train/test flags if different behavior is desired depending on the mode, and perhaps other fields as well, depending on a set of most common needs. I imagine there would be some flag, `write_cache` that when `True`, would lead  `__call__()` to accept a list of corresponding data (e.g. `[img, labels]` or something like that) and cache it to the desired location. In `write_cache=False`, it reads from the the cache according to the dataloader, if the requested file exists in cache, and reads from the original location if the file is not in cache.\r\n\r\n**UPDATE:** After posting this, I just came across this [similar open request](https://github.com/pytorch/pytorch/issues/35642). I guess I'm not the only one interested in this idea :-). Feel free to close my feature request as you see fit.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nCaching can speed up training significantly for many datasets and networks. I am often finding that data loading speed is the biggest bottleneck to my training routines. When I write my own caching tool into the dataloader, I can sometimes see 10x+ faster training (of course it depends on the network, datasets, preprocessing, etc).\r\n\r\nFrom what I gather from forums and simply looking at other people's code, many people write their own caching functions already. It would be great if there were a canonical one, tested and optimized for PyTorch dataloaders.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nI would like to see a `Cacher` class developed that can be instantiated and extended similar to the `Sampler` classes and accepted by PyTorch dataloaders. This class would fit well in the existing PyTorch design principle of extensible abstraction: providing a high-level library, accessible even to novices, that is also malleable and easily extended by users with more advanced needs.\r\n\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\nThe alternative really is for people to keep doing what they're doing. Folks who know how to do it probably are fine continuing doing so. However, this will help the new users and potentially save time for the power users.\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nAdd per-cluster biases in addition to the main head bias in Pytorch's implementation of adaptive softmax.\r\n\r\n## Motivation\r\n\r\n[TransformerXL](https://arxiv.org/abs/1901.02860)'s implementation of adaptive softmax incorporates a bias for all the clusters. It would be nice to have Pytorch at parity with that SOTA.\r\n\r\n## Pitch\r\n\r\nI would like to add a flag to the class constructor to add per-cluster biases, and then use them in the logprob calculations.\r\n\r\n## Additional context\r\n\r\nHugging Face already uses code by @thomwolf that only deviates from Pytorch code in this regard. I can quickly fix up a PR that adds the possibility of cluster biases if this feature is deemed interesting.\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸ› Bug\r\n\r\nUsing `nn.Bilinear` inside a `nn.Sequential` raises a `TypeError` because `nn.Sequential` doesn't support multiple inputs.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nbilinear = torch.nn.Bilinear(4, 8, 12)\r\nbilinear_seq = torch.nn.Sequential(bilinear)\r\n\r\ninput_1 = torch.zeros((10, 4))\r\ninput_2 = torch.zeros((10, 8))\r\n\r\nprint(f'bilinear output dims: {bilinear(input_1, input_2).shape}')\r\nprint(f'sequential bilinear output dims: {bilinear_seq(input_1, input_2).shape}')\r\n```\r\nOutput:\r\n```bash\r\nbilinear output dims: torch.Size([10, 12])\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n in \r\n      9 print(f'expected output dims:\\n{(10, 12)}')\r\n     10 print(f'bilinear output dims:\\n{bilinear(input_1, input_2).shape}')\r\n---> 11 print(f'sequential bilinear output dims:\\n{bilinear_seq(input_1, input_2).shape}')\r\n\r\n~/anaconda3/envs/ae/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    538             result = self._slow_forward(*input, **kwargs)\r\n    539         else:\r\n--> 540             result = self.forward(*input, **kwargs)\r\n    541         for hook in self._forward_hooks.values():\r\n    542             hook_result = hook(self, input, result)\r\n\r\nTypeError: forward() takes 2 positional arguments but 3 were given\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nOne would expect `nn.Sequential` to work with any number of inputs as long as the inputs are passed correctly from one module to the next one inside `nn.Sequential`.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.5.0.dev20200210+cu100\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Ubuntu 18.10\r\nGCC version: (Ubuntu 8.3.0-6ubuntu1~18.10.1) 8.3.0\r\nCMake version: version 3.12.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration: \r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: GeForce RTX 2080\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[conda] _pytorch_select           0.2                       gpu_0  \r\n[conda] blas                      1.0                         mkl  \r\n[conda] cudatoolkit               10.0.130                      0  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] numpy                     1.17.4           py37hc1035e2_0  \r\n[conda] numpy-base                1.17.4           py37hde5b4d6_0  \r\n[conda] torch                     1.5.0.dev20200210+cu100          pypi_0    pypi\r\n[conda] torch-nightly             1.2.0.dev20190805          pypi_0    pypi\r\n[conda] torchsnooper              0.7.1                    pypi_0    pypi\r\n[conda] torchvision               0.6.0.dev20200210+cu100          pypi_0    pypi\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"},{"labels":["enhancement",null,null,null],"text":"We have seen several requests for adding Windows support to `torch.distributed` package recently. ([1](https://discuss.pytorch.org/t/torch-distributed-for-windows-7-10/77855), [2](https://discuss.pytorch.org/t/pytorch-windows-is-parallelization-over-multiple-gpus-now-possible/59971), [3](https://discuss.pytorch.org/t/distributed-error-module-torch-distributed-has-no-attribute-is-initialized/31872), [4](https://discuss.pytorch.org/t/module-torch-distributed-has-no-attribute-is-initialized/55391/2), [5](https://github.com/pytorch/pytorch/issues/31363)) This issue is created to track how many people need this feature. Please add a reaction or comment below if your project needs `torch.distributed` on Windows.\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @peterjc123 @nbcsm @guyang3532"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nAdd a numerical stable implementation of the logit function, the inverse of the sigmoid function, and its derivative.\r\n```\r\ny = ln(x/(1-x))\r\n```\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n## Motivation\r\nIt should be as easy to use the inverse of the sigmoid as it is to use the sigmoid function without having to worry about a numerical stable implementation.\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\nAdd `torch.logit`.\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\nA current version of the logit could look like this:\r\n```\r\ny = torch.log(x) - torch.log1p(-x)\r\n```\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\nhttps://de.wikipedia.org/wiki/Logit\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null],"text":"## Summary\r\n\r\nThis project aims at decomposing existing `DistributedDataParallel` (DDP) implementation into multiple smaller pluggable and customizable building blocks. So that applications can customize DDP to best serve specific applications. More specifically, each type of individual building block exposes a fixed set of APIs but can implement different algorithms. The new DDP constructor takes building blocks to assemble a DDP instance. Existing DDP API can stay intact and internally calls into the new DDP implementation.\r\n\r\nThis project is motivated by the following requirements:\r\n\r\n* Enable new features without polluting existing DDP API\r\n    * Many of the new features would require adding more configurations at initialization. Instead of adding all of them into DDP constructor, these configurations should be self-contained inside the implementation of a building block.\r\n* Make DDP/Reducer code more readable\r\n    * Most of DDP core lives in `reducer.cpp`. It starts from a nicely organized implementation, but as we are gradually adding more features, the growing complexity starts to detrimental to readability.\r\n* Allow applications to customize their DDP by picking and extending sub-components\r\n    * Different applications have vastly different requirements, which we will elaborate in the *Targeted Features* section. Packing everything in one implementation would inevitably leads to unnecessary slowdowns. Instead, applications should be able to just pick the algorithms they need and get rid of those unnecessary complexities. \r\n* Add C++ DDP API\r\n* Gradually remove legacy features, e.g., `device_ids`, `dim`, `check_reduction`, etc. \r\n    * `device_ids` is necessary for single-process multi-GPU mode, which is not the recommended mode as GIL contention and the per-iteration scatter and gather could introduce additional overhead. The recommended one process per module replica/device use case does not need `device_ids` or `dim`.\r\n* Gradually move DDP from `torch.nn.parallel` to `torch.distributed` \r\n    * Currently DDP API lives in `torch.nn.parallel` but the core `Reducer` implementation is in the distributed package, which creates a weird dependency that the single-machine package depends on the distributed package. We can resolve this by moving DDP into the `torch.distributed` package.\r\n\r\n\r\n\r\n## Compositional Building Blocks\r\n\r\nWe propose to decompose the main logic of `DistributedDataParallel` into the following four building blocks\r\n\r\n\r\n* **Grad Reader**: responsible for reading grad from the local model, which is done by manipulating DDP hooks, including\r\n    * Where to read the grad from: e.g., `parameter.grad` or from distributed autograd context.\r\n    * When to read grad: e.g., `AccumlateGrad` , forward output tensor, etc.\r\n    * Which grad to expect: e.g., `find_unused_parameters` should skip hooks on unused parameters\r\n* **Grad Bucketer**: responsible for organizing grads into buckets, including\r\n    * How do grads maps to buckets: e.g., static mapping determined at construction time, dynamic mapping based on grad-ready order, etc.\r\n    * Determine when a bucket is ready for communication\r\n    * Grad/bucket compression\r\n* **Comm Scheduler**: responsible for how to communicate buckets, including\r\n    * What comm strategy to use: e.g., `AllReduce` or gossip\r\n    * How to prioritize communication\r\n* **Grad Writer**: responsible for writing the communicated grad back to the local model, including\r\n    * Where to read the grad from: e.g., `parameter.grad` or from distributed autograd context.\r\n    * Trigger post DDP-grad hooks if there are any: e.g., this is useful to overlap optimizer `step` with DDP communication. \r\n\r\n\r\n\r\n## Targeted New Features\r\n\r\n* **Mix DDP with RPC**\r\n    * Use DDP in conjunction with RPC functions.  Currently, DDP hook is only registered to the `AccumulateGrad` function and looks for grad from `param.grad`. To support RPC, DDP need to read grad from and write grad to the distributed autograd context.\r\n    * Compositional building blocks: \r\n        * Grad Reader\r\n        * Grad Writer\r\n* **Delay Allreduce**\r\n    * There are certain scenarios where we cannot make assumptions on how many times a `AccumulateGrad` hook will be called or we cannot traverse the real autograd graph from the outputs (e.g., with checkpoints + reused layers). For these cases, we can have a slower version of DDP which does not overlap comm with comp, so that DDP should be able to support all use cases that would work with local autograd. (see https://github.com/pytorch/pytorch/pull/35083)\r\n    * Compositional building blocks: \r\n        * Grad Reader\r\n        * Grad Writer\r\n* **Tune parameter to bucket mapping**\r\n    * This tunes the order of `AllReduce` parameters. Ideally, the `AllReduce` order should be the same as gradient ready order. So that DDP can launch communications as soon as possible. We might have to do some profiling to detect the gradient ready order in the backward pass, and might need to change it over iterations. All processes would need to agree on the mapping, but as the mapping adjustment should occur infrequently, we should be able to afford introducing one more `AllReduce` to achieve consensus. (see https://github.com/pytorch/pytorch/pull/35137)\r\n    * Compositional building blocks: \r\n        * Grad Bucketer\r\n* **Layer Drop/Skip Comm for Unused Params**\r\n    * Currently, DDP always `AllReduce` all buckets (i.e., all grads) even if some parameters are not used in the forward pass. Some applications intentionally skip layers to speed up training and prevent overfitting. In some other applications, different inputs might reach different part of the model.  Allowing skip unused parameters in the communication as well will help to speed up training. One challenge is that all processes need to have a global consensus on which gradients to skip. (see [paper](https://arxiv.org/pdf/1909.11556.pdf) as one example use case)\r\n    * Compositional building blocks:\r\n        * Grad Reader\r\n        * Grad Bucketer\r\n        * Grad Writer\r\n* **Overlap DDP with optimizer**\r\n    * Existing DDP enforces a hard barrier between backward and optimizer step. However, this is not necessary, as optimizers should be able to start working as soon as some grad is ready. Therefore, we should allow optimizer step to overlap with DDP backward. \r\n    * Compositional building blocks:\r\n        * Grad Writer\r\n* **Overlap DDP backward with next forward**\r\n    * Description: the reason for computing grad is to use the updated parameter in the next iteration. The next forward does not have to be block until all parameter are updated. Instead, it can start as long as the first layer parameters are ready. So we should allow overlapping DDP backward with next iterationâ€™s forward, and only block forward on a layer if its parameter hasnâ€™t been updated yet.\r\n    * Challenge: API design\r\n    * Compositional building blocks:\r\n        * per sub-module pre-forward hooks (to block forward until parameter is updated, this is already available)\r\n* **Prioritize and preempt allreduce**\r\n    * Grads becomes ready in the backward order, but parameters are consumed by the next forward in the opposite order. If we want to overlap backward with next forward well, we need to assign higher priorities to `AllReduce` operations for first layers. However, as buckets for last layers will be ready first, they will start early. Hence, we need some way to implement *preemption* in allreduce operations.  (see [paper1](https://dl.acm.org/doi/10.1145/3341301.3359642) and [paper2](https://arxiv.org/abs/1905.03960))\r\n    * Compositional building blocks:\r\n        * Comm Scheduler\r\n* **Gossip DDP**\r\n    * Instead of using `AllReduce` to globally sync gradients, itâ€™s proven that gossip and async param sync also works and can potentially lead to considerable training speedups. \r\n    * Compositional building blocks:\r\n        * Comm Scheduler\r\n* **Gradient Compression**\r\n\r\n    * Mixed precision training has been proven to help accelerate training considerably. DDP should support this use case as well by allowing use less bits to store and communicate grads.\r\n    * Compositional building blocks:\r\n        * Grad Bucketer\r\n\r\n\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nI would like to see a PyTorch feature to generate a dataset, ideally Pandas DataFrame with the image name as index and lists of labels for each image.\r\n\r\n## Motivation\r\nFor multi-label image classification, it is important to generate the dataset from scratch given a custom datasets. Most likely, the images would be duplicated in multiple folder if the image has multiple label, similar to how `ImageFolder` works. Unfortunately, I have not seen any of such features offered in most PyTorch implementation.\r\n\r\n## Pitch\r\nMulti-label image classification is definitely an important topic as most images can be classified with multiple labels. It is also much easier to implement compared to object detection if such level of detection is not required.\r\n\r\n## Alternatives\r\n@SpandanMadan had proposed a [working solution](https://discuss.pytorch.org/t/multi-label-classification-in-pytorch/905/26). I would like to clean it up and make it consistent with PyTorch implementation.\r\n\n\ncc @fmassa"},{"labels":[null,"enhancement",null],"text":"## ðŸš€ Feature\r\n\r\nI would like to be able to use a xLARFG function, or alternatively it would be great to be able to have a small extension for calling an arbitrary LAPACK function.\r\n\r\n## Motivation\r\n\r\nI want to generate transformation vectors using xLARFG and I do not want to do it in pure python. Even with JITing i believe that this will be significantly slower than the LAPACKs implementation. It would be great if xLARFG could be implemented. \r\n\r\nAlso I think that it would be really beneficial to have some way to access specific LAPACK functions which are not yet implemented.\r\n\r\n## Pitch\r\n\r\nThe implementation of the mirrored function for xLARFG. Or a way to implement/call native LAPACK functions without a new release on your end. \r\n\r\n## Alternatives\r\n\r\nImplement the function in python and JIT. Although this would increase the efficiency of the function itself it will likely not reach the same level as a native implementation"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nBased on training checkpoint data, allow training to restart at the epoch and **precise** batch, after the last batch trained on when the training checkpoint data was saved.\r\n\r\n## Motivation\r\n\r\nI'm training (relatively) large sequence to sequence models (> 100M parameters) with a (relatively large) dataset (1M conversations), using limited training hardware (one machine with 4x 1080Ti); one epoch can easily take 12 hours or (much) longer.\r\n\r\nThere are situations where I want to restart training based on a training checkpoint I saved during the training process that was stopped before, for some reason. In order to not lose many hours of training time, and to continue in exactly the same way, as if I would if the training process had not stopped, I want to be able to continue at the epoch and **precise** batch, after the last batch trained on when the training checkpoint data was saved.\r\n\r\n## Pitch\r\n\r\nAll relevant components in of a PyTorch training process, such as `nn.Module`s, optimizers and schedulers have `state_dict` and `load_state_dict` methods in order to retrieve and load its states, such that training can be restarted with the exactly the same state as it was stopped.\r\n\r\nThe only exception, as far as I can tell, is for `DataLoader` and `Sampler`. These (base) classes do not have `state_dict` and `load_state_dict` methods.\r\n\r\nIt would be very consistent and concise, if a PyTorch user could do the following in order to save and load training state:\r\n\r\n1. Get the state of your `DataLoader`s, models, optimizers and schedulers and save it together with any other training process variables (e.g. current epoch and batch index) as a training checkpoint.\r\n\r\n2. Load your training checkpoint and subsequently load the state of `DataLoader`s, models, optimizers and schedulers\r\n\r\n3. Restart your training loop at the loaded epoch index and batch index and continue training\r\n\r\nPlease note that it is assumed that the `Sampler` state is nested as a part of the `DataLoader` state.\r\n\r\n## Alternatives\r\n\r\n`DistributedSampler` has the possibility to set the current epoch, such that the random shuffling of samples is deterministic. However, I can't tell the DataLoader from which specific batch to continue.\r\n\r\nEdit:\r\n## Additional thoughts\r\n\r\nAssume that `k` of `N` batches of an epoch have been trained on, before training was stopped and a training checkpoint was saved. When the state of a `DataLoader` is reloaded in to an `DataLoader` instance `my_data_loader`, to stay consistent, `len(my_data_loader)` should be the total number of batches in an epoch, `N`.\r\n\r\n```\r\nmy_sampler = SomeSampler(...)\r\nmy_data_loader = torch.utils.data.DataLoader(my_dataset, ... sampler=my_sampler ...) \r\n\r\nmy_data_loader.load_state_dict(checkpoint['data_loader'])\r\n\r\n# Should print the number of batches in a normal epoch, N \r\nprint(len(my_data_loader))\r\n```\r\n\r\nHowever, when iterating over batches using the first `iter` object after restarting training:\r\n```\r\nfor batch in iter(my_data_loader):\r\n    ...\r\n```\r\nThe number of iterations will be `N-k`, for the last `N-k` batches in the epoch from where we stopped training and saved our training checkpoint. In the next epoch `iter(my_data_loader)` will again yield in the normal `N` batches.\n\ncc @SsnL"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nPossibility to add channels to linear layer --> nn.Linear(Input_size, output_size, n_channels). If possible, extend that to RNNs.\r\n\r\n## Motivation\r\nMany architectures, specially those related to multi task learning, can have multiple branches. Instead of processing each branch sequentially, they could be computed in parallel.\r\n\r\n## Pitch\r\nParallel processing will leverage GPU efficiency. With the code below, time reduced from 375us to 183us on cpu. If the tensor is small, using LinearWithChannel can be over 5x faster than creating 4 different linear layers.\r\n\r\n## Additional context\r\n![image](https://user-images.githubusercontent.com/23004953/79258665-7c404f80-7e59-11ea-8d1e-4286fd87ec8c.png)\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass LinearWithChannel(nn.Module):\r\n    def __init__(self, input_size, output_size, channel_size):\r\n        super(LinearWithChannel, self).__init__()\r\n        \r\n        #initialize weights\r\n        self.w = torch.nn.Parameter(torch.zeros(channel_size, output_size, input_size))\r\n        self.b = torch.nn.Parameter(torch.zeros(1, channel_size, output_size))\r\n        \r\n        #change weights to kaiming\r\n        self.reset_parameters(self.w, self.b)\r\n        \r\n    def reset_parameters(self, weights, bias):\r\n        \r\n        torch.nn.init.kaiming_uniform_(weights, a=math.sqrt(3))\r\n        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(weights)\r\n        bound = 1 / math.sqrt(fan_in)\r\n        torch.nn.init.uniform_(bias, -bound, bound)\r\n    \r\n    def forward(self, x):\r\n        \r\n        b, ch, r, c  = x.size()\r\n        return (( x * self.w).sum(-1 ) + self.b).view(b,ch,1,-1)\r\n\r\nclass FourDifferentLayers(nn.Module):\r\n    def __init__(self, input_size, output_size):\r\n        super(FourDifferentLayers, self).__init__()\r\n        \r\n        self.L1 = nn.Linear(input_size, output_size)\r\n        self.L2 = nn.Linear(input_size, output_size)\r\n        self.L3 = nn.Linear(input_size, output_size)\r\n        self.L4 = nn.Linear(input_size, output_size)\r\n    \r\n    def forward(self, x1, x2, x3, x4):\r\n        \r\n        return self.L1(x1), self.L2(x2), self.L3(x3), self.L4(x4)\r\n\r\nbatch_size, channel_size, input_size, output_size = 16, 4, 256, 128\r\nL_channel = LinearWithChannel(input_size = input_size, output_size = output_size, channel_size = channel_size)\r\nmy_tensor = torch.ones(batch_size, channel_size, 1, input_size)\r\n\r\n%timeit L_channel.forward(my_tensor)\r\n#output --> 183 Âµs Â± 5.91 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)\r\n\r\nL_four = FourDifferentLayers(input_size, output_size)\r\nmy_tensor1 = torch.ones(batch_size, 1, input_size)\r\nmy_tensor2 = torch.ones(batch_size, 1, input_size)\r\nmy_tensor3 = torch.ones(batch_size, 1, input_size)\r\nmy_tensor4 = torch.ones(batch_size, 1, input_size)\r\n\r\n%timeit L_four.forward(my_tensor1, my_tensor2, my_tensor3, my_tensor4)\r\n#output --> 375 Âµs Â± 2.39 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\r\n```"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nI would like `__array__` to always implicitly detach and transfer to CPU before returning a numpy array, so that `np.asarray(mytensor)` is guaranteed to work.\r\n\r\n## Motivation\r\n\r\nFor good reasons detailed in [this Discourse thread](https://discuss.pytorch.org/t/should-it-really-be-necessary-to-do-var-detach-cpu-numpy/35489), a `torch.Tensor` with gradients needs to be `.detach()`ed before it is converted to NumPy, and further, if the Tensor is on the GPU it needs to be explicitly transferred back. Specifically:\r\n\r\n> People not very familiar with `requires_grad` and cpu/gpu Tensors might go back and forth with numpy. For example doing pytorch -> numpy -> pytorch and backward on the last Tensor. This will backward without issue but not all the way to the first part of the code and wonâ€™t raise any error.\r\n\r\nAs someone not very familiar with `requires_grad`, I am fully on board with this. :sweat_smile: \r\n\r\nHowever, the purpose of `__array__` is to allow functions to be written against a unique API (NumPy) to work on arrays of other types *without having to know anything about said array*. Having to go through `.detach()[.cpu()]` breaks this assumption.\r\n\r\n## Pitch\r\n\r\nThe specific use case I have in mind is viewing tensors. In [napari](https://napari.org), we pass input arrays through `__getitem__` and then through `np.asarray`, which lets us lazily view n-dimensional arrays as long as they satisfy the `__array__` API. This works for NumPy, dask, zarr, and Xarray, but I stumbled when I was trying to demo it with torch Tensors. You can see a brief demo in this noise2self notebook:\r\n\r\nhttps://github.com/jni/noise2self/blob/napari/notebooks/Intro%20to%20Neural%20Nets.ipynb\r\n\r\n(You need the usual suspects, plus `pip install napari` or `conda install -c conda-forge napari` for the viewer.)\r\n\r\n![pytorch](https://user-images.githubusercontent.com/492549/79187638-73606700-7de2-11ea-977e-114041d76c8f.gif)\r\n\r\n## Alternatives\r\n\r\nThings could remain as is, and there are advantages (articulated in the Discourse post). However, this means that anyone else in the NumPy ecosystem that wants to play well with `torch.Tensor`s then needs to include PyTorch-specific code in their code base, which is not super nice. (Or monkey-patch it, which is also not super nice!)\r\n\r\n## Additional context\r\n\r\noriginal `__array__` proposal: #2935\r\noriginal `__array__` implementation: #2945\r\npossibly related issue: #9648\r\n\r\nI couldn't find too much discussion on the issues or PRs about the decision not to `.detach()` automatically in `__array__`.\n\ncc @ngimel"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nDecouple the Stream and Event with the CUDA at python.\r\n\r\n## Motivation\r\nWhile the stream based synchronization mechanism are widely used and generalized in the c10, c10 stream to cuda stream, there is no reason that the python code still coupled with cuda stream, only torch.cuda.stream and torch.cuda.event are available.\r\n\r\n1. Need to decouple the python stream and event to support more hardware type other than the cuda devices. \r\n2. Generalize the inter-node operation synchronization mechanism.\r\n\r\n## Pitch\r\n1. Add a python class to expose the c10 Stream and the torch cuda stream should inherit from the common stream.\r\n2. Add a new class to abstract the CUDAEvent and the torch cuda event should inherit from the common base event.\r\n3. Generalize the specific CUDA code for common abstraction: like move the hard coded THPVariable_record_stream to the ATen operation.\r\n\r\n\r\n\n\ncc @ngimel"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nAdd sparse tensor support for torch.blkdiag.\r\n\r\n## Motivation\r\n\r\nDense matrix support was implemented for issue #31932, but we need sparse as well.\r\n\r\n## Pitch\r\n\r\nSee #31932\r\n\r\n## Alternatives\r\n\r\nSee #31932\r\n\n\ncc @vincentqb"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nFor convenience, a module torch.nn.Parallel could exist for applying a list of operations parallely to a tensor instead of sequentially.\r\n\r\n## Motivation\r\n\r\nWhen models branch (imagine a variational autoencoder model with reconstruction in one branch and classification with another), nn.Parallel could be really useful for simplifying the definition of these types of models.\r\n\r\n## Pitch\r\n\r\nThe modification (as far as I can see) is almost trivial. In fact it only needs a minor modification over the Sequential code and the addition of Parallel (which is almost copy/paste). Here's an example of Parallel, where the only difference with Sequential is the forward function:\r\n\r\n```python\r\n[docs]class Parallel(Module):\r\n    r\"\"\"A parallel container.\r\n    Modules will be added to it in the order they are passed in the constructor.\r\n    Alternatively, an ordered dict of modules can also be passed in.\r\n\r\n    To make it easier to understand, here is a small example::\r\n\r\n        # Example of using Parallel\r\n        model = nn.Parallel(\r\n                  nn.Conv2d(1,20,5),\r\n                  nn.ReLU(),\r\n                  nn.Conv2d(20,64,5),\r\n                  nn.ReLU()\r\n                )\r\n\r\n        # Example of using Parallel with OrderedDict\r\n        model = nn.Parallel(OrderedDict([\r\n                  ('conv1', nn.Conv2d(1,20,5)),\r\n                  ('relu1', nn.ReLU()),\r\n                  ('conv2', nn.Conv2d(20,64,5)),\r\n                  ('relu2', nn.ReLU())\r\n                ]))\r\n    \"\"\"\r\n\r\n    def __init__(self, *args):\r\n        super(Parallel, self).__init__()\r\n        if len(args) == 1 and isinstance(args[0], OrderedDict):\r\n            for key, module in args[0].items():\r\n                self.add_module(key, module)\r\n        else:\r\n            for idx, module in enumerate(args):\r\n                self.add_module(str(idx), module)\r\n\r\n    def _get_item_by_idx(self, iterator, idx):\r\n        \"\"\"Get the idx-th item of the iterator\"\"\"\r\n        size = len(self)\r\n        idx = operator.index(idx)\r\n        if not -size <= idx < size:\r\n            raise IndexError('index {} is out of range'.format(idx))\r\n        idx %= size\r\n        return next(islice(iterator, idx, None))\r\n\r\n    @_copy_to_script_wrapper\r\n    def __getitem__(self, idx):\r\n        if isinstance(idx, slice):\r\n            return self.__class__(OrderedDict(list(self._modules.items())[idx]))\r\n        else:\r\n            return self._get_item_by_idx(self._modules.values(), idx)\r\n\r\n    def __setitem__(self, idx, module):\r\n        key = self._get_item_by_idx(self._modules.keys(), idx)\r\n        return setattr(self, key, module)\r\n\r\n    def __delitem__(self, idx):\r\n        if isinstance(idx, slice):\r\n            for key in list(self._modules.keys())[idx]:\r\n                delattr(self, key)\r\n        else:\r\n            key = self._get_item_by_idx(self._modules.keys(), idx)\r\n            delattr(self, key)\r\n\r\n    @_copy_to_script_wrapper\r\n    def __len__(self):\r\n        return len(self._modules)\r\n\r\n    @_copy_to_script_wrapper\r\n    def __dir__(self):\r\n        keys = super(Parallel, self).__dir__()\r\n        keys = [key for key in keys if not key.isdigit()]\r\n        return keys\r\n\r\n    @_copy_to_script_wrapper\r\n    def __iter__(self):\r\n        return iter(self._modules.values())\r\n\r\n    def forward(self, input):\r\n        output = []\r\n        for module in self:\r\n            output.append(module(input))\r\n        return tuple(output)\r\n```\r\n\r\nThe only extra modification would be in Sequential, then, also in the forward function, to unpack the input if needed:\r\n\r\n```python\r\n[docs]class Sequential(Module):\r\n    r\"\"\"Another sequential container.\r\n    Modules will be added to it in the order they are passed in the constructor.\r\n    Alternatively, an ordered dict of modules can also be passed in.\r\n\r\n    To make it easier to understand, here is a small example::\r\n\r\n        # Example of using Sequential\r\n        model = nn.Sequential(\r\n                  nn.Conv2d(1,20,5),\r\n                  nn.ReLU(),\r\n                  nn.Conv2d(20,64,5),\r\n                  nn.ReLU()\r\n                )\r\n\r\n        # Example of using Sequential with OrderedDict\r\n        model = nn.Sequential(OrderedDict([\r\n                  ('conv1', nn.Conv2d(1,20,5)),\r\n                  ('relu1', nn.ReLU()),\r\n                  ('conv2', nn.Conv2d(20,64,5)),\r\n                  ('relu2', nn.ReLU())\r\n                ]))\r\n    \"\"\"\r\n\r\n    def __init__(self, *args):\r\n        super(Sequential, self).__init__()\r\n        if len(args) == 1 and isinstance(args[0], OrderedDict):\r\n            for key, module in args[0].items():\r\n                self.add_module(key, module)\r\n        else:\r\n            for idx, module in enumerate(args):\r\n                self.add_module(str(idx), module)\r\n\r\n    def _get_item_by_idx(self, iterator, idx):\r\n        \"\"\"Get the idx-th item of the iterator\"\"\"\r\n        size = len(self)\r\n        idx = operator.index(idx)\r\n        if not -size <= idx < size:\r\n            raise IndexError('index {} is out of range'.format(idx))\r\n        idx %= size\r\n        return next(islice(iterator, idx, None))\r\n\r\n    @_copy_to_script_wrapper\r\n    def __getitem__(self, idx):\r\n        if isinstance(idx, slice):\r\n            return self.__class__(OrderedDict(list(self._modules.items())[idx]))\r\n        else:\r\n            return self._get_item_by_idx(self._modules.values(), idx)\r\n\r\n    def __setitem__(self, idx, module):\r\n        key = self._get_item_by_idx(self._modules.keys(), idx)\r\n        return setattr(self, key, module)\r\n\r\n    def __delitem__(self, idx):\r\n        if isinstance(idx, slice):\r\n            for key in list(self._modules.keys())[idx]:\r\n                delattr(self, key)\r\n        else:\r\n            key = self._get_item_by_idx(self._modules.keys(), idx)\r\n            delattr(self, key)\r\n\r\n    @_copy_to_script_wrapper\r\n    def __len__(self):\r\n        return len(self._modules)\r\n\r\n    @_copy_to_script_wrapper\r\n    def __dir__(self):\r\n        keys = super(Parallel, self).__dir__()\r\n        keys = [key for key in keys if not key.isdigit()]\r\n        return keys\r\n\r\n    @_copy_to_script_wrapper\r\n    def __iter__(self):\r\n        return iter(self._modules.values())\r\n\r\n    def forward(self, input):\r\n        for module in self:\r\n            if isinstance(input, tuple):\r\n                input = module(*input)\r\n            else:\r\n                input = module(input)\r\n        return input\r\n```\r\n\r\nThen, an example of its usage: \r\n\r\n```python\r\nmodel = torch.nn.Sequential(*[\r\n    torch.nn.Linear(in_features=200,out_features=100),\r\n    torch.nn.Parallel(*[\r\n        torch.nn.Linear(in_features=100, out_features=16),\r\n        torch.nn.Linear(in_features=100, out_features=16),\r\n    ]),\r\n    torch.nn.Bilinear(in1_features=16,in2_features=16,out_features=100),\r\n])\r\nmodel(torch.randn(200,))\r\n```\r\n\r\nMaybe it's a pointless addition, but I think it might be useful to some.\r\n\n\ncc @albanD @mruberry"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nIt would be great to have the bitwise operation 'Population Count' that counts the number of 1 bits to be exposed.\r\n\r\nNot sure if this exists in the back end of torch? It is exposed in [tensorflow](https://github.com/tensorflow/tensorflow/issues/1592)\r\n\r\n## Motivation\r\n\r\nThis is a key operation in implementing binarised neural nets, for example in [XNOR-Net](https://arxiv.org/pdf/1603.05279.pdf). Binarised linear and conv layers can be performed quickly using XNOR and population counts. XNOR is already exposed in the pytorch API.\r\n"},{"labels":["enhancement",null,null,null,null,null],"text":"## ðŸš€ Feature\r\nhttps://github.com/pytorch/pytorch/pull/35055/files is adding support for RPC profiling in functions `rpc.remote` and `rpc.rpc_sync/asyc`, but the implementation should be moved out of the main function into a context manager/helper function. This will improve readability and abstract away the profiling from the main code.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar @jiayisuse"},{"labels":["enhancement",null,null],"text":"\r\n## Issue description\r\n\r\nUpdate the `torch.quantization.fuse_modules` API to accept custom module for fusion \r\nUser should also provide the relevant code for the fused module.\r\n\r\nShould work for both training (with DDP) and inference.\r\n\r\n\r\n\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nWhen a user is doing `torch.ceil` and `torch.floor`, it is likely that the user is asking for a long tensor instead of a floating tensor\r\n\r\n`torch.ceil(x, dtype=torch.long)` would make this possible."},{"labels":["enhancement",null,null,null],"text":"After #28068, we now support 3rd party c10d extensions. However, DDP still hard-code calls into`torch.cuda.comm` APIs.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/8afa001d898914a48d6b9e3d944a99607d2819c1/torch/nn/parallel/distributed.py#L493-L496\r\n\r\nTo allow DDP to decouple from cuda, we have a few options:\r\n\r\n1. Deprecating single-process-multi-device training support, i.e., deprecating `device_ids` arg in DDP. \r\n2. Use c10d libraries to do local broadcast. We probably can do this by creating a subgroup for each DDP instance that only contains its own pg instance and then using the `broadcast_multigpu` API to perform local broadcast.\r\n3. Allow `torch.cuda.comm` to support 3rd party extensions. \n\ncc @ngimel @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar"},{"labels":["enhancement",null,null],"text":"## Impact\r\n\r\nCurrently, every RPC request occupies an RPC thread on the server side until done. However, if there are nested RPC calls or other IO operations, we don't have any feature to allow user functions to yield on the server side. As a result, threads processing those requests would idle wait. The _Async User Function_ feature_ aims to fix this by allowing applications to mark a function as async. This function must return an `rpc.Future`, and RPC framework would install response processing/handling as a callback to that Future.\r\n\r\n\r\n## Pitch\r\n\r\n### API\r\n\r\n```python\r\nimport torch.distributed.rpc as rpc\r\n\r\ndef my_add_2(a, b):\r\n    return a + b\r\n\r\n@rpc.async_function\r\ndef my_add_3(a, b, c):\r\n    # final output future of this user function\r\n    future_ret = rpc.Future()\r\n    # nested RPC calls\r\n    future_value = rpc.rpc_async(dst2, my_add_2, args=(a, b))\r\n    # finish processing asynchronously when the nested RPC is done.\r\n    future_value.add_callback(\r\n        lambda ret: future_ret.mark_complete(ret + c)\r\n    )\r\n    return future_ret\r\n    \r\nrpc.rpc_sync(dst1, my_add_3, args=(1, 2, 3))\r\n```\r\n\r\n### New Concepts\r\n\r\n1. `@rpc.async_function` decorator marks a function as an async function. \r\n    1. It gives the server side a hint that this is async user function, which can be done by registering the function name on the receiver, similar to how we handle `TorchScript` functions. \r\n    2. All async functions must return a future. \r\n2. `rpc.Future()` creates a future object which can be marked complete by the user function when the value is ready. This is not exactly a new concept, we just need to expose it and allow applications to create an empty Future.\r\n3. `Future.add_callback` this is already available in C++, we need to expose it to Python.\r\n\r\n## Examples\r\n\r\n### Nested Async User Functions\r\n\r\n```python\r\nimport torch.distributed.rpc as rpc\r\n\r\ndef identity(a):\r\n    return a\r\n\r\n@rpc.async_function\r\ndef my_add_2(a, b):\r\n    fut_ret = rpc.Future()\r\n    rpc_async(dst, identity, args=(a, )).add_callback(\r\n        lambda ret: fut_ret.mark_complete(ret + b)\r\n    )\r\n    return fut_ret\r\n    \r\n\r\n@rpc.async_function\r\ndef my_add_3(a, b, c):\r\n    fut_ret = rpc.Future()\r\n    rpc.rpc_async(dst, my_add_2, args=(a, b)).add_callback(\r\n        lambda ret: fut_ret.mark_complete(ret + c)\r\n    )\r\n    return fut_ret\r\n```\r\n\r\n### Multiple Async Calls in One User Function\r\n\r\n```python\r\ndef my_add_2(a, b):\r\n    return a + b\r\n\r\n@async_function\r\ndef my_add_3(a, b, c):\r\n    fut_ret = rpc.Future()\r\n    def bottom_half(x):\r\n        fut2 = rpc.rpc_async(dst, my_add_2, args=(x, x))\r\n        fut2.add_callback(\r\n            lambda ret: fut_ret.mark_complete(ret)\r\n        )\r\n    \r\n    rpc.rpc_async(dst, my_add_2, args=(a, b)).add_callback(bottom_half)\r\n    return fut_ret\r\n```\r\n\r\n### Async RPC Fan Out\r\n\r\n```python\r\ndef my_add_2(a, b):\r\n    return a + b\r\n\r\n@async_function\r\ndef my_add_4(a, b, c, d):\r\n    fut_ret = rpc.Future()\r\n    rets = []\r\n    lock = threading.Lock()\r\n    \r\n    def barrier(x):\r\n        flag = False\r\n        with lock:\r\n            rets.append(x)\r\n            if len(rets) == 2:\r\n                flag = True\r\n        if flag:\r\n            fut_ret.mark_complete(rets)\r\n            \r\n    rpc.rpc_async(dst2, my_add_2, args=(a, b)).add_callback(barrier)\r\n    rpc.rpc_async(dst2, my_add_2, args=(c, d)).add_callback(barrier) \r\n    return fut_ret\r\n```\r\n\r\n### Batch RPC Requests\r\n\r\n```python\r\nlock = threading.Lock()\r\na_list = []\r\nb_list = []\r\nfut_rets = []\r\nbatch_size = 10\r\n\r\n@async_function\r\ndef batch_add(a, b):\r\n    flag = False\r\n    fut_ret = rpc.Future()\r\n    with lock:\r\n        a_list.append(a)\r\n        b_list.append(b)\r\n        fut_rets.append(fut_ret)\r\n        \r\n        if len(a_list) == batch_size:\r\n            tmp_a_list, a_list = a_list, []\r\n            tmp_b_list, b_list = b_list, []\r\n            tmp_fut_rets, fut_rets = fut_rets, []\r\n            flag = True\r\n            \r\n    if flag:\r\n        aa = torch.stack(a_list)\r\n        bb = torch.stack(b_list)\r\n        cc = aa + bb\r\n        for i in range(len(fut_rets)):\r\n            fut_rets[i].mark_complete(cc[i])\r\n    \r\n    return fut_ret\r\n```\r\n\r\n## Discussion\r\n\r\nQ: why not using native Python `asyncio` API?\r\nA: The `asyncio` proposal is started in Python 3.3. The `async`/`wait` syntax is first added in Python 3.5, and then becomes in Python 3.6. As RPC also aim to support earlier Python 3 releases, we cannot rely on a feature that is only stabilized for 3.6+.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null,null,null],"text":"For now, RPC could either send tensor data or tensor **storage** data to the destination depending on the sparsity of the tensor. Say, if the tensor is just a view of the 1st row of another tensor that contains 100 rows, RPC would clone the tensor and just send the data in the 1st row. If, however, if the tensor is a view of the first 60 rows, RPC would send the entire storage. This is an internal optimization to balance memory overhead vs comm overhead. As @lw pointed out in [this comment](https://github.com/pytorch/pytorch/pull/35483#discussion_r400196335), this will have user-visible differences and it's important that we have a clear design on this behavior and clarify that in our API docs. \n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null,null],"text":"@jjlilley brought this up in #35331. Currently, if `UserRRef` encountered an error internally or when running user-facing APIs, it would report the error but nothing prevents the application from using the `UserRRef` again. However, as some error could have messed up RRef internal states, especially for the ones that have impact on reference counting, we should store those error in the UserRRef, delete the UserRRef and prevent subsequent usage on the RRef. \n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nThis function would mirror `np.unravel_index`; a basic Torch implementation and todo to formally expose such an implementation were previously added [here](https://github.com/pytorch/pytorch/blob/c73e97033a3aef97a5685588ea014d54a5cc11cc/torch/testing/__init__.py#L41-L49).\r\nA few more suggestions for implementation may be found at this [SO thread](https://stackoverflow.com/questions/53212507/how-to-efficiently-retrieve-the-indices-of-maximum-values-in-a-torch-tensor).\r\n\r\n## Motivation\r\n\r\nAs per the SO thread, functions like `argmax` now return a flat index, while sometimes one may want an unravel (/ multi-dimensional) index.\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\n\r\nPyTorch Large Model Support (LMS) is a feature in the PyTorch provided by IBM here: [here (official IBM repo)](https://github.com/IBM/pytorch-large-model-support/) and [here (fork of the main maintener of LMS)](https://github.com/mtbrandy/pytorch) that allows the successful training of deep learning models that would otherwise exhaust GPU memory and abort with \"out-of-memory\" errors. LMS manages this oversubscription of GPU memory by temporarily swapping tensors to host memory when they are not needed.\r\n\r\nWith LMS, deep learning models can scale significantly beyond what was previously possible and, ultimately, generate more accurate results.\r\n\r\n## Motivation\r\n\r\n* When training recurrent models with back-propagation through time (BPTT) it is often useful to 'truncate' the sequence length as little as possible, especially when dealing with audio inputs or EEG data that have high temporal resolution. This results in a larger memory footprint, and this is where LMS can save the day.\r\n* Also, the amount of compute needed to train state-of-the-art models doubles on average every 3.5 months (see https://openai.com/blog/ai-and-compute/). This comes both from the use of larger batch sizes and the use of larger models (like the now famous GPT-2 with 1.5B parameters). For instance the Transformer-XL can have a big memory footprint (https://openai.com/blog/sparse-transformer/). Using LMS is very useful when you want to test something out without using gradients checkpointing right away.\r\n* LMS can be extremely beneficial to anyone who cannot afford access to high-end GPUs (within small startups or in academic research). Using cloud services or buying the Titan RTX ($2,499) to run models is often too expensive.\r\n* GPU RAM is most of the time limited to about 8GB and is not extensible. Regular RAM on the other hand can easily be increased up to 128GB or more and is underused during trainings.\r\n* Finally, LMS could be useful when smoke testing runs with small GPUs (either manually or within the context of a CI). This leaves the small (often older) GPUs still busy while the larger ones are used for real runs with or without LMS.\r\n\r\n## Pitch (copy/paste from the doc of LMS)\r\n\r\nOne or more elements of a deep learning model can lead to GPU memory exhaustion.\r\n\r\nThese include:\r\n\r\n * Model depth and complexity\r\n * Base data size (for example, high-resolution images)\r\n * Batch size\r\n\r\nTraditionally, the solution to this problem has been to modify the model until it fits in GPU memory. This approach, however, can negatively impact accuracy â€“ especially if concessions are made by reducing data fidelity or model complexity.\r\n\r\n## Alternatives\r\n\r\nCheckpointing can some sometimes helps. But that not always the cases...\r\n\r\n## Additional context\r\n\r\nThis feature is maintained for a while (since at least PyTorch 1.1) by @mtbrandy and is proposed for contribution to PyTorch since at least August 2019 (I did not found any mention of it on this repo):\r\n\r\nhttps://www.reddit.com/r/pytorch/comments/cgyppk/large_model_support_for_pytorch/\r\nhttps://discuss.pytorch.org/t/very-large-model-on-single-gpu/28962\r\n\r\nIt is as well mentionned here:\r\nhttps://www.ibm.com/support/knowledgecenter/SS5SF7_1.5.4/navigation/pai_getstarted_pytorch.html\r\n\r\nOfficial repos:\r\nhttps://github.com/IBM/pytorch-large-model-support/\r\nhttps://github.com/mtbrandy/pytorch\r\n\r\n-----\r\n\r\nI am basically creating this issue because I really like LMS. So far I have waited the support of LMS for each version of PyTorch. Each time I had to manually compile PyTorch (and create wheels) to have the support of it. (BTW, many thanks to @mtbrandy that still maintains this fork).\r\n\r\nThe thing that I am missing is why this feature has not been integrated in PyTorch even though the code is made by IBM (and maintained) :sweat_smile:.\r\n\r\nI mean, it needs an \"opt-in\" from the user, so it is not enabled by default! If the reason is \"it can reduce the speed performance\". I agree with you, but it can also allows people to experiment more without the need of a super-expansive GPU. I really think that the community, small start-ups, students etc. would benefits from this even if they will surely not use that most of the time."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nSupport a Bazel build of pytorch, equal in capacity to the cmake build\r\n\r\n## Motivation\r\n\r\npytorch integration with bazel projects is substantially easier with a bazel ruleset and target surface. Additional uses include from-source build capacity, with unique global configurations like sanitizers.\r\n\r\n## Pitch\r\n\r\nbazel building and testing with clear instructions for using pytorch as an external repository without patches, direct from release/git commit.\r\n\r\n## Alternatives\r\n\r\nUsing binary-only releases and rules_foreign_cc\r\n\r\n## Additional context\r\n\r\nThis is currently in use at Uber ATG, where a successful build and test of cpu and gpu (nvcc) targets occurs on a daily basis. The changes required for this will be adapted, improved, and released as the starting point. Further work should be detailed in additional tasks.\r\n"},{"labels":["enhancement",null,null,null],"text":"### What is TensorPipe?\r\n\r\nTensorPipe (https://github.com/pytorch/tensorpipe) is a library that provides a new primitive for point-to-point communication for machine learning. A so-called pipe is an ordered bidirectional stream of structured messages between two processes. Each message contains a list of payloads/tensors, which are represented as buffers: by a pointer and a length, residing on CPU or GPU. The pipeâ€™s job is to copy the contents of those buffers to the other endpoint in the most efficient way possible. TensorPipe comes with a collection of â€œbackendsâ€ which implement different ways of transferring data (TCP between machines in a network, shared memory inside the same machine, CUDA device-to-device memcpy, ...). The pipe is backend-agnostic and will autodiscover the capabilities of the two endpoints and negotiate the best available backend.\r\n\r\n### What problems is it trying to solve?\r\n\r\nFor long, networking in PyTorch was synonymous with torch.distributed, which performs collective communication that works great for data parallelism. Some projects however have been pushing the envelope of that paradigm. Think model parallelism, parameter servers, actor-learner separation, elastic training, .... Those applications would benefit from point-to-point communication. This is what PyTorchâ€™s RPC module has recently enabled. However, the backbone of RPC, its so-called â€œagentâ€, is based on Gloo, one of the libraries that were developed for the collective communication of torch.distributed. TensorPipe aims to power an alternative RPC agent designed from the group up for point-to-point communication and optimized for that scenario. For example, TensorPipe doesnâ€™t require rendezvous between all pairs of processes (it supports arbitrary topologies and discovery methods), it offers asynchronous and asymmetric communication, and more.\r\n\r\nAnother challenge TensorPipe aims to address is the burden of supporting and leveraging all the transport techniques and interconnect fabrics available in modern hardware. It often falls to the user to be aware of where their code is run (threads of the same process, processes on the same machine, servers on the same rack, ...) and what their communication capabilities are (shared memory, NVLink, InfiniBand, GPUDirect, ...), and use that information to make informed choices in their code. TensorPipe wants to ship with builtin support for all of these backends and automatically figure out at runtime which one it should use based on the capabilities and the circumstances.\r\n\r\n### What would we like to do?\r\n\r\nWe would like to implement an additional RPC agent that leverages TensorPipe. This would live side-to-side with the current Gloo-based one (ProcessGroup), with that one remaining the default one. As TensorPipe matures and gains more in-the-field experience we can see how it holds up against Gloo in real-world scenarios and decide whether to consolidate behind it.\r\n\r\n### Who are we?\r\n\r\nThe team working on TensorPipe includes @jiayisuse, @beauby, @javier-m and @osalpekar . The project is the brainchild of @pietern, the author of Gloo.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jjlilley"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nPyTorch now recommends to use DistributedDataParallel over DataParallel for all sorts of multi-GPU trainings (#35063). However, it has one limitation comparing to old DataParallel module - currently it cannot handle forward/backward hooks in a user convenient way.\r\nProposed workaround\r\nhttps://github.com/pytorch/pytorch/blob/95ad94c75b09ad2438141e4eb52e83e737966e60/torch/nn/parallel/distributed.py#L146-L149\r\nrequires users to edit each model's forward propagation code for using hooks with model wrapped into DDP.\r\nAs I understand, it wasn't initially designed having this limitation in mind and was discovered during fixing another issue #5061. So, I am wondering, maybe there are some possibilities to implement some sort of hook synchronization mechanism across distributed model replicas?\r\n\r\n## Motivation\r\n\r\nAlso with current workaround possibilities to use hooks dynamically is lost for DistributedDataParallel module. For example, in my current code with DataParallel I am able to place and remove hooks dynamically: during validation phase of training process I am placing hooks to extract additional bottleneck features to calculate some complementary evaluation metrics which are not calculated during training phase.\r\nIn general, current hooking mechanism looks not fully compatible with DDP.\r\n\r\n## Pitch\r\n\r\nHooking mechanism for DistributedDataParallel module working from the user perspective as in DataParallel module.\r\n\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nA generic API that encapsulates certain operators that require contexts. For example weights are prepacked in certain format.\r\n\r\n\r\n## Pitch\r\n\r\n* Instead of using _xnnpack::conv2d_prepack and _xnnpack::conv2d_packed, prepacked::.., prepacked::..\r\n    * Conv:\r\n        * prepacked::conv2d_clamp_prepack -> Conv2dOpContext\r\n        * prepacked::conv2d_clamp_run\r\n    * Linear:\r\n        * prepacked::linear_clamp_prepack -> LinearOpContext\r\n        * prepacked::linear_clamp_run\r\n    * OpContext:\r\n        * Stores the engine info or backend info to be used by _packed ops during op execution.\r\n    * Context creation during deserialization can use heuristics to decide which backend to chose from available ones.\r\n        * e.g. if mobile has both XNNPACK and GPU backend available, some heuristics may help decided which backend to use and create context accordingly.\r\n        * For now this can be just based on global context. e.g. if xnnpack backend enabled (and is only enabled on mobile) pack for xnnpack.\r\n\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nI would like to contribute to the pytorch (in python and c++) by adding the [ contrastive loss/siamese_margin_loss](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf) function which is usually implemented  for siamese networks \r\n\r\n## Motivation\r\n\r\nI have been working on siamese networks and have implemented constrastive loss/siamese_margin_loss to measure similarity between bottleneck embeddings of the autoencoder.\r\n\r\n## Pitch\r\n\r\nThe output of the loss function is similar to that of  [TripletMarginLoss](https://github.com/pytorch/pytorch/blob/d9b97a4ffd3d65eeaccb5f8729926976fa868981/torch/nn/functional.py#L3544) but for siamese network instead of triplet network.\r\n\r\nIt returns a float value signifying the distance between the bottleneck embedding.\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n\r\n## Additional context\r\n\r\nThis loss implemented for all siamese network implementations (such as facial similarity, Omniglot examples)\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nAllow setting the number of `CUDAStream` in the stream pools\r\n\r\n## Motivation\r\nWhen running multiple inferences using different streams from different threads, the current concurrency limit is 32 (as stated in https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDAStream.h). We're using T4 which allow Concurrent Kernel Execution of 128. and would like to use more than 32 logical streams.\r\n\r\n## Pitch\r\nAdd an API to override the `kStreamsPerPool` to allow more than 32 streams\r\n\r\n## Additional context\r\n\r\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications__technical-specifications-per-compute-capability\r\n\n\ncc @ngimel"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nC++ API should support Famous Open Datasets CIFAR10 and CIFAR100.\r\n**Dataset Reference**: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)\r\n\r\n## Motivation\r\nWhile creating PyTorch C++ tutorials [https://github.com/prabhuomkar/pytorch-cpp](https://github.com/prabhuomkar/pytorch-cpp), I came to know that PyTorch has support for these datasets via [vision](https://github.com/pytorch/vision/tree/master/torchvision/datasets). As of now, [C++ API for Datasets](https://github.com/pytorch/pytorch/tree/master/torch/csrc/api/src/data/datasets) only has MNIST.\r\nSupport for standard datasets like CIFAR, COCO, ImageNet, etc out of the box will help developers play with C++ API easily.\r\n\r\n## Pitch\r\n- Add [cifar.h](https://github.com/pytorch/pytorch/tree/master/torch/csrc/api/include/torch/data/datasets) header with _`TORCH_API`_ `CIFAR` class.\r\n- Add [cifar.cpp](https://github.com/pytorch/pytorch/tree/master/torch/csrc/api/src/data/datasets) with both `CIFAR10` and `CIFAR100` support similar to vision for reading images, targets and getting train/test data. \r\n\r\n## Alternatives\r\nN/A\r\n\r\n## Additional context\r\nReference C++ Implementation: [cifar10.h](https://github.com/prabhuomkar/pytorch-cpp/blob/master/tutorials/intermediate/deep_residual_network/include/cifar10.h) and [cifar10.cpp](https://github.com/prabhuomkar/pytorch-cpp/blob/master/tutorials/intermediate/deep_residual_network/src/cifar10.cpp)\r\nReference Python Implementation: [cifar.py](https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py)\r\n\n\ncc @yf225 @fmassa"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nImplementing the SLIDE algorithm.\r\n\r\n## Motivation\r\n\r\nI came across [this](https://arxiv.org/abs/1903.03129v2) new paper while browsing through the AI group on facebook. I haven't read it completely yet, but I believe what they have manged to pull off(based on my current understanding) is, they have gotten rid of matrix multiplication all together  from Network and replace it with hashing based approach which runs on CPU but can train the  model with the speed at which a typical GPU would do. It also has almost linear scaling i.e more cores = faster training. \r\n\r\n## Pitch\r\n\r\nA custom implementation of SLIDE algorithm in Pytorch which is built purely for supporting the  Pytorch ecosystem.\r\n\r\n## Alternatives\r\n\r\nI went on [their](https://github.com/keroro824/HashingDeepLearning/issues/7) Git repo and asked them for a Cython implementation and they agreed and are interested in making it happen. So maybe Pytorch team can port their Cython implementation(once it's available) within Pytorch. \r\n\r\n\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nImplement a dataloading functionality for reinforcement learning state, action pairs, with assigned policy scores, transitional probabilities and rewards.\r\nImplement a set of gradient algorithms (qlearning, actor-critic, etc) in order to populate policy maps and use them or export them for other models.\r\n\r\n## Motivation\r\n\r\nI feel there is a gap in pytorch, which doesn't address a relatively interesting field of machine learning. I understand that reinforcement learning falls outside deep learning, and that simple examples of DQN already exist in tutorials, blogs and such, but I feel that impementing a GPU and CPU based dataloader which is easy to use and understand, combined with the algorithms that calculate the policy values and maps, would benefit pytorch as well as those who already use pytorch for a variety of reasons. I think that there's a lot of software domains where this would be applicable.\r\n\r\n## Pitch\r\n\r\n1. develop a dataloader class or set of classes which deal with deterministic state-action spaces and with probabilistic state-action spaces.\r\n2. develop a set of reinforcement learning algorithm classes which can be applied to those dataloaders, and populate policy values, based on rewards and such\r\n\r\n## Alternatives\r\n\r\nNo much really.\r\n\r\n## Additional context\r\n\r\nI think that pytorch has grown to be a wonderful tool used in so many different places, and that reinforcement learning seems to be missing. I know theres a tutorial for DQN, and I know that a lot of the algorithms are trivial and simple, yet it seems to me that users rely on pytorch as a simple and easy to use tool for machine learning, and as such don't always have the time (or know-how) on how to implement that functionality.\r\n\r\n**Just to be clear** I propose **I implement this** with some guidance, as I've already done this in C, C++ and Python before. I've got a few ideas on how to do this with the underlying C++ aten and autograd libraries and then move on to the python bindings.\r\n\n\ncc @SsnL"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nAs mentioned in https://github.com/pytorch/pytorch/pull/33987/files, the tests for `export_chrome_trace` in `test/test_autograd.py` are currently disabled on Windows due to a file permission issue.\r\n\r\nA correct fix here would be to pass the file object directly to `export_chrome_trace`, however, this would require modifying its API. \r\n\r\nWhen this is done we should re-enable the two tests that are currently skipped for windows:\r\n`test_profiler_aggregation_lstm` and `test_profiler_tracing`\r\n"},{"labels":["enhancement",null,null],"text":"I am currently working with a model which has a fixed set of heavy operations applied on different versions of the same input. It would be really nice if I could perform these operations in parallel as each one of them takes a big chuck of total execution time. In Theano, with graph optimizations, this would have been automatically executed in parallel. But I am not sure how to parallelize these operations in PyTorch. I looked into Pythonâ€™s multiprocessing module and PyTorchâ€™s wrapper for it. But I am not sure if itâ€™s usage would maintain the autogradâ€™s graph integrity and how theyâ€™d work while performing backprop (which is actually an even more expensive than forward propagation).\r\n\r\nThe example of the model that captures the problem.\r\n`def forward(x):`\r\n    `    #process each transformed input by the same function`\r\n    `    # my_func here is time consuming and the current bottleneck, but is internally optimized`\r\n    `    # ideally, all four my_func calls below should execute in parallel`\r\n    `    a = my_func1(x1)`\r\n    `    b = my_func2(x2)`\r\n    `    c = my_func3(x3)`\r\n    `    result = a + b + c `\r\n    `    return result`\r\n\r\nIs there any way I could speed up this function? Not being able to parallelize this hurts me even more when I stack many such steps, which I am planning to do next. So any suggestions to speed this up in any way would help a lot.\r\n\r\nThanks!\r\n\r\n"},{"labels":["enhancement",null],"text":"There are two motivations. First, the low precision inference is becoming popular, int4 could make full advantage of latest NV GPUs. Second, we are doing some quantum simulation stuff based on PyTorch, int4 based on NV GPUs will significantly improve the simulation speed in our framework."},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nVariable learning rate matrices option.\r\n\r\n## Motivation\r\n\r\nMany neural nets don't make full use of a computer's resources. Variable learning rate matrices could decrease time to reach optima while making use of otherwise wasted resources. Variable learning rate matrices are especially useful with attention based neural nets in case those are added in future versions.  A variable learning rate matrix could be as large as the matrix of weights or neurons ( perhaps larger if secondary learning rates were used ). It would be interesting to impose a super-intelligence  to control the values of the matrix.\r\n\r\n## Pitch\r\n\r\nI want to create an option for variable learning rate matrices. I will do all of the coding"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nAdd `reverse` option to `torch.cumsum`, such as in [tensorflow](https://www.tensorflow.org/api_docs/python/tf/math/cumsum)\r\n\r\n## Motivation\r\nThis would compute right to left cumulative sum more efficiently.  Currently, as far as I know, the only way to do it is \r\n\r\n```Python\r\nx = torch.arange(9).view(3, 3)\r\nr2lcumsum = th.flip(th.cumsum(th.flip(x, [1]), 1), [1])\r\n```\r\n\r\nResult should be:\r\n\r\n```Python\r\ntensor([[ 3,  3,  2],\r\n        [12,  9,  5],\r\n        [21, 15,  8]])\r\n```\r\n\r\n## Pitch\r\nAdd `reverse` arg to native `cumsum` function\r\n\r\n## Alternatives\r\n\r\n## Additional context\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸ› Bug\r\nWhen using `IterableDataset` with `num_workers > 0` and `drop_last=True`, DataLoader drops more instances than expected.\r\nIt seems like the drop of incomplete batches are occurred at each worker.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport torch\r\nfrom torch.utils.data import IterableDataset\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.utils.data import get_worker_info\r\n\r\nclass MyIterableDataset(IterableDataset):\r\n    def __iter__(self):\r\n        worker_info = get_worker_info()\r\n        worker_id = worker_info.id\r\n        yield from range(100*worker_id, 100*(worker_id+1))\r\n\r\ndataset = MyIterableDataset()\r\ndata_loader = DataLoader(dataset, batch_size=128, num_workers=10, drop_last=True)\r\nnext(iter(data_loader))\r\n\r\n>>>\r\n(Expect this to return some batch, but this just raises StopIteration)\r\n```\r\n\r\n## Expected behavior\r\nDrops only the incomplete batch of complete iteration, not per each worker.\r\n\r\n## Environment\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: TITAN Xp\r\nGPU 1: TITAN Xp\r\nGPU 2: TITAN Xp\r\nGPU 3: TITAN Xp\r\n\r\nNvidia driver version: 440.44\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] torch==1.3.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.4.2                py36_cu101    pytorch\r\n\n\ncc @SsnL"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nHi, dear developers, I think,\r\n<!-- A clear and concise description of the feature proposal -->\r\nIt is not good to separate the steps of modules making and forward computation,\r\nwhich should be combined like a circuit diagram\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nBecause separating  the steps of modules making and forward computation will make code highly redundant, as the module initiation and forward computation are a procedure essentially the same.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nAdd a `groups` argument to the `Fold` and `Unfold` modules (and the corresponding functions in `functional`)\r\n\r\n## Motivation\r\n\r\nCurrently it is a little bit cumbersome to implement e.g. a custom `AvgPool2d` or `MaxPool2d` filter using `Fold` and `Unfold`, as the channel information is not retained. For an input of shape `[B, C, H, W]` `Fold` will output an array of `[B, #pixels_per_window, #windows]` and it is cumbersome to divide that up into the channels again in this stage.\r\n\r\n## Pitch\r\n\r\nAdding a `group` argument (just like in the `Convxd` modules) would simplify that (by choosing `groups=C`), and provide a natural extension of these modules. I'd suggest for `Fold` to output a shape of `[B, groups, #pixels_per_window, #windows]` instead. \r\n\r\n## Alternatives\r\n\r\nI don't think there are any really elegant alternatives. `Fold` and `Unfold` are provide to implement custom filtering and pooling functionality.\r\n\r\n## Additional context\r\n\r\nnone\r\n"},{"labels":["enhancement",null,null,null],"text":"## Issue description\r\n\r\ntorch.multinomial only works on single and double dimension tensors, without giving ability for choosing probability dimension.\r\nThus, multinomial is not directly usable on rnn output tensor, which has three dimensions (the third one being the probability dimension).\r\n\r\nA practical improvement would be to allow torch.multinomial to work on the third dimension of any 3-dimension tensorr, in order to get the action for every batch sample and every sequence time.\r\n\r\nA more generic improvement would be to make it work regardless of the input tensor dimension, by allowing to choose probability dimension in a new 'dim=' parameter.\r\n\r\nOn the following example, multinomial is used on a 3-dimension tensor.\r\nThere is no crash, but the result is a 1-dimension tensor of size SEQ_SIZE, as if dimension 2 & 3 had been flattened into one single dimension.\r\nThe expected results would have been a SEQ_SIZE  x BATCH_SIZE  tensor.\r\n\r\n## Code example\r\n```python\r\nimport torch\r\n\r\nclass Network(torch.nn.Module):\r\n\r\n    def __init__(self,input_size,hidden_size,output_size):\r\n        super(Network,self).__init__()\r\n        self.gru         = torch.nn.GRU(input_size,hidden_size)\r\n        self.softmax     = torch.nn.functional.softmax\r\n        self.multinomial = torch.multinomial\r\n\r\n\r\n    def forward(self,input,hidden):\r\n        output,hidden = self.gru(input,hidden)\r\n        policy        = self.softmax(output,dim=2)\r\n        action        = self.multinomial(policy,num_samples=1)\r\n        return hidden,policy,action\r\n    \r\nSEQ_SIZE = 100\r\nBATCH_SIZE = 20\r\nINPUT_SIZE = 10\r\nHIDDEN_SIZE = 20\r\nOUTPUT_SIZE = 15\r\n\r\nnetwork = Network(INPUT_SIZE,HIDDEN_SIZE,OUTPUT_SIZE)\r\ninput = torch.rand(SEQ_SIZE,BATCH_SIZE,INPUT_SIZE)\r\nhidden = torch.rand(1,BATCH_SIZE,HIDDEN_SIZE)\r\n\r\nhidden,policy,action = network(input,hidden)\r\n\r\nprint(policy.size())\r\nprint(action.size())\r\n```\r\n\r\n## System Info\r\n\r\nPyTorch version: 1.0.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Microsoft Windows 10 Professionnel\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.5\r\n[pip] numpydoc==0.9.1\r\n[pip] torch==1.0.1\r\n[conda] _pytorch_select           1.1.0                       cpu  \r\n[conda] _tflow_select             2.3.0                       mkl  \r\n[conda] blas                      1.0                         mkl  \r\n[conda] libmklml                  2019.0.5                      0  \r\n[conda] mkl                       2019.4                      245  \r\n[conda] mkl-service               2.3.0            py37hb782905_0  \r\n[conda] mkl_fft                   1.0.14           py37h14836fe_0  \r\n[conda] mkl_random                1.1.0            py37h675688f_0  \r\n[conda] pytorch                   1.0.1           cpu_py37h39a92a0_0  \r\n[conda] tensorflow                2.0.0           mkl_py37he1bbcac_0  \r\n[conda] tensorflow-base           2.0.0           mkl_py37hd1d5974_0\n\ncc @fritzo @neerajprad @alicanb @vishwakftw @ezyang @SsnL @gchanan"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nhttps://github.com/pytorch/pytorch/blob/1b746b95fb6f3df45bd2883045de9fbd82b72100/torch/nn/functional.py#L1362\r\nexpects the weight and bias tensors to have shapes `(out_features, in_features)`  and `(out_features)`, respectively. We would like for this function to optionally support `(N, out_features, in_features)` and `(N, out_features)`, as well.\r\n\r\nThis request extends to other `torch.nn.functional` functions, but this is the painful one to us right now.\r\n\r\n## Motivation\r\n\r\nIt's currently a bit of a pain to use `torch.nn.functional` with batch networks.\r\n\r\n## Pitch\r\n\r\nThis request is effectively asking to replace \r\nhttps://github.com/pytorch/pytorch/blob/1b746b95fb6f3df45bd2883045de9fbd82b72100/torch/nn/functional.py#L1379 with\r\n```py\r\noutput = input.matmul(weight.transpose(-1, -2))\r\n```\r\n\r\n## Alternatives\r\n\r\nWriting our own `torch.nn.functional.linear`\r\n\r\n## Additional context\r\n\r\nIn writing this request, github helpfully pointed out https://github.com/pytorch/pytorch/issues/7500 as being similar. I suppose this is a request to take that beyond algebra operators and into nn operators."},{"labels":["enhancement",null,null,null],"text":"Large networks like for instance cycleGAN, will be unable to run with a batch size larger than 1-10 usually, due to the severe memory requirements during backpropagation. However this is only because the backpropagation is done for all images at once. If instead there was an optional parameter when calling backprop that set the number of images to backprop at once and then just looped through all the images then arbitrarily large batch sizes could be handled.\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["enhancement",null,null],"text":"A usecase: storing a full backtracking pointer matrix can be okay for needleman/ctc alignment (4x memory saving compared to uint8 representation), if 2bit data type is used. Currently it's possible to do this with bit manipulation magic, but probably not very efficient (store and load will require masking and shifting, not fused)\r\n\r\nAnother usecase: compressed BoolTensor for binary neural networks\r\n\r\nAnother usecase: extremely low-bit quantized representations.\r\n\r\nIs something like this already implemented for quantization? Probably a simple version of this feature could be providing some explicitly utility functions like calculating size of the holder `uint8` tensor, fused store and load functions (potentially explicitly batched, e.g. actual store is delayed until some aligned number of memory lines has arrived)\r\n\r\nIn NumPy the related functionality is `np.packbits` and `np.unpackbits`, however these are designed to work only with 1-bit contained type. 2-bit/4-bit would be cool as well.\r\n\r\nOn 1-bit side, another related project is RoaringBitmap https://github.com/RoaringBitmap/RoaringBitmap (http://roaringbitmap.org/) - for compressed bitsets for set operations.\n\ncc @izdeby"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nSupport for quantized leaky relu\r\n\r\n## Motivation\r\n\r\nLeaky relu is an extremely useful and common activation, which does not have a quantized op yet. It would be extremely useful to support that for many networks.\r\n\r\n## Pitch\r\n\r\nAdd quantized operator for leaky relu\r\n\r\n## Alternatives\r\n\r\nNone\r\n\r\n## Additional context\r\n\r\nNone\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nUsage of Julia to access Torch backend apiâ€™s. \r\n\r\n## Motivation\r\n\r\nArray Programming is a paradigm, just like saying â€œObject Orientated Programmingâ€ or â€œFunctional Programmingâ€, along w/ a compiler that can handle Dispatch and mathematical types makes it a language perfect for deep learning \r\n\r\n## Pitch\r\n\r\nCreate Julia bindings to access torch backend while leveraging the capabilities of the Julia language itself. \r\n\r\n## Additional context\r\n\r\nInclude GPU support for NVIDIA/CUDA. (Julia has this)\r\n"},{"labels":["enhancement",null,null,null],"text":"Some changes would make small-scale prototyping more user-friendly in PyTorch:\r\n\r\n- Python types are automatically converted to PyTorch types\r\n- integer types are treated numerically equivalent to their floating point version\r\n- device mismatches are handled automatically\r\n\r\nThe last one is probably the most controversial. Implicit movement between devices causes hard-to-debug performance problems in large models. However, this problem could be solved with better tooling. Forgetting to add `.cuda()` will always trip people up\r\n\r\nAn example of workflow I just went through to check if my result is close to 1\r\n```\r\n\r\ntorch.allclose(tensor.mean(), 1) # allclose(): argument 'other' (position 2) must be Tensor, not int\r\n \r\ntorch.allclose(tensor.mean(), torch.Tensor(1)) # output with device cuda:0 doesn't match the desired device cpu\r\n \r\ntorch.allclose(tensor.mean(), torch.Tensor(1).cuda()) # False....huh?...oh, need tensor, not Tensor...\r\n \r\ntorch.allclose(tensor.mean(), torch.tensor(1).cuda()) # Float did not match Long\r\n \r\ntorch.allclose(tensor.mean(), torch.tensor(1.).cuda()) # Success!\r\n \r\n ```\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nIt would be cool if the Java bindings worked on Windows.  They don't right now.  If you are interested in this feature, please comment on this issue with information on how/why you want to use it.  We will use this information to prioritize the work.\n\ncc @peterjc123"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nOption to provide a seed (random_state) for random_split() like the sklearn API https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html.\r\n\r\n## Motivation\r\n\r\nUseful for deterministic sampling & reproducible data generation (easily, without affecting the PRNG for other uses).\r\n\r\n## Pitch / implementation proposal\r\n\r\nIn `dataset.py`\r\n```diff\r\n+ from torch.random import manual_seed, get_rng_state, set_rng_state\r\n\r\n-def random_split(dataset, lengths):\r\n+def random_split(dataset, lengths, random_state=None):\r\n    r\"\"\"\r\n    Randomly split a dataset into non-overlapping new datasets of given lengths.\r\n\r\n    Arguments:\r\n        dataset (Dataset): Dataset to be split\r\n        lengths (sequence): lengths of splits to be produced\r\n+       random_state (int, optional): seed used by the random number generator\r\n    \"\"\"\r\n    if sum(lengths) != len(dataset):\r\n        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\r\n\r\n+   if random_state is not None:\r\n+       state = get_rng_state()\r\n+       manual_seed(random_state)\r\n\r\n    indices = randperm(sum(lengths)).tolist()\r\n-   return [Subset(dataset, indices[offset - length:offset]) for offset, length in zip(_accumulate(lengths), lengths)]\r\n+   splits = [Subset(dataset, indices[offset - length:offset]) for offset, length in zip(_accumulate(lengths), lengths)]\r\n+   if random_state is not None:\r\n+       set_rng_state(state)\r\n+   return splits\r\n```\r\n\r\nIt could also allow a `ByteTensor` to be compatible with `get_rng_state()`.\r\n\r\n## Alternatives\r\n\r\nManually call `manual_seed()` before splitting, without forgetting to save and restore the state with `get/set_rng_state()` or with `fork_rng()` (by the way it's not obvious from the docs that it should used with `with`, maybe add a snippet?)\r\n"},{"labels":["enhancement",null],"text":"Raising here to solicit feedback on a proposal for External Memory Management support in [Numba]( https://numba.pydata.org ) (also called NBEP 7). Discussion is occurring in [this repo]( https://github.com/gmarkall/nbep-7 ). Please take a look and raise issues/PRs as you see fit. Thanks in advance for your feedback!\r\n\r\ncc @gmarkall (for awareness)"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nAdding a warning/error message if you had created DDP module from different models.\r\n\r\n## Motivation\r\nRecently I faced weird behavior of DDP module â€“ in case if you have different models (i.e., some weights differed) on different nodes, you will receive no error/warning message about this while the whole training process is going to halt.\r\n\r\nI forgot that some pre-trained models differed in shapes (the first node had an embedding layer with shape [100, 500], and the second one had an embedding layer with shape [10000, 500]). I had received no error message while my model halted during the training before first forward propagation. \r\n\r\nThus I had to debug this case for about a day, while it could be significantly easier if I had received a warning message that my models differs.\r\n\r\n## Pitch\r\n\r\nI'm not sure what is the reason to allow somebody to create DDP model from different modules. I believe that users should receive a warning about that.\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski"},{"labels":["enhancement",null,null],"text":"current scenario -\r\nwhen I do \r\n```\r\nimport torch\r\ntorch.nn.Sigmoid()\r\n```\r\nthen it gives output\r\n```\r\nSigmoid()\r\n```\r\nexpected scenario -\r\nI do something like\r\n```\r\nimport torch\r\ntorch.nn.Sigmoid(symbolic=True)\r\n```\r\ngives\r\n![Screenshot (230)](https://user-images.githubusercontent.com/43755000/72700241-abaf2b00-3b8e-11ea-9acc-ac274f25d23b.png)\r\nor\r\n![Screenshot (232)](https://user-images.githubusercontent.com/43755000/72700665-df3e8500-3b8f-11ea-9a15-ebdfbbd6593e.png)\r\n\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n- Extend the KL divergence method for two `torch.distributions.Independent` to be as the one in [pyro](https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/kl.py#L19). This recursively falls back to the base distribution to compute the KL divergence but is more flexible in terms of the shared dimensions than the current solution.\r\n- Implement KL divergence for diagonal covariance normals defined by wrapping `torch.distributions.Normal` with `torch.distributions.Independent` and `torch.distributions.MultivariateNormal` as in [pyro](https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/kl.py#L32).\r\n\r\n## Motivation\r\n`torch.distributions.kl_divergence` raises a `NotImplementedError` when called on a diagonal Normal when it is defined by wrapping a `torch.distributions.Normal` with `torch.distributions.Independent`. It works fine when the `torch.distributions.MultivariateNormal`  is used but this can be memory and compute intensive when defining diagonal covariance matrices.\r\n\r\nKL divergences between diagonal Gaussians and typically other diagonal Gaussians are widely used in variational methods for generative modelling but currently, there is no efficient way to represent a multivariate diagonal Gaussian that allows computing a KL divergence.\r\n\r\n## Pitch\r\n\r\nThe implementation is extremely straightforward:\r\n\r\nIn `torch.distributions.kl` the `kl_independent_independent` method (https://github.com/pytorch/pytorch/blob/master/torch/distributions/kl.py#L734) is expanded to match https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/kl.py#L19 from pyro.\r\n\r\nSimilarly, the `kl_independent_MultivariateNormal` method is added as a copy of the method's implementation in pyro (https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/kl.py#L32) keeping only the `Normal` distribution special case (disregarding the `Delta` distribution special case).\r\n\r\n## Alternatives\r\n\r\nBoth of the above changes have been discussed in relation to #11178 which suggests adding a special case distribution for the diagonal covariance Gaussian. This is generally unwanted since any other diagonal covariance distributions (or distributions wrapped with `Independent`) would need a special diagonal case class as well. The suggestion at hand does not require any new distribution but still enables efficient KL divergence computation for `Independent` distributions and specifically diagonal Gaussians.\r\n\r\n\r\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nImplement truncated normal distribution.\r\n\r\n## Motivation\r\n\r\nA truncated normal distribution is useful as initializer of weights or when sampling from ReLU potentials. This has been requested before (https://github.com/pytorch/pytorch/issues/2129, https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/15, etc.).\r\n\r\nMany packages have this: [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.truncnorm.html), [matlab](https://www.mathworks.com/help/stats/prob.normaldistribution.truncate.html), [R](https://cran.r-project.org/web/packages/truncnorm/truncnorm.pdf), [julia](https://juliastats.org/Distributions.jl/stable/truncate/#Distributions.TruncatedNormal), [tensorflow](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/TruncatedNormal), and others.\r\n\r\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw\r\n\r\nRelated: https://github.com/pytorch/pytorch/issues/31945"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nSupport strategy to train large model that exceeds GPU mem and DRAM mem.\r\n\r\n## Motivation\r\nModels [e.g. **Factorization Machines**(1) or **DeepFM**(2)] in **recommendation task**s are usually very large , **billions of features**(including user id and product id) are used.\r\n\r\nTraining data in recommendation scenario is very sparse and the large embedding table consumes large memory, the model size may **exceed both GPU mem and DRAM mem** in one Machine. \r\n\r\nBoth **DataParallel**  and **DistributedDataParallel**  strategy can't support this scenario. \r\n\r\nThis kind of scenario usually use **Parameter Server** strategy.\r\n\r\nI wonder how **Facebook** use Pytorch to serve **Recommendation and Ads products**.\r\n\r\n1. Rendle, Steffen. \"Factorization machines.\" 2010 IEEE International Conference on Data Mining. IEEE, 2010.  https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\r\n2. Guo, Huifeng, et al. \"DeepFM: a factorization-machine based neural network for CTR prediction.\"Â arXiv preprint arXiv:1703.04247Â (2017). https://arxiv.org/abs/1703.04247\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["enhancement",null],"text":"\r\n## Feature\r\n\r\nWe would like to propose a replacement for `torch/utils/checkpointing.py` which provides more efficient checkpointing strategies. We already have a working prototype called `rotor`, and available [on our gitlab](https://gitlab.inria.fr/hiepacs/rotor). We are opening an issue to discuss the opportunity to include `rotor` into `torch/utils`, as well as the possible changes to the code that this would require.\r\n\r\n## Motivation\r\n\r\nThe checkpointing strategy available in `torch/utils/checkpoint.py` are not optimal: as mentioned in [this comment](https://github.com/pytorch/pytorch/pull/4594#pullrequestreview-88079144), more efficient strategies would allow faster training times for a given memory usage. Additionnally, the number of segments used in `checkpoint.py` needs to be hand-tuned, whereas it would be more practical to specify a limit on memory usage.\r\n\r\n## Pitch \r\n\r\nJust like in `checkpoint.py`, our code only handles `nn.Sequential` types of networks. We start with performing measures of time and memory usage of each module in the sequence on a sample input. Then, given a memory limit, we compute provably optimal strategies with a dynamic programming algorithm. The resulting strategy is then implemented in a way which is similar to what is done in `checkpoint.py`.\r\n\r\nOur working prototype is called `rotor`, and is available [on our gitlab](https://gitlab.inria.fr/hiepacs/rotor). The README file of the project provides a sample code to explain its basic usage. The algorithms used in our code and an experimental evaluation based on the models of torchvision are described in [our research report](https://hal.inria.fr/hal-02352969). \r\n\r\n## Notes on the current version\r\n\r\n* The code is not yet sufficiently documented and tested, we have plans to improve on that, but wanted to discuss this feature proposal during this time.\r\n\t\r\n* Just like `torch/utils/checkpoint.py`, our code preserves RNG states by default at each checkpoint, and restores it during the backward phase.\r\n"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nscikit's implementation of logsumexp accepts a supplemental array of scaling factors that allow for subtraction instead of just addition.\r\nhttps://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.misc.logsumexp.html\r\n\r\n## Motivation\r\n\r\nI'm doing work with log probabilities and need to subtract some likelihoods from each other. Currently I have to move to linear probabilities first.\r\n\r\n## Pitch\r\n\r\nIf logsumexp is operating over a dimension of size N, accept an additional tensor of size (N,) of scaling factors, to match the functionality in scikit\r\nhttps://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.misc.logsumexp.html\r\n\r\n## Alternatives\r\n\r\nsubtraction of probabilities in linear space, but this has numerical stability issues obviously.\r\n"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nGiven a batch of images, having a shape (b, c, h, w).\r\nGiven a batch of weight tensors, concatenated together with shape (b, out_features, in_features, kernel_height, kernel_width) which is not contiguous (can not view as (b * out_features, in_features, kernel_height, kernel_width) without copying).\r\nPerform a convolution that uses each 4d weight tensor to convolve the corresponding 3d image.\r\ntorch.conv2d with parameter groups=b can be used to solve the problem when the 5d weight tensor is contiguous. See https://github.com/pytorch/pytorch/issues/17983 .\r\n\r\n## Motivation\r\nFor example, if one want to calculate the Jacobian matrix (i.e. the gradient with respect to parameters for each input in a batch), he can first create a 5d weight tensor using torch.expand() efficiently, \r\n then perform batch convolutional layer, and finally backpropagate. In such case, weight tensor is not contiguous.\r\nFor another application, consider an encoder-decoder network. If an encoder layer shares the same convolutional kernel with the decoder layer, then the weight tensor of the encoder layer is the transpose of  the decoder layer, thus not contiguous.\r\n\r\n## Alternatives\r\nOf course one can use torch.conv2d with group parameter groups=b. However, reshaping the 5d tensor to 4d (b * out_features, in_features, kernel_height, kernel_width) is needed. If the tensor is not contiguous, it will involve a copy operation which is really time consuming.\r\n\r\n## Additional context\r\nFor implementing batch convolutional layer using conv2d with group, see\r\nhttps://github.com/pytorch/pytorch/issues/17983 .\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nImplement the `erfcx(x)` special function, which computes `exp(x^2) * erfc(x)` in a numerically stable way. Also for convenience, add `logerfc(x) = log(erfc(x))` and `logerfcx(x) = log(erfcx(x))`.\r\n\r\n`erfcx` is available in many numerical packages, such as [Matlab](https://www.mathworks.com/help/matlab/ref/erfcx.html), [Julia](https://juliamath.github.io/SpecialFunctions.jl/latest/functions_list/#SpecialFunctions.erfcx), [SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.erfcx.html) [R](https://www.rdocumentation.org/packages/pracma/versions/1.9.9/topics/erf), and others.\r\n\r\nFrom `erfcx` it is easy to obtain `logerfc` and `logerfcx`, but this involves a conditional which can be slow in pure Python code. So I recommend adding `logerfc` and `logerfcx` as well, which can be implemented as:\r\n\r\n```\r\ndef logerfc(x): \r\n    if x > 0.0:\r\n        return log(erfcx(x)) - x**2\r\n    else:\r\n        return log(erfc(x))\r\n\r\ndef logerfcx(x):\r\n    if x < 0.0:\r\n        return log(erfc(x)) + x^2\r\n    else:\r\n        return log(erfcx(x))\r\n```\r\n\r\n## Motivation\r\n\r\nThese special functions are very useful whenever we have to work with truncated normal distributions.\r\n\r\nRelated: https://github.com/pytorch/pytorch/issues/2129, https://github.com/pytorch/pytorch/issues/32293\r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nThe blkdiag method is defined clearly in https://github.com/pytorch/pytorch/issues/31932\r\n\r\nhttps://github.com/pytorch/pytorch/issues/31932 suggests blkdiag should create a dense Tensor, which may also be helpful in some case.\r\n\r\nHowever, considering graph neural networks, we always want a sparse block tensor rather than a dense one, since a dense block tensor will be even slower than multiply submatrix one by one and will easily cause OOM.\r\n\r\nA use case can be found in https://stackoverflow.com/a/59641321/7699035\r\n\r\nIt's consistent with the most popular pytorch_geometric module, where node features x1, x2, x3, ..., xn of different graphs are concatenated to a large tensor and a batch index is given. I've also asked the author of pytorch_geometric on this problem here https://github.com/rusty1s/pytorch_scatter/issues/95 .\r\n\r\n## Pitch\r\n\r\nThis issue is for something like torch.spase.blkdiag rather than torch.blkdiag.\r\n\r\n## Alternatives\r\n\r\nThe operation in https://stackoverflow.com/a/59641321/7699035 is clearly parallelizable, I want an efficient solution in pytorch, however a torch.sparse.blkdiag method seems the best solution.\r\n\r\n\n\ncc @vincentqb"},{"labels":["enhancement",null,null],"text":"It might be helpful to allow using RPCs to do some non-autograd actions within an autograd context. For example, it will be useful to support the following use case:\r\n\r\n```python\r\nwith dist_autograd.context():\r\n    # this RPC should add send/recv functions\r\n    rpc_sync(dst, some_func, args=(some_tensor))\r\n\r\n    with torch.no_grad():\r\n        # this RPC should NOT add send/recv functions or carry context id\r\n        rpc_sync(dst, some_other_func, args=(some_tensor))\r\n\r\n    # this RPC should add send/recv functions\r\n    rpc_sync(dst, some_func, args=(some_tensor))\r\n```\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nA way to create a block-diagonal matrix:\r\n```\r\na = torch.tensor([\r\n  [1, 2],\r\n  [3, 4]\r\n])\r\nb = torch.tensor([\r\n  [5, 6],\r\n  [7, 8]\r\n])\r\n\r\nc = torch.blkdiag(a, b)\r\n```\r\n## Motivation\r\n\r\nGraph deep learning is getting more and more attention. The commonly used acceleration operation is to merge the adjacency matrices of subgraphs into a large adjacency matrix. No doubt we need a faster functionality of `bkldiag`.\r\n\r\nThe name comes from the function `bkldiag` in matlab: https://www.mathworks.com/help/matlab/ref/blkdiag.html\r\n\r\nThe implementation of this method has been discussed in the community:\r\nhttps://discuss.pytorch.org/t/creating-a-block-diagonal-matrix/17357\r\nhttps://discuss.pytorch.org/t/creating-a-block-diagonal-matrix/22592\r\nhttps://stackoverflow.com/questions/54856333/pytorch-diagonal-matrix-block-set-efficiently/56638727#56638727\r\n\r\n## Pitch\r\n```\r\na = torch.tensor([\r\n  [1, 2],\r\n  [3, 4]\r\n])\r\nb = torch.tensor([\r\n  [5, 6],\r\n  [7, 8]\r\n])\r\n\r\nc = torch.blkdiag(a, b)\r\n# torch.tensor([\r\n#   [1, 2, 0, 0],\r\n#   [3, 4, 0, 0],\r\n#   [0, 0, 5, 6],\r\n#   [0, 0, 7, 8],\r\n# ])\r\n```\r\n\r\n## Alternatives\r\n#31942 discussed the sparse version of this method, which I think is also necessary.\r\n```\r\na = a.to_sparse()\r\nb = b.to_sparse()\r\n\r\nc = torch.sparse.blkdiag(a, b)\r\n# tensor(indices=tensor([[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 2, 3, 2, 3]]),\r\n# values=tensor([1, 2, 3, 4, 5, 6, 7, 8]),\r\n# size=(4, 4), nnz=8, layout=torch.sparse_coo)\r\n```\n\ncc @ezyang @gchanan @zou3519"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nAllow multidimensional index in `index_add_`\r\n\r\n## Motivation\r\n`index_add_`  is similar to `scatter_add_` with the index applied on RHS rather than LHS. Unfortunately, currently `scatter_add_` allows multidimensional index while `index_add_` allows 1D index only.\r\n\r\nAllowing multidimensional index can make this function much more useful. For example, if a problem involves some kind of (batch) sorting, one might need to permute another tensor the same way as the sorted tensor. In this case, the 1D `index_add_` becomes useless and explicit advanced indexing will waste time doing copy.\r\n\r\nI see a similar issue #30574. This is kind of different compared to that because that is more-or-less an api that converts advanced indexing to proper language, while this is a request of a fused-operation.\r\n\r\n## Pitch\r\nMake `index_add_` exactly the same as `scatter_add_`, except that the index is applied on the RHS, like:\r\n```python\r\nself[i][j][k] += src[index[i][j][k]][j][k]  # if dim == 0\r\nself[i][j][k] += src[i][index[i][j][k]][k]  # if dim == 1\r\nself[i][j][k] += src[i][j][index[i][j][k]]  # if dim == 2\r\n```\r\n\r\n## Alternatives\r\nThe alternative is directly do advanced indexing:\r\n```python\r\nself += src[index, arange(src.size(1))[:,None], arange(src.size(2))]\r\nself += src[arange(len(src))[:,None,None], index, arange(src.size(2))]\r\nself += src[arange(len(src))[:,None,None], arange(src.size(1))[:,None], index]\r\n```\r\nBut this creates permuted copy of `src` and is less efficient.\r\n\r\nCC @ngimel "},{"labels":["enhancement",null,null],"text":"### Problem\r\n\r\nI frequently run into the situation where I have an `RRef` referencing a remote object, and I need to call a function on that remote object. Currently, we would need to first implement a wrapper function, and then call RPC on that function. For example:\r\n\r\n```python\r\nclass ExampleClass:\r\n    def some_func(self, arg1, arg2):\r\n        pass\r\n\r\ndef call_some_func(rref, *args):\r\n    rref.local_value().some_func(*args)\r\n\r\nrref = remote(\"worker1\", ExampleClass)\r\nrpc_sync(rref.owner(), call_some_func, args=(1, 2))\r\n```\r\n\r\nOr more generically:\r\n\r\n```python\r\ndef _run_remote_func(rref, func_name, *args, **kwargs):\r\n    getattr(rref.local_value(), func_name)(*args, **kwargs)\r\n```\r\n\r\n### Proposal\r\n\r\nIt will be easier if `RRef` provides this feature natively. We cannot directly support `RRef.some_func(1, 2)`, as there could be name collisions between user functions and `RRef` API. And as discussed in #28882, it might be bad to trigger an implicit RPC here. \r\n\r\nWhat about something like the following?\r\n\r\n```python\r\nrref.remote_value().some_func(1, 2)\r\n```\r\n\r\nThe API `remote_value()` pairs with the existing `local_value()` API, meaning that it is trying to access the value reference but remotely. It also avoids the name collision and implicit RPC problems above.  \r\n\r\nIn terms of implementation, we could let `RRef.remote_value()` return a special object `X` and also implement a utility function, e.g., `call_remote_value(rref, func_name, *args, **kwargs)`. In `X`'s `__getattr__` method, it partially applies the rref and the attribute name to `call_remote_value`, and returns the partially applied function. And `call_remote_value` uses RPC to invoke the `func_name` remotely.\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley"},{"labels":["enhancement",null,null,null,null],"text":"Hello. Iâ€™m currently working on spherical convolutional network topic. Right now Iâ€™m trying to develop a new kind of kernel used for the convolutional layer.\r\nThe usual kernel is 3x3 matrix. But for spherical images, after being projected onto a plane using equirectangular projection, there will be distortion. So I want to define the kernel as a spherical cap and project it on plane according to its position.\r\nFor example, the kernel at different positions of the sphere perspective to the panorama pictures will look like this:\r\n![image](https://user-images.githubusercontent.com/51077545/71574908-c6f5c000-2b25-11ea-80e1-1387ccdcfc82.png)\r\nIs there any way to determine the shape of the kernels in these ways? I have already had the whole coordinate of the points in every case. I would very appreciate any help and information.\r\nThank you guys very much!\r\n\n\ncc @csarofeen @ptrblck"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nParallelization: more balanced work distribution among workers\r\n\r\n## Motivation\r\n\r\nI recently checked the code for `at::parallel_for` method and this is what I observed.\r\nSuppose there are `N` indices and `T` workers, then worker `i (0 <= i < T)` receives\r\nindices `[i * ceil(N/T), min(N, (i+1) * ceil(N/T)) )`.\r\n\r\nNow, suppose `N=10, T=4`, then this scheme will have the following distribution : `3|3|3|1`.\r\nDefinitely, something like `3|3|2|2` looks better. The issues is that the current approach is biased against the last worker(s) - it (them) always receive(s) the least work, which **could be even zero**. To elaborate, the whole situation becomes even worse if, for example, `N=100, T=40`. Then work of size 3 is scheduled for each worker, meaning that only 34 workers are going to do something useful, while 6 workers do nothing. And, in general, the situation gets worse if the number of workers scales up with the size of an input.\r\n\r\n## Pitch\r\nWhat about a slightly different distribution? This one **uses all the workers**!\r\nFor a worker `i: 0 <= i < T` let\r\n```\r\nbegin(i) = ceil(N*i/T),\r\nend(i) = begin(i+1),\r\n```\r\nand the worker receives indices `[begin(i), end(i) )`\r\n\r\nIt can be shown that for any `N >= T >= 1`, any `i,j: 0 <= i,j < T`:\r\n```\r\n|(end(i) - begin(i)) - (end(j) - begin(j))| <= 1, \r\nend(i) - begin(i) >= 1, begin(i) < N,\r\n```\r\nso this new scheme is optimally balanced, and each worker will do at least something!\r\n\r\nThe only issue I see is a more likely chance of overflow in computing `begin`\r\n\r\ncc @yf225 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nUsing the sequence (or any other similar Python abstract class) for the `Dataset` class in order to tackle the note in lines 30-32 of `torch.utils.data.dataset`:\r\n\r\n```python\r\n    # No `def __len__(self)` default?\r\n    # See NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\r\n    # in pytorch/torch/utils/data/sampler.py\r\n```\r\n\r\nBy using the `Sequence` abstract classes, we can see in the official Python documentation that they have the exact abstract methods that the `Dataset` class expects, i.e. `__getitem__` and `__len__`.\r\n\r\n## Motivation\r\n\r\nMy motivation is three-fold: (1) tackling the note mentioned above left by the PyTorch contributors, (2) let the code be auto-documentedâ€”since accessing this code states explicitly that you need to define those two methods and, (3) using Python abstract classes instead of inheritting from object (which is a rather deprecated practice for when Python did not have abstract classes capabilities).\r\n\r\n## Pitch\r\n\r\nImplement the Dataset class using new Python abstract classes capabilities, or at least discuss about it.\r\n\r\n## Alternatives\r\n\r\nI currently do not have alternative proposals.\r\n\r\n## Additional context\r\n\r\n`None`.\r\n\n\ncc @SsnL"},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\nI would like to propose that the API expose an option to weight the gradients during backpropagation in when using DistributedDataParallel.\r\n\r\n\r\n## Motivation\r\nLetâ€™s say I have 4 GPUs and I am training a semantic segmentation network on a dataset with an ignore class. As I understand it, in the DataParallel setting, the outputs are aggregated on GPU0, the loss computed, and then the gradient is backpropagated back through each GPUâ€™s model. In the DistributedDataParallel case, L0, L1, L2, L3 are each computed for each GPUâ€™s share of the batch, the losses are backpropagated back through their respective GPUâ€™s model, and the gradients are averaged along the way.\r\n\r\nUsing DataParallel, the presence of an ignore class makes no difference. Even if one GPUâ€™s mini-batch has a lopsided amount of ignore pixels, the total loss is computed as the weighted average before any backprop occurs. However, what happens when you have a lopsided distribution of ignore pixels on one GPU using DistributedDataParallel? There does not seem to be any mechanism for weighting the average of the gradients; it seems that the averaging is unweighted. Yet in this case, L0, L1, L2, and L3 ought to have their contributions weighted by the ratio of valid pixels when averaging gradients during backpropagation.\r\n\r\n## Pitch\r\n\r\nI would like to see an optional field to `backward()` to provide weights for each backpropagation channel in DistributedDataParallel.\r\n\r\n## Alternatives\r\n\r\nUnclear what the alternatives are.\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @aazzolini @xush6528"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nAllow allocation of a second parameter when assigning devices to parallel implementations to control how the batch will be split between the devices involved.\r\n\r\n## Motivation\r\n\r\nThe main limitation in any multi-GPU or multi-system implementation of PyTorch for training i have encountered is that each GPU must be of the same size or risk slow downs and memory overruns during training.\r\n\r\n## Pitch\r\n\r\nnew parameter for data_parallel and distributed to set batch size allocation to each device involved\r\n\r\n## Alternatives\r\n\r\nMy current work around is assigning multiple instances of a particular GPU to DataParallel but this is not ideal because it still carry a significant speed and batch size overhead on large models.\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nThe LSTM layer in torch.nn should have the option to output the cell states of all time steps along with the hidden states of each time step.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nWhen implementing Recurrent Replay Distributed DQN (R2D2), [See here.](https://openreview.net/forum?id=r1lyTjAqYX) I need access to all the hidden states and cell states of all the time steps of a sequence. Those hidden states and cell states are stored in an array to later on continue training the LSTM based network from a different time step. This is possible using the LSTMCell and a for-loop. But this painfully slow, around 15-30 times slower than using an LSTM layer, even when using a GPU. \r\n\r\nThis is the first major roadblock that I encountered when using PyTorch because it hinders me from using the R2D2 technique, as the current technique of using LSTMCells is simply way too slow.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nI would like a flag in the LSTM layer which can be toggled to True, which will change the output behavior of the LSTM layer from ` output, (h_n, c_n)` to `output, cell_states, (h_n, c_n)`. Output comprises all hidden states (h_1, ... h_n) and cell_states comprises all cell states (c_1, c_n). The name of the flag could be \"return_states\" or \"return_cell_states\".\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\nAn alternative solution would be to somehow add a way to make the speed of using LSTMCells match the speed of the LSTM layer. This seems quite difficult to me to achieve.\r\n\r\n## Additional Context:\r\n\r\nOther people seem to have this issue as well, see [here](https://discuss.pytorch.org/t/lstm-internal-cells-for-each-state/9987/3) or [here](https://discuss.pytorch.org/t/how-to-get-cell-states-for-each-timestep-from-nn-lstm/10907/5).\n\ncc @zou3519"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nWindows support for distributed training (multiple GPUs on the same host)\r\n\r\n## Motivation\r\n\r\nI use distributed training with Pytorch on Linux and it is really easy and works well. I would like the same for Windows. I work alot with images and I find it easier to debug and see what I am doing in general on Windows and it would be great if I didn't have to dualboot to Linux when I want to multi-gpu train my models.\r\n\r\n## Pitch\r\n\r\nBeing able to use distributed training on Windows. I know it wasn't requested much by users, but lets have this feature request open and see if others would like it.\r\n\r\n## Alternatives\r\n\r\nLinux\r\n\r\n## Additional context\r\n\r\n-\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @peterjc123"},{"labels":["enhancement",null,null],"text":"In my opinion, the pytorch needs faster data loader via GPU, like [this project](https://github.com/tanglang96/DataLoaders_DALI). Please consider this feature. Thank you!\n\ncc @SsnL"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nA number of optimization and performance tuning for DLRM on CPU\r\n\r\n## Motivation\r\n\r\nRecommendation systems are one of the most common DL workloads in the cloud or enterprise server room. Very often the recommendation system burns most compute cycles in the data center among all DL workload.  DLRM is a state-of-the-art deep learning recommendation model which is composed of compute intensive MLP layers and memory intensive and capacity limited embedding layers.  Due to the memory capacity, people have found CPU's large DRAM capacity helpful training large DLRM models with very large embedding tables. \r\n\r\nSince the introdution of DLRM workload, Intel Pytorch team has been working to improve the performance using the configuration described in the DLRM paper. The work has generated ~90x performance improvement for DLRM trainning workload on CPU. This RFC summarizes the work we have done on the DLRM optimization and performance tuning, including a number code improvements and command option BKMs.   \r\n\r\n1. Parallelize the Embedding_bag. The embedding_bag operation contains a table look up and reduction operation. We parallelize the index_select_add  and uses caffe2's Embedding look up function caffe2::EmbeddingLookupIdx(). https://github.com/pytorch/pytorch/pull/24385  Jianyu further improve this implementation by adding a new parameter into to EmbeddingBag API  https://github.com/pytorch/pytorch/pull/27477\r\n2. Optimize the backward path of embedding_bag operation.  We parallelized the backward embedding_bag specialized for float-point tensor with SUM reduction operation with continuous stride.  This greatly improves the performance by avoid calling the original code which involves offset2bag(), cumsum(), index_select(), index_add(), and _embedding_bag_sparse_backward(). https://github.com/pytorch/pytorch/pull/27804\r\n3. Optimize the gradient update. For the non-coalesced hybrid sparse tensor input, the add_out_dense_sparse() function originally does coalescing involves an expensive sort and merge operation. We parallelized gradient update by using fine-grain locked critical sections which adds the non-coalesced hybrid sparse tensor directly to the dense weight tensor. This bring more than 100x improvement on gradient update and 20x performance improvement for whole DLRM benchmark.https://github.com/pytorch/pytorch/pull/23057Â  \r\n4. Parallelize the index_put. index_put is called by the backward of Zflat = Z[:, li, lj] in interaction. We optimize the accumulation path by using atomic add float to parallelize it. This improved about 10x on index_put accumulate operation in DLRM. https://github.com/pytorch/pytorch/pull/29705\r\n5. Parallelize the cat operation. On the contiguous path, we pre-calculate the cat offset and then perform cat copy in parallel. This gives about 8x improvement on cat operation in DLRM.https://github.com/pytorch/pytorch/pull/30806\r\n6. Parallelize Index_select. Index_select is used in the backward path of embedding_bag operation. The current implementation is in th module and only parallelize for contiguous input. We found that for DLRM, the input tensor could be non-contiguous, and the implementation go to a serialized version. We optimize the non-contiguous input and re-implement the operation in Aten library using parallel_for.  https://github.com/pytorch/pytorch/pull/30598\r\n7. Uses all threads for all openmp tasks. We found that the best performance is achieve when all the threads running for all parallel_for tasks(). Having some light operation running at few threads doesnâ€™t help the performance. https://github.com/pytorch/pytorch/issues/30803\t\r\n\r\nTo further tune  the performance, you may consider the following command line options and software configuration. \r\n1. Export KMP_BLOCKTIME=1   KMP_BLOCKTIME sets the time that an OPENMP worker thread should wait ,after completing the execution of a parallel region, before sleeping. The default time is 200ms and can hurt performance. Setting to 0 is not good also since we don't want to put worker thread to sleep too often.  \r\n\r\n2. Export KMP_AFFINITY=\"granularity=fine,compact,1,0\"  \r\nKMP_AFFINITY binds OpenMP threads to physical processing units.Â  In this setting, The OpenMP* threadÂ n+1 is bound to a different physical core to OpenMP* threadÂ n.Â \r\n\t\r\n3. Use je_malloc().  jemalloc is a general purpose malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support. We found this helps performance, mostly due to the reduction of calling OS's zero_pages() and better cache locality. \r\n\t\r\n\r\n\r\n\n\ncc @VitalyFedyunin @ngimel @mruberry"},{"labels":["enhancement",null,null,null],"text":"I see that 2D align-pooling has been implemented, but there is no 3D one. I do target detection of 3D voxels. The input data format is (batch, channel, depth, height, w). I customized a 3D align-pooling Op.  However, there is a big problem in the deployment. The custom OP needs to be registered in the torchscript. In this way, the model can be exported normally, and finally the model can be loaded on the C + + side.I tried for months, but it didn't work.  I hope you can provide 3D align-pooling layer directly. Pray.\n\ncc @fmassa"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nA lint pass to help user detect any violation of the immutability of the models during inference time.\r\n\r\n## Motivation\r\nWe assume that in most of the cases, the model should be stateless during inference, which means the model itself is thread-safe. However, user may update the parameters/attributes accidentally. We need some way to help users identify such cases.\r\n\r\n## Pitch\r\nA lint pass for TorchScript models (maybe with some annotation to whitelist something)\r\n\r\n## Additional context\r\nThis may help users detect problems at early stage.\r\n\r\n\r\ncc: @suo @dzhulgakov \r\n\n\ncc @suo"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nCommon lookup of generic types across full script and mobile parsers\r\n\r\n## Motivation\r\n\r\nTo maximize the codes that can be shared between the two parsers.\r\n\r\n## Pitch\r\n\r\nAs in the title. Build the lookup of generic types (especially Dict, Tuple, List and other composing types) and make it shared by both of the parsers. \r\n\r\n## Alternatives\r\n\r\n## Additional context\r\n\r\nIn future the type part of script parsers can be pulled out so that it can be directly used by both full script and mobile interpreter. "},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nIntroduce a torch.max() function that will allow the use of tuples when specifying the dimension over which to evaluate the maximum values in a tensor.\r\n\r\n## Motivation\r\n\r\nNumpy supports tuples, making this a simple one line problem:\r\n`max_vals = array.max(axis=(1,2,3))`\r\n\r\nWhereas, with PyTorch, one must use a more bloated approach:\r\n`max_vals = tensor.max(1)[0].max(1)[0].max(1)[0]`\r\n\r\n\r\n## Pitch\r\n\r\nThe fact we have to index the output of `torch.max()` operations to get the maximum values is actually a blessing in disguise as (for up to 2D inputs) you can see what indices the maximum values are located at. If we want to preserve this feature then perhaps an alternative function called torch.maxNDim() could be created for the integration of tuple dimensions without the functionality of returning the indices of those maximum values\r\n\r\n## Alternatives\r\n\r\nAlternatively, updating the current `torch.max()` function such that the second returned tensor gives an ND set of indices for the location of maximum values in an ND tensor would be fantastic. e.g. for getting the indices of the maximum values in a 3D tensor along the third axis, one can imagine you would return a 2D array of rows and column indices.\r\n\r\n## Additional context\r\n\r\nHere is some example code to show the current issue more clearly:\r\n\r\n```\r\n### Numpy version of the problem\r\n\r\nimport numpy as np\r\n\r\na = np.random.uniform(0,1,(5,1,3,3))\r\nmax_vals = a.max(axis=(1,2,3)) ### Clear and concise\r\n\r\n### PyTorch version of the problem (currently)\r\n\r\nimport torch \r\n\r\nat = torch.tensor(a)\r\nmax_vals = at.max(1)[0].max(1)[0].max(1)[0] ### Bloated \r\n```\r\n\r\nAdditionally, this functionality could be extended for common numerical operations such as `torch.min()`, `torch.std()`, etc. \r\n\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nI noticed that torch.save() has already saved the model definition file. However, a model still needs to be defined in advance when loading. It is inconvenient to the user-defined  model structure. Since the relative paths must be consistent.\r\n\r\n## Motivation\r\nI always keep model in one place and often loaded failed.\r\n\r\n## Pitch\r\ntorch.load can save forward process.\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nLet [`index_select`](https://pytorch.org/docs/stable/torch.html#torch.index_select) work with a multidimensional `index`.\r\n\r\n## Motivation\r\n\r\nIndex the input tensor along a given dimension using the entries in a multidimensional array of indices. For example,\r\n```\r\na = b.index_select(-1, c)\r\n```\r\nshould mean\r\n```\r\na[i, j, k] = b[i, j, k, c[i, j, k]]\r\n```\r\n\r\n## Alternatives\r\n\r\n**Option 1:**\r\n```\r\na = b.gather(-1, c[..., None])[..., 0]\r\n```\r\n\r\n**Option 2:** Create a meshgrid with the indices `i,j,k,...`. Then create an empty array `a` of the appropriate size. Then assign to `a` as follows:\r\n```\r\na[i, j, k] = b[i, j, k, c[i, j, k]]\r\n```"},{"labels":["enhancement",null,null],"text":"The current Python RRef API does not expose RRefId and ForkId information, as those are internal implementation details that user code should not worry about. However, without these info, it's hard to debug if there are any RRefs that did not get garbage collected in time and leaking memory. \r\n\r\n\r\n### Proposal\r\n\r\n1. Expose RRefId and ForkId APIs on RRef\r\n2. Expose an API on RRefContext to return living OwnerRRefs and UserRRefs, and also show which worker is holding on to each UserRRef.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nMemory-Efficient Adam Optimizer \r\n- Implement compressed, adaptive optimizer for training large-scale transformer models\r\n\r\n## Motivation\r\n\r\nYou can get out-of-memory issues when training large-scale transformer models (e.g. BERT, GPT-2, RoBERTa). Since the models are so large, adaptive optimizers (e.g. Adam) can take up too much space. A memory-efficient adaptive optimizer would complement activation checkpointing and mixed-precision training. \r\n\r\nFor example, you cannot train a GPT-2 774M parameter model using Nvidia V100 16 GB cards with the standard Adam optimizer, activation checkpointing, and FP16 mixed precision training. \r\n\r\nTensorflow already has memory-efficient optimizers. See [SM3](https://github.com/google-research/google-research/tree/master/sm3).\r\n\r\n## Pitch\r\n[Count-Sketch Optimizers](https://github.com/rdspring1/Count-Sketch-Optimizers) is a compressed adaptive optimizer for PyTorch. It includes results on successfully training BERT-Large models.\r\n\r\nUsage Examples:\r\n**Original Adam Optimizer**\r\n```\r\nimport torch.optim.Adam as Adam\r\noptimizer = Adam(model.parameters(), betas=(0.9, 0.999))\r\n```\r\n**Compressed RMSprop Optimizer**\r\n```\r\nimport torch.optim.compressed_rmsprop as cRMSprop\r\noptimizer = cRMSprop(model.parameters())\r\n```\n\ncc @vincentqb"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nofficial Adafactor optimizer\r\n## Motivation\r\nan efficient optimizer Adafactor is currently widely used in some big models, it saves a lot of memory due to its sub-linear running average of the gradient, sometimes result in a sigificant memory footprint reduce and larger batch size.\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @vincentqb"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nI am interested in contributing an [exponentiated gradient descent optimizer](https://ttic.uchicago.edu/~tewari/lectures/lecture4.pdf)  to pytorch.\r\n\r\n## Motivation\r\n\r\nOptimizing variables / parameters with a simplex constraint on the parameters.\r\n\r\n## Pitch\r\n\r\nContribute a new optimizer\r\n\r\n## Alternatives\r\n\r\nProjected gradient descent.\r\n\r\n\n\ncc @vincentqb"},{"labels":["enhancement",null,null,null],"text":"Not only C + + interface on IOS\r\nPython may also call torch on IOS\r\nThis can be used for demonstration or introduction to basic learning\r\n\r\nI made some changes when compiling. At present, most of the examples are available through\r\nIt's just that I don't know if pytorch is compatible with this pattern\r\n\r\nDownload APP:\r\n- US https://apps.apple.com/us/app/id1471351733\r\n- CN  https://apps.apple.com/cn/app/id1471351733\r\n\r\nScreenshots:\r\n- ![](https://github.com/goodclass/PythonAI/raw/master/image/torch1.jpg)\r\n- ![](https://github.com/goodclass/PythonAI/raw/master/image/torch2.jpg)\n\ncc @yf225"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nAdd a function in distributed package to uninitialize the default process group.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nWe are trying to make a fault-tolerant distributed training mode for pytorch. Or even more generally, the number of workers can be adjusted during the training process without checkpoint. That asks us to rebuild the default process group, which might need to come after the existing process group uninitialized.\r\n\r\nWe tested with simple NCCL ops and found that creating a second NCCL communicator results an error, which can be remedied by calling [`ncclCommAbort`](https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/api/comms.html#ncclcommabort).\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nAfter the program realized a loss or join of a worker, the uninitialization function will be called to nullify the existing default process group and function `torch.distributed.init_process_group` will be called for again with a new configuration to create a new default process group. \r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nThis RFC is to upgrade MKL-DNN 0.20.1 to DNNL 1.1. DNNL is MKL-DNN's new name.  The major release DNNL 1.0 introduced a number of new features, and DNN 1.1 continues improvement on performance, quality, and usability. Below is an incomplete list, please refer to https://github.com/intel/mkl-dnn/releases for a full description.   \r\n\r\n    DNNL 1.0 added SGEMM copy-based kernels for Intel SSE 4.1, Intel AVX, Intel AVX 2 and Intel AVX 512 architectures. With this optimization Intel MKL-DNNâ€™ JIT SGEMM implementation achieves comparable performance to Intel MKL.\r\n\r\n    DNNL 1.0 introduced bfloat16 training and inference support in reorders, (de-)convolution, pooling, batch normalization, local response normalization, eltwise, inner product, shuffle, sum, and concat. The implementation relies on new instructions targeting future Intel Xeon Scalable processor (codename Cooper Lake). On Intel Xeon processors with Intel AVX512 support bfloat16 arithmetic is emulated.\r\n\r\n    DNNL 1.0 introduced Intel Processor Graphics support covering fp16 and fp32 inference, and fp32 training. Intel MKL-DNN relies on OpenCL* runtime to execute computations on Intel Processor Graphics and provides interoperability with userâ€™s OpenCL code.\r\n\r\n    DNNL 1.1 includes (experimental) IntroducedÂ caching mechanismÂ that reduces primitive creation time for repeated primitive creation. The functionality is disabled by default and has to be enabled in compile time.\r\n\r\n## Additional context\r\n\r\nDNNL 1.0 comes with a new API design, but the most change is absorbed by ideep, the DNNL integration layer.  Below is a list of changes to Pytorch and Caffe2 to enable the DNNL 1.1. \r\n\r\n\t\tâ—‹ Build option change with new name (OMP:COMP vs. OMP)\r\n\t\tâ—‹ Implement ideep op using new DNNL operation (mul, add)\r\n\t\tâ—‹ Enable DNNL built-in primitive cache, remove hashing key to primitive cache in caffe2  \r\n\t\tâ—‹ change DNNL format representation (no explicit format tag like nchw). \r\n\t\tâ—‹ Repalce passing allocator as op template with registering global allocator \r\n\t\tâ—‹ Tensor construction code clean-up\r\n\t\tâ—‹ deconv IOHW workaround cleanup\r\n\t\tâ—‹ Simplify group conv interface\r\n\r\nDNNL v1.1 PyTorch upgrade patch contains 3 components: \r\n\t1. Upgrade underlying DNNL (original MKL-DNN) v0.21.1 to DNNL v1.1\r\n\t2. Upgrade DNNL integration layer (iDeep)\r\n        3. Change build system and update PyTorch/Caffe2 integration\r\n\n\ncc @VitalyFedyunin"},{"labels":["enhancement",null,null],"text":"I'm doing some manual inplace tensor manipulation in custom autograd functions. I have to work with `.data` directly because otherwise version tracking errors out. It would be nice to allow save/restore tensor versions when it's needed. Currently `tensor._version` is not writable. (related https://github.com/pytorch/pytorch/issues/23756, cc @ezyang @SsnL @albanD @zou3519 @gqchen)"},{"labels":[null,"enhancement",null,null],"text":"## ðŸš€ Feature\r\nPer @pritamdamania87's comment on https://github.com/pytorch/pytorch/pull/30020, we can have duplicated code in the shutdown implementation of different RPC backends. We should unify this code in RpcAgent's shutdown function, and then each backend can have a `shutdownImpl` for their custom shutdown implementation details.\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nProposed in: https://github.com/pytorch/pytorch/pull/29743\r\n\r\nTensorIterator is performance-critical but complicated. Changing some parts of it might\r\ncause hard-to-realize regression on the other parts. It would be very convenient to have a script that benchmark all the case we could imagine:\r\n\r\n**Memory layout**: trivial 1D, (contiguous dim, non-contiguous dims)\r\n**Problem size**: small, medium, large\r\n**Type of computation**: unary ops, binary ops, compare ops, reduction\r\n**Data type**: all dtypes with/without promotion\r\n**Inplace**: True, False\r\n**Device**: CPU, CUDA\r\n\r\n## The designed usage of the script could be:\r\n\r\n**Step 1**:\r\nInstall a PyTorch build of the master branch, and run\r\n```\r\npython main.py benchmark baseline.json\r\n```\r\n\r\n**Step 2**:\r\nGo to your branch, build install and run\r\n```\r\npython main.py benchmark new.json\r\n```\r\n\r\n**Step 3**:\r\nRun the following command to get the report:\r\n```\r\npython main.py compare baseline.json new.json report.html\r\n```\r\n\r\n\r\n\r\ncc: @ngimel @VitalyFedyunin \r\ncc: @csarofeen \n\ncc @VitalyFedyunin @ngimel @mruberry"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n`torch.einsum` errors when trying to broadcast a size-1 dimension. Both Numpy and Tensorflow support einsum broadcasting:\r\n\r\nNumpy:\r\n```python\r\nimport numpy as np\r\nX = np.random.randn(4, 2)\r\nY = np.random.randn(2)\r\nZ = np.einsum(\"ab,b->ab\", X, Y)\r\n\r\nY2 = Y.reshape(1, 2)\r\nZ2 = np.einsum(\"ab,ab->ab\", X, Y2)\r\nassert np.all(Z == Z2)\r\n```\r\n\r\nTensorflow:\r\n```python\r\nimport tensorflow as tf\r\nX = tf.random.normal((4, 2))\r\nY = tf.random.normal((2,))\r\nZ = tf.einsum(\"ab,b->ab\", X, Y)\r\n\r\nY2 = tf.reshape(Y, (1, 2))\r\nZ2 = tf.einsum(\"ab,ab->ab\", X, Y2)\r\nassert tf.reduce_all(Z == Z2)\r\n```\r\n\r\nPytorch:\r\n```python\r\nimport torch\r\nX = torch.randn(4, 2)\r\nY = torch.randn(2)\r\nZ = torch.einsum(\"ab,b->ab\", X, Y)\r\n\r\nY2 = Y.reshape(1, 2)\r\nZ2 = torch.einsum(\"ab,ab->ab\", X, Y2)\r\n```\r\ngets\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-12-31a4a0f0f2d6> in <module>\r\n      5 \r\n      6 Y2 = Y.reshape(1, 2)\r\n----> 7 Z2 = torch.einsum(\"ab,ab->ab\", X, Y2)\r\n\r\n~/anaconda3/envs/gpu/lib/python3.7/site-packages/torch/functional.py in einsum(equation, *operands)\r\n    199         # the old interface of passing the operands as one list argument\r\n    200         operands = operands[0]\r\n--> 201     return torch._C._VariableFunctions.einsum(equation, operands)\r\n    202 \r\n    203 \r\n\r\nRuntimeError: size of dimension does not match previous size, operand 1, dim 0\r\n```\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nIntel have announced their new oneAPI, which provides hardware acceleration support on a diverse range of underlying devices. Supporting it would give a huge boost to performance on a wide range of devices that do not contain an NvidiaÂ® GPU.\r\n\r\n## Motivation\r\nNvidiaÂ® GPUs are expensive, and I do not currently own one. I do however own an AMD Radeon and, of course, Intel Integrated Graphics. It's really frustrating that I'm limited to slow CPU-based training when I have the hardware _right in front of me_ that _could_ be used to make it go faster, but I _can't_ because PyTorch doesn't support it.\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\n[Intel Announced](https://arstechnica.com/gadgets/2019/11/intels-oneapi-aims-to-unify-ai-code-efforts-across-disparate-hardware/) their new oneAPI to abstract the complexity of hardware acceleration away from higher-level libraries and code. I propose that PyTorch implements support for this, to bring hardware-accelerated GPU training to everyone who doesn't own an NvidiaÂ® GPU.\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\nImplementing support for OpenCL was discussed in #488, but for some reason that I don't quite understand the suggestion was dismissed. I don't know the PyTorch internals, but by contrast it looks like implementing support for oneAPI would bring a similar benefit to implementing OpenCL, while supporting more different devices.\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\nI can't afford to buy a new Â£1000 device just to get an NvidiaÂ® GPU - and I suspect that I'm not the only one here. I find it supremely frustrating that the device I have now contains the hardware necessary to greatly accelerate machine-learning workloads without being able to use it. Had I know a few years ago that I'd be getting into machine learning and AI, I might have bought a different device.\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @ngimel"},{"labels":["enhancement",null,null],"text":"I'd like to discuss here the implementation of the multivariate normal cumulative distribution function (CDF), as the following code\r\n```\r\nfrom torch.distributions import MultivariateNormal\r\nmvn = MultivariateNormal(torch.zeros(2), torch.eye(2))\r\nmvn.cdf(torch.ones(3))\r\n```\r\nraises a _NotImplementedError_.\r\n\r\nThe cdf of the mvn has no closed-form solution, and is implemented [in scipy](https://github.com/scipy/scipy/blob/master/scipy/stats/mvndst.f) (Fortran code) and [in Matlab](https://fr.mathworks.com/help/stats/mvncdf.html) based on the work by Genz (paper [here](http://www.math.wsu.edu/faculty/genz/papers/mvn.pdf)).\r\n\r\nIf needed, we can discuss the derivatives of the cdf w.r.t. the location and the correlation coefficient here. There exist formulas in the bivariate case but not in the general multivariate case (at least to my knowledge).\r\n\r\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw "},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nImplement GPU INT8 matrix multiplication in PyTorch.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nIn order to save time by using less GPU memory per data (hence, being able to use bigger batch sizes), I think it would be nice to be able to use int8 when representing the data, for example, for combinatorial problems, since the combinatorial space is vast.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nI'd like to be able to perform a matrix multiplication in GPU when using dtype = torch.uint8\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\nA current alternative is to use float32 or float16 dtypes\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nAs far as I'm aware, INT8 GPU matrix multiplication is already supported for CUDA and cuBLAS, but I'm not sure if it is competitive with using half precision.\n\ncc @ngimel"},{"labels":["enhancement",null,null,null],"text":"## ðŸ› Bug\r\nWhen trying to use `torch.distributions.normal.Normal` in a JIT function, you get `torch.jit.frontend.NotSupportedError: comprehension ifs not supported yet`.\r\n\r\n## To Reproduce\r\nI am writing a function and calling `torch.distributions.normal.Normal.sample()`.\r\n\r\nSteps to reproduce the behavior:\r\n1. Call `torch.distributions.normal.Normal` in a function tagged with `@torch.jit.script`.\r\n\r\n```\r\ntorch.distributions.normal.Normal(torch.mean(waveform), 1).sample()\r\n```\r\n\r\n## Expected behavior\r\n\r\nJIT to compile funciton\r\n\r\n## Traceback\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test/test_functional.py\", line 6, in <module>\r\n    import torchaudio\r\n  File \"/private/home/cwillycs/audio/torchaudio/__init__.py\", line 7, in <module>\r\n    from torchaudio import transforms, datasets, kaldi_io, sox_effects, compliance, _docs\r\n  File \"/private/home/cwillycs/audio/torchaudio/transforms.py\", line 6, in <module>\r\n    from . import functional as F\r\n  File \"/private/home/cwillycs/audio/torchaudio/functional.py\", line 904, in <module>\r\n    def dither(waveform, probability_density_function=\"TPDF\", noise_shaping=False, ns_filter=\"\"):\r\n  File \"/private/home/cwillycs/.local/lib/python2.7/site-packages/torch/jit/__init__.py\", line 1226, in script\r\n    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))\r\n  File \"/private/home/cwillycs/.local/lib/python2.7/site-packages/torch/jit/__init__.py\", line 1075, in _compile_and_register_class\r\n    ast = get_jit_class_def(obj, obj.__name__)\r\n  File \"/private/home/cwillycs/.local/lib/python2.7/site-packages/torch/jit/frontend.py\", line 148, in get_jit_class_def\r\n    self_name=self_name) for method in methods]\r\n  File \"/private/home/cwillycs/.local/lib/python2.7/site-packages/torch/jit/frontend.py\", line 169, in get_jit_def\r\n    return build_def(ctx, py_ast.body[0], type_line, self_name)\r\n  File \"/private/home/cwillycs/.local/lib/python2.7/site-packages/torch/jit/frontend.py\", line 209, in build_def\r\n    build_stmts(ctx, body))\r\n  File \"/private/home/cwillycs/.local/lib/python2.7/site-packages/torch/jit/frontend.py\", line 127, in build_stmts\r\n    stmts = [build_stmt(ctx, s) for s in stmts]\r\n  File \"/private/home/cwillycs/.local/lib/python2.7/site-packages/torch/jit/frontend.py\", line 185, in __call__\r\n    return method(ctx, node)\r\n  File \"/private/home/cwillycs/.local/lib/python2.7/site-packages/torch/jit/frontend.py\", line 283, in build_Assign\r\n    rhs = build_expr(ctx, stmt.value)\r\n  File \"/private/home/cwillycs/.local/lib/python2.7/site-packages/torch/jit/frontend.py\", line 185, in __call__\r\n    return method(ctx, node)\r\n  File \"/private/home/cwillycs/.local/lib/python2.7/site-packages/torch/jit/frontend.py\", line 681, in build_ListComp\r\n    raise NotSupportedError(r, \"comprehension ifs not supported yet\")\r\ntorch.jit.frontend.NotSupportedError: comprehension ifs not supported yet:\r\nat /private/home/cwillycs/.local/lib/python2.7/site-packages/torch/distributions/distribution.py:263:23\r\n    def __repr__(self):\r\n        param_names = [k for k, _ in self.arg_constraints.items() if k in self.__dict__]\r\n                        <--- HERE\r\n        args_string = ', '.join(['{}: {}'.format(p, self.__dict__[p]\r\n                                if self.__dict__[p].numel() == 1\r\n                                else self.__dict__[p].size()) for p in param_names])\r\n        return self.__class__.__name__ + '(' + args_string + ')'\r\n'Normal' is being compiled since it was called from 'dither'\r\nat /private/home/cwillycs/audio/torchaudio/functional.py:952:8\r\n        random_tensor = int(torch.randint(wave_size, [1, ]).item())\r\n        RPDF_dither = waveform[0][random_tensor] - 0.5\r\n\r\n        signal_scaled_RPDF_dithered = signal_scaled + RPDF_dither\r\n        quantised_signal_scaled_RPDF_dithered = torch.round(signal_scaled_RPDF_dithered)\r\n        quantised_signal_RPDF_dithered = quantised_signal_scaled_RPDF_dithered / down_scaling\r\n\r\n        dithered = quantised_signal_RPDF_dithered\r\n    elif (probability_density_function == \"GPDF\"):\r\n        gaussian_dither = torch.distributions.normal.Normal(torch.mean(waveform), 1).sample()\r\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n\r\n        signal_scaled_gaussian_dithered = signal_scaled + gaussian_dither\r\n        quantised_signal_scaled_gaussian_dithered = torch.round(signal_scaled_gaussian_dithered)\r\n        quantised_signal_gaussian_dithered = quantised_signal_scaled_gaussian_dithered / down_scaling\r\n\r\n        dithered = quantised_signal_gaussian_dithered\r\n    else:\r\n        TPDF_dither = torch.bartlett_window(wave_size + 1)\r\n```\r\n\r\n## Additional context\r\n\r\nRelevant to [PR #319](https://github.com/pytorch/audio/pull/319)\n\ncc @suo"},{"labels":["enhancement",null,null],"text":"## ðŸ› Bug\r\nUnfold can't be exported.\r\nsimilar issue #22862\r\n\r\n## To Reproduce\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nimport numpy as np\r\n\r\nclass Demo(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.unfold = nn.Unfold(kernel_size=3, padding=1)\r\n\r\n    def forward(self, x):\r\n        n,c,h,w = x.shape\r\n        unfold_x = self.unfold(x).view(n,-1,h,w)\r\n        return unfold_x\r\n\r\nif __name__ == \"__main__\":\r\n    input_tensor = torch.zeros((1,16,100,100))\r\n    demo = Demo()\r\n    out = demo(input_tensor)\r\n    torch.onnx.export(demo, input_tensor, \"debug.onnx\", verbose=True,\r\n                        input_names=['data'],\r\n                        opset_version=11,\r\n                        # do_constant_folding=True,\r\n                        dynamic_axes={'data':{0:'batch', 2:'width', 3:'height'},\r\n                        })\r\n```\r\n## Expected behavior\r\n```\r\nFile \"/home/godricly/python36/lib/python3.6/site-packages/torch/onnx/symbolic_registry.py\", line 91, in get_registered_op\r\n    return _registry[(domain, version)][opname]\r\nKeyError: 'im2col'\r\n```\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.4.0.dev20191103\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.15.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 430.50\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.3\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] torch==1.4.0.dev20191103\r\n[pip3] torchvision==0.4.2\r\n[conda] Could not collect\n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof"},{"labels":["enhancement",null,null],"text":"```\r\ntorch.jit.frontend.NotSupportedError: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:\r\nat /usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py:138:32\r\n    def forward(self, *inputs, **kwargs):\r\n                                ~~~~~~~ <--- HERE\r\n        if not self.device_ids:\r\n            return self.module(*inputs, **kwargs)\r\n\r\n        for t in chain(self.module.parameters(), self.module.buffers()):\r\n            if t.device != self.src_device_obj:\r\n                raise RuntimeError(\"module must have its parameters and buffers \"\r\n                                   \"on device {} (device_ids[0]) but found one of \"\r\n                                   \"them on device: {}\".format(self.src_device_obj, t.device))\r\n```\r\nAs you can see,this varargs is in torch lib itself:\r\n`/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py:138:32`\n\ncc @suo"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nBeing able to specify where the result of an op should be written to, rather than having ops allocating their own buffers.\r\n\r\n## Motivation\r\nI'm working with context blocks, as defined in https://arxiv.org/pdf/1511.07122.pdf\r\n\r\n```python\r\nclass MultiConvolution(torch.nn.modules.Module):\r\n    def __init__(self, inputChannels, outputChannels, stride, padding = 'zeros'):\r\n        super(MultiConvolution, self).__init__()\r\n        self.padding = padding\r\n        self.a = torch.nn.Conv2d(inputChannels, outputChannels // 2, 3, stride=stride, padding=0, dilation=1)\r\n        self.b = torch.nn.Conv2d(inputChannels, outputChannels // 4, 3, stride=stride, padding=0, dilation=2)\r\n        self.c = torch.nn.Conv2d(inputChannels, outputChannels // 8, 3, stride=stride, padding=0, dilation=4)\r\n        self.d = torch.nn.Conv2d(inputChannels, outputChannels // 8, 3, stride=stride, padding=0, dilation=8)\r\n\r\n    def pad(self, x, p):\r\n        if self.padding == 'circular':\r\n            return padding.circular_pad(x, p)\r\n        return padding.reflection_pad(x, p)\r\n\r\n    def forward(self, x):\r\n        a = self.a(self.pad(x, 1*2))\r\n        b = self.b(self.pad(x, 2*2))\r\n        c = self.c(self.pad(x, 4*2))\r\n        d = self.d(self.pad(x, 8*2))\r\n        return torch.cat([a, b, c, d], 1)\r\n```\r\nIn this situation, it seems that PyTorch allocate four buffers for the output of a, b, c, and d.\r\nAnd then a fifth one for the result of the concatenation.\r\n\r\n## Pitch\r\n\r\nI'd like a bit more control, with the option of manually specifying where an op writes it's output.\r\nSomething like :\r\n```python\r\nclass MultiConvolution(torch.nn.modules.Module):\r\n    def __init__(self, inputChannels, outputChannels, stride, padding = 'zeros'):\r\n        super(MultiConvolution, self).__init__()\r\n        self.outputChannels = outputChannels\r\n        self.padding = padding\r\n        self.a = torch.nn.Conv2d(inputChannels, outputChannels // 2, 3, stride=stride, padding=0, dilation=1)\r\n        self.b = torch.nn.Conv2d(inputChannels, outputChannels // 4, 3, stride=stride, padding=0, dilation=2)\r\n        self.c = torch.nn.Conv2d(inputChannels, outputChannels // 8, 3, stride=stride, padding=0, dilation=4)\r\n        self.d = torch.nn.Conv2d(inputChannels, outputChannels // 8, 3, stride=stride, padding=0, dilation=8)\r\n\r\n    def pad(self, x, p):\r\n        if self.padding == 'circular':\r\n            return padding.circular_pad(x, p)\r\n        return padding.reflection_pad(x, p)\r\n\r\n    def forward(self, x):\r\n        ret = torch.Tensor(x.shape[0], self.outputChannels, x.shape[2], x.shape[3]) # One large memory allocation here\r\n        a = self.a(self.pad(x, 1*2), outputBuffer=ret[:,0:outputChannels // 2]) # No need for alloc\r\n        b = self.b(self.pad(x, 2*2), outputBuffer=ret[:,outputChannels // 2:outputChannels // 4 * 3]) # No need for alloc\r\n        c = self.c(self.pad(x, 4*2), outputBuffer=ret[:,outputChannels // 4 * 3:outputChannels // 8 * 7]) # No need for alloc\r\n        d = self.d(self.pad(x, 8*2), outputBuffer=ret[:,outputChannels // 8 * 7:]) # No need for alloc\r\n        return ret\r\n```\r\n\r\n\r\n## Alternatives\r\nSee pitch\r\n\r\n## Additional context\r\nSee linked paper\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168"},{"labels":[null,"enhancement",null,null,null],"text":"## ðŸš€ Feature -N dimensional histogram function in pytorch as in  numpy\r\n \r\nIt  would be great to have support for multi-dimensional histograms  with similar functionality  as in the \r\n\r\n```numpy.histogramdd```\r\n\r\nSee discussion within https://discuss.pytorch.org/t/n-dimensional-histogram-function-in-pytorch-cpu-gpu/59888 \r\n\r\n## Motivation\r\nWe are working on interactive visualization of multi-dimensional data\r\nUsually O(2-7) dimensions (still dense) Currently we are using numpy implementation for multidimensional histogram filling, slicing, summing:\r\n\r\n* creating NDimensional histogram once:\r\n```H, axis = np.histogramdd(inputArray.values, bins=bins, range=hRange)```\r\n* querying interactively - many times\r\n```y = np.sum(H[hSliceLocal], axis=axis)```\r\n\r\nWe would like to use PyTorch (or Numba) to get faster GPU implementation\r\n\r\n## Pitch\r\nI checked numpy implementation in https://github.com/numpy/numpy/blob/v1.17.0/numpy/lib/histograms.py#L935-L1113\r\nAt the end algorithm in (step 3) is using 1D histogramming  - ``` np.bincount```\r\nHowever, numpy implementation assume non uniform binning - to find bins they use internally ```np.searchsorted```.\r\n\r\n## Alternatives\r\n\r\nIn case searchsorted will be difficult to implement (I did not find it in pytorch= only external https://github.com/aliutkus/torchsearchsorted), uniform binning will be sufficient. \r\n\r\n\r\n## Additional context\r\n###  Current numpy Algorithm:\r\n\r\n0. Calculate bin edges if not provided \r\n1. Compute the bin number each sample falls into - assuming non uniform binning\r\n```np.searchsorted```\r\n2. Compute the sample indices in the flattened histogram matrix\r\n```np.ravel_multi_index```\r\n3. Compute the number of repetitions in xy and assign it to the flattened histmat.\r\n```np.bincount```\r\n4. Shape into a proper matrix\r\n```hist.reshape(nbin)```\r\n\r\n\n\ncc @mruberry @rgommers"},{"labels":["enhancement",null,null],"text":"Following up on https://github.com/pytorch/pytorch/issues/29028#issuecomment-548968891, there exists `torch.masked_fill_` but cannot be accessed through `out` argument, which makes writing generic (inplace / out-of-place) code less straighforward\r\n\r\nThis is just one example, it would be nice to make a summary table for all ops and see if they are consistent about out-argument (allows for generic code) and underscore-suffixed variants (nice semantics, but leads to method signature duplication)\r\n\r\nIt would be nice to have clear guidelines wrt inplace / out-variants across all PyTorch: tensor instance methods, functions, nn.functional functions, nn.Module classes. "},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nImplement the positive-definite-matrix-valued Wishart and inverse-Wishart distributions, currently missing from `torch.distributions`.\r\n\r\n## Motivation\r\n\r\nThese distributions often pop up in Bayesian analysis as the conjugate priors for the covariance or precision matrix of the multivariate Gaussian. It would be great to have an official, GPU-accelerated, `torch.distributions`-compatible implementation available to everyone. This feature already exists in [TensorFlow Probability](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Wishart), for example.\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\n\r\nI already have a working implementation I wrote for a personal project (involving Bayesian mixture models), extending [`ExponentialFamily`](https://pytorch.org/docs/stable/distributions.html#exponentialfamily). Other than the basic methods, it currently includes:\r\n- reparametrised sampling (`rsample`), based on some background research (some refs. below);\r\n- efficient computations using Cholesky factors;\r\n- unit tests for output shapes, for different combinations of batch and sample shapes;\r\n- unit tests for correctness of `log_prob` and `entropy` vs. [`scipy.stats.wishart`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wishart.html);\r\n- statistical tests show sample statistics (e.g. means, determinants) are indistinguishable vs. [`scipy.stats.wishart`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wishart.html).\r\n\r\nWould there be any interest in integrating this into the `torch.distributions` codebase? I'd be happy to make the necessary changes to harmonise style, documentation, and unit-test format.\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\nCurrently the way to use Wishart distributions in a PyTorch project would be to go through NumPy and use [`scipy.stats.wishart`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wishart.html). This has a couple of big limitations:\r\n- need to copy data between CPU and GPU;\r\n- no support for batched parameters or arguments (e.g. to `wishart.logpdf()`).\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## References\r\n- Sawyer, S. (2007). Wishart Distributions and Inverse-Wishart Sampling. https://www.math.wustl.edu/~sawyer/hmhandouts/Wishart.pdf\r\n- Anderson, T. W. (2003). An Introduction to Multivariate Statistical Analysis (3rd ed.). John Wiley & Sons, Inc.\r\n- Odell, P. L. & Feiveson, A. H. (1966). A Numerical Procedure to Generate a Sample Covariance Matrix. Journal of the American Statistical Association, 61(313):199-203.\r\n- Ku, Y.-C. & Blomfield, P. (2010). Generating Random Wishart Matrices with Fractional Degrees of Freedom in OX.\r\n\r\ncc `torch.distribution` maintainers @fritzo @neerajprad @alicanb @vishwakftw \n\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw"},{"labels":["enhancement",null,null],"text":"I'd like to read audio file by using a sox utility directly. Currently I can do it as:\r\n```python\r\nsignal = torch.from_numpy(np.frombuffer(subprocess.check_output(['sox', '-V0', audio_path, '-b', '16', '-e', 'signed', '--endian', 'little', '-r', str(sample_rate), '-c', '1', '-t', 'raw', '-'], dtype = np.int16))\r\n```\r\n\r\nIs there a way to reinterpret a torch byte tensor (or python's `bytearray(...)`) as another type without invoking first NumPy? (or a generic numpy.view(dtype) functionality) If not, I think it'd be quite useful (mainly for integral types, but also maybe for some integer bit-tricks on float32 tensors, which can hopefully be fused by jit)\r\n\r\nAlso reported in https://discuss.pytorch.org/t/reinterpret-pytorch-array-as-a-different-dtype/24256\r\n\r\nThis probably can also be helpful for conversions between complex <-> pair or real/imag."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\ntensor.stack outputs named tensors.\r\n\r\n## Motivation\r\n\r\nStack is one of those times when you are mushing together a bunch of tensors and getting the end result to match what you intended can be surprisingly tedious and finicky. Named tensors is a beautiful idea that provides names for each dimension.\r\n\r\n## Pitch\r\n\r\nThe input tensors should be named and the names should match in all the dimensions.\r\ne..g.\r\n```\r\ntest1 = torch.from_numpy(np.array([[4,1,5,3],[0,6,-1,3.]])).refine_names('H', 'W')\r\ntest2 = torch.from_numpy(np.array([[1,1,2,2],[1,1,3,8.]])).refine_names('H', 'W')\r\nnewstack = torch.stack((test1,test2), dim=0,  names=('B', 'H', 'W'))\r\n```\r\n'B' is the new dimension you are stacking, other dimensions should match by name.\r\n \r\n## Alternatives\r\n\r\nThe alternative is the current implementation where you get a clear error.\r\n`RuntimeError: stack is not yet supported with named tensors. Please drop names via `tensor = tensor.rename(None)`, call the op with an unnamed tensor, and set names on the result of the operation.`\r\n\r\n## Additional context\r\n\r\nWrite up about named tensors. [Named Tensors for Deep Learning](http://nlp.seas.harvard.edu/NamedTensor2)\n\ncc @zou3519"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nWhen calling `save_for_backward` inside a built-in or custom `torch.autograd.Function`, it would be nice to have the option to store these tensors on disk rather than in memory.\r\n\r\n## Motivation\r\n\r\nSome operations, such as matrix decompositions, require the storage of multiple intermediate tensors using the `save_for_backward(*tensors)` method each time they are called. In applications where these operations are called many times (e.g. tensor networks), and the matrix sizes are medium to large, the memory consumption of `autograd` can quickly become overbearing. In some cases, simply saving/loading these intermediate tensors to/from disk could be an ideal solution. If the `forward()` call of each decomposition is very time consuming, then the additional computation time incurred by using checkpointing techniques (to reduce memory usage in an alternative way) could far exceed the time it takes to simply load the tensors from disk.\r\n\r\n## Pitch\r\n\r\nAdding a kwarg to `save_for_backward`  that allows for disk storage would permit custom `Function`s to use this feature. Exposing this option to the user for built-in `Function`s seems like it might require a bit more careful design consideration. Ideally, one would be able to specify which function calls use disk storage and which use memory, so that `backward()` for operations on small tensors can still be evaluated efficiently with no I/O time.\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nAdd torch.nn.GELU as the module for GELU activation.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nCurrently we have a torch.nn.functional.gelu op but no corresponding module like we do for the other activations (e.g. nn.ReLU()).\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nA class that implements the following pattern:\r\n```\r\nx = nn.Parameter(torch.rand(4,5))\r\ny = f(x)\r\n# Use y here\r\n```\r\ncomputing `f` and its derivative only once per minibatch. Note that the value of `y` and the gradient of `x` do not depend on the elements of the minibatch.\r\n\r\n## Motivation\r\n### A useful example\r\nThe exponential of matrices maps the skew-symmetric matrices onto the orthogonal matrices with positive determinant. One way to perform optimization with orthogonal constraints is the following:\r\n```\r\nx = nn.Parameter(torch.rand(4,4))\r\naux = x.triu(diagonal=1)\r\naux = aux - aux.t()     # aux is a skew-symmetric matrix \r\ny = exp(aux)            # y is an orthogonal matrix\r\n```\r\n\r\n### Problems implementing this under the current API\r\nIf one implements this naÃ¯vely within a `forward` method, the exponential of matrices will be computed every time the `forward` function is called, even if  the value of `x` has not changed from one call to the next one (imagine this as parametrizing the kernel of an RNN).\r\n\r\nOne way of avoiding this is to have some kind of cache:\r\n```\r\ndef Parametrized(nn.Module):\r\n  def __init__(self, x): # x is a torch.Tensor here\r\n    self.x = nn.Parameter(x)\r\n    self._y_cached = self.register_buffer(f(x))\r\n    self.computed = True\r\n  @property\r\n  def y(self):\r\n    if not self.computed:\r\n      self._y_cached = f(x)\r\n      self.computed = True\r\n    return self._y_cached\r\n  def set_dirty(self):\r\n    self.computed = False\r\n```\r\n\r\nThis does the trick in most situations, but it still fails in the case when `self.computed ==  False` and we call `self.y` in a `torch.no_grad()` context. In that case, `self._y_cached` is updated, but `grad_fn` won't be computed, so we will get an error when calling `loss.backward()`. This can be solved modifying the if statement as:\r\n```\r\n    if not self.computed or (not self.y_cached.grad_fn and torch.is_grad_enabled):\r\n```\r\nTo make this implementation work, one still hast to set `self._y_cached.retain_grad()` to make it a leaf variable, and call the `set_dirty` function appropriately for every instance of `Parametrized` in the model.\r\n\r\nThis looks like a very convoluted solution that is full of nuances, like the implementation of most of caches.\r\n\r\n### The bigger picture\r\nOne can perform optimization on a manifold M (the orthogonal matrices, or the positive definite, the matrix with non-zero determinant, the Grassmannian...) through a surjective function f : R^{n x m} -> M just by parametrizing M in terms of a linear space. This class would allow for a rather direct implementation of optimization constrained to a manifold just by choosing an appropriate function. The exponential of matrices is just one example of this but, for example, we also have another widely used example as `L.mm(L.t())` where L is a lower-triangular matrix with positve elements on the diagonal. This function maps this set surjectively to the manifold of symmetric positive matrices.\r\n\r\nA simpler application would be to have a tensor that has entries in [-1, 1], parametrized through `tanh` or non-negative entries, through `relu`. In these cases, the improvement would not be very big in the computational side of things, but in how concise the model is, expressing better what one wants to do.\r\n\r\nOne could also think of having a few constraints implemented for a few manifolds, like some of those mentioned above. This way Pytorch would support optimization on manifolds without any major changes in the API.  \r\n\r\n## Pitch [EDITED]\r\nThe class `nn.Module` would have a new method `register_constrained_parameter` that could be used as follows:\r\n```\r\nlin = nn.Linear(10,20)\r\nlin.register_constrained_parameter(name=\"weight\", function=f, update=\"auto\")\r\n```\r\nWhen called, `register_constrained_parameter` would create an inner attribute of type `nn.Parameter` named `_{name}_unconstrained` and a `buffer` named `_{name}_cache`. When assigning to `lin.{name}`, this would be deviated through `nn.Module.__setattr__` to `lin._{name}_unconstrained`. The caching would be implemented in `nn.Module.__getattr__`.\r\n\r\n `update` is a flag that takes `\"auto\"` or `\"manual\"`. If `update == \"manual\"`, the user has to notify the tensor that has been updated like\r\n```\r\nlin.weight.updated()\r\n```\r\nThis has to be done after `optim.step()` has been called, but also if the user wants to modify the inner-variable manually (more on this below).\r\n\r\nIf the `update == \"auto\"`, then we will set an updated flag to `True` when the gradients with respect the parametrization are computed. This can easily be done with a `register_hook` on the tensor. This idea was proposed and discussed in #7313. This solution solves most practical use cases, when the user does not fiddle with the parameters manually during training, even less between the call to `loss.backwards()` and `optim.step()`.\r\nThe implementation `\"auto\"` would not work correctly by itself if `module.{name}` is called between these two. Another case would be the following:\r\n```\r\nclass MyModule(nn.Module):\r\n  def __init__(self, f):\r\n    self.W = nn.Parameter(torch.tensor(3,4))\r\n    # Register the constrained parameter\r\n    self.register_constrained_parameter(name=\"W\", function=f, update=\"auto\")\r\n    # Retrieve W\r\n    a = self.W \r\n    # update _W_unconstrained\r\n    torch.nn.init.eye_(self._W_unconstrained) \r\n```\r\nIn these cases the user would have to call the `module.W.update()` manually. after `self._W_unconstrained` has been updated.\r\n\r\nFurther details:\r\n- When using a parameter constrained by a function, the parameter will change shape as to that of the image of `f`. In other words, if `f` creates a constant vector from a real number, R -> R^n, and we use this `f` to constrain a parameter that is a real number called \"number\", then accessing `mymodule.number` would return a vector.\r\n\r\n- Assignments to the variable:\r\nI am not sure about the `__setattr__` part being deviated to `_{name}_unconstrained`, as it might not feel natural. This is a tricky one, as the user can edit the variable `module.{name}` in many ways (assignment, in-place operations, `module.{name}.data`...) and all these changes would not be reflected in the inner state of the module. Maybe this can be just put in the documentation as, at the end of the day, we are creating a buffer `_{name}_cache` and a buffer `_{name}_unconstrained`. The user should be aware of these, as they are a natural abstraction for a parametrization, and should access these if they want to reinitialise the variable or modify the cached buffer temporally. \r\nA safer way to go about this would be to raise an exception if the user tries to modify a constrained variable in any way (this might not be easy at all). On the other hand, if we are willing to go down this road, we might as well notify directly to all the tensors whenever they have been updated, and the trick with the `register_hook` described before would not be necessary.\r\n\r\nThis whole scheme is implemented following this approach in\r\nhttps://github.com/Lezcano/expRNN/blob/master/parametrization.py\r\nas part of a slightly more involved scheme to parametrize manifolds."},{"labels":["enhancement",null,null,null],"text":"torch.distributed.rpc uses future heavily and we are planning to make local autograd engine non blocking as well. \r\n\r\nThe plan is to move FutureMessage in torch.distributed.rpc to a more generic Future<T> that can be used for torch.distributed package and non blocking autograd first, then can be used for other module potentially as well. \r\n\r\nper @zdevito's suggestion, whole pyTorch codebase should have only one Future<T> implementation, we can instantiate Future<T> to Future<IValue>, make Future<IValue> to be an Ivalue and kill ivalue::Future eventually. \r\n\r\n@aazzolini also suggested to make torch::utils::Future<T> as a shared_ptr, as most of cases future is used as shared_ptr. \r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nIs it possible to reinitialize communication with the cuda drivers once they have been made? At the moment, `torch.cuda.init()` calls a lazy init function which sets up communication. Is it possible to close that communication and open it again? \r\n\r\n## Motivation\r\nI want to set the `CUDA_VISIBLE_DEVICES` env variable programmatically. This currently works if it is set before any calls to `torch.cuda.init`, but if any `torch.cuda` functions are called first, then the comms are open and it ignores the env variable.\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nPerhaps a function called `torch.cuda.close()` which calls `nvmlShutdown` and resets the `_initialized` flag. Then we could make a subsequent call to `torch.cuda.init()` to reopen communications.\r\n\r\n## Alternatives\r\nPerhaps a way to reset the device counts based on the CUDA_VISIBLE_DEVICES environment variable. \r\n"},{"labels":["enhancement",null],"text":"\r\n\r\n## ðŸš€ Feature\r\npytorch can be compiled in `mips64` arch successfully, and then utilize related API to develop :+1: \r\n\r\n## Motivation\r\nI'm working on compiling pytorch in mips64 arch, but it doesn't work\r\n\r\n```\r\n(pytorch-venv) [root@lx-app-1 build]# git branch\r\n* v1.3.0\r\n\r\n(pytorch-venv) [root@lx-app-1 build]# cmake -DUSE_DISTRIBUTED=OFF -DBLAS=ATLAS -DAtlas_BLAS_LIBRARY=/usr/lib64/atlas/ -DAtlas_CBLAS_INCLUDE_DIR=/usr/include/atlas-mips64el-base/ -DAtlas_CBLAS_LIBRARY=/usr/lib64/atlas/USE_MPI=OFF -DUSE_FBGEMM=OFF -DCMAKE_SYSTEM_PROCESSOR=mips64  -DUSE_PYTORCH_QNNPACK=OFF -DBUILD_TEST=ON -DUSE_NNPACK=OFF -DUSE_QNNPACK=OFF -DUSE_NUMA=OFF  -DBUILD_CAFFE2_MOBILE=OFF  -DBUILD_BINARY=ON -DUSE_OPENCV=ON -DUSE_CUDA=OFF -DCMAKE_BUILD_TYPE=Release ..\r\n(pytorch-venv) [root@lx-app-1 build]# make -j $(nproc)\r\n... ...\r\nCMake Warning at third_party/cpuinfo/CMakeLists.txt:71 (MESSAGE):\r\n  Target processor architecture \"mips64\" is not supported in cpuinfo.\r\n  cpuinfo will compile, but cpuinfo_initialize() will always fail.\r\n... ...\r\n\r\n... ...\r\n[  6%] Generating contrib/playground/resnetdemo/override_no_test_model_no_checkpoint.py\r\n[  6%] Building C object sleef/src/libm/CMakeFiles/sleefpurecfma_scalar.dir/sleefsimdsp.c.o\r\n[  6%] Building C object sleef/src/libm/CMakeFiles/sleefdetpurec_scalar.dir/sleefsimddp.c.o\r\n[  6%] Building C object sleef/src/libm/CMakeFiles/sleefpurec_scalar.dir/sleefsimdsp.c.o\r\n[  6%] Building C object sleef/src/libm/CMakeFiles/sleefdetpurecfma_scalar.dir/sleefsimdsp.c.o\r\n[  6%] Built target c10\r\n[  6%] Building C object sleef/src/libm/CMakeFiles/sleefdetpurecfma_scalar.dir/sleefsimddp.c.o\r\n[  6%] Generating contrib/playground/resnetdemo/rendezvous_filestore.py\r\nIn file included from /home/yancy/pytorch/third_party/sleef/src/libm/sleefsimdsp.c:187:0:\r\n/home/yancy/pytorch/third_party/sleef/src/arch/helperpurec_scalar.h:56:2: error: #error FP_FAST_FMA or FP_FAST_FMAF not defined\r\n #error FP_FAST_FMA or FP_FAST_FMAF not defined\r\n... ...\r\n```\r\nI have tried add `-I /home/yancy/pytorch/third_party/sleef/src/arch/` in `build/CMakeCache.txt` and `make rebuild_cache`, same error happened.\r\n\r\n## Pitch\r\npytorch can be compiled in `mips64` arch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n1) When I see warning info in `cmake ...` for `third_party/cpuinfo`, I plan to fake match rule in `third_party/cpuinfo/CMakeLists.txt`, for example:\r\n```\r\n... ...\r\n-ELSEIF(NOT CMAKE_SYSTEM_PROCESSOR MATCHES \"^(i[3-6]86|AMD64|x86(_64)?|armv[5-8].*|aarch64)$\")\r\n+ELSEIF(NOT CMAKE_SYSTEM_PROCESSOR MATCHES \"^(i[3-6]86|AMD64|x86(_64)?|armv[5-8].*|aarch64|mips64|mips64el)$\")\r\n... ...\r\n```\r\nThe ultimate result will also failed since `cpuinfo` . So, whether or not `cpuinfo` support `mips64` arch, too? Thanks!\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nCurrently, there is no easy way of visualizing the graph generated by autodiff in jit. We want something that could do\r\n\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef f(a):\r\n    return (a + 100) * 5\r\n\r\na = torch.tensor(0., requires_grad=True)\r\nresult = f(a)\r\n\r\nprint(result.grad_fn.graph)\r\n```\r\n\r\n## Additional context\r\n\r\nIn autograd, the `Node` for `grad_fn` of a JIT graph is defined as\r\n\r\n```\r\nstruct DifferentiableGraphBackward : public autograd::Node\r\n``` \r\nin `torch/csrc/jit/graph_executor.cpp`\r\n\r\nThis class does not have python correspondence, therefore python only views it as a general `CppFunction`. We need to create a Python class for it so that we could access additional information.\r\n\r\n**Please confirm if this feature sounds useful and if yes, assign me to this issue and I will work on it.**\n\ncc @suo @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["enhancement",null,null,null,null,null],"text":"We should add a centralized thread local handling (in c10).\r\nThis would have two main advantages:\r\n- Make the code cleaner as all thread local elements can easily be identified and worked with.\r\n- Allow improvement for example in `at::parallel_for` where it will be able to handle any Tensor operation within the loop.\r\n\r\n(Setting high priority only to be discussed for triage)\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @SsnL @albanD @gqchen @VitalyFedyunin @ngimel @mruberry"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nRelated: #22402 \r\n\r\nThis feature proposes passing through `Tensor` subclasses via `__torch_function__`.\r\n\r\n### Desired Behaviour\r\nExample desired behavior would be:\r\n\r\n```python\r\nclass MyTensor(torch.Tensor):\r\n    _additional_attribute = \"Kartoffel\"\r\n\r\na = MyTensor([0, 1, 2, 3])\r\n# b should be a MyTensor object, with all class attributes passed through.\r\nb = torch_function(a)\r\n```\r\n\r\n### Goals\r\n\r\nQuoting #22402 \r\n\r\n> These are _potential_ goals that have been collected from the above referenced PRs, other PyTorch issues (referenced in the relevant sections), as well as from discussions with mainly Edward Yang, and also other PyTorch and NumPy maintainers:\r\n> \r\n> 1. Support subclassing `torch.Tensor` in Python\r\n> 2. Preserve `Tensor` subclasses when calling `torch` functions on them\r\n> 3. Preserve `Tensor` subclasses when calling `numpy` functions on them\r\n> 4. Use the NumPy API with PyTorch tensors (i.e. NumPy API calls dispatch to `torch` functions)\r\n> 5. Use the PyTorch API with `torch.Tensor`-like objects that are _not_ `Tensor` subclasses\r\n> 6. Reuse NumPy ufunc implementations directly from PyTorch\r\n> 7. Allow operations on mixed array types, e.g. `tensor + ndarray`\r\n\r\nAdditionally, from https://github.com/pytorch/pytorch/issues/28361#issuecomment-544520934:\r\n\r\n> * Preserve `Tensor` subclasses when calling `Tensor` methods \r\n> * Propagating subclass instances correctly also with operators, using views/slices/etc.\r\n\r\n### Rough Sketch of Implementation\r\nAnything with a type like a built-in tensor will bypass `__torch_function__` via their fast path (although they will have a default implementation) but anything else defined by an external library will have the option to allow it.\r\n\r\nThe following code snippet shows what the default `__torch_function__` on `TensorBase` would look like.\r\n\r\n```python\r\nclass Tensor:\r\n    def __torch_function__(self, f, t, a, kw):\r\n        if not all(issubclass(ti, TensorBase) for ti in t):\r\n            return NotImplemented\r\n        result = f._wrapped(*a, **kw)\r\n        return type(self)(result)\r\n```\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @jph00 @rgommers "},{"labels":["enhancement",null,null,null],"text":"## Feature\r\nAbstraction for lazy evaluation of tensors that can make use of special matrix structure for linear algebra operations, similar to Tensorflow's / scipy's `LinearOperator`.\r\n\r\n\r\n## Motivation\r\nIn many situations, sequences of operations involving tensors can often be expressed and computed without ever instantiating the involved tensors. This can be achieved using lazy evaluation, so that a compiler can optimize the operations (â€œoperator fusionâ€). In the context of linear algebra, the involved (input or intermediate) tensors often have special structure (e.g. diagonal, block diagonal, triangular, Toeplitz, â€¦). Exploiting this structure using (semi-)symbolic evaluation and reductions can significantly decrease time and memory complexity, often by many orders of magnitude (see example below).\r\n\r\nWhile TensorFlowâ€™s `LinearOperator` (and to a lesser degree, scipyâ€™s) provide a basic abstraction for implementing such operators, PyTorch does not have this concept. This is a significant shortcoming for researchers whose work involves a lot of linear algebra.\r\n\r\n####  General Wishlist\r\nIn order for this to be general useful, we should have the following:\r\n- PyTorch autograd functions can accept the operator abstraction\r\n- The operator abstraction accept the same methods as Tensors, with the default implementations being to evaluate the operator and perform standard operations on the materialized tensors.\r\n\r\n## Alternatives / 3rd party solutions\r\nThird-party implementations include KeOpsâ€™s LazyTensor, which compiles the symbolic operations (basically a JIT) and GPyTorchâ€™s LazyTensor, which performs reductions in a pair-wise fashion using the PyTorchâ€™s python frontend.\r\n\r\n#### Illustration: Simple example using GPyTorch LazyTensor\r\n```\r\nimport torch\r\nfrom gpytorch.lazy import DiagLazyTensor\r\n\r\nd = torch.rand(10000)\r\nD = d.diag()  # 10000 x 10000 tensor\r\nD_lazy = DiagLazyTensor(d)  # same object, but represented only by d\r\n\r\n(D @ D).diag()  # 1.6 seconds \r\n(D_lazy @ D_lazy).diag()  # 0.000027 seconds (D_lazy @ D_lazy is a DiagLazyTensor)\r\n```\r\nMany other operations can similarly be accelerated when exploiting known tensor structure, including matrix decompositions (Cholesky) and spectral operations.\r\n\r\n#### Limitations \r\nBoth of these solutions have significant shortcomings. KeOps requires setting up an external compiler toolchain (which can be painful), and does not straightforwardly support sparse / structured representations. GPyTorch, being implemented in pure python, incurs overhead that for smaller tensors / simpler operations often outweighs the benefits.\r\n\r\nPyroâ€™s suggested [Funsor](https://github.com/pyro-ppl/funsor) abstraction also addresses the issue, but is much more general in scope (it generalizes the tensor interface to also cover arbitrary functions of multiple variables, where variables may be integers, real numbers or themselves tensors).\r\n\r\n## Existing plans for Lazy Tensors\r\n\r\nThe proposed implementation for lazy tensors in #25753 lays the groundwork for lazy evaluation. However, so far it only deals with the basic dispatch mechanism, not with any actual code optimizations. It should be possible to implement optimization of the involved tensors by manipulating PyTorch Internal Representation generated during the evaluation of the lazy operations. \r\n\r\nIn order to be able to exploit special structure, it seems that one would need to have certain structured tensor primitives (similar to  GPyTorchâ€™s `BlockDiagLazyTensor` or TensorFlowâ€™s `LinearOperatorKronecker`). \r\n\r\n\r\n@gpleiss, @jacobrgardner, @vishwakftw, @bwasti\n\ncc @vincentqb @vishwakftw @SsnL @jianyuh"},{"labels":[null,"enhancement",null,null],"text":"## ðŸš€ Feature\r\nThere is no way to perform the functionality of `torch.where()` in-place. This feature would either add a `.where_()` method to `Tensor` or an `out` parameter to the existing `torch.where()`, or both.\r\n\r\n## Motivation\r\n\r\nAll of the usual reasons for doing operations in-place.\r\n\r\n## Pitch\r\n\r\nAs stated above, add a `.where_()` method to `Tensor` or an `out` parameter to the existing `torch.where()`.\r\n\r\n## Alternatives\r\n\r\nOne could use the out-of-place `torch.where()` for a less efficient alternative.\r\n\r\nNote that the `.masked_scatter_()` method does not have the same functionality as an in-place `where()`.\r\n"},{"labels":["enhancement",null,null],"text":"[from [https://discuss.pytorch.org/t/training-a-model-via-a-train-method/58567](https://discuss.pytorch.org/t/training-a-model-via-a-train-method/58567)]\r\n\r\n## ðŸš€ Feature\r\nAdd a .fit() method to nn.Module, which trains the model using the parameters given.\r\n\r\n## Motivation\r\nWriting out a full training loop every time I'm testing a model is frustrating, if I'm simply using the same standard template each time.\r\n\r\n## Pitch\r\nEither add the .fit() method to nn.Module directly, or create an extension of nn.Module which contains it. The method should be as general as possible, and the focus should be ease of use, with reasonable defaults. I'd like to be able to write something like\r\n\r\n```\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        (...)\r\n\r\n    def forward(self, x):\r\n        (...)\r\n\r\nmodel = Model()\r\nmodel.fit(traindata, valdata, criterion = 'mse', epochs = 5)\r\n```\r\n\r\n## Alternatives\r\nAn alternative could be a train function which inputs the model along with all the relevant parameters, but it feels like the training belongs more to the internals of the model, which would also allow it to store its own training logs internally, if that's desired.\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nContext for Model Parallel: https://github.com/pytorch/pytorch/issues/23110\r\n\r\n## Motivation\r\n\r\nWhen applications are using complex distributed primitives like RPC, RRef and Distributed Autograd, debugging issues can be cumbersome. We should have a way of exposing metrics to applications. This could simply be a `def get_metrics() -> Dict[str, int]` API that returns information about various things. The full list of metrics needs to be decided, although a few examples could be number of owner rrefs, number of user rrefs, RPC latency, Distributed autograd latency etc.\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\n`Tensor.max()` and `Tensor.min()` support a `dim` argument to reduce over a single axis. They should support multiple axes just like `Tensor.sum()`.\r\n\r\n## Motivation\r\n\r\nSometimes I want to pool over all spatial dimensions, or an arbitrary set of dimensions. Currently I have to decide myself whether to loop over dimensions (and in which order), or reshape the tensor to collapse multiple dimensions into one.\r\n\r\n## Pitch\r\n\r\n`max()` and `min()` should support a list of dimensions. In this case, the returned indices tensor would be a matrix giving the coordinates along each dimension in their given order.\r\n\r\n## Alternatives\r\n\r\nFor the specific case of max-pooling across spatial dimensions, I could use torch.nn.functional.adaptive_max_pool2d with an output size of 1.\r\n\r\nWhen extending `max()` and `min()`, there are different options for the returned indices tensor, ordered from easiest to most complex implementation:\r\n1. not returning any indices if there are multiple dimensions\r\n2. returning a vector of indices that index into a flattened view of the dimensions to reduce (this is what adaptive_max_pool2d_with_indices does)\r\n3. returning a matrix of indices that give the indices along each of the reduced dimensions, in ascending order of dimensions\r\n4. returning a matrix of indices that give the indices along each of the reduced dimensions, in the given order of dimensions\r\n\r\nThe first would avoid some headaches, but cannot be undone again without breaking the signature. It would also complicate user code that has to work both with single and multiple dimensions. So that's probably not a good route.\r\nThe second would be compatible to adaptive max-pooling, but other than that, not very practical to deal with (think of advanced indexing).\r\nThe third and forth are similarly useful. Most of the time they will be equivalent. I don't really know what users would expect who provide `dim` in a not-ascending order.\r\n\r\nWe cannot really look at numpy for guidance -- `np.max()` supports multiple axes, but `np.argmax()` doesn't. If `np.argmax()` is called without specifying an axis, it will reduce over all axes and return an index into the flattened array (i.e., option 2, the one I deemed impractical).\r\n\r\nDisclaimer: I don't need the indices anyway.\r\n\r\n## Additional context\r\n\r\nFor summation, this was suggested in #2006 and implemented in #6152.\r\nThere also is a generic multi-dimensional operators issue in #9703, but the indices warrant some extra discussion."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nI would like to suggest new stochastic optimizer additions to Pytorch.\r\n\r\n### For non-convex loss functions\r\nIt is known that adaptive stochastic optimizers like Adam, Adagrad, RMSprop can indeed fail to converge. Moreover the test error of the trained models can be larger when compared to the models trained by SGD even though if they attain lower training error than with SGD - in other words they can overfit. See [[1](http://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.pdf)].\r\n\r\nRecently, ADABOUND was proposed [[2](https://openreview.net/pdf?id=Bkg3g2R9FX)] to overcome the above mentioned problems of the adaptive optimizers. The algorithm combines the best of both worlds : (a) Makes fast progress initially like the adaptive methods, and (b) attains similar or better test accuracy to that of SGD. In terms of code size and complexity, it is close to Adam's.\r\n\r\n### For convex loss functions\r\nWhile Pytorch is generally used for deep learning purposes, I see it is as a tensor-based Machine Leaning (ML) library with automatic differentiation that can run on both CPU and GPU and which allows easy implementation of both convex and non-convex ML models. As such, I think an ML engineer working on convex ML models would be interested to see faster state-of-the-art stochastic optimizers implemented in Pytorch like the variance reduced optimizers. Among them, SVRG [[3](https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf)] is most widely known. In terms of code size and complexity, it s somewhat close to SGD's with few modifications.\r\n\r\n### Alternatives\r\nI recommended SVRG since it is the most popular variance reduced algorithm. There are more complex variance reduced algorithms that are faster than SVRG and if they are of interest I can suggest some. Also, I would like to know if there are interests on batch optimizers other than LBFGS which is within Pytorch. For large scale ML models, there are cases when LBFGS requires more memory than the system can offer. An alternative would to be try out the accelerated gradient descent that consumes the same amount of memory as Batch gradient descent while performing significantly faster than it. Accelerated gradient descent's code size and complexity is close to batch gradient descent and as such it is way easier to maintain than LBFGS's. Lastly, if there are interests in other specific optimizers, I would be interested to hear out. I can try implementing them.\r\n\r\n## References\r\n[1] http://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.pdf\r\n[2] https://openreview.net/pdf?id=Bkg3g2R9FX\r\n[3] https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf\n\ncc @vincentqb"},{"labels":["enhancement",null,null],"text":"## Motivation\r\nExpand Pytroch C10D backend to allow dynamic loading non-built-in communication libraries,  as a preparation step to integrate Intel CCL (aka MLSL) to Pytorch as another c10d backend for supporting BFloat16 and future HW.\r\nÂ \r\n## Pitch\r\nEnrich Pytorch for better scaling efficiency on multi-node training\r\n\r\n## Additional Context\r\nExpand Pytorch c10d built-in communication module mechanism to support dynamic loading 3rd communication python modules.  The change is very small and made to c10d Python query mechanism. User needs specify a backend name and pass it to init_process_group() as a parameter in the python code, which calls c10d Python query mechanism. The c10d query mechanism is expanded to imported third party library according to the passed backend name. The third party library  implements the process_group interface. \r\n\r\nIntel CCL is added as third plug-in through Pytorch C++ extension.  CCL threads can be pinned to specific cores through environment variables. It supports bfloat16 all reduce (bfloat16 gradient reduce to fp32)  in the roadmap. \n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n`transformer.py` in [https://github.com/pytorch/pytorch/tree/master/torch/nn/modules](https://github.com/pytorch/pytorch/tree/master/torch/nn/modules) lacks `transformer.pyi.in` file, resulting in no code hints in PyCharm. I have no idea how to generate such one.\r\n\r\n## Motivation\r\n\r\n## Pitch\r\n\r\nAdd the missing `transformer.pyi.in` file.\r\n\r\n## Alternatives\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @ezyang"},{"labels":["enhancement",null,null],"text":"## ðŸ› Bug\r\n\r\nMy model (for object detection) uses a function which computes padding to trim excess tensor size after pooling with ceil_mode then upsampling gives a size which is off by 1. However pytorch onnx exporter complains that this is a dynamic operation and can't export the model.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\n\r\nfrom torch import nn\r\nimport torch.nn.functional as F\r\n\r\nimport torch\r\nimport torch.onnx\r\n\r\n\r\ndef match_size_2d(t, sized):\r\n    assert t.dim() == 4 and sized.dim() == 4\r\n    dh = sized.size(2) - t.size(2)\r\n    dw = sized.size(3) - t.size(3)\r\n\r\n    pad = (dw // 2, dw - dw // 2, dh // 2, dh - dh // 2)\r\n    return F.pad(t, pad)\r\n\r\n\r\nclass MyModel(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def forward(self, input):\r\n\r\n        x = F.avg_pool2d(input, 2, ceil_mode=True)  \r\n        upscaled = F.interpolate(x, scale_factor=2)\r\n\r\n        # input is 51x51, upscaled is 52x52, trim to match\r\n        upscaled = match_size_2d(upscaled, input) \r\n                \r\n        return torch.cat([input, upscaled], dim=1)\r\n\r\nmodel = MyModel()\r\n\r\ndummy = torch.ones((1, 64, 51, 51))\r\nout = model(dummy)\r\ntorch.onnx.export(model,               # model being run\r\n                  dummy,                         # model input (or a tuple for multiple inputs)\r\n                  \"model.onnx\",   # where to save the model (can be a file or file-like object)\r\n                  export_params=True,        # store the trained parameter weights inside the model file\r\n                  opset_version=11,          # the ONNX version to export the model to\r\n                  do_constant_folding=True,  # wether to execute constant folding for optimization\r\n                  input_names = ['input'],   # the model's input names\r\n                  output_names = ['output'], # the model's output names\r\n                  dynamic_axes={'input_1':{0:'batch'}})\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n```\r\n  File \"/home/oliver/miniconda3/lib/python3.7/site-packages/torch/onnx/__init__.py\", line 143, in export\r\n    strip_doc_string, dynamic_axes, keep_initializers_as_inputs)\r\n  File \"/home/oliver/miniconda3/lib/python3.7/site-packages/torch/onnx/utils.py\", line 66, in export\r\n    dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)\r\n  File \"/home/oliver/miniconda3/lib/python3.7/site-packages/torch/onnx/utils.py\", line 382, in _export\r\n    fixed_batch_size=fixed_batch_size)\r\n  File \"/home/oliver/miniconda3/lib/python3.7/site-packages/torch/onnx/utils.py\", line 262, in _model_to_graph\r\n    fixed_batch_size=fixed_batch_size)\r\n  File \"/home/oliver/miniconda3/lib/python3.7/site-packages/torch/onnx/utils.py\", line 132, in _optimize_graph\r\n    graph = torch._C._jit_pass_onnx(graph, operator_export_type)\r\n  File \"/home/oliver/miniconda3/lib/python3.7/site-packages/torch/onnx/__init__.py\", line 174, in _run_symbolic_function\r\n    return utils._run_symbolic_function(*args, **kwargs)\r\n  File \"/home/oliver/miniconda3/lib/python3.7/site-packages/torch/onnx/utils.py\", line 619, in _run_symbolic_function\r\n    return op_fn(g, *inputs, **attrs)\r\n  File \"/home/oliver/miniconda3/lib/python3.7/site-packages/torch/onnx/symbolic_helper.py\", line 123, in wrapper\r\n    args = [_parse_arg(arg, arg_desc) for arg, arg_desc in zip(args, arg_descriptors)]\r\n  File \"/home/oliver/miniconda3/lib/python3.7/site-packages/torch/onnx/symbolic_helper.py\", line 123, in <listcomp>\r\n    args = [_parse_arg(arg, arg_desc) for arg, arg_desc in zip(args, arg_descriptors)]\r\n  File \"/home/oliver/miniconda3/lib/python3.7/site-packages/torch/onnx/symbolic_helper.py\", line 75, in _parse_arg\r\n    raise RuntimeError(\"Failed to export an ONNX attribute, \"\r\nRuntimeError: Failed to export an ONNX attribute, since it's not constant, please try to make things (e.g., kernel size) static if possible\r\n\r\n```\r\n\r\n## Expected behavior\r\n\r\nSince sizes are constant, they're not actually dynamic - is there a way to work around this restriction?\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 418.87.00\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.3\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.5\r\n[pip] pytorch-tools==0.1\r\n[pip] pytorchcv==0.0.50\r\n[pip] torch==1.3.0\r\n[pip] torch2trt==0.0.1\r\n[pip] torchvision==0.4.1a0+d94043a\r\n[conda] mkl                       2019.4                      243  \r\n[conda] pytorch                   1.3.0           py3.7_cuda10.0.130_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-tools             0.1                       dev_0    <develop>\r\n[conda] pytorchcv                 0.0.50                   pypi_0    pypi\r\n[conda] torch2trt                 0.0.1                    pypi_0    pypi\r\n[conda] torchvision               0.4.1                py37_cu100    pytorch\r\n\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nWould it be possible to train the model with quantization?\r\n\r\n## Motivation\r\nMemory issues are common and quantized training can help\r\n\r\n## Pitch\r\nUser can set a flag for quantized training and it happens. Might require backprop over the expectation. If it's impossible to do quantized training, perhaps then docs could point to nevergrad (we could just evolve the weights in this case)\r\n\r\n## Alternatives\r\n\r\nNVIDIA APEX has reduced-precision; You can train in fp16 in tensorflow\r\nhttps://github.com/NVIDIA/apex\r\n\r\nBrevitas appears to offer this feature (havent tested personally)\r\nhttps://github.com/Xilinx/brevitas\r\n\r\n## Additional context\r\nSmaller weights = More cool models\n\ncc @jerryzh168 @jianyuh @dzhulgakov"},{"labels":["enhancement",null,null,null,null,null,null],"text":"I would like to add vsx support to aten Vec256 for power le platform.\r\nThe work is still in progress. I open this issue to discuss this topic as I saw Vec256 is kept modified.\r\nFor now \r\nI am simulating  256bit using 2 128 bit vector variables.\r\nFor supporting vsx inside pytorch \r\n-cmake for vsx detection\r\n-pytorch modifications for handling power  \r\n-Vector codes for Vec256<float>, Vec256<double>, Vec256<int16_t>, Vec256< int32_t>, Vec256< int64_t> ,Vec256<qint>\r\n\r\nI keep those Vec256 changes isolated from PyTorch for testing purposes and it was not fully incorporated and ready to ship. As there are not any benchtests I just looked at generated codes. Under gcc8 -Ofast compiler-generated assemblies were quite good. \r\n\r\nFor now, for me, the main obstacle is the way to implement trigonometric, other mathematical functions. Firstly, I went with avx_mathfun and translated its codes into vsx:\r\n-all avx_mathfun sin,cos,exp.log was implemented inside Vec256<float> \r\n-I also implemented pow, sin, cosh and other functions.\r\n\r\nPytorch team is using sleef library for some functions. I checked sleef library and they have support for 128bit vsx functions. I could either translate those into simulated 256bit or I could use [cephes implementations](http://www.netlib.org/cephes/) as it was done with avx_mathfun.\r\n\r\nI am planning to add vsx support as separate headers like vec256_*_vsx.h.\r\n\r\nI also think that we could add test cases for Vec256 using either google test or catch. I only used the later.\r\nAny guidance on this topic is appreciated. I want that my changes fit both ibm and pytorch team.\r\n\n\ncc @ezyang @gchanan @zou3519 @VitalyFedyunin @ngimel @mruberry"},{"labels":["enhancement",null,null],"text":"We have used ReflectionPad2d for cyclegan and it is very important to solve the checkboard artifact. \r\n\r\nConsidering the 5D case, I also meet the checkboard artifact with cyclegan. I try to find the ReflectionPad3d with the newest PyTorch version but it still in the checklist. Could you tell me when the function will available? It is very useful if it can integrate it in PyTorch"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\n\r\n**Document: https://torchserve-docs.s3-us-west-2.amazonaws.com/docs/torchserve_architecture_v0.pdf**\r\n\r\n**NOTE: The above document is a draft and subject to change**\r\n\r\nRequest to build a Serving framework to host and serve trained PyTorch models. \r\n@alexwong\r\n\r\n## Motivation\r\n\r\nPyTorch provides an excellent and easy-to-use interface to train a model and also provides an easy to use optimized inference interface through JIT. Currently, there is a need for an optimized model serving solution to take any model trained using PyTorch framework into production. There exists multiple solutions to serve a PyTorch model in production, but most of the solutions are generic model serving solutions. There are multiple pain points that the PyTorch community currently face when trying to take a PyTorch model into production. \r\n\r\n* Building a high performance web serving component to host PyTorch models is difficult to build and requires experience and domain knowledge.\r\n* Adding custom pre-processing and post-processing for a model in service currently requires significant rework on the model server itself.\r\n* Supporting multiple accelerators requires additional work.\r\n* Any customization to the model server would require significant understanding of the existing serving framework itself and would also require significant rework.\r\n\r\nWe think the following are the most important requirements of a good PyTorch model serving framework.\r\n\r\n\r\n## Pitch\r\n\r\nWe want to contribute to building a serving framework, which addresses the above pain points and more. We foresee the \r\n\r\nA PyTorch model serving solution will have the following capabilities: \r\n\r\n1. **Performance**: The server component should be highly-performant with low overhead from the serving framework. This implies that the average throughput must be high and the P90 latencies should be low. Its also important to get P50 and P90 latencies comparatively flat, signifying that all the requests are treated equally.\r\n2. **Host Multiple Models**: The server component should be able to host multiple models at the same time and customers should be able to load/unload a model at runtime. The model serving framework should expose an endpoint for each model, which can be reached by any customer.\r\n3. **High Availability**: The serving framework should be robust. Runtime errors of one model shouldnâ€™t affect other models running on the server or the runtime of the server itself. There should be mechanisms to recuperate from any system out of resource errors.\r\n4. **Metrics and Logs**:  A production grade serving framework should provide insight into the runtime of the model. The serving framework should provide easy access to logs and metrics and also provide easy hooks to add new logs and metrics without needing to understand the serving framework at a deep level.\r\n5. **Support both Eager mode and Scripted mode models**: The serving framework should support means to run PyTorch models in scripted mode for optimized execution.\r\n6. **Support for multiple bindings**: The serving framework should have support for models loaded via Python (eager/torchscript) or C++ bindings (JIT IR traces).\r\n7. **Supports HTTP and gRPC Endpoints**: The serving framework should come with a full set of HTTP endpoints for managing models as well as running inference on the models. PyTorch serve would also come with an SDK to easily customize the endpoints. The serving framework would also support gRPC endpoints. \r\n8. **Ease of use and access**: The serving framework should be easy to set up on any platform (MacOS, Linux, Windows) and should be testable on these systems. Users should have the same experience to containerize the Serving framework and launch into production using any container orchestration mechanism. The PyTorch serve framework would also have a fully featured CLI to start and run the model server.\r\n9. **Lightweight**: This implies that the serving component itself shouldnâ€™t have multiple dependencies.\r\n10. **Supports features such as request batching**: The serving framework would have features such as request batching, to optimally run inference on accelerators.\r\n11. **Support model Versioning and A/B testing**: The serving framework should have capabilities to load multiple versions of the same model and run A/B tests on the model. This is very useful for when rolling out newer versions of a model into production. This can also be used to roll back if the new model is not as performant. \r\n12. **Zero code serving**: While providing the feature to customize the pre-processing and post-processing of inference requests, the PyTorch serve framework should also allow customers to simply drop their trained models into the server and use it for inference. In other words, the PyTorch serving framework should come with sensible defaults for pre processing and post processing. \r\n13. **Easy customizability**: The serving framework must be easy to customize for endpoints. This means easily modifying and adding new management endpoints, defining custom request batching algorithms, defining custom AuthZ and AuthN mechanisms. \r\n14. **Support Accelerators**: A production grade model server should be able to run on GPU hosts as well as any other custom accelerator host. \r\n15. **Web UI**: The PyTorch serving framework should come with a Web UI to allow interaction with a served model.\r\n\r\n### Proposed Technical Details\r\n\r\nThe model server will be based on a micro-service based solution rather than a monolithic approach. This architecture would bring us the benefit of decoupling the work of handling ingress requests and running the actual inference. This also allows the PyTorch serving framework to scale beyond a single host. The high-level components of the serving framework are divided into a frontend and backend which share different responsibilities.\r\n\r\n#### Frontend responsibilities:\r\n\r\n1. **Manage connections**:  In other words, the incoming requests and outgoing responses are managed by the frontend.\r\n2. **Manage models** : The frontend is responsible for the lifecycle of the model. Each hosted model will have its unique endpoint created, which could take any data type and return any data-type back. \r\n3. **Manage the backend**: The frontend is responsible for providing the models to be loaded onto the backend workers and also managing the backend workers themselves. \r\n4. **Manage requests**: Requests coming into the serverâ€™s frontend will be queued in model-sepecific queues for handling. \r\n5. **Request distribution**: The frontend will be responsible for request distribution, to backend workers. \r\n6. **Metrics and Logs** : Frontend will be responsible for metrics management, logs management and capturing any custom metrics and logs that come from the backend.  \r\n7. **Retrieve models from anywhere** : The frontend is also responsible for retrieving the models from cloud or local storage. \r\n\r\n#### Backend responsibilities:\r\n\r\n1. **Running inference**: Tightly integrate with PyTorch backend and also responsible for running any preprocessing required, running the *forward* method of the model with the incoming request and running post process on the inference response. \r\n2. **Default pre-process, inference and post-process**: If no custom processing logic is provided to the backend, it would have default logic to run preprocess, inference and post process on the model.\r\n3. **Publish custom metrics and logs**: Backend will have the capabilities to publish custom model level metrics and logs.\r\n\r\n### Proposed sequence diagrams\r\n\r\n#### Monitoring status of the Server\r\n![monitoring](https://user-images.githubusercontent.com/36211508/66499650-153e7a00-ea75-11e9-9d41-31a00b89452f.png)\r\n\r\n#### Checking status of the models\r\n![Model status](https://user-images.githubusercontent.com/36211508/66499662-196a9780-ea75-11e9-9275-83337104c3de.png)\r\n\r\n#### Running inference\r\n![Inference](https://user-images.githubusercontent.com/36211508/66499668-1cfe1e80-ea75-11e9-98ef-00120b268104.png)\r\n\r\n#### Loading model\r\n![](https://user-images.githubusercontent.com/36211508/66499722-33a47580-ea75-11e9-92e3-0658e57b1bae.png)\r\n\r\n#### Deleting a model\r\n![delete_model](https://user-images.githubusercontent.com/36211508/66499725-356e3900-ea75-11e9-8e5a-ae032e231e69.png)=\r\n\r\n## Next Steps\r\n\r\n* We are looking for feedback/comments on this proposal. Specifically, we are looking for feedback on the list of capabilities outlined in this RFC and their priority. We also welcome feedback on our proposed design and implementation. \r\n* Add details on proposed architecture and details on endpoint.\r\n* Add additional sequence diagrams.\r\n* Target Q4 2019 for Experimental release.\r\n\r\n"},{"labels":["enhancement",null,null],"text":"If I understand correctly `sum(tensor_list)` will allocate and keep O(N) intermediate tensors (same with a for loop) where N is number of tensors, which can be quite large in the case of big DenseNet. I propose to maybe generalize `torch.add` to support more than two tensors as input.\r\n\r\nCurrently one can do: `functools.reduce(lambda acc, x: acc.add_(x), tensor_list, torch.zeros_like(tensor_list[0]))`, so it's not super-urging, but a more idiomatic way may be nice"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\njavascript API to train and deploy models\r\n\r\n## Motivation\r\n\r\nAI for react / react native / web apps / node APIs\r\n\r\n## Pitch\r\n\r\nJust imagine you could make interactive AI instead of toy / research experiments\r\n\r\n## Alternatives\r\n\r\nTensorflow. You could train your own model and try to deploy it with onnx or something...but then you canâ€™t train it in the browser, so youâ€™d better make a good guess about how that model should work, and youâ€™d better have the money to pay for compute. \r\n\r\n## Additional context\r\n\r\nLeverage external resources: let users train the website AI, while theyâ€™re running it.\r\n\r\nAlso: async\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nAdd a mode to `Dataset` that enables fetching data in batches instead of item-by-item.\r\n\r\n## Motivation\r\nIf model training takes relatively small individual examples as an input, like in the case of training on tabular data, the python interpreter overhead of fetching data becomes so large that hinders training performance. In other words, training becomes CPU-bound (even with multiprocessing enabled).\r\n\r\nThis came up in a real scenario of the StarSpace model from FAIR.\r\n\r\n## Pitch\r\n\r\nAdd an optional `__getbatch__` method to the `Dataset` that's analogous to `__getitem__` but takes a collection of indices as an input. Make the `Dataloader` aware of `BatchedDataset`. Once the `Dataloader` recognizes that the `__getbatch__` is present, that method is used for fetching data, one batch at the time.\r\n\r\nAs a result, the user receives an ability to pass data in batch end-to-end and avoid the high cost (per byte read) of python interpreter.\r\n\r\nI implemented a variant of batch loading for aforementioned StarSpace model and got the training down from 5.5 days to under 24 hours. The person who originally implemented it used standard PyTorch data loading abstractions and fall into the trap of low performance. \r\n\r\nThis is a type of issue anybody working on e.g. tabular data will be running into. Unfortunately, there's no natural way out given current PyTorch abstractions.\r\n\r\n## Alternatives\r\n\r\nImplement this on top of existing abstractions by \"smuggling\" batches values wrapped as a single value and unwrapping them in a custom collate function. The code, that I provide below, is fairly subtle and a bit hacky (abusing current abstractions). The code is fully functional and used in production, though.\r\n\r\nEdit: I found also this: https://github.com/pytorch/pytorch/pull/19228 which a different way of implementing what I need. The downside of IterableDataset is that it essentially throws through the window the nice decomposition into Dataset, Sampler and Dataloader. Suddenly, you're responsible for implementing all of the logic. Having said that, this is a big improvement over my rather hacky solution I posted below.\n\ncc @SsnL"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n(Long term request, mostly to gather feedback on our current experiment)\r\nWe would like to extend the `Tensor` class with the description of its dimensions shape and values, to enable static checking of tensor  operations w.r.t. to shapes (e.g. detecting an illegal call to `Tensor.mm` statically rather than with a runtime exception). The new syntax would look  like `Tensor[int32, dim0, dim1, dim2]`\r\n\r\n## Motivation\r\nAt the moment, PyTorch is more or less untyped: everything is a `Tensor` and there is no information whatsoever on the dimensions of these tensors. By being more descriptive, we could statically check (aka at compile time, rather  than at runtime) that tensor operations are executed on arguments with the right shape. For example, we could catch this kind  of errors during type checking that:\r\n```\r\nT0 : Tensor[int32, D3, D4] = ...\r\nT1 : Tensor[int32, D5,  D5] = ...\r\nT2 = Tensor.mm(T0, T1) # mismatch: D4 != D5\r\n```\r\nThis could save  lots of computer time (less runtime errors) along with debugging time.\r\n\r\n## Pitch\r\n\r\nRecently [Pyre](https://github.com/facebook/pyre-check) added an experimental support for variadic type variable which allows to describe the shape of tensors.\r\nIt allowed me to write some initial stubs for PyTorch where the tensor type has a documented shape. This shape can be check statically by Pyre to prevent most mis-usage of PyTorch operators.\r\n\r\nAs an example, I took [this script](https://github.com/pytorch/examples/blob/29c2ed8ca6dc36fc78a3e74a5908615619987863/regression/main.py#L43) and translated it into this [typed version](https://github.com/facebook/pyre-check/blob/master/examples/pytorch/sources/linear_regression.py). The main stubs are located [here](https://github.com/facebook/pyre-check/blob/master/examples/pytorch/stubs/_torch/__init__.pyi).\r\n\r\nWe already got some very positive feedback for the Python types community last Friday during Facebook MPK's Python meetup. So now I'm asking the PyTorch community :D\r\n\r\nKnown limitations: this is an early draft of the project, so we can't type everything at the moment. For example we only support simple broadcasting (like `Tensor.__add__` when the rhs is scalar. Nothing for `Tensor.matmul` yet). Also there are some functions that just can't be statically check (like `Tensor.cat`) and which require manual annotation.\r\n\r\n## Alternatives\r\n\r\nCurrent known  alternative are all runtime check (like the Named Tensor proposal) which address the same problem, but still at runtime, which could be less  efficient when programs run for several hours/days.\r\n\r\n\r\n## Additional context\r\nI don't expect PyTorch to migrate to this solution  right now, I'm gathering feedback on the experiment to see where to go next. Our next stop is  to support broadcasting, and I would gladly have some direction on which killer feature we should try to support next."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nA function `convmtx2` such that\r\n\r\n    convmtx2(kernel, image.shape[1:]) @ image.flatten() == torch.nn.functional.conv2d(image[None], kernel)[0].flatten()\r\n\r\nfor any tensors `image` of shape `(in_channels, image_height, image_width)` and `kernel` of shape `(out_channels, in_channels, kernel_height, kernel_width)`.\r\n\r\n## Motivation\r\n\r\nLike its [MATLAB counterpart](convmtx2), `convmtx2` computes the matrix corresponding to convolution by a 2D kernel. This matrix is a doubly block [Toeplitz matrix](https://en.wikipedia.org/wiki/Toeplitz_matrix#Discrete_convolution).\r\n\r\nThis feature has been requested elsewhere. See [here](https://discuss.pytorch.org/t/obtaining-toeplitz-matrix-for-convolution/52968/3) and [here](https://stackoverflow.com/questions/56702873/is-there-an-function-in-pytorch-for-converting-convolutions-to-fully-connected-n) for example."},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nProvide pytorch traced model graph syntax specification.\r\n\r\n## Motivation\r\nWe plan to deploy pytorch on our inference platform.\r\nSo far the only path is pytorch --> onnx --> target platform.\r\nBut this path may break in sometimes.\r\nWe want to parse pytorch traced model to target platform directly.\r\nWe can see traced model have it's syntax, tokens define, but there's no document can refer.\r\nLike this graph output:\r\ngraph(%x : Tensor) {\r\n  %5 : bool = prim::Constant[value=1]()\r\n  %1 : int = prim::Constant[value=0]()\r\n  %result.1 : Tensor = aten::select(%x, %1, %1)\r\n  %4 : int = aten::size(%x, %1)\r\n  %result : Tensor = prim::Loop(%4, %5, %result.1)\r\n    block0(%i : int, %7 : Tensor) {\r\n      %10 : Tensor = aten::select(%x, %1, %i)\r\n      %result.2 : Tensor = aten::mul(%7, %10)\r\n      -> (%5, %result.2)\r\n    }\r\n  return (%result);\r\n}\r\n\r\nThe tokens, the built-in functions the syntax, without a clear document we can't build a correct parser which support all torch-traced model.\r\n\r\n\r\nThanks\r\n8086\r\n\n\ncc @suo"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nOften times I find myself writing this pattern:\r\n```py\r\nif training:\r\n  with torch.enable_grad():\r\n    # build a graph, with out of place ops on input tensors\r\nelse:\r\n  with torch.no_grad():\r\n    # update inplace on input tensors\r\n```\r\n\r\nThe context manager can be unified via `torch.set_enabled_grad(training)`, yet there is no easy way to switch between in-place and out-of-place ops. \r\n\r\nMaybe we could, similar to `nn.ReLU(inplace)`, adding a kwarg to ops that supports inplace variants?\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nWe have added MKLDNN+AMD BLIS path for PyTorch and want to upstream our changes to master branch\r\n\r\n## Motivation\r\n\r\nPyTorch with MKLDNN+AMD BLIS would give higher performance for DNN applications on AMD Platforms \r\n\r\n## Pitch\r\n\r\nWe want to upstream our changes to the master branch. We have added new CMake file for taking the AMD BLIS path\r\n\r\n## Alternatives\r\n\r\n BLIS path is alternative to the OpenBLAS path which already exists \r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh"},{"labels":["enhancement",null,null,null,null,null],"text":"## ðŸš€ Feature\r\nAdd numerically stable cumulative logsumexp function. Also we have associated PR on `cummax` that is needed for numerically stable implementation (https://github.com/pytorch/pytorch/issues/20240).\r\n## Motivation\r\nThis is useful when computing sum of probabilities and have different applications.\r\n\r\n## Pitch\r\nTorch has `cumsum` and `cumprod` so I suggest `logcumsumexp` to be added.\r\n\r\n## Alternatives\r\nCurrent workaround for me and for people that need it can be written as follows (it is quite fast, only overhead for converting between numpy/torch):\r\n```\r\nimport numpy as np\r\nimport torch\r\n\r\ndef cummax(x, dim):\r\n    x_np = x.detach().cpu().numpy()\r\n    ret = np.maximum.accumulate(x_np, axis=dim)\r\n    return torch.from_numpy(ret).to(x)\r\n\r\n\r\ndef logcumsumexp(x, dim=-1):\r\n    if (dim != -1) or (dim != x.ndimension() - 1):\r\n        x = x.transpose(dim, -1)\r\n\r\n    init_size = x.size()\r\n    last_dim_size = init_size[-1]\r\n    x_resized = x.contiguous().view(-1, last_dim_size)\r\n    d1, d2 = x_resized.size()\r\n    x_cummax = cummax(x_resized, -1).view(d1, d2, 1)\r\n    x_expand = x_resized.unsqueeze(1).expand(d1, d2, last_dim_size)\r\n    mask = torch.tril(torch.ones(last_dim_size, last_dim_size)).unsqueeze(0)\r\n    ret = torch.log(torch.sum(torch.exp(x_expand - x_cummax) * mask, dim=-1)) + x_cummax.view(d1, d2)\r\n    ret = ret.view(*init_size)\r\n\r\n    if (dim != -1) or (dim != x.ndimension() - 1):\r\n        ret = ret.transpose(-1, dim)\r\n\r\n    return ret\r\n```\r\n\r\nUPDATE: code above is not always numerically stable. Still most numerically stable, but slow way to compute `logcumsumexp` is use for-loop:\r\n\r\n```\r\ndef logcumsumexp(x, dim):\r\n    # slow implementation, but ok for now\r\n    if (dim != -1) or (dim != x.ndimension() - 1):\r\n        x = x.transpose(dim, -1)\r\n\r\n    out = []\r\n    for i in range(1, x.size(-1) + 1):\r\n        out.append(torch.logsumexp(x[..., :i], dim=-1, keepdim=True))\r\n    out = torch.cat(out, dim=-1)\r\n\r\n    if (dim != -1) or (dim != x.ndimension() - 1):\r\n        out = out.transpose(-1, dim)\r\n    return out\r\n```\r\n"},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\n\r\nMy request is regarding the new nn.Transformer modules, more precisely the [TransformerEncoderLayer](https://github.com/pytorch/pytorch/blob/2dac67386128b4023f25dad6a8456aaf3029b7a8/torch/nn/modules/transformer.py#L235) and `TransformerEncoderLayer`. My problem is that the layer normalization is not customizeable in these modules, contrary to `TransformerEncoder` and `TransformerDecoder` where it is passed as a parameter.\r\n\r\n## Motivation\r\n\r\nThis feature would be useful if I would like to use a different layer normalization layer like [FusedLayerNorm](https://github.com/NVIDIA/apex/blob/3ae89c754d945e407a6674aa2006d5a0e35d540e/apex/normalization/fused_layer_norm.py#L70) from NVIDIA/apex.\r\n\r\n## Pitch\r\n\r\nA simple `layer_norm_builder_fn` passed as a parameter to Transformer*Layer __init__.\r\n\r\n```python\r\nclass TransformerEncoderLayer(Module):\r\n\r\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\", layer_norm_builder_fn=None):\r\n        super(TransformerEncoderLayer, self).__init__()\r\n        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\r\n        # Implementation of Feedforward model\r\n        self.linear1 = Linear(d_model, dim_feedforward)\r\n        self.dropout = Dropout(dropout)\r\n        self.linear2 = Linear(dim_feedforward, d_model)\r\n\r\n        self.norm1 = LayerNorm(d_model) if layer_norm_builder_fn is None else layer_norm_builder_fn()\r\n        self.norm2 = LayerNorm(d_model) if layer_norm_builder_fn is None else layer_norm_builder_fn()\r\n        self.dropout1 = Dropout(dropout)\r\n        self.dropout2 = Dropout(dropout)\r\n\r\n        self.activation = _get_activation_fn(activation)\r\n\r\ndef main():\r\n    d_model = 512\r\n    layer_norm_builder = lambda: FusedLayerNorm(d_model)\r\n    encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=8, layer_norm_builder_fn=layer_norm_builder)\r\n    \r\n```\r\n\n\ncc @ezyang @gchanan @zou3519"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nInput-cell-attention for LSTM\r\n\r\n## Motivation\r\nRecent efforts to improve the interpretability of deep neural networks use saliency to characterize the importance of input features in predictions made by models. However, RNN saliency vanishes over time, biasing detection of salient features only to later time steps and are, therefore, incapable of reliably detecting important features at arbitrary time intervals. \r\n\r\n\r\n## Pitch\r\n\r\nTo address this vanishing saliency problem, we propose a novel RNN cell structure (input-cell-attention), which can extend any RNN cell architecture. At each time step, instead of only looking at the current input vector, input-cell-attention uses a fixed-size matrix embedding, each row of the matrix attending to different inputs from current or previous time steps.  \r\nOur proposed mechanism has been accepted as a paper in NeurIPs 2019.\r\n\r\n\r\n## Additional context\r\n\r\n![cellAttentionLstm-6](https://user-images.githubusercontent.com/9405315/65023407-c16ec400-d900-11e9-99ac-598c6eb419e8.png)\r\n\r\nLSTM with input-cell-attention, at time t matrix X<sub>t</sub>=[x<sub>0</sub>,x<sub>1</sub> ,...,x<sub>t</sub> ] is passed to an attention mechanism; the output  A<sub>t</sub> is multiplied with X<sub>t</sub> to produce  M<sub>t</sub> (i.e  M<sub>t</sub>=A<sub>t</sub>X<sub>t</sub>). Matrix M<sub>t</sub> is now the input to LSTM cell (M<sub>t</sub> has dimension r x N, where r is the attention parameter and N is the number of inputs).\n\ncc @zou3519"},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\nPublish the CPU version of PyTorch on PyPI to make it installable in a more convenient way.\r\n\r\n## Motivation\r\nUsing PyTorch in production does not require necessarily the (~700 MB big) GPU version, but installing the much smaller CPU version as suggested on the website:\r\n```\r\npip3 install torch==1.2.0+cpu torchvision==0.4.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\r\n```\r\nmakes it hard to use tools like [Poetry](https://poetry.eustace.io/), which do not work with `pip` itself and therefore do not support an argument like `-f https://download.pytorch.org/whl/torch_stable.html`.\r\n\r\n## Pitch\r\n\r\nPublish the CPU version (e.g. as `torch-cpu`) on PyPI."},{"labels":[null,"enhancement",null,null],"text":"Currently if the the argument of `__rmatmul__` is not a Tensor, PyTorch throws `*** TypeError: matmul(): argument 'other' (position 1) must be Tensor, not Vec`\r\n\r\nPython data model asks that it return `NotImplemented` in such case. This allows interpreter to dispatch op to the proper implementation.\r\n(ie see https://docs.python.org/3/library/numbers.html#implementing-the-arithmetic-operations)\r\n\r\nCurrent behavior makes it hard to implement custom types. IE\r\n```\r\nclass FactoredMatrix:\r\n  def __init__(self, mat):\r\n    self.mat = mat\r\n  def __matmul__(self, other):\r\n    return 0\r\n  def __rmatmul__(self, other):\r\n    return 1\r\nx = torch.ones((2,2))\r\nprint(FactoredMatrix(x) @ x)  # works\r\nprint(x @ FactoredMatrix(x))  # fails\r\n```\r\n\n\ncc @ezyang @gchanan @zou3519"},{"labels":[null,"enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\n`mycudabooltensor.where(...)`\r\nreturns:\r\n`RuntimeError: \"where_cuda\" not implemented for 'Bool'`\r\n\r\n## Motivation\r\n\r\nIn order to do masked operations on bool tensors, I have to cast those to byte tensors first, which looks unelegant.\r\n\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nEnhance the runtime dispatch to support AVX512\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nWe are optimizing some quantized kernels by using AVX2 or AVX512 instruction set. It gave significant performance win for server side quantized model. Currently in OSS, we already have support for dispatching for AVX and AVX2 in pytorch/aten/src/ATen/native/DispatchStub.h.  \r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\r\n\r\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a"},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\nA function \r\n```\r\ntrain_model(model, optimizer, scheduler, dataset, batch_size, shuffle=False, \\\r\n                snaphot_prefix = None, num_iters=None, \\\r\n                num_epochs=None, iter_size=1, display_iter = 20, snaphost_interval=None, \\\r\n                load_snapshot_path=None, restore_dataloader=True, display_gpu=False, schedule_on_iter=False)\r\n```\r\nthat allows the user to easily train a model with a dataset without the hassle of coding himself the training loop. It contains the possibility to spread the batch size over several iterations `iter_size`, the possibility to schedule per epoch or per iteration `schedule_on_iter`, the possibility ot train for a set number of iterations or epochs and the possibility to save a reload snapshots containing the model, scheduler, and optimizer states including a few other parameters to properly restart from the snapshot (except for the dataloader since it does not have a state dict).\r\nThere is a requirement that the models contains its own loss function.\r\nIt does only training, not validation.\r\n\r\nSomething similar for a learning rate finder.\r\nBoth also available for multi node, multi gpu training with automated support for sync batch norm (with simple distributed configuration). It is using DistributedDataParallel with one process per GPU.\r\n\r\nAnd a utility to easily configure learning rates for each parameter: `make_param_groups`.\r\n\r\n## Motivation\r\n\r\nSome of these features were easily available in caffe and we needed all of them for our work at Pixelz (the company I work for). We have it all implemented already (without unit tests though). We thought it could be useful to the community :-)\r\n\r\nWe didn't do the validation because we use the snapshots to do it outside of the training instances.\r\n\r\n## Pitch\r\nIs anyone interested in this?\r\nIs the scope ok for a single pull request? Or should I make several issues and pull requests?\r\nCan someone help to make the unit tests? (we don't need them ourselves and don't have tons of time available for opensourcing this code)\r\nDoes anyone want to add the validation to it?\r\nIn which module/submodule should it go?\r\n\r\n## Additional context\r\nExample usage:\r\nConfigure optimizer and learning rates\r\n```\r\n custom_lr = { \\\r\n        'conv12.weight' : 1e-3, \\\r\n        'conv12.bias' : 1e-3  \\\r\n    }\r\n    optimizer = optim.SGD(make_param_groups(model, custom_lr), lr=1e-2)\r\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\r\n   ```\r\nconfigure multi gpu settings on a single node and train\r\n```\r\n    os.environ[\"RANK\"] = \"0\"\r\n    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\r\n    os.environ[\"MASTER_PORT\"] = \"35003\"\r\n    os.environ[\"WORLD_SIZE\"] = \"1\"\r\n    \r\n    model = train_model_multigpu(model, optimizer, scheduler, train_dataset, \\\r\nbatch_size=6, shuffle=False, num_epochs=5, iter_size=2, schedule_on_iter=False, \\\r\nsnaphost_interval=0.5, snaphot_prefix='/data/pixelz_train_6/snapshot')\r\n```\r\nSo far we used it for single or multi gpu on a single node, with several computer vision architectures (VGG, resnet and variants, etc) without problem.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @vincentqb"},{"labels":["enhancement",null,null,null],"text":"PyTorch seems to use `gesdd` for singular value decomposition. Alternative `gesvd` is slower, but more accurate/robust. PyTorch could follow scipy syntax and provide a way to choose between them using `lapack_driver` kwarg\r\n\r\nSome background on gesvd vs gesdd\r\nhttps://epubs.siam.org/doi/10.1137/17M1117732\r\nhttps://savannah.gnu.org/bugs/?55564\r\n\r\n\r\n```\r\nimport numpy as np\r\nimport torch, scipy\r\nfn = \"pinv2_crash.txt\"\r\nif not os.path.exists(fn):\r\n  import urllib.request\r\n  url=\"https://s3.amazonaws.com/yaroslavvb2/data/\"+fn\r\n  response = urllib.request.urlopen(url)\r\n  body = response.read()\r\n  print(\"Read %d bytes\"%(len(body),))\r\n  open(fn, \"wb\").write(body)\r\n\r\nmat = np.genfromtxt(fn, delimiter= \",\").astype(np.float32)\r\nscipy.linalg.svd(mat, lapack_driver=\"gesdd\")  # crashes\r\ntorch.svd(torch.tensor(mat))  # crashes\r\nscipy.linalg.svd(mat, lapack_driver=\"gesvd\")  # works\r\n```\n\ncc @vishwakftw @SsnL @jianyuh"},{"labels":["enhancement",null,null],"text":"I released Mish activation function a couple of weeks ago and it has a significant improvement in performance over ReLU, Swish and other commonly used activation functions. Full details along with the paper link provided in my repository here - https://github.com/digantamisra98/Mish\r\nMish was also used along with Ranger Optimizer to beat 12 records on the Fast.ai ImageNette and ImageWoof benchmarks. \r\nThe link to the relevant fast.ai forum - https://forums.fast.ai/t/meet-mish-new-activation-function-possible-successor-to-relu/53299/245\r\n\r\nI have the pytorch implementation of Mish here - https://github.com/digantamisra98/Mish/tree/master/Mish/Torch\r\n\r\nHopefully, if validated, I would like Mish to be a part of PyTorch by submitting a PR for ease of use. \r\n\r\nThank You!"},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\nbuild the pyTorch package with with c++11 ABI\r\n\r\n## Motivation\r\nSince in libtorch, we have release with Pre-cxx11 ABI and c++11 ABI, it is also necessary that we have the PyTorch package with both the Pre-cxx11 ABI and c++11 ABI.\r\n\n\ncc @ezyang"},{"labels":["enhancement",null,null,null],"text":"torch.distributed package currently implements two-sided point to point communication, i.e. send and recv. However, one-sided communication, i.e., put, get and accumulate routines are getting popular in mpi due to performance benefits and dynamic communication patterns. It would be great to implement these routines in pytorch.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nBy using DataParallel, the engine creates a thread for each GPU. It should do this for backpropagation too, including within python-implemented autograd.Functions.\r\n\r\n## Motivation\r\nWe're trying to implement synchronized batchnorm for one machine/one \"main\" process using an interface like the traditional DataParallel. To synchronize the forward pass (computation of the mean+variance) we use a threading.Barrier to sync the threads for each GPU.\r\n\r\nAs it turns out, torch's batchnorm uses a custom backwards function which gives a different gradient than you'd get just from doing a linear transform with the mean and variance: https://github.com/pytorch/pytorch/blob/v1.1.0/aten/src/ATen/native/Normalization.cpp\r\n\r\nTo implement a SyncBatchNorm that behaves identically on multiple GPUs as torch's batchnorm does on one GPU it looks like we'd need to synchronize things like the mean of grad_output across all GPUs. But we can't get our .backward to be entered by multiple threads simultaneously to pull this off.\r\n\r\n## Additional context\r\nAt first we were concerned that torch might not multithread backprop at all, which would be a huge slowdown for multi-GPU training, but it looks like it does for C implemented functions only? I can't find any clear documentation.\r\n"},{"labels":["enhancement",null,null],"text":"## Feature Request\r\n\r\nIs it possible to implement sparse Adam with weight decay?\r\n\r\ncc @vincentqb"},{"labels":["enhancement",null,null,null],"text":"This is needed when incorporating curvature information into optimization\r\n\r\nThere's PyTorch implementation of symmetric matrix square root op [here](https://github.com/msubhransu/matrix-sqrt) but they use PyTorch for backward pass only, and use scipy for forward pass\r\n\r\nI've hacked something together by mirroring `scipy.linalg.pinvh`\r\n\r\n```\r\n def symsqrt(a, cond=None, return_rank=False):\r\n    \"\"\"Computes the symmetric square root of a positive definite matrix\"\"\"\r\n\r\n    s, u = torch.symeig(a, eigenvectors=True)\r\n    cond_dict = {torch.float32: 1e3 * 1.1920929e-07, torch.float64: 1E6 * 2.220446049250313e-16}\r\n\r\n    if cond in [None, -1]:\r\n        cond = cond_dict[a.dtype]\r\n\r\n    above_cutoff = (abs(s) > cond * torch.max(abs(s)))\r\n\r\n    psigma_diag = torch.sqrt(s[above_cutoff])\r\n    u = u[:, above_cutoff]\r\n\r\n    B = u @ torch.diag(psigma_diag) @ u.t()\r\n    if return_rank:\r\n        return B, len(psigma_diag)\r\n    else:\r\n        return B\r\n\r\ndef symsqrt_test():\r\n    def randomly_rotate(X):\r\n        \"\"\"Randomly rotate d,n data matrix X\"\"\"\r\n        d, n = X.shape\r\n        z = torch.randn((d, d), dtype=X.dtype)\r\n        q, r = torch.qr(z)\r\n        d = torch.diag(r)\r\n        ph = d / abs(d)\r\n        rot_mat = q * ph\r\n        return rot_mat @ X\r\n\r\n    n = 20\r\n    d = 10\r\n    X = torch.randn((d, n))\r\n\r\n    # embed in a larger space\r\n    X = torch.cat([X, torch.zeros_like(X)])\r\n    X = randomly_rotate(X)\r\n    cov = X @ X.t()\r\n    sqrt, rank = symsqrt(cov, return_rank=True)\r\n    assert rank == d\r\n    assert torch.allclose(sqrt @ sqrt, cov, atol=1e-5)\r\n```\r\n\r\n\r\ncc @vishwakftw @SsnL @jianyuh"},{"labels":["enhancement",null,null],"text":"@jfc4050 added CPU `allreduce_coalesced` in #24949. The current version flattens all input tensors into one, and then allreduce that tensor. It works, even if, say, process 0 provides a tensor of size 4 and process 1 provides two tensors of size 2 each. This is a reasonable shortcut to avoid using additional communications to check tensor sizes, but it will be good to add size checking in the default mode, and asking users to explicitly set `check_sizes=False` to they indeed want to skip that.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera"},{"labels":["enhancement",null,null],"text":"@jfc4050 added CPU `allreduce_coalesced` in #24949. The next would be adding GPU support to `ProcessGroupNCCL` and `ProcessGroupGloo`.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera"},{"labels":["enhancement",null,null],"text":"https://arxiv.org/ftp/arxiv/papers/1908/1908.08681.pdf\r\nð‘“(ð‘¥) = ð‘¥ â‹… ð‘¡ð‘Žð‘›â„Ž(ð‘ ð‘œð‘“ð‘¡ð‘ð‘™ð‘¢ð‘ (ð‘¥)) = ð‘¥ â‹… ð‘¡ð‘Žð‘›â„Ž(ln(1 + ð‘’ð‘¥))\r\n\r\nUsing this Non-Monotonic Neural Activation Function, the author has shown the best Test Top-1 Accuracy on CIFAR-10 and CIFAR-100\r\n\r\nFast.ai students have also beaten the 5 epoch ImageWoof leaderboard score using the same activation.\r\nhttps://forums.fast.ai/t/how-we-beat-the-5-epoch-imagewoof-leaderboard-score-some-new-techniques-to-consider/53453\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nsome way to see the code for builtin_function_or_method\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nconsider this scenario\r\n1) I am using google colab, I want to see the implementation of nn.Dropout\r\n2) I ctrl+click on nn.Dropout, it takes me to dropout.py, where I find Dropout class, which returns\r\n`\r\nF.dropout(input, self.p, self.training, self.inplace)\r\n`\r\n3) so, I ctrl+click on F.dropout, it takes me to functional.py, where I see this\r\n`\r\nreturn (_VF.dropout_(input, p, training)\r\n            if inplace\r\n            else _VF.dropout(input, p, training))\r\n`\r\n4) now, I am stuck, because this is a builtin, and only way for me to look at its code, is to search for this in github.\r\n5) same thing happens when I want to see the implementation of nn.CrossMapLRN2d, it redirects to torch.group_norm, which is builtin.\r\n6) similarly for view, permute, this way to understand what these builtins are doing becomes difficult\r\n\r\nbecause of this I raised an issue in google colab, to provide a way to look at builtin_function_or_method, they told \r\n\r\n\"\r\nNo, there's no way to get directly to the source in this case.\r\n\r\nIn particular, for a .so, it could be a cython file, or it could be another kind of C extension (built via vanilla C/C++ code, or SWIG'd in, or something else completely). All we have is the installed package -- there's no way for us to trace that back to source, and in cases like this (torch is distributed as a .whl file) the source isn't included.\r\n\"\r\n\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\neuclidean distance as used in prototypical networks, included in standard PyTorch library.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\ncode from prototypical networks\r\n\r\n```\r\ndef euclidean_dist( x, y):\r\n    # x: N x D\r\n    # y: M x D\r\n    n = x.size(0)\r\n    m = y.size(0)\r\n    d = x.size(1)\r\n    assert d == y.size(1)\r\n\r\n    x = x.unsqueeze(1).expand(n, m, d)\r\n    y = y.unsqueeze(0).expand(n, m, d)\r\n\r\n    return torch.pow(x - y, 2).sum(2)\r\n```\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null,null],"text":"In PyTorch 1.2,\r\n\r\nFor `nn.Transformer`, the `src` dimensions correspond to `S, N, E` (sequence position, batch index, embedding feature), but the `src_key_padding_mask` dimensions correspond to `N, S`. That is, the batch is the second dimension of `src` but the first dimension of `src_key_padding_mask` (and other masks). I find this pretty confusing and inconsistent: either all parameters should be batch-first or they all should be batch-second.\r\n\r\nRelatedly, there should probably be an option analogous to the LSTM's `batch_first` argument. "},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nRequest to add python modules for executing quantized models on mobile.\r\n@ljk53 @dreiss \r\n \r\n## Motivation\r\nCurrently we don't have a way to execute quantized pytorch models on mobile. Mobile has different operator backends for quantized ops (QNNPACK) compared to server side (FBGEMM). This proposal outlines an approach similar to FBGEMM to add support for new mobile specific modules in `torch.nn.quantized` namespace.\r\n\r\n## Pitch\r\n\r\nIn order to execute quantized models on mobile, we can take two approaches - \r\n* Eager mode + TorchScript - Do calibration in eager mode and collect quantization statistics on FP32 model. Create quantized model which replaces FP32 ops with quantized ops. Call torch.jit.script on the quantized model.\r\n* Graph mode - Quantization pass in graph mode will insert quant, dequant nodes and perform the calibration. Finally it will replace the fp32 ops with the corresponding quantized backend ops. This will generate a torchscript that we can execute on mobile.\r\n\r\nSample change to add a new module -\r\n```\r\nclass QNNPackLinear(torch.nn.Module):\r\n   \r\n    _FLOAT_MODULE = nn.Linear\r\n\r\n    def __init__(self, in_features, out_features):\r\n        super(QNNPackLinear, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n\r\n        qweight = torch._empty_affine_quantized(\r\n            [out_features, in_features], scale=1, zero_point=0,\r\n            dtype=torch.quint8)\r\n        qbias = torch._empty_affine_quantized(\r\n            [out_features], scale=1, zero_point=0, dtype=torch.qint32)\r\n\r\n        #self.register_buffer('out_scale', torch.Tensor([1]))\r\n        #self.register_buffer('out_zero_point', torch.Tensor([0]))\r\n        self.out_scale = 1.0\r\n        self.out_zero_point = 0\r\n        self.set_weight_bias(qweight, qbias)\r\n        self.bias = qbias\r\n\r\n    def forward(self, x):\r\n        Y_q = torch.ops.quantized.qnnpack_linear_op(\r\n            x, self._packed_weight,\r\n            self.out_scale,\r\n            self.out_zero_point)\r\n        return Y_q\r\n\r\n    @torch.jit.export\r\n    def __getstate__(self):\r\n        return (\r\n            self.in_features,\r\n            self.out_features,\r\n            self.bias,\r\n            self._orig_weight,\r\n            self.out_scale,\r\n            self.out_zero_point\r\n        )\r\n\r\n    @torch.jit.export\r\n    def __setstate__(self, state):\r\n        # type: (Tuple[int, int, torch.Tensor, torch.Tensor, float, int]) -> None\r\n        self.in_features = state[0]\r\n        self.out_features = state[1]\r\n        self.bias = state[2]\r\n        self.set_weight_bias(state[3], state[2])\r\n        self.out_scale = state[4]\r\n        self.out_zero_point = state[5]\r\n\r\n    def weight(self):\r\n        return self._orig_weight\r\n\r\n    def set_weight_bias(self, weight, bias):\r\n        self._orig_weight = weight\r\n        self.bias = bias\r\n        self._packed_weight = torch.ops.quantized.qnnpack_linear_prepack(weight, bias)\r\n```\r\n\r\n\r\n## Alternatives\r\n\r\nUse the graph mode execution to add a quantization pass specific for mobile. This pass will replace the FP32 ops with the quantized counterparts based on the backend. \r\nUsing this approach will mean that we won't be able to support eager mode + torchscript approach to run models on mobile.\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov"},{"labels":["enhancement",null,null,null,null,null],"text":"# ðŸš€ Feature\r\n\r\nWe would like Pytorch to support the automatic mixed precision training recipe:  auto-casting of Cuda operations to FP16 or FP32 based on a whitelist-blacklist model of what precision is best for each operation, as well as gradient scaling to avoid underflow during the backward pass.\r\n\r\n# Work in progress\r\n\r\n* [PR #33366](https://github.com/pytorch/pytorch/pull/33366) for the gradient scaling API (merged!)\r\n* [PR #35102](https://github.com/pytorch/pytorch/pull/35102) for the autocasting API and eager-mode backend (merged!)\r\n* [Dedicated issue tracking jit support](https://github.com/pytorch/pytorch/issues/25387) (@jjsjann123 is driving this on our side)\r\n* [Jit scripting for context managers (`with` statements)](https://github.com/pytorch/pytorch/issues/28762 )\r\n\r\n# Motivation\r\n\r\nMixed precision is essential to achieve good performance on Tensor Core GPUs (Volta + Turing).  We've been trying to educate and evangelize for it since Volta's release.  We have [Apex](https://github.com/nvidia/apex), our own repository of tools for mixed precision training, which has seen moderate adoption.  However, forcing users to install a separate toolkit is burdensome, especially if their environments aren't set up to build extensions, and we'll never be able to achieve the same performance as a native integration.  Native, well documented support in Pytorch core is certainly the most convenient way to enable mixed precision training for the largest number of users.\r\n\r\n# Pitch\r\n\r\nAfter initial discussions with @mruberry, @zdevito, @jamesr66a, @gchanan, and @jjsjann123 we believe the UX should permit auto-casting and gradient scaling as modular and independent components.\r\n\r\n## Auto-casting\r\n\r\n### Background\r\n\r\nThe current philosophy of Apex's Automatic Mixed Precision (Amp, in recommended mode \"O1\") is that when training with mixed precision, the user never needs to manually alter the precision of their model or data.  The user declares their model in default (FP32) precision.  **The model parameters (leaves) are and remain FP32 for the duration of training.  These leaves are also directly owned and stepped by the optimizer,** which is identical to the ordinary behavior of Pytorch in the absence of mixed precision.\r\n\r\nTo ensure that FP16 is used for operations that benefit from it, `Tensor.*` methods and `torch.*` and `torch.nn.functional.*` functions are patched to cast data to a certain type before running the actual op.  Which type depends on what precision is best for that op.  For example, `torch.mm` is patched to cast the incoming input and weight to FP16, which enables Tensor Cores.  `torch.log` is patched to cast input to fp32, because log's forward and backward may require a large dynamic range.  This casting-as-data-flows-through-functions is the strategy used by Amp in Apex today, and achieves accuracy comparable to pure FP32 training on a wide range of networks.  It is also the strategy used by MXNet and Tensorflow's Amp integrations.  However, Apex's Amp is implemented by Python-side monkey-patching of `torch.*` and `torch.nn.functional.*` functions, which is invasive and not ideal for performance.\r\n\r\n### Proposed Implementation\r\n\r\nFor eager execution, we propose to integrate the same casting-as-data-flows-through-functions strategy that Apex's Amp uses.  We propose to implement this by inserting the casts in some of the autogenerated C++ functions on the Cuda dispatch path.  Each such function will be given a few additional lines by the autogeneration script.  For example, a whilelist function with 1 argument would be given something like\r\n```python\r\nif(autocasting_is_enabled())\r\n  input = input.half()\r\n```\r\nThese casts should be autograd-exposed, so they will be reversed in backward().  They should also precede the autograd-exposed call of the whitelist or blacklist op itself, so if the op saves its inputs for backward, the inputs are saved as the correct (post-cast) type.\r\n\r\nOn the Python side, the user shall request auto-casting by running the forward pass (or any portions of the forward pass where auto-casting is desired) under a nestable context manager, for example\r\n```python\r\n@contextlib.contextmanager\r\ndef autocast(whitelist_type=torch.float16, enabled=True):\r\n    old_whitelist_type, old_status = torch.get_autocasting_state()\r\n    torch.set_autocasting_state(whitelist_type, enabled)\r\n    try:\r\n        yield\r\n    finally:\r\n        torch.set_autocasting_state(original_whitelist_type, old_status)\r\n```\r\n`torch.get_` and `set_autocasting_state` will get/set a backend state that is queryable within C++ dispatch functions.\r\n\r\n`whitelist_type` can be changed to request that whitelist functions be autocast to types other than FP16.  The `enabled` argument can be used to locally disable autocasting if the user wishes to have manual control over the types that are used in some regions of their model, while permitting autocasting in others.\r\n\r\nMy initial thought is that `with autocast()` may be used to wrap any regions of code where a graph is being constructed via explicit Python invocations of Pytorch ops (ie, the forward pass), but shall not wrap any region where a previously constructed graph is being backwarded through.  All desired casting will have been recorded as part of the graph construction, and will be correctly reversed by a bare backward call without needing to be under an `autocast` context.  Backward passes with `create_graph=True` also belong to the latter category (ie, they should not be under a `with autocast()` context). [Gradient Penalty](#gradient-penalty) under End to End Examples below shows this philosophy more clearly.\r\n\r\nIt's possible that running both forward and backward under the context manager won't do any harm (ie, any resulting casts requested during backward will be no-ops, because the type flow is already being properly/faithfully reversed by autograd) and it can be permissible (but not required) to also allow the backward pass to take place under the context manager.\r\n\r\nWhen training with FP16, gradient scaling and auto-casting must both be requested by the user.  We envision that when autocasting to formats other than FP16, gradient scaling will not be necessary, and the auto-casting context manager can be used without any gradient scaling.\r\n\r\n### Example Usage\r\n```python\r\nwith autocast():\r\n    output = model(input)\r\n    loss = loss_fn(output, target)\r\n# The backward pass should be invoked outside the context manager.  All casting has been appropriately recorded as part of the forward pass.\r\n```\r\nWithin `model.forward`, if the user has regions where they wish explicit control over the precision, they may nest an invocation of `with autocast(enabled=False)`:\r\n```python\r\ndef forward(self, x):\r\n    x = self.layer_permitting_autocasting(x)\r\n    with autocast(enabled=False):\r\n        x = x.float()\r\n        x = self.explicitly_float_layer(x)\r\n    x = self.another_layer_permitting_autocasting(x)\r\n    return x\r\n```\r\n\r\n### Gotchas/Challenges\r\n\r\nThe Amp casts need to be recorded by autograd, so they'll be properly reversed in backward.  Unfortunately, for many ops, dispatch into an autograd-disabled region currently occurs in `VariableType*.cpp`, at a higher level of the call chain than the Cuda-specific dispatch functions.  `VariableType*.cpp` is also the level at which necessary data is saved for backward.  In other words, by the time we've reached the Cuda-specific dispatch functions, it's too late to invoke autograd-exposed casts.  The alternative is to insert the\r\n```python\r\nif(autocasting_is_enabled())\r\n  input = input.half() or float()\r\n```\r\nsnippets at the level of `VariableType*.cpp`, before we dive into autograd-disabled regions, but then these if statements will be on the hot path taken by all dispatches (Cuda and non-Cuda).  I'd like to avoid having the if statement on any non-Cuda code path.  This is a tricky problem and I need to think hard about it.\r\n\r\n## Gradient scaling\r\n\r\n### Background\r\n\r\nLate in training, FP16 gradients can underflow, halting convergence and in some cases causing destabilization.  Apex's Amp mitigates underflow via \"dynamic gradient scaling.\"  The implementation creates scaled gradients by handing the user `scaled_loss  =  loss*scale_factor`, then requiring that the user invoke `scaled_loss.backward()`. By the chain rule, all gradients flowing backward through the network are then scaled by `scale_factor`.  Apex's Amp attempts to maximize use of FP16's full dynamic range by choosing the highest `scale_factor` that can be used without incurring inf/nan gradients, which is accomplished as follows:  Initially, a high `scale_factor` is chosen.  Each iteration, after backward() returns, gradients are checked for infs/nans.  If any infs/nans are found, the optimizer skips the step, and the scale factor is reduced.  The scale factor is also periodically increased if a streak of successful (inf/nan free) iterations occurs.  Gradients are unscaled in FP32 before being applied to FP32 model parameters.\r\n\r\n### Proposed API\r\n\r\nUser scripts will implement gradient scaling with an instance of a helper class.  Typical use would look like\r\n```python\r\nscaler = torch.cuda.amp.AmpScaler()\r\n\r\nfor input, target in data:\r\n    optimizer.zero_grad()\r\n    output = model(input)\r\n    loss = loss_fn(output, target)\r\n    scaler.scale(loss).backward()\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\n\r\nCreating scaled gradients with `torch.autograd.backward` would look like\r\n```python\r\ntorch.autograd.backward(scaler.scale((output0, output1)), grad_tensors=(grad0, grad1))\r\n```\r\n\r\nCreating scaled gradients with `torch.autograd.grad` would look like\r\n```python\r\ntorch.autograd.grad(scaler.scale((output0, output1)), model.parameters(), grad_outputs=(grad0, grad1))\r\n```\r\n\r\n[The best explanation for `AmpScaler` is the class itself, as found in the gradient scaling PR](https://github.com/pytorch/pytorch/blob/35829f24ef2a71eacedcd6307b2fe9325e4a6a94/torch/cuda/amp/amp_scaler.py#L6).  It's lightweight and each function is documented.\r\n\r\n`scaler` internally initializes and maintains the scale value, and updates it each iteration based on whether `optimizer`'s gradients contained infs or NaNs.  If infs/NaN gradients are encountered in a given iteration, `scaler.update` reduces the scale.  If no inf/NaN gradients are encountered, `scaler.update` increases the scale slightly.  This approach achieves what dynamic gradient scaling intends:  over time, riding the edge of the highest gradient scale that can be used without incurring overflow.\r\n\r\nThe user also has the freedom to manually reset the scale value at the end of any iteration, by passing a new scale to `scaler.update` as a Tensor or Python float.\r\n\r\n`AmpScaler` instances contain at most a few Tensors and Python scalars.  When checkpointing, the user can easily save an `AmpScaler` instance as part of the checkpoint, alongside `model.state_dict()` and `optimizer.state_dict()`.  The model and optimizer instance(s) are not affected by AmpScaler, and remain exactly what they would be in the absence of mixed precision.  Therefore, behaviors and invocations of `model.state_dict()` and `optimizer.state_dict()` themselves may remain unaltered.\r\n\r\n### Interaction with Existing Optimizers\r\n\r\nAs long as training scripts adhere to the proposed API, existing optimizers (whether native or custom) will immediately work with mixed precision.  In particular, the gradient scaling API does not rely on changes to the Python source of `step()` in any existing optimizers.\r\n\r\n`scaler.step(optimizer)` [wraps `optimizer.step()` with logic to make it scaling-safe](https://github.com/pytorch/pytorch/blob/35829f24ef2a71eacedcd6307b2fe9325e4a6a94/torch/cuda/amp/amp_scaler.py#L146-L192).  Specifically, `scaler.step(optimizer)`\r\n- makes sure gradients are unscaled before `optimizer.step()`\r\n- skips `optimizer.step()` if inf/nan gradients are found.\r\n\r\n`optimizer.step()` itself is untouched, and again, does not need to change for existing optimizers.\r\n\r\n### Interaction with New Custom Optimizers\r\n\r\n`AmpScaler` defines a contract such that custom optimizers _may_ implement their own scaling-safe `step` methods if they choose to.  If an optimizer obeys this contract, [`AmpScaler.step` will call the optimizer's `step` method directly](https://github.com/pytorch/pytorch/blob/35829f24ef2a71eacedcd6307b2fe9325e4a6a94/torch/cuda/amp/amp_scaler.py#L179-L184).  This gives custom optimizer authors a control point to implement ninja optimizations like sync-free dynamic gradient scaling.\r\n\r\n### Gotchas/Challenges\r\n\r\nWhen the user invokes a backward pass with a scale factor, all gradients produced by this backward pass (leaf gradients produced by `loss.backward` or `torch.autograd.backward`, or out-of-place gradients produced by `torch.autograd.grad`) will be scaled.  Therefore, anything that manipulates the gradients between `scaler.scale(loss).backward()` and the `scaler.step(optimizer)` will require proper awareness of the scale factor.  Examples of operations that require scale-factor-aware treatment are \r\n- gradient clipping\r\n- gradient penalty computations.\r\n\r\nIn our opinion, requiring the user be aware of the scale factor when making direct use of the gradients is not terribly burdensome.  The user has explicitly requested gradient scaling; they should not be surprised when they end up with scaled gradients on their hands.  The treatment of such cases is also not difficult from a code-change perspective, as long as it is clearly documented.\r\n\r\nClosures are also a challenge.  Do we need to support explicit closure use?  Based on the scripts and issues I've seen, closure use is not terribly common, and LBFGS is the only native optimizer that requires a closure.  However, I think I can torture the proposed API into supporting closures if it turns out to be in high demand.\r\n\r\n## End to End Examples (Auto-Casting + Gradient Scaling)\r\n\r\n### Typical Use (1 loss, 1 optimizer)\r\n```python\r\nscaler = AmpScaler()\r\n...\r\nfor input, target in data:\r\n    optimizer.zero_grad()\r\n    with autocast():\r\n        output = model(input)\r\n        loss = loss_fn(output, target)\r\n    scaler.scale(loss).backward()\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\n\r\n### Gradient Clipping\r\n\r\nGradient clipping requires awareness that the gradients resulting from `scaler.scale(loss).backward()` are scaled. One simple way to account for the scale factor is by clipping to `max_norm*scaler.get_scale()` instead of max_norm:\r\n\r\n```python\r\nscaler = AmpScaler()\r\n...\r\nfor input, target in data:\r\n    optimizer.zero_grad()\r\n    with autocast():\r\n        output = model(input)\r\n        loss = loss_fn(output, target)\r\n    scaler.scale(loss).backward()\r\n\r\n    # Gradients are scaled, so we clip to max_norm*scale\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm*scaler.get_scale())\r\n\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\nHere the scaled gradients are clipped. `scaler.step(optimizer)` is aware that gradients have not yet been unscaled, and unscales them under the hood before calling `optimizer.step()`.\r\n\r\n### Gradient Clipping with Explicit Unscaling\r\n\r\nThe specific case of clipping scaled gradients isnâ€™t so hard (all you have to do is clip to `max_norm*scaler.get_scale()`). However, in general, between the backward pass and the optimizer step you may wish to manipulate gradients in some way thatâ€™s not so easy to translate to scaled gradients. In such cases, you can unscale and step separately. Hereâ€™s how that looks, using gradient clipping as an example once more:\r\n\r\n```python\r\nscaler = AmpScaler()\r\n...\r\nfor input, target in data:\r\n    optimizer.zero_grad()\r\n    with autocast():\r\n        output = model(input)\r\n        loss = loss_fn(output, target)\r\n    scaler.scale(loss).backward()\r\n\r\n    scaler.unscale(optimizer)\r\n    # Since the optimizer's owned gradients are unscaled, we can clip to max_norm directly:\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\r\n\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\n\r\n### Gradient Penalty\r\n(based on **Higher order gradients** from the [release notes](https://github.com/pytorch/pytorch/releases?after=v0.3.0))\r\n\r\nGradient penalty also requires awareness that the gradients are scaled in certain places. Additionally, gradient penalty demonstrates:\r\n- `torch.autograd.grad`\r\n- Double-backward\r\n- Some clarification of what counts as a \"forward pass\" for the purpose of using `with autocast` (in other words, when exactly it's appropriate to use `with autocast`).\r\n\r\nThe following shows an implementation of gradient penalty under the proposed API.\r\n```python\r\nscaler = AmpScaler()\r\n...\r\nfor input, target in data:\r\n    optimizer.zero_grad()\r\n    with autocast():\r\n        output = model(input)\r\n        loss = loss_fn(output, target)\r\n\r\n    # We should scale outputs for the out-of-place backward pass\r\n    grad_params = torch.autograd.grad(scaler.scale(loss), model.parameters(), create_graph=True)\r\n\r\n    # In general, the penalty term may depend nonlinearly on the out-of-place gradients, so to be safe,\r\n    # manually unscale them before computing the penalty.  This unscale should be autograd-exposed.\r\n    grad_params = [p*(1./scaler.get_scale()) for p in grad_params]\r\n\r\n    # Compute the penalty term and add it to the loss.\r\n    # The penalty term computation is effectively another snippet of forward pass, so it makes\r\n    # sense to enable autocasting for this section as well:\r\n    with autocast():\r\n        grad_norm = 0\r\n        for grad in grad_params:\r\n            grad_norm += grad.pow(2).sum()\r\n        grad_norm = grad_norm.sqrt()\r\n        loss = loss + grad_norm\r\n\r\n    # The usual scaling for backward will now accumulate leaf gradients that are appropriately scaled.\r\n    scaler.scale(loss).backward()\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\n\r\nGradient penalty is a tricky case to think about but writing the code is simple once the right pattern is established.  Compared to the [example in the release notes](https://github.com/pytorch/pytorch/releases?after=v0.3.0), the only extra line for gradient penalty computation is the unscaling `grad_params = [p*(1./scaler.get_scale()) for p in grad_params]`.  I think this can be considered a documentation problem, and addressed by providing clear examples.\r\n\r\n### Multiple Models/Optimizers/Losses\r\n\r\nNetworks must use the same `AmpScaler` instance (and therefore the same scale) to create gradients for all backward passes in a given iteration, otherwise we open the door to nasty corner cases.  For example, if two different losses, with different gradient scales, accumulate into the same parameters' .grads, the accumulation math breaks.  If two different losses, with different gradient scales, accumulate into different parameters owned by the same optimizer, then when you invoke `scaler.unscale(optimizer)`, there's no single correct value that can be used to unscale all the gradients owned by that optimizer, and handling multiple scale factors for different parameters within the same optimizer would get ugly fast.  Requiring that networks use the same `AmpScaler` instance for all backward passes avoids all such control flow difficulties, while still achieving what loss scaling is meant to achieve.\r\n\r\n`scaler.update()` must be called only at the end of the iteration, after `scaler.step(optimizer)` has been called for all optimizers used this iteration.  This requirement allows `update` to account for infs/nans found among any of the optimizers' gradients.\r\n\r\n```python\r\nscaler = torch.cuda.amp.AmpScaler()\r\n...\r\nfor input, target in data:\r\n    optimizer0.zero_grad()\r\n    optimizer1.zero_grad()\r\n    with autocast():\r\n        output0 = model0(input)\r\n        output1 = model1(input)\r\n        loss0 = loss_fn(2 * output0 + 3 * output1, target)\r\n        loss1 = loss_fn(3 * output0 - 5 * output1, target)\r\n\r\n    scaler.scale(loss0).backward(retain_graph=True)\r\n    scaler.scale(loss1).backward()\r\n\r\n    # Users can choose which optimizers receive explicit unscaling\r\n    scaler.unscale(optimizer0)\r\n\r\n    scaler.step(optimizer0)\r\n    scaler.step(optimizer1)\r\n    scaler.update()\r\n```\r\n\r\nI had to write Apex's Amp to handle arbitrary combinations of multiple models/optimizers/losses.  I'm painfully aware of the complicated combinations of models/optimizers/losses people want to implement.  In my opinion, the proposed interface permits a great deal of flexibility in network design.\r\n\r\n### Gradient accumulation\r\n\r\nGradient accumulation across iterations (between steps) is a common use case.  The proposed API accommodates gradient accumulation without trouble:\r\n```python\r\nscaler = AmpScaler()\r\n...\r\nfor i, (input, target) in enumerate(data):\r\n    with autocast():\r\n        output = model(input)\r\n        loss = loss_fn(output, target)\r\n        loss = loss/iters_to_accumulate\r\n    scaler.scale(loss).backward()\r\n    if (i + 1) % iters_to_accumulate == 0:\r\n        scaler.step(optimizer)\r\n        scaler.update()\r\n        optimizer.zero_grad()\r\n```\r\n\r\n### Switching automatic mixed precision on and off\r\n\r\nIf users want to run with or without autocasting+gradient scaling, they shouldn't have to litter their code with if statements.  The API should allow one code path that accommodates easily switching autocasting+gradient scaling on and off.\r\n\r\nThe autocasting context manager and `AmpScaler` constructor provide such convenience by accepting an `enabled=False` argument.\r\n\r\nIn the following example, autocasting and gradient scaling can be switched on and off by flipping `args.use_mixed_precision` with no additional control flow required.\r\n\r\n```python\r\nscaler = AmpScaler(enabled=args.use_mixed_precision)\r\n...\r\nfor input, target in data:\r\n    optimizer.zero_grad()\r\n    with autocast(enabled=args.use_mixed_precision):\r\n        output = model(input)\r\n        loss = loss_fn(output, target)\r\n    scaler.scale(loss).backward()\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\n\r\n### Batch replay\r\n\r\nSometimes every iteration/batch is valuable enough that users don't want to skip any.  Instead, it's preferable to replay the batch with a reduced loss scale until gradients do not contain infs/nans.  Batch replay control flow is not provided by the API alone, but with the proposed gradient scaling PR, it would be easy to rig:\r\n\r\n```python\r\nscaler = AmpScaler()\r\n...\r\nfor input, target in data:\r\n    # Replay the batch, updating the scale if necessary, until we receive gradients that aren't inf/nan.\r\n    while True:\r\n        optimizer.zero_grad()\r\n        with autocast():\r\n            output = model(input)\r\n            loss = loss_fn(output, target)\r\n        scaler.scale(loss).backward()\r\n        scaler.unscale(optimizer)\r\n        if scaler._found_inf(optimizer).item():\r\n            scaler.update()\r\n        else:\r\n            break\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\n\r\n# Alternatives\r\n\r\n### Python-side alternatives for gradient scaling and unscaling\r\n\r\nThe supplementary information contains an in-depth discussion of some alternatives I considered for the [gradient scaling](https://gist.github.com/mcarilli/43445260404c8d7cd79d84439808250e#file-scaling_apis_considered-md) and [gradient unscaling](https://gist.github.com/mcarilli/43445260404c8d7cd79d84439808250e#file-unscaling_apis_considered-md) API. \r\n\r\n### Gradient scaling in the autograd backend\r\n\r\nI recently submitted [a PR](https://github.com/pytorch/pytorch/pull/24893) that implemented gradient scaling directly in the autograd engine (Engine::execute).\r\n\r\nBenefits:\r\n - It automatically enabled gradient scaling for all typical single- and double-backward use cases (including complex cases like gradient penalties) without requiring any change to user scripts.\r\n- Gradients returned visibly to the user, either in-place via `.grad` attributes or out-of-place via a call to `torch.autograd.grad`, were only ever unscaled, eliminating the need to change gradient clipping or manually unscale before computing gradient penalties.\r\n\r\nDrawbacks:\r\n- Modifying engine.cpp, especially for a GPU-specific purpose, is not to be done lightly (ie, not until alternatives are exhausted).\r\n- It's unclear what the user-facing API would look like.  I figured the implementation was general enough to permit many options, and the exact API could be nailed down later.  However, it certainly requires maintaining a global \"amp state,\" which is not as clean as the purely functional approach to gradient scaling described above. \r\n- Altering the backend to black-box gradient scaling and require no change to user scripts is a double-edged sword.  Explicit Python-side control and visibility of gradient scaling, as we propose above, is not a bad thing.  @cbcase, this seems like an instance of the https://www.jwz.org/doc/worse-is-better.html thing you told me about...\r\n\r\ncc @ezyang @gchanan @vincentqb\r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"### Purpose of this issue\r\nThis issue tracks RFCs and PRs related to the design and implementation of NestedTensors. It reflects the current state of the project. It is however **not** to discuss details of the projects, but solely for information purposes. For discussion please refer to the corresponding RFCs and PRs. You can expect this issue to be edited frequently to reflect the current state of the project.\r\n\r\n\r\n### RFCs\r\n\r\n- [x] [0.0.1](https://github.com/pytorch/pytorch/issues/22169) - Introduction, tensor-wise operations, implementation considerations.\r\n\r\n- [x] [0.0.2](https://github.com/pytorch/pytorch/issues/25112) - Definition of nested and tensor dimensions; tensor dimension only broadcasting and reductions; tensor+mask construction functions; prototype dispatch implementation; implicit autograd support; unbind, to_tensor; as_nested_tensor; nested_dim; nested_size(i)\r\n\r\n- [ ] 0.0.3 - Definition of ```__iter__```, ```__getitem__```, flatten, tensor-wise tuple/list arguments, contiguous. Migration plan into pytorch/pytorch. Rename tensorwise to vmap.\r\n\r\n- [ ] 0.0.4 - Definition of expand, reshape, view, narrow, squeeze, select.\r\n\r\n- [ ] 0.0.5 - Performance benchmarks and implementations, explicit autograd support (if necessary), improved dispatch (if necessary/available), release.\r\n\r\n### <s>PR\r\n\r\n- [ ]  Prototype implementation of 0.0.1 and 0.0.2 to support current feature branches https://github.com/pytorch/pytorch/pull/22783</s>\r\n\r\n### Repository\r\n\r\n- [pytorch/nestedtensor](https://github.com/pytorch/nestedtensor) with pypi as ```pip install nestedtensor``` (needs update).\r\n\r\n### Examples\r\n\r\n- [pytorch/nestedtensor/examples](https://github.com/pytorch/nestedtensor/tree/master/examples)\r\n\r\n### Feature branches\r\nThese are updated frequently. They don't always comply with the RFCs and might exhibit usage of experimental or undocumented features. They mainly serve as playgrounds for dogfooding and as examples. They won't land before NestedTensors are actually part of a PyTorch release and up to the respective project standards. \r\n\r\n- https://github.com/facebookresearch/ParlAI/compare/master...cpuhrsch:nestedtensor\r\n- https://github.com/cpuhrsch/vision/compare/formatting...cpuhrsch:ntsegmentation2\r\n- https://github.com/pytorch/vision/compare/master...cpuhrsch:imglist2\r\n\r\n### POCs\r\n@cpuhrsch \r\n"},{"labels":[null,"enhancement",null,null],"text":"Currently, `final_callbacks_` of an autograd engine is a vector shared by all backward passes on this engine.  The autograd engine [clears](https://github.com/pytorch/pytorch/blob/936632b120aafd2d12233c1096d9000fe9554b77/torch/csrc/autograd/engine.cpp#L666) final callbacks at the beginning of the backward call. With reentrant backward calls, the final callback enqueued by the original backward call will be cleared by the reentrant ones (learnt from @pritamdamania87). I was a little surprised on this API behavior as I queued a callback but it never fires. Would it be better if we attach these final callbacks to each graph task? For example, we could use a thread local stack to hold the current graph task, and then push and pop accordingly similar to what we did for `current_depth`. Thoughts?\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["enhancement",null,null],"text":"As pointed out by @zzzwen, current `distributed.rpc` implementation always serializes a tensor into bytes and creates a new tensor (and storage) during deserialization. For processes on the same server, it will be much more efficient if we can enable shared memory across processes. The basic idea is to do something similar to [`sharedFd`](https://github.com/pytorch/pytorch/blob/master/torch/csrc/generic/StorageSharing.cpp#L153) (thanks @yf225 ), and serialize the shared-memory tensor into `fd` and `size`. During deserialization, it will just attach the same storage to the new tensor. \r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera"},{"labels":["enhancement",null,null],"text":"#22907 addressed NCCL error detection for multi-server use cases. However, for some unknown reason, the current solution does not work for single-server multi-process use cases. There are indeed many scenarios where applications spawn one process per gpu on a multi-gpu server. So we do need the error detection work for single-server use cases.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera"},{"labels":[null,"enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\n\r\nA new test structure that:\r\n\r\n- allows backends to specify which tests (and which variants of those tests) should be run when testing them\r\n- lets backends specify which (other) backend to compare against (where applicable)\r\n- lets new backends quickly register themselves for testing\r\n\r\nFor example, a backend like XLA  should be able to register itself for a suite of backend-generic tests simply, a backend like CUDA should be able to mark some test inputs, like bfloat16, as unsuitable, and an FPGA backend might list some generic tests as unsuitable because they use unsupported ops.\r\n\r\nUsers should be able to request that tests be run for a specific backend and compared to another. E.g. \"run the CUDA tests and compare outputs with the CPU backend.\" Ideally users can also specify input data types, too, like \"run only the bfloat16 XLA tests.\" \r\n\r\n## Motivation\r\n\r\nHistorically PyTorch has only seriously supported CPU and CUDA (plus HIPMasqueradingasCUDA) backends. We want to support a variety of backends, however, and providing them a simple mechanism to hook into PyTorch's existing test infrastructure will be immensely helpful in ensuring they correctly implement PyTorch's interface. Giving backend developers more control over the tests they run should also improve their speed of development.\r\n\r\n## Pitch\r\n\r\nSee feature. I'll create an initial proposal for us to review and post it back to this issue.\r\n\r\n## Alternatives\r\n\r\nWe need to let new backends test their implementation of the PyTorch interface, the only question is how much work we do to support this. This issue is to discuss possible improvements. \r\n\r\n@ailzhang @VitalyFedyunin @izdeby \r\n@dlibenzi \r\n@csarofeen\r\n\n\ncc @ezyang @gchanan"},{"labels":["enhancement",null,null],"text":"## â“ Can pytorch 1.2 support cuda 9.0?\r\n\r\nI try to install pytorch 1.2.0 with anaconda, but found that `torch.cuda.is_available()` always returns False. Turns out our server environment can only support CUDA 9.0, but I do not have administrator right to upgrade the nvidia driver.\r\nGive that pytorch 1.2.0 introduced Bool type, and I have already updated my code for corresponding change of API, I do not want to undo this efforts.\r\nWill pytorch 1.2.0 support CUDA 9.0 backend in the future?\n\ncc @ezyang"},{"labels":["enhancement",null,null,null],"text":"The Transformer implementation docs (https://pytorch.org/docs/stable/nn.html?highlight=transformer#torch.nn.Transformer) state that they implement the original paper but fail to acknowledge that they donâ€™t implement the following: \r\n* Layer Norm as the default normalization option. \r\n* Positional Encodings\r\n* Embeddings before the encoding and decoding step \r\n\r\nItâ€™s fine that these are all not implemented directly in the module but making it more clear that they arenâ€™t and were in the original paper would be helpful. "},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nONNX Supports ROIAlign (see https://github.com/onnx/onnx/blob/master/docs/Operators.md#RoiAlign)\r\n\r\nROIAlign is implemented in torchvision, but you cannot export it using the onnx exporter.\r\n\r\n\r\n## Motivation\r\n\r\nROIAlign is important for most 2 Stage object detection algorithms.\r\n\r\n## Alternatives\r\n\r\nA ugly way would be to use adaptive_max_pool2d with a for loop, but i cannot export it to onnx, too.\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nIntroduce a `verbose=False` flag for `torch.hub.load`\r\n\r\n## Motivation\r\n\r\nWhen running inference using torch hub models on multiple files, it becomes a bit too verbose when always being confronted with the message\r\n\r\n`Using cache found in ...`\r\n\r\nthis is produced by https://github.com/pytorch/pytorch/blob/0a12ff7c5b59cbd47dc865603e45178903f6dfa8/torch/hub.py#L156\r\n\r\n## Pitch\r\n\r\n`torch.hub.load(..., verbose=False)` should suppress the prints (and possibly also the loading progress bars)\n\ncc @ailzhang"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nWhen creating a SummaryWriter, specifying a path in an S3 or GCP bucket should directly write to the bucket instead of the local filesystem\r\n\r\n## Motivation\r\n\r\nBoth in tensorflow and tesorboardX you can specify s3:// or gs:// paths in your logdir, which greatly simplifies distributed training and monitoring, you also can launch tensorbaord directly from your local machine, or a notebook pointing directly to the bucket, which means no need to launch a machine to share results, just the URL to the results inside the bucket\r\n\r\n## Additional context\r\n\r\ntensorboardX implementation https://github.com/lanpa/tensorboardX/blob/master/tensorboardX/record_writer.py#L57\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nFor the type of  _src_mask_ , _memory_mask_ and  _tgt_mask_  in nn.Transformer module, I suggest using **torch.bool**  instead of  **torch.float32** while **torch.bool** was already introduced in v1.2.\r\n\r\n## Motivation\r\nDocument mentioned _[src/tgt/memory] mask should be filled with float(â€˜-infâ€™) for the masked positions and float(0.0) else_.\r\nFor the **torch.float32** type  of masks , it is not very convenient when we set the mask.\r\nIf it is **torch.bool**, we can easily write:\r\n`mask = ~ (src > 0)`\n\ncc @SsnL"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nI assume `register_backward_hook` should catch the gradients. At the moment I was not able to get the gradients, unless reading them from a loop. I am providing the complete example you can run and test.\r\n\r\n## Motivation\r\nThis would be great feature for telemetry.\r\n\r\n## Additional context\r\nI am providing the gist https://gist.github.com/dejanbatanjac/86d02413118cd7fb10b479c1d4c8f318 \r\n\n\ncc @ezyang @gchanan @SsnL @albanD"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n```python\r\nimport torch\r\n\r\na = torch.tensor([[0,0,1,1,1],[0,0,1,1,2],[3,3,3,3,2]])\r\nc = 2\r\nfeat = torch.randn(c, *a.size())\r\nnum_sp = a.max().long() + 1\r\n\r\n# For now, I have to use torch.masked_select for many times\r\noutput = []\r\nfor i in range(num_sp):\r\n    # (c*1), mean of each SP.\r\n    output.append(torch.masked_select(feat, a==i).view(c, -1).mean(-1, keepdim=True)) \r\noutput = torch.stack(output)\r\nprint('feat')\r\nprint(feat)\r\nprint('ref_mask')\r\nprint(ref_mask)\r\nprint('output')\r\nprint(output)\r\n\r\n# Something like this will be better\r\noutput, return_index = torch.value_select(feat, mask_ref = a, values=torch.arange(num_sp))\r\n\r\n```\r\n```bash\r\nfeat\r\ntensor([[[-0.6140, -1.5337,  1.4974, -1.2668, -0.3866],\r\n         [-0.1943,  0.0868,  1.0380,  0.1135,  1.1820],\r\n         [-1.4744,  1.5889, -0.2627, -1.7951, -0.0045]],\r\n\r\n        [[-0.8168, -0.6949, -1.9554, -0.5807, -0.2200],\r\n         [-0.7170, -0.8191,  0.5452,  1.6736, -0.6598],\r\n         [-0.6220,  0.8880, -0.6597,  0.6438, -1.5148]]])\r\nref_mask\r\ntensor([[0, 0, 1, 1, 1],\r\n        [0, 0, 1, 1, 2],\r\n        [3, 3, 3, 3, 2]])\r\noutput\r\ntensor([[[-0.5638],\r\n         [-0.7619]],\r\n\r\n        [[ 0.1991],\r\n         [-0.1075]],\r\n\r\n        [[ 0.5887],\r\n         [-1.0873]],\r\n\r\n        [[-0.4858],\r\n         [ 0.0625]]])\r\n```\r\n\r\n## Motivation\r\nIn some cases, label matrix (ref_mask above) indicates which class the pixel should be categorized to. Suppose we get some labels from a super-pixel algorithm, then I want to calculate the mean of SP region or perform other operations for each region. Then `value_select` function is quite useful. In a word, it performs `region-wise` selection and will be useful for other tasks too.\r\n\r\n## Pitch\r\nAs the pixels contained in each region is not constant, then we need to return the indexes for each region, logged in `return_index`. "},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nComputation of a subset of eigenvalues and eigenvectors\r\n\r\nInspired by [MATLAB eigs](https://it.mathworks.com/help/matlab/ref/eigs.html#bu2_q3e-sigma):\r\n\r\n    o = torch.eigs(A, B, k, sigma)\r\n\r\nSolves the generalized eigenvalue problem `A * V = d * B * V `\r\n\r\n- `k`: Number of eigenvalues to compute, specified as a positive scalar integer.\r\n- `sigma`: The eigenvalues closest to the numberÂ sigma.\r\n\r\n\r\n## Motivation\r\nI need the first `k` smallest eigenvalues of a `n * n` matrix, currently it's an intractable problem in PyTorch.\r\n\r\ne.g. with `k=30` and `n=500`\r\n\r\nMATLAB:\r\n```matlab\r\n[phi, e] = eigs(S, L, 30, -1e-5);\r\n% Elapsed time is 0.074229 seconds.\r\n```\r\n\r\nNumPy:\r\n```python\r\ne, phi = scipy.sparse.linalg.eigs(S, 30, L, sigma=-1e-5)\r\n# Elapsed time is 2.5 seconds.\r\n```\r\n\r\nPyTorch:\r\n```python\r\ne, phi = torch.symeig(C, eigenvectors=True)\r\n\r\n# cpu\r\n# Elapsed time is 5 seconds plus the time for the cholesky decomposition. \r\n\r\n# cuda\r\n# Elapsed time is 2 seconds plus the time for the cholesky decomposition. \r\n```\r\n\r\n## Pitch\r\nI want to be able to compute the subset of eigenvalues/eigenvectors of a matrix that I need, without computing all the others.\r\n\r\n## Alternatives\r\nComputing all the eigenvalues/eigenvectors and then selecting the relevant ones.\r\n\r\n## Usecase \r\nThe [Laplace-Beltrami](https://en.wikipedia.org/wiki/Laplace%E2%80%93Beltrami_operator) operator is a key tool in geometry processing, and its eigenvalues/eigenvectors are an optimal basis to represent smooth functions on surfaces.\r\nFor example, it's common to project functions into the basis of LB eigenvectors, truncating the base to the first k-elements. This is equivalent to the low-pass filtering of the Fourier series:\r\n\r\n![spec8](https://user-images.githubusercontent.com/11019190/62874586-d2d11900-bd21-11e9-8bbd-cb646af36568.png)\r\n\r\nMoreover, the associated eigenvalues encode [properties of the surface](https://arxiv.org/abs/1811.11465), e.g. when sorted they lie on a line and the slope of this line is proportional to the surface's area.\r\n\r\nIf only `k` elements of the spectrum will be used, it is extremely wasteful computing all the eigenvalues/eigenvectors of the Laplace-Beltrami operator, since when discretized their number is equal to the number of vertices in the mesh, yielding an intractable problem in practice.\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nNote, this is one of the features discussed in gh-22402, see that issue for full context.\r\n\r\nImplement a `__torch_function__` method on `Tensor` (public) and a `torch_function_dispatch` decorator (private). The method contains a dispatch mechanism that can be used to dispatch to `Tensor`-like objects, that can then handle the `torch.somefunction` call the way they see fit. I.e. torch functions that take at least one tensor input parameter become overridable.\r\n\r\nShould be able to do this without any additional overhead when the input *is* `Tensor`, and sub-microsecond when it is `Tensor`-like.\r\n\r\nThe mechanism is completely analogous to NumPy's `__array_ufunc__` and `__array_function__` methods.\r\n\r\n## Motivation\r\n\r\n`torch` functions need to become overridable. One concrete user that @ezyang mentioned is `NestedTensor`, and there will be others. See also gh-22402.\r\n\r\n## Plan\r\n\r\nFirst build a prototype and apply it to only a couple of functions in the main `torch` namespace (to review/evaluate). Make sure they have different signatures from each other. E.g. `max`, `dot`, `svd`.\r\n\r\nUse a toy `Tensor` ducktype class, along the lines of `DiagonalArray` in https://numpy.org/devdocs/user/basics.dispatch.html, to implement a working example with the functions that are overridden.\r\n\r\n- First make it work (parts can be in Python) with the couple of functions chosen\r\n- Then make it fast - reuse the existing checks to ensure zero overhead on `func(Tensor, ...)`- needs to all be in C++\r\n- Once that's good, expand coverage to the whole API."},{"labels":["enhancement",null,null],"text":"## ðŸ› Bug\r\n\r\nUsing  `torch.utils.checkpoint.checkpoint_sequential` and `torch.autograd.grad` breaks when used in combination with `DistributedDataParallel` resulting in the following stacktrace\r\n\r\n```\r\nTraceback (most recent call last):                                                                    \r\n  File \"minimal_buggy_2.py\", line 198, in <module>                                                    \r\n    train(hps)                                                                                        \r\n  File \"minimal_buggy_2.py\", line 179, in train                                                       \r\n    loss.backward()                                                                                   \r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/tensor.py\", line 107, in backward                \r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)                               \r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 93, in backward      \r\n    allow_unreachable=True)  # allow_unreachable flag                                                 \r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/autograd/function.py\", line 77, in apply         \r\n    return self._forward_cls.backward(self, *args)                                                    \r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py\", line 99, in backward       \r\n    torch.autograd.backward(outputs, args)                                                            \r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 93, in backward      \r\n    allow_unreachable=True)  # allow_unreachable flag                                                 \r\nRuntimeError: has_marked_unused_parameters_ ASSERT FAILED at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:181, please report a bug to PyTorch.\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Running `python -m torch.distributed.launch --nproc_per_node=2 minimal_buggy.py` works\r\n1. Running `python -m torch.distributed.launch --nproc_per_node=2 minimal_buggy.py --parallel` breaks\r\n\r\nThe code for `minimal_buggy.py` is here:\r\n```python\r\nfrom argparse import ArgumentParser, Namespace\r\nimport numpy as np\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.distributed as dist\r\nfrom torch.utils.checkpoint import checkpoint_sequential\r\n\r\nimport torchvision.datasets as ds\r\nfrom torchvision import transforms\r\n\r\n\r\nclass GradientStep(nn.Module):\r\n\r\n    def __init__(self, energy):\r\n        super(GradientStep, self).__init__()\r\n        self._energy = energy\r\n        self.step_size = 0.1\r\n\r\n    def forward(self, x: torch.Tensor):\r\n        with torch.enable_grad():\r\n            x.requires_grad_()\r\n            omega = self._energy(x).sum()\r\n            grad_out = torch.ones_like(omega).to(x.device)\r\n            dx = torch.autograd.grad(outputs=omega, inputs=x, grad_outputs=grad_out,\r\n                                     create_graph=True, retain_graph=True,\r\n                                     allow_unused=True)[0]\r\n            dx.requires_grad_()\r\n            x = (x - self.step_size ** 2 * dx)\r\n        return x\r\n\r\n\r\nclass FFN(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(FFN, self).__init__()\r\n        self.n_hidden = 1024\r\n        self._energy = nn.Sequential(\r\n            nn.Linear(28**2, self.n_hidden),\r\n            nn.LeakyReLU(),\r\n            nn.Linear(self.n_hidden, self.n_hidden),\r\n            nn.LeakyReLU(),\r\n            nn.Linear(self.n_hidden, self.n_hidden),\r\n            nn.LeakyReLU(),\r\n            nn.Linear(self.n_hidden, 1))\r\n        self.L = 10\r\n\r\n    def forward(self, x: torch.Tensor):\r\n        y = x.clone().to(x.device)\r\n        y.requires_grad_()\r\n        fwd = nn.Sequential(*[GradientStep(self._energy) for _ in range(self.L)])\r\n        y = checkpoint_sequential(fwd, self.L, y)\r\n        return y\r\n\r\n\r\ndef get_distributed_mnist_iterators(batch_size, **kwargs):\r\n    def _worker_init_fn(worker_id):\r\n        np.random.seed(np.random.get_state()[1][0] + worker_id)\r\n\r\n    base_transforms = [\r\n        transforms.RandomHorizontalFlip(),\r\n        transforms.ToTensor(),\r\n        transforms.Lambda(lambda x: x.reshape(-1, ))]\r\n\r\n    train_dataset = ds.MNIST(\r\n        '/tmp/mnist/train',\r\n        train=True,\r\n        download=True,\r\n        transform=transforms.Compose(base_transforms))\r\n\r\n    test_dataset = ds.MNIST(\r\n        '/tmp/mnist/test',\r\n        train=False,\r\n        download=True,\r\n        transform=transforms.Compose(base_transforms))\r\n\r\n    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\r\n    test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset)\r\n\r\n    train_loader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        batch_size=batch_size,\r\n        shuffle=False,\r\n        num_workers=kwargs.get('workers', 4),\r\n        pin_memory=True,\r\n        sampler=train_sampler,\r\n        worker_init_fn=_worker_init_fn)\r\n\r\n    test_loader = torch.utils.data.DataLoader(\r\n        test_dataset,\r\n        batch_size=kwargs.get('test_batch_size', 100),\r\n        shuffle=False,\r\n        num_workers=kwargs.get('workers', 4),\r\n        pin_memory=True,\r\n        sampler=test_sampler,\r\n        worker_init_fn=_worker_init_fn)\r\n\r\n    return train_loader, test_loader, train_sampler, test_sampler\r\n\r\n\r\ndef get_mnist_iterators(batch_size, **kwargs):\r\n\r\n    base_transforms = [\r\n        transforms.RandomHorizontalFlip(),\r\n        transforms.ToTensor(),\r\n        transforms.Lambda(lambda x: x.reshape(-1, ))]\r\n\r\n    train_dataset = ds.MNIST(\r\n        '/tmp/mnist/train',\r\n        train=True,\r\n        download=True,\r\n        transform=transforms.Compose(base_transforms))\r\n\r\n    test_dataset = ds.MNIST(\r\n        '/tmp/mnist/test',\r\n        train=False,\r\n        download=True,\r\n        transform=transforms.Compose(base_transforms))\r\n\r\n    train_loader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        batch_size=batch_size,\r\n        shuffle=True,\r\n        num_workers=kwargs.get('workers', 4),\r\n        pin_memory=True,\r\n        sampler=None)\r\n\r\n    test_loader = torch.utils.data.DataLoader(\r\n        test_dataset,\r\n        batch_size=kwargs.get('test_batch_size', 100),\r\n        shuffle=True,\r\n        num_workers=kwargs.get('workers', 4),\r\n        pin_memory=True,\r\n        sampler=None)\r\n\r\n    return train_loader, test_loader\r\n\r\n\r\ndef parse_args() -> Namespace:\r\n    parser = ArgumentParser()\r\n    parser.add_argument('--local_rank',\r\n                            type=int,\r\n                            default=0,\r\n                            help=\"Is being set by the pytorch distributed launcher\")\r\n\r\n    parser.add_argument('--parallel',\r\n                        default=False,\r\n                        action='store_true')\r\n\r\n    hps = parser.parse_args()\r\n    return hps\r\n\r\n\r\ndef train(hps: Namespace):\r\n\r\n    if hps.parallel:\r\n        train_loader, test_loader, _, _ = get_distributed_mnist_iterators(\r\n            batch_size=32)\r\n    else:\r\n        train_loader, test_loader = get_mnist_iterators(\r\n            batch_size=32)\r\n\r\n    model = FFN()\r\n    model.cuda()\r\n    if hps.parallel:\r\n        model = nn.parallel.DistributedDataParallel(model)\r\n    crit = nn.MSELoss()\r\n\r\n    opt = torch.optim.Adam(model.parameters(), lr=1e-4)\r\n    for epoch in range(100):\r\n        for step, (b, lbl) in enumerate(train_loader):\r\n\r\n            model.train()\r\n            opt.zero_grad()\r\n            model.zero_grad()\r\n            corrupt_b = (b + 0.3 * torch.randn_like(b)).cuda()\r\n            recons = model(corrupt_b)\r\n            loss = crit(recons, b.cuda())\r\n            loss.backward()\r\n            opt.step()\r\n\r\n            if dist.get_rank() == 0:\r\n                if step % 10 == 0:\r\n                    print(f'epoch {epoch}, batch: {step}, loss: {float(loss.cpu())}')\r\n\r\n\r\ndef setup():\r\n    hps = parse_args()\r\n    dist.init_process_group(backend='nccl', init_method=f'env://', rank=hps.local_rank)\r\n\r\n    size = dist.get_world_size()\r\n    group = torch.distributed.new_group(ranks=list(range(size)))\r\n    return group, hps\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    group, hps = setup()\r\n    train(hps)\r\n```\r\n\r\n## Expected behavior\r\n\r\nExpect either command to work independent of using `DistributedDataParallel` or not.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.11.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] pytorch-memlab==0.0.3\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.3.0\r\n[conda] pytorch-memlab            0.0.3                    pypi_0    pypi\r\n[conda] torch                     1.1.0                    pypi_0    pypi\r\n[conda] torchvision               0.3.0                    pypi_0    pypi\r\n```\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nWhen we fuse the conv2d and its followed batchnorm, we need the quantized operator support in PyTorch quantization flow.\r\n\r\n## Motivation\r\nWe want to fuse convolution and batch norm to improve the quantization accuracy. \r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nType promotion for ``Bool`` type when operated with other types (especially for ``Byte``).\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nThe``Bool`` type has been introduced into pytorch type system, but currently they are not compatible with other types. However, before ``Bool`` becomes an independent type, many people use ``Byte`` as ``Bool``. It would cause many problems if ``Byte`` and ``Boool`` are not incompatible, which is currently the case.\r\n\r\n## Pitch\r\nBefore type promotion, many people write something like:\r\n```\r\na = torch.tensor([1], dtype=torch.uint8)\r\nb = torch.tensor([1.0]) # whatever, does not matter\r\nc = a & ( b < 2.0)\r\n```\r\nIt works when comparison returns ``Byte`` type, but causes error when comparison returns ``Bool`` type:\r\n`Expected object of scalar type Byte but got scalar type Bool for argument #2 'other'`\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nMy pytorch version (self-built) : ``acc5ced``\n\ncc @izdeby"},{"labels":["enhancement",null,null,null,null],"text":"Inplace BatchNorm seems to be developed by Mapillary here: https://github.com/mapillary/inplace_abn\r\n\r\nThis would be a very nice addition to core PyTorch (for memory savings).\r\n\r\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw @SsnL"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\n## Motivation\r\n\r\nTypically, in Deep Learning, the training process revolves around running consecutive epochs with a training and development dataset. Today for each new epoch, the DataLoader needs to be restarted and data loading is momentarily paused. \r\n\r\nThis, in our experience, causes a non-trivial amount of overhead.\r\n\r\n## Pitch\r\n\r\nThe proposed feature would be for a data loader object that is continuous. It's always loading data for the current epoch or the next epoch. The user of the `DataLoader` is notified of a transition via some event or flag change.\r\n\r\nHere is some example sudo code of this experience:\r\n```python\r\ndata_loader = DataLoader(datasets={'train_dataset': train_dataset,\r\n                                   'validation_dataset': validation_dataset}, \r\n                         batch_sizes={'train_dataset': 256,\r\n                                      'validation_dataset': 1024},\r\n                         repeat=True)\r\n\r\nwhile training:\r\n    while data_loader.dataset == 'train_dataset':\r\n        batch = next(data_loader)\r\n        predictions = nn.Model(batch)\r\n        nn.Loss(predictions).backward()\r\n    \r\n    while data_loader.dataset == 'validation_dataset':\r\n        batch = next(data_loader)\r\n        predictions = nn.Model(batch)\r\n        evaluate(predictions)\r\n```\r\n\r\nThis `DataLoader` design means the `DataLoader` would never need to restart; furthermore, it would always be preloading the next batch of data.\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nAccelerate PyTorch just-in-time compilation using MKL-DNN\r\n \r\n## Motivation\r\nPyTorch's just-in-time (JIT) compiler rewrites and runs Pytorch model at production-efficiency. MKL-DNN is built to accelerate deep learning applications in production environment. With the high performance primitives like conv, rnn, and gemm, MKL-DNN accelerates most deep learning models significantly on multiple Intel CPU generations using AVX2, AVX512, AVX512-VNNI and future deep learning acceleration technology. \r\n \r\nWith MKL-DNN enabled in JIT compiler, user can use JIT mode to get best performance with MKL-DNN with minimum change of Pytorch code. In imperative mode, user needs to explicitly insert format conversion for MKL-DNN operations using tensor.to_mkldnn() and to_dense(). In JIT mode, user doesnâ€™t have to do so. User may need to pass an explicit flag or invoke a specific MKL-DNN optimization pass. It automatically converts CPU path op to MKL-DNN op, and propagates mkl-dnn format across neighbor MKL-DNN operations. It includes all performance benefits possibly achieved in imperative mode and additional graph optimization.\r\n \r\n## Pitch\r\nUse PyTorch just-in-time compilation to get MKL-DNN acceleration with one flag (or function call)\r\n \r\n## Additional context\r\n \r\nThe MKL-DNN optimization pass includes mkl-dnn format propagation and fusion as initial step. The mkl-dnn formation propagation converts CPU ops to MKL-DNN ops. Format conversion ops are added in-between CPU and MKL-DNN ops.\r\n\r\nThe implementation of PyTorch MKL-DNN JIT backend will be located in the â€˜backendâ€™ directory in JIT sub-directory\r\n"},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\nIt would be nice if `torch.sparse` will be extended to include `torch.sparse.min(...)` and `torch.sparse.max(...)` with same interface as existing `torch.sparse.sum(...)`.\r\n\r\nFor example\r\n```python\r\ns = torch.sparse.FloatTensor(i, v, (height, width))  # make a sparse float tensor\r\ntorch.sparse.min(s, dim=1)\r\n# returns minimum along dim=1\r\ntorch.sparse.max(s, dim=0)\r\n# returns maximum along dim=0\r\n```\r\n\r\n## Motivation\r\nTaking min/max (argmin/argmax if possible) is quite rudimentary. \r\nFor instance suppose I have a sparse matrix `s` with sparse logits and I wish to *robustly* compute the output probabilities using `softmax`. If one wants to avoid overflow/underflow in the exp-sum process, one needs to subtract the `max` across the relevant dimension.\r\n \r\n## Pitch\r\nMin/max over a selected dimension is quite basic and exists for dense tensor. It would be nice to have similar functionality for sparse tensors as well.\r\n\r\n## Alternatives\r\nVery painfully iterate over the entries of the relevant dimension. **Painfully**.\r\n\r\n## Additional context\r\nThinking this over, there is an issue about the \"sparse\" entries: How should the \"sparse\" zeros be taken into account? For instance, if all the non-zeros entries in a row are strictly positive, does `min` over this row return 0 (value of \"sparse\" elements) or the minimal value of the greater than zero elements of the row?\r\n\r\nI think, what I am actually looking for is to *ignore* the sparse zeros of the matrix and only consider the values stored in `s._values()` and do `min/max` on these values based on their row/col arrangement in `s`.  \r\nIn that case, it is very possible to get `torch.sparse.min(s, dim=0) != torch.min(s.to_dense(), dim=0)`.\n\ncc @ezyang @gchanan @zou3519 @vincentqb"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n```python\r\n>>> torch.arange(torch.tensor([3, 4, 5]))\r\ntensor([0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4])\r\n```\r\n\r\n## Motivation\r\n\r\nOne possible use case is to help to deal with sequences concatenated to each other. For example:\r\n\r\nLet's say I have a concatenated sequence, with data stored in `data`, and a long tensor `seq_id` storing which sequence the data belongs to. The `data` looks like:\r\n```\r\n[ --- sequence 1 ---, ----- sequence 2 -----, -- sequence 3 --]\r\n```\r\nand the `seq_id` looks like:\r\n```\r\n[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2]\r\n```\r\nI want to convert this to padded sequence. Then the extended arange would help:\r\n```\r\n_, seq_size = torch.unique_consecutive(seq_id, return_count=True)\r\nintra_seq_index = torch.arange(seq_size)\r\nzero_padded_sequence = torch.zeros(seq_size.max(), seq_id.max() + 1)  # layout (sequence, batch)\r\nzero_padded_sequence[intra_seq_index, seq_id] = data\r\n```"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nsupport python sub-interpreters and maintains all status of the torch library.\r\n\r\n## Motivation\r\n\r\nas #10950 demonstrates, the current ``torch`` library cannot lives on multiple sub-interpreter  simultaneously within the same process. But we do need to run python codes on multiple \"threads\" at the same time for the very reasons why ``torch`` introduces ``torch.multiprocessing`` and ``DistributedDataParallel`` (the single node scenario). As [PEP 554](https://www.python.org/dev/peps/pep-0554/) is proposed back in 2017 and maybe available by 2019 or 2020, I think it is necessary to make use of it because:\r\n- It is easier to sharing data between interpreters than between processes\r\n- It will reduce gpu memory overhead (Every subprocess consume at least 400~500MB gpu memory)\r\n- It can help avoid relatively complex process management problems\r\n\r\nAnd between multi-interpreter and multi-process, there is almost no difference on user coding experience and front-end design, and the changes will be made behind the scene. \r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nI think works need to be done on following aspects:\r\n- Changes any global status that should bind to a interpreter to a per-interpreter status set. (the ``detach`` method mentioned in #10950, for example)\r\n  ``Tensor`` lifecycle management maybe not a good example, because it is also a choice that ``Tensor`` can be shared across interpreters.\r\n- Prevent re-initializing and double-finalize for those status that are indeed global. (CUDA initialization, for example)\r\n- Create interface and infrastructure for controlling communication and sharing ``Tensor`` between interpreters. \r\n- Deprecate ``torch.multiprocessing`` module\r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nWe hope to get a **parallel** implementation of **batched** Jacobian like tensorflow, e.g. \r\n\r\n```\r\nfrom tensorflow.python.ops.parallel_for.gradients import jacobian\r\njac = jacobian(y, x)\r\n```\r\n\r\n```\r\nwith tf.GradientTape() as g:\r\n  x = tf.placeholder(tf.float32,  shape=(None, 3, 32, 32))\r\n  g.watch(x)\r\n  y = f(x)\r\n\r\nbatch_jacobian = g.batch_jacobian(y, x)\r\n```\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["enhancement",null],"text":"## ðŸš€ Lookahead Optimizer\r\nhttps://arxiv.org/pdf/1907.08610.pdf\r\n\r\nIn this paper they show new Optimizer can save time, improve accuracy than SGD and work both on CV/NLP.\r\nI think it may worth to try.\r\n"},{"labels":["enhancement",null,null],"text":"## Motivation\r\n\r\nThereâ€™s a lot of cases in framework design where we want to be able to treat components like lego blocks when building various model architectures. The following case studies in PyText are the 90% common cases, not the exceptions, ie. most people build models from these architecture families.\r\n\r\nDocument classifiers in PyText follow a general pattern of Embedding â†’ Representation â†’ Decoder.\r\n\r\n* Embedding\r\n    * Word embedding\r\n    * BiTransformer\r\n    * Character embedding\r\n* Representation\r\n    * Encoder\r\n        * DocNN\r\n        * LSTM (possibly bidirectional)\r\n    * Pooling layer\r\n        * Max pool\r\n        * Attention\r\n        * etc\r\n* Decoder\r\n    * Almost always an MLP, needs to have an output dimension of #classes\r\n\r\nWithin the 4 categories of Embedding/Encoder/Pooling/Decoder though, each boundary depends on the dimensionality of the previous component â€” the encoder depends on the embedding dimension, the pooling depends on the encoder dimension (including eg. whether the LSTM is bidirectional), and the decoder depends on the pooling layer output dimension (and #classes).\r\n\r\nHowever, from a user standpoint, it is extremely desirable to not have to make sure that these dimensions match up exactly. For instance, if a product team wants to experiment with DocNN vs BiLSTM with self attention and two different sizes of word embeddings, they would need to know how to compute the correct input dimensions for their encoders, pooling layers, and MLPs for each of the 4 cases. If they have dense features, those might also factor into the computation for the MLP layer.\r\n\r\nPyText solves this problem by having modules of each of these categories compute and track the appropriate dimensionality internally to their constructor, and then constructing the model and wiring the various blocks together before training. See [DocNN](https://github.com/facebookresearch/pytext/blob/a64370c7f1aac1c0491ccf5b2b0987909c9e0d94/pytext/models/representations/docnn.py#L30) representation_dim computation for an example of this, and [Model.from_config](https://github.com/facebookresearch/pytext/blob/a64370c7f1aac1c0491ccf5b2b0987909c9e0d94/pytext/models/model.py#L167) for where we use this to construct the model.\r\n\r\nThis solution works for us, but only in the case where we take the responsibility of constructing the model, along with all of its subcomponents, away from the user. If we wanted to eg. allow the user to pass in their own representation layer, we wouldnâ€™t be able to ensure the dimensionality match, the user is therefore responsible for also constructing _every other model component_. As a direct result of this design, PyText is abysmally hard to use in a notebook, and we built a Config system for managing hyperparameters that essentially boils down to allowing users to partially construct modules, then let us take over and finish building the whole model. See one of our [BiLSTM Config](https://github.com/facebookresearch/pytext/blob/master/pytext/models/representations/bilstm_doc_attention.py#L35) classes for an example.\r\n\r\nThis issue is not specific to PyText. Consider this snippet from the demo example from the AllenNLP website:\r\n\r\n```python\r\nEMBEDDING_DIM = 6\r\nHIDDEN_DIM = 6\r\n\r\ntoken_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\r\nembedding_dim=EMBEDDING_DIM)\r\nword_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\r\nlstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\r\nmodel = LstmTagger(word_embeddings, lstm, vocab)\r\n```\r\n(source - https://allennlp.org/tutorials)\r\n\r\nNote that similar to PyText, we need to match up the EMBEDDING_DIM between token_embedding and the LSTM module; we therefore couldnâ€™t construct an LstmTagger object with a default argument for just one of word_embeddings or lstm, because there would be no way to create them with the appropriate dimensions.\r\n\r\n## Proposal - Lazy Modules\r\n\r\nWe propose adding lazily inferred dimensions to some core Torch components, such as Linear, ConvNd, and RNN. This syntax should allow constructing these components (and any derived or more complex user components containing them) to instantiate the models without fully defining their dimensions, and have them inferred from the tensors passed in the first time their forward function is called. We propose using -1 to represent these dimensions similar to how -1 is used in reshape now (None has also been suggested, covered in alternatives below). For example\r\n\r\n```python\r\n>>> l = nn.Linear(-1, 3)\r\n>>> l(torch.rand(2, 4)).size()\r\n[2, 3]\r\n>>> l(torch.rand(2, 5))\r\nTypeError(...)\r\n```\r\n\r\nThis would allow building more complex components that could also infer their input dimensions; consider a simple MLP implementation, much like what PyText uses for its decoders:\r\n\r\n```python\r\nclass MLP(nn.Module):\r\n    def __init__(self, dims: List[int], dropout: float = 0.):\r\n        super().__init__()\r\n        layers = []\r\n        for in_dim, dim in zip([-1] + dims, dims):\r\n            layers.extend([\r\n                nn.Linear(in_dim, dim),\r\n                ReLU(),\r\n                nn.LayerNorm(dim),\r\n                nn.Dropout(dropout),\r\n            ])\r\n        self.layers = nn.Sequential(*layers)\r\n\r\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\r\n        return self.layers(input)\r\n```\r\n\r\nNow this MLP can be used in any PyText model, regardless of what the output shape of representation/pooling is:\r\n\r\n```python\r\nmlp1 = MLP([4, 5, 6])\r\nmlp2 = MLP([4, 5, 6])\r\n\r\nmlp1(torch.rand(2, 4))  # finalizes mlp1 as expecting input dimension 4\r\nmlp2(torch.rand(2, 5))  # finalizes mlp2 as expecting input dimension 5\r\n```\r\n\r\nAnd in the cases of PyText and AllenNLP these could allow interfaces that are much simpler and more modular:\r\n\r\n```python\r\nclass Model(nn.Module):\r\n    def __init__(\r\n        self,\r\n        embedding = WordEmbedding(),\r\n        representation = BiLSTM(),\r\n        decoder = MLP([4, 5, 6]),\r\n    ):\r\n        self.embedding = embedding\r\n        self.representation = representation\r\n        self.decoder = decoder\r\n    \r\n    def forward(self, tokens):\r\n        embedded = self.embedding(tokens)\r\n        representation = self.representation(embedded)\r\n        return self.decoder(representation)\r\n        \r\nmodel1 = Model()\r\nmodel2 = Model(representation=DocNN())\r\nmodel3 = Model(\r\n    embedding=WordEmbedding(embedding_dim=200, vocab_size=10000),\r\n    decoder=MLP([2, 50, 10]),\r\n)\r\n```\r\n\r\n## Sample implementation\r\n\r\nThis will be supplied shortly following this proposal as a pull request in torch.nn.lazy, but for now here is the proposed implementation.\r\n\r\n```python\r\nfrom torch import nn\r\n\r\n# LazyModuleMeta re-implements the type construction semantics for objects to allow\r\n# a slight variant on syntax. Essentially anything with this metaclass can optionally\r\n# execute a single yield statement during its constructor (normal constructors also work fine).\r\n# If it does yield during construction, then a __lazy_init__ function is populated;\r\n# any code occurring before yield in the constuctor will be called as normal during object creation,\r\n# and any code after yield will instead be deferred to the first call of __lazy_init__.\r\n\r\nclass LazyInitMeta(type):\r\n    def __call__(cls, *args, **kwargs):\r\n        if hasattr(cls, '__new__'):\r\n            obj = cls.__new__(cls, *args, **kwargs)\r\n        else:\r\n            obj = object.__new__(cls)\r\n        \r\n        def initialize(obj):\r\n            res = obj.__init__(*args, **kwargs)\r\n            if isinstance(res, types.GeneratorType):\r\n                next(res, None)\r\n                def lazy_init(call_args):\r\n                    try:\r\n                        res.send(call_args)\r\n                    except StopIteration:\r\n                        pass\r\n                    finally:\r\n                        obj.__lazy_init__ = None\r\n                obj.__lazy_init__ = lazy_init\r\n            else:\r\n                obj.__lazy_init__ = None\r\n            \r\n        if isinstance(obj, cls):\r\n            initialize(obj)\r\n        return obj\r\n        \r\n# Here's a Lazy nn.Module implementation using LazyInitMeta, calling __lazy_init__ before the first\r\n# forward pass.\r\n    \r\nclass LazyModule(nn.Module, metaclass=LazyInitMeta):\r\n    def __init__(self):\r\n        nn.Module.__init__(self)\r\n    \r\n    def __call__(self, *args, **kwargs):\r\n        if self.__lazy_init__:\r\n            self.__lazy_init__(call_args=(args, kwargs))\r\n        return nn.Module.__call__(self, *args, **kwargs)\r\n        \r\n# Optionally lazy Linear module, based on the current torch implementation of nn.Linear.\r\n\r\nclass Linear(nn.Linear, LazyModule):\r\n    def __init__(self, in_features, out_features, bias=True):\r\n        LazyModule.__init__(self)\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        if bias:\r\n            self.bias = nn.Parameter(torch.Tensor(out_features))\r\n        else:\r\n            self.register_parameter('bias', None)\r\n            \r\n        if in_features == -1:\r\n            self.register_parameter('weight', None)\r\n            ([input], _) = yield  # lazy init remainder\r\n            in_features = self.in_features = input.size()[-1]\r\n            \r\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\r\n        self.reset_parameters()\r\n```\r\n\r\n## Drawbacks\r\n\r\nLazy modules donâ€™t work perfectly with everything in torch. In particular, if theyâ€™re not yet finalized (ie. -1 was passed to a dimension to be inferred, but forward hasnâ€™t been called yet), the following behaviors might be unintuitive:\r\n\r\n* torch.save/torch.jit.script wonâ€™t work properly\r\n    * both confirmed to work fine with the sample implementation once initialization finishes\r\n* module.apply()\r\n* any parameter initialization functions\r\n* module.parameters() wonâ€™t return all of the model parameters, so\r\n    * creating an optimizer/scheduler normally, ie torch.optim.Adam(module.parameters()) will potentially have unintended side-effects, as not all of the moduleâ€™s eventual parameters will optimized\r\n    * calling module.to() will move the parameters that exist, but once forward is called and new parameters are created, they will be on the default device\r\n        * this can potentially be solved with some tighter init behavior and unit tests, as thereâ€™s a finite number of components that need to be made lazy, and they can usually look at other parameters on the module and assume that users want them to be on the same device\r\n        * alternatively it may make sense to have a concept of module-level device residency\r\n\r\n## Alternatives\r\n\r\n### None instead of -1\r\n\r\nUse the above proposal, but pass None to indicate an inferred dimension rather than -1. -1 was chosen for the proposal because it mirrors the semantics of reshape and a few other operators, but None may be more intuitive.\r\n\r\n### Support -1 (or None) for Tensor dimensions\r\n\r\nInstead of implementing this at the Module level with lazy modules, implement this at the Tensor level. Have Tensors be able to be lazy- or partially-initialized, and then be able to be finalized with their final dimensions. This would solve most of the above drawbacks above, as for instance weâ€™d always be making and setting all parameters, and it would also likely simplify its integration as all known useful cases of lazy dimensions eventually boil down to passing them to Tensors. However, itâ€™s less clear what this implementation would look like, for instance how the Tensors would generically be able to infer the parameters.\r\n\r\n### Lazy Parameters\r\n\r\nIf this were implemented at the Parameter level, it would require pretty radically increasing the complexity of how Parameters work, but could solve most of the drawbacks of this proposal while also leaving Tensor untouched. We can flesh out this design more if there is significant feedback in that direction.\r\n\r\n### Support via metaclass vs non-metaclass\r\n\r\nThe current proposal implementation uses a metaclass for Lazy modules to introduce a novel initialization pattern for lazy components (this same pattern could eg. also be used to solve an analogous problems for Optimizer/Scheduler in the future). However, using metaclasses always comes at a cost in Python, especially for libraries that are being built on-top-of, because multiple inheritance in the case of multiple metaclasses is a problem with no generic solution. In other words, if any libraries built on top of pytorch want to use metaclasses in their own Module subclasses, theyâ€™ll need to do non-trivial work to make sure those metaclasses are compatible with any metaclass used in pytorch.\r\nThe proposal could likely be modified to, for example, use a decorator on the __init__ function instead of a metaclass."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nAdd new layer - Gabor Layer - to PyTorch library. It is a layer with learnable Gabor fuction parameters, where Gabor function generates 2D convolutional kernel, thus number of parameters stays the same no matter the kernel size.\r\n\r\n## Motivation\r\n\r\nI've made a research, you may find it on [arxiv](https://arxiv.org/abs/1904.13204).\r\n\r\nThis research on deep convolutional neural networks proposes a modified architecture that focuses on improving convergence and reducing training complexity. The filters in the first layer of network are constrained to fit the Gabor function. The parameters of Gabor functions are learnable and updated by standard backpropagation techniques. The proposed architecture was tested on several datasets and outperformed the common convolutional networks\r\n\r\n## Pitch\r\nAdd new layer as standard layer to PyTorch library. I already implemented the layer using PyTorch, you may find code here: https://github.com/iKintosh/GaborNet .\r\nHowever, I couldn't solve one problem: fix GaborLayer weights in evaluate or no_grads mode. \r\nThe issue: https://github.com/iKintosh/GaborNet/issues/2\r\n"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nextending the .eval() to  the sub models of an ensemble of neural networks .\r\n\r\n## Motivation\r\nI trained an ensemble of CNN models ( Squeezenet1_1 trained on different training sets ) and saved the checkpoint , when loading the checkpoint for inference *Even if I called the .eval() on the ensemble model*  I had inconsistent results because the sub models were not set to the evaluation mode . I think is because the models were loaded as a list of models inside of the ensemble model aggregating them. \r\n\r\n\r\n## Pitch\r\nDefining a structure to aggregate different checkpoints into one model ( instead of a list ) or defining a way we can extend the .eval() effect on the sub models inside of an ensemble model .\r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nNew operations `gatherv` and `allgatherv` to support gathering tensors of different shapes.\r\n\r\n## Motivation\r\n\r\nThis is not uncommon (e.g. different size batches) and one has to jump through hoops to get this done today. Native support means a better user experience.\r\n\r\n## Pitch\r\n\r\nWe'll add `gatherv` and `allgatherv` functions on the base `ProcessGroup` class and provide an implementation for every backend. The Gloo and MPI backends can implement this natively. The NCCL backend has to perform the padding trick (described below) under the hood to make this work. \r\n\r\n## Alternatives\r\n\r\nCurrently, to do the same, you first need to establish the maximum number of elements across processes. Then you allocate a big output tensor to hold that maximum. Then you pad your local contribution, if necessary. Then you run a conventional gather/allgather. Finally you chop off the padding to get the result. All these steps \r\n\r\n## Additional context\r\n\r\nSee https://discuss.pytorch.org/t/mpi-gatherv-and-mpi-scatterv-feature-request-idea/50878.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nSupport `gather`, `all_gather`, `broadcast`, and `scatter` general python objects. Instead of overloading existing API, it might be clearer to make them explicit, e.g.: `gather_object`, `all_gather_object`, `broadcast_object`, and `scatter_object`.\r\n\r\n\r\n## Motivation\r\n\r\nSharing objects across processes is a very common use case. People actually started building their own solutions by serializing meta and object into tensors and then using tensor collective comm APIs to share them (see, `_broadcast_object ` in #22490, `_collect_worker_names` in #23228). \r\n\r\n## Pitch\r\n\r\nPython object on different processes might not be of the same size. So this needs to be done in multiple steps:\r\n\r\n1. Pickle local python object into byte tensors. \r\n2. Communicate the tensor sizes across processes. Then, each process allocates local buffer tensors using the max size.\r\n3. Copy local tensor to the buffer tensor and communicate. \r\n4. Extract results from the buffer tensors and unpickle them back into Python objects.\r\n\r\ncc @pietern @zhaojuanmao @pritamdamania87 \r\n"},{"labels":["enhancement",null,null,null],"text":"This issue comes from the discussion started [here](https://discuss.pytorch.org/t/possible-solution-to-the-reproducibility-issues-using-scatter-add-operation/48989).\r\n\r\n## ðŸš€ Feature\r\n\r\nA GPU deterministic mode for scatter_add and similar operations \r\n\r\n## Motivation\r\n\r\nI know that there is a general idea that non-deterministic algorithms do not have a lot of influence on the results, but, in some cases, there is a huge impact. It is a little bit frustrating not be able to reproduce an experiment even though you have all seeds, random states, init weights and so on saved. Moreover, this behavior sometimes can lead to wrong conclusions, when you are trying new methodologies. For instance, I started this thread because in pytorch_geometric some of the operations are based on non-deterministic implementations of pytorch. As you can see in this [github](https://github.com/rusty1s/pytorch_geometric/issues/446) issue this behavior is impacting the results. At the end this non-deterministic behavior has an impact in other libraries like pytorch geometric, causing the lack of reproducibility.\r\n\r\n## Pitch\r\n\r\nI would like to be able to have a deterministic mode for the previously mentioned operations. \r\n\r\n## Alternatives\r\n\r\nI know that the deterministic mode for these kinds of operations will come with a huge runtime trade-off. However, I found this [article](https://people.eecs.berkeley.edu/~hdnguyen/public/papers/ARITH21_Fast_Sum.pdf) that can help with the deterministic mode of scatter_add. \r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"## BFloat16 doesn't work with CUDA yet\r\n\r\nI'd like to use bfloat16 for numeric stability within some cuda kernels that I'm working on. It seems like @izdeby added bfloat16 support in #21523 (thanks!) but I think that `tensor.copy_` is not yet exposed to the python API.\r\n\r\n## To Reproduce\r\n\r\n```\r\n>>> ary = torch.randn(5, dtype=torch.float32)\r\n>>> ary.bfloat16() # CPU bfloat16 is fine\r\ntensor([ 0.2383, -1.4531,  0.2461,  2.5156,  1.1719], dtype=torch.bfloat16)\r\n\r\n>>> ary.bfloat16().cuda() # CUDA  bfloat16 is sad\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/tensor.py\", line 82, in __repr__\r\n    return torch._tensor_str._str(self)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/_tensor_str.py\", line 300, in _str\r\n    tensor_str = _tensor_str(self, indent)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/_tensor_str.py\", line 200, in _tensor_str\r\n    self = self.float()\r\nRuntimeError: \"copy_\" not implemented for 'BFloat16'\r\n```\r\n## Expected behavior\r\n\r\nI'd like torch to copy the memory from CPU memory to GPU memory.\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.2.0.dev20190721\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.11.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-SXM2-16GB\r\nGPU 1: Tesla V100-SXM2-16GB\r\nGPU 2: Tesla V100-SXM2-16GB\r\nGPU 3: Tesla V100-SXM2-16GB\r\nGPU 4: Tesla V100-SXM2-16GB\r\nGPU 5: Tesla V100-SXM2-16GB\r\nGPU 6: Tesla V100-SXM2-16GB\r\nGPU 7: Tesla V100-SXM2-16GB\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-memlab==0.0.3\r\n[pip3] tcop-pytorch==0.0.0\r\n[pip3] torch==1.2.0.dev20190721\r\n[pip3] torch-bst==0.0.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl_fft                   1.0.12           py37ha843d7b_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] pytorch-memlab            0.0.3                    pypi_0    pypi\r\n[conda] pytorch-nightly           1.2.0.dev20190721 py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n[conda] tcop-pytorch              0.0.0                     dev_0    <develop>\r\n[conda] torch                     1.1.0                    pypi_0    pypi\r\n[conda] torch-bst                 0.0.0                     dev_0    <develop>\r\n```\r\n\r\n\r\n"},{"labels":["enhancement",null,null],"text":"with @pritamdamania87 @zhaojuanmao @aazzolini @gqchen @pietern @satgera @ezyang @zdevito @suo @manojkris @gchanan @soumith @dzhulgakov @yifuwang @bddppq @joxu-cn @dwarakrajagopal @jspisak \r\n\r\nPyTorch currently provides simple APIs for single machine data parallel, distributed data parallel, and single machine model parallel. However, when it comes to distributed model parallel, applications have to build their own scaffold to stitch together local autograd graphs into one global graph. This proposal aims to fill in that gap by providing an RPC-Based distributed model parallel API. In short, applications may run RPC to execute code remotely in the forward pass, and autograd will automatically travel across RPC boundaries in the backward pass. \r\n\r\n# API\r\n\r\n## Core Concepts\r\n\r\n\r\n**RRef[T] -** (abbreviation ref)  A *reference *to a value of some type `T` (e.g. Tensor) on a remote worker. This handle keeps the referenced remote tensor value alive on the owner, but there is no implication that the value will be transferred to the local worker in the future. It is valid to have a reference to local value as well, and values of type `T` can be* implicitly converted* to `RRef[T]`. This implicit conversion will be critical later to allow the expression of different types of RPC. Think of it like the implicit conversion from `std::string` to `const std::string &`. See System Design section for more details about `RRef`.\r\n\r\n```python\r\nref.owner() # what is the worker this value lives on\r\n\r\nv = ref.local_value() # if ref.owner() is local worker, then\r\n                      # this returns the the underlying value, otherwise error.\r\n# you can create a ref to a local tensor\r\nt = torch.rand(3, 4)         \r\nref2 = torch.RRef(t)\r\n\r\n# in TorchScript, T can be automatically converted to RRef[T]\r\nref3 : RRef[Tensor] = t\r\n```\r\n\r\n\r\n**Future[T] -** (abbreviation fut) a guarantee that at some future point in time the value of type `T` will be available locally. The action to create `T` locally is assumed to be scheduled and in-progress.  Future is already supported in TorchScript and we are extending this to remote calls. \r\n\r\n```python\r\nv = fut.wait() # block the current thread until v is ready\r\n\r\n# local cpu task creation returns a future to the computed tensors\r\nfut = torch.fork(lambda x, y: x + y, torch.rand(3, 4), torch.rand(3, 4))\r\n```\r\n\r\n## Core Functions\r\n\r\n```python\r\n# synchronous\r\nresult : T = torch.rpc(on : Worker, remote_callable : Callable, *args)\r\n# asynchronous\r\nresult : Future[T] = torch.async_rpc(on : Worker, remote_callable : Callable, *args)\r\n# remote reference\r\nresult : RRef[T] = torch.remote(on : Worker, remote_callable : Callable, *args)\r\n```\r\n\r\nEach function above invokes `remote_callable` on a remote worker. Value types in the `args` list are *copied by value* to the remote worker. `RRef[T]` types in the `args` list are *copied by reference* to the remote worker (again see the analogy between `std::string` and `const std::string&`). \r\n\r\nThe *synchronous* variant copies the result value back, blocking the calling thread until the response occurs. The *asynchronous* variant returns immediately with a future. The remote knows that the call *will expect to receive the value* so it will send a message back at some point with the result without further prompting.\r\n\r\nThe *remote reference* variant returns immediately with an `RRef` of the return value. The remote knows that the caller *does not expect to receive the result value*. \r\n\r\n\r\nBelow shows how these functions are used:\r\n\r\n\r\n```python\r\n# make some local tensors\r\na : Tensor = torch.rand(3, 4)\r\nb : Tensor = torch.rand(3, 4)\r\n\r\n# define a remote function, visible to all machines.\r\n# type annotations define expected input/output types.\r\ndef remote_function(a : Tensor, b : RRef[Tensor]) -> Tensor:\r\n   # 'b' in the type signature is a remote reference, so we must copy it here\r\n   # to use it locally.\r\n   # to_here() is defined later in the syntax sugar section, it synchronously\r\n   # copies the tensor to this worker.\r\n   b_l  : Tensor = b.to_here()\r\n   return a + b_l\r\n\r\n# run remote_function on a different device. \r\n# a is copied by value since it is a Tensor\r\n# b is copied by reference remote machine due to the RRef[Tensor] \r\n# type annotation in the signature, which causes an implicit conversion to a\r\n# reference type.\r\n\r\n# torch.remote always creates an RRef of the result type. \r\n# It does not wait for the remote's response. \r\n# There is no implied copy of the tensor data yet.\r\nc : RRef[Tensor] = torch.remote(\"worker1\", remote_function, a, b)\r\n\r\n# we can explicitly request the data to be copied back here:\r\nc_l : Tensor = c.to_here()\r\n\r\n# another example:\r\ndef remote_function2(a : Tensor, b : Tensor) -> Tensor:\r\n   return a + b\r\n\r\n# Here we call torch.rpc which returns the value directly without\r\n# creating a remote reference.\r\n# we synchronously wait for remote_function2 to return.\r\nc : Tensor = torch.rpc(\"worker2\", remote_function2, a, b)\r\n\r\n# When the RPC call is returning a non-reference type, we need to wait for \r\n# a response from the remote host. To avoid synchronously waiting, use the\r\n# async flag to get a future instead.\r\nc_f : Future[Tensor] = torch.async_rpc(\"worker2\", remote_function2, a, b)\r\n# even before calling wait, the remote knows that the data should be sent back\r\n# to the caller as soon as it is ready.\r\n\r\n# force the local thread to wait for the remote's response\r\nc = c_f.wait()\r\n\r\n\r\n# if you omit type annotations in the remote function, the assumption is that \r\n# arguments are passed without any implicit conversions\r\ndef remote_function3(a, b):\r\n   # no annotations mean that a, b will be Tensor since there is no conversion\r\n   return a + b\r\n\r\nc: Tensor = torch.rpc(\"worker2\", remote_function3, a, b)\r\n\r\n```\r\n\r\n### RRef Forks\r\n\r\n### Implicit Conversions for RRef Arguments\r\n\r\n\r\nWe allow implicit conversion between `T` and `RRef[T]` for arguments of RPC functions.  Both the actual and formal parameter can either be a `T` or an `RRef[T]`, leading to four cases that might occur:\r\n\r\n**T â†’ T (passing a T to an rpc that accepts a T):** the value T is copied by value, and send over the wire as part of the message invoking the RPC\r\n\r\n**T â†’ RRef[T] (passing a T to an rpc that accepts  RRef[T]):** The caller constructs a remote reference to the argument, and sends the *reference* over the wire to the callee. The data is not sent. The callee can then use the reference as a handle to either request the data later or to make further remote calls.\r\n\r\n**RRef[T] â†’ T (passing an RRef[T] to an rpc that accepts T):** The callee expects to get an actual value, so the callee needs to turn the reference into a value. The network behavior depends on where the `RRef[T]` lives.\r\n\r\n* If the `RRef[T]` lives on the caller, then the implementation looks up the actual value of `T` locally and pass it by value along the wire similar to the T â†’ T case.\r\n* If the `RRef[T]` lives on the callee, then the implementation just sends the reference and the callee does the lookup locally.\r\n* If the `RRef[T]` lives on some third machine, then the caller sends 2 messages. One to the third machine telling it to send the data in the remote reference directly to the callee, and one to the callee telling it to start the RPC and expect this input to be coming from the third machine. This effectively forward value of the `RRef[T]` to the callee without the caller having to load it or the callee having to request it later\r\n\r\n\r\nExamples:\r\n\r\n```python\r\ndef remote_function1() -> Tensor:\r\n    return torch.ones(2)\r\n    \r\ndef remote_function2(a : Tensor) â†’ Tensor:\r\n    b = a * 2\r\n    return b\r\n\r\naref : RRef[Tensor] = remote(\"worker1\", remote_function1)\r\n\r\n# this local worker will make two RPC calls: one to tell worker1 to send the \r\n# tensor to worker2, and another one to tell worker2 to expect this Tensor input\r\n# from worker1. remote_function2 will run on worker2 only after it received the \r\n# tensor from worker1.  \r\nbref : RRef[Tensor] = remote(\"worker2\", remote_function2, aref) \r\n```\r\n\r\n\r\n\r\n**RRef[T] â†’ RRef[T]** (**passing an RRef[T] to an RPC that accepts RRef[T]): **The callee expects an `RRef[T]`, but we must make sure we correctly keep track of references to the value on a remote. So the actual behavior depends on where the `RRef[T]` lives.\r\n\r\n* If `RRef[T]` lives on the caller, then we simply pass it to the remote and record that this remote now has a live reference to the value.\r\n* If the `RRef[T]` lives on the callee, then we pass it to the remote, and it becomes a local reference on the remote.\r\n* If `RRef[T]` lives on some third machine, then we must forward the reference. To do this the caller sends two messages. One to the third machine telling it to create a remote reference and send it to the callee, and one to the callee telling from where to expect the remote. The callee code is not invoked until the remote is transferred to ensure sane reference counting.\r\n\r\nExamples: \r\n\r\n```python\r\ndef remote_function1() -> Tensor:\r\n    return torch.ones(2)\r\n    \r\ndef remote_function2(a : RRef[Tensor]) -> Tensor:\r\n    int delta = 10\r\n    return a.to_here() + delta\r\n\r\naref : RRef[Tensor] = remote(\"worker1\", remote_function1)\r\n\r\n# this local worker will make two RPC calls: one to tell worker1 to create a \r\n# remote reference and send it to worker2, and another one to tell worker2 to \r\n# expect this remote reference input from worker1. remote_function2 code will \r\n# not run on worker2 until it receives the remote reference from worker1 to \r\n# ensure proper reference counting.   \r\nbref : RRef[Tensor] = remote(\"worker2\", remote_function2, aref) \r\n```\r\n\r\n\r\nWhen an `RRef[T]` goes dead on machine A, a message is sent to the owner of `T` telling it that the reference from machine A is dead.\r\n\r\n### Explicit RRef type for return values\r\n\r\nThe above implicit `RRef` argument conversion does not apply to return values. If `remote_function` returns `RRef[T]`, calling it remotely using `torch.remote` would return `RRef[RRef[T]]` instead of `RRef[T]`. This is because when the return value `RRef` of `torch.remote` is first created on the caller who does not know the owner of the real data `T`. T could be stored on the callee of `torch.remote`, but it could also be on a different worker as callee may also make another remote call within `remote_function` and return an `RRef[T]` owned by a different worker. Moreover, the caller is allowed to share the returned `RRef` with other workers immediately after `torch.remote` returns.  However, as by then, the caller does not know the real owner of `T` yet, sharing the `RRef` would break the reference count algorithm.\r\n\r\nExamples:\r\n\r\n\r\n```python\r\ndef remote_function3() -> RRef[Tensor]:\r\n    return torch.remote(\"Worker2\", torch.ones, 2, 2)\r\n \r\ncref : RRef[RRef[Tensor]] = remote(\"worker1\", remote_function3) \r\n```\r\n\r\n\r\n\r\n## Initialization API\r\n\r\nUsers may choose communication backend for RPC, and users are responsible for setting up the backend properly before calling the `init_rpc` method. \r\n\r\n\r\n```python\r\n# backend: specifies the underlying communication implementation\r\n# init_method: contains the information to initialize/connect a name store to \r\n#              resolve names\r\n# name: is a unique identifier for the current worker\r\ntorch.distributed.init_rpc(backend=\"pg\", init_method=\"file:///...\", name=\"worker1\")\r\n```\r\n\r\nThe `init_rpc` method will create an `RpcAgent` under the hood and will make the current worker ready to send and receive RPC calls. If you call `init_rpc` and use the `ProcessGroup` (`pg`) backend, it acts as a global barrier, where all the node names as collectively synchronized before continuing. This is not the case if you use a peer to peer backend (e.g. tensor pipes), where calling `init_rpc` will register the node name in the specified store and start serving.  \r\n\r\nApplications donâ€™t need to explicitly register functions for remote execution, but we do assume same functions are defined on both caller and callee. This is often true as all workers can import the same set of libraries or even share the same Python script. \r\n\r\n\r\n## Syntax Sugar\r\n\r\nOther operations are now implementable using syntax sugar.\r\n\r\n### Retrieving Value From RRef\r\n\r\n```python\r\n# helper private RPC functions\r\ndef _identity(v : Tensor) -> Tensor:\r\n    # copy the tensor by value to this remote,\r\n    return v\r\n\r\ndef _to_here(v : RRef[T]) -> T:\r\n    # take a reference, send it to the device that owns it\r\n    # and have that device return the actual tensor by value\r\n    return v.local_value()\r\n\r\nclass RRef[T]:\r\n    ...\r\n    # copy a remote tensor to the local worker, sync version\r\n    def to_here(self) -> T:\r\n        return torch.rpc(_to_here, self, on=self.owner())\r\n\r\n```\r\n\r\n### Builtin Operators\r\n\r\n```python\r\n# proxy methods for all builtin functions exist on references for \r\n# existing TorchScript types like Tensors. They always follow a fixed pattern:\r\ndef _mm(a : RRef[Tensor], b : RRef[Tensor]) -> RRef[Tensor]:\r\n    return a.local_value() + b.local_value()\r\n     \r\nclass RRef[Tensor]:\r\n    def mm(self : RRef[Tensor], other : RRef[Tensor]) -> RRef[Tensor]:\r\n        on = same_worker(self.owner(), other.owner())\r\n        return torch.remote(on, _mm, self, other)\r\n\r\n\r\nc : Tensor = a.mm(b).to_here()\r\n\r\n```\r\n\r\n### Callable and RRef\r\n\r\nIf `RRef[T]` holds a callable object `T`, the application may directly call  the `RRef` which will be translated into `torch.remote` call to the owner of the callable.\r\n\r\n```python\r\n# if T is callable for RRef[T], rref(x) will be translated to calling T(x) \r\n# on the owner of the RRef\r\ndef _call_rref(v : RRef[T], *args):\r\n    return v.local_value()(*args)\r\n\r\nclass RRef[T]:\r\n    def __call__(self, *args):\r\n        return torch.remote(self.on(), _call_rref, self, *args)\r\n\r\nnet = torch.remote(\"Worker1\", Net)\r\nnet(inputs)\r\n```\r\n\r\n### Optimizer and RRef\r\n\r\nAs models might have remote sub-modules (i.e., `RRef[nn.Module]`), we should provide an optimizer sugar to handle it. The optimizer sugar (`torch.optim.remote`) takes a local optimizer constructor, a distributed model parallel model, and an argument list for the local optimizer constructor. The `torch.optim.remote` recursively creates a local optimizer on every remote sub-module owner, and exposes the same step API as a local optimizer which recursively calls every local optimizer. \r\n\r\n\r\n```python\r\nclass Net1(nn.Module):\r\n    ...\r\n\r\nclass Net2(nn.Module):\r\n    ...\r\n\r\nclass DMP(nn.Module):\r\n    def __init__(self):\r\n        self.net1 = dist.remote(\"worker1\", Net1)\r\n        self.net2 = dist.remote(\"worker2\", Net2)\r\n        \r\ndmp = dist.remote(\"worker0\", DMP)\r\n# dist.optimizer creates an optimizer on all RRef owners\r\noptimizer = dist.optimizer(torch.optim.SGD, dmp, lr=0.1)\r\n\r\nwith dist.autograd.context():\r\n  loss = dmp(inputs)\r\n  dist.autograd.backward(loss)\r\n  optimizer.step()\r\n```\r\n\r\n## Model Parallel Training Examples\r\n\r\n### Multi-Machine Model Training\r\n\r\n```python\r\n# 1. load data\r\ninputs_rref = torch.remote(\"worker1\"ï¼Œ load_inputs, path_to_inputs) \r\nlabels_rref = torch.remote(\"worker2\"ï¼Œ load_labels, path_to_inputs)\r\n\r\n# 2. define model\r\nclass Net1(nn.Module):\r\n    ...\r\n\r\nclass Net2(nn.Module):\r\n    ...\r\n\r\nclass DMP(nn.Module):\r\n    def __init__(self):\r\n        self.net1 = torch.remote(\"worker1\", Net1)\r\n        self.net2 = torch.remote(\"worker2\", Net2)\r\n        \r\n    def forward(self, inputs_rref):\r\n        # RRef[T].__call__(args) is a sugar that translates to \r\n        # dist.remote(T, RRef.on(), args)\r\n        outputs1_rref = self.net1(inputs_rref)\r\n        outputs2_rref = self.net2(outputs1_rref)\r\n        return outputs2_rref\r\n        \r\n# 3. training, run it where you want to call autograd\r\ndef train(inputs_rref, labels_rref):\r\n    dmp = DMP()\r\n    # torch.optim.remote creates an optimizer on every RRef destination\r\n    optimizer = dist.optimizer(torch.optim.SGD, dmp, lr=0.1)\r\n    outputs_rref = dmp(inputs_rref)\r\n    loss = loss_func(outputs_rref.to_here(), labels_rref.to_here())\r\n    autograd_ctx_id = dist.autograd.backward(loss)\r\n    optimizer.step(autograd_ctx_id)\r\n    \r\ndist.rpc(dev2, train, args=(inputs_rref, labels_rref))\r\n\r\n```\r\n\r\n### Parameter Server Training\r\n\r\n\r\n```python\r\nclass ParameterServer:\r\n    def __init__(self):\r\n        self.params = torch.zeros(100, 100).to(0)\r\n        \r\n    def get_params(self) -> Tensor:\r\n        return self.params\r\n        \r\n    def add_grads(self, grad: Tensor):\r\n        return self.params += grad.to(0)\r\n        \r\ndef train(ps)\r\n    for _ in range(10):\r\n        params = torch.rpc(\"ps\", ParameterServer.get_params, args=(ps, ))\r\n        # run forward and backward\r\n        torch.rpc(\"ps\", ParameterServer.add_grads, args=(ps, params.grad))\r\n        torch.distributed.barrier(group=TRAINER_GROUP)\r\n        \r\nps = torch.remote(\"worker1\"ï¼ŒParameterServer)\r\ntorch.remote(\"worker2\", train, args=(ps,))\r\ntorch.remote(\"worker3\", train, args=(ps,))\r\n            \r\n```\r\n\r\n# System Design\r\n\r\n## Distributed Autograd\r\n\r\n### Basic Idea\r\n\r\nIn the first version, `dist.autograd.backward` does not support `RRef` arguments, but `RRef` can still help build the autograd graph. The overall idea is as follows.\r\n\r\n* When calling  `torch.rpc` or `RRef.to_here()`, `send` and `recv` autograd functions will be inserted to connect local autograd graphs on multiple workers into one distributed autograd graph. \r\n* Every distributed backward pass is assigned a globally unique id (***`autograd_context_id`***), and every participating worker will keep a dedicate context for it. \r\n* When the backward computation reaches a `recv` function, it packs the gradient and the `autograd_context_id` in the message, and pass it to its `send` counterpart. \r\n* Upon receiving a message for a `send` function in the backward pass, it uses the `autograd_context_id` in the message to identify which backward pass it belongs to, and uses the gradient in the message to continue autograd computation locally.\r\n\r\n\r\n\r\n### Send and Recv Autograd Functions \r\n\r\nLetâ€™s start with a simple example where there is just one synchronized RPC call and there is only one tensor passed across worker boundaries. Code is on the left and the autograd graph is on the right where `AccumulateGrad` autograd functions for leaf nodes are omitted for simplicity.\r\n\r\n```python\r\n# the add function should be \r\n# defined on both workers\r\ndef add() -> Tensor:\r\n    a = torch.rand(2, 2)\r\n    b = torch.rand(2, 2)\r\n    c = a + b\r\n    return c\r\n    \r\n# make RPC call from worker0\r\n# to execute add on worker1\r\nc1 = dist.rpc(add, on=\"worker1\")\r\nd = torch.ones_like(c1)\r\ne = c1 * d\r\ne.sum().backward()\r\n```\r\n\r\n![Screen Shot 2019-07-16 at 11 19 31 AM](https://user-images.githubusercontent.com/16999635/61566619-d4712f00-aa4a-11e9-922d-8124f17fbef7.png)\r\n\r\n\r\nThe `send` and `recv` autograd functions are inserted during the forward pass, which connect two local graphs into one distributed graph. In the backward pass, the gradient will be passed to the `recv` autograd function on `worker0`, and the `recv` autograd function will then transmit the gradient tensor to `worker1`â€™s `send` autograd function. Then, `worker1` can kick off the local autograd engine to resume the backward pass. There are a few more details need to be clarified in this simple example:\r\n\r\n* On `worker1`, how do we keep the autograd graph alive after the RPC call returns?\r\n    * In short, the distributed autograd engine on `worker1` will keep a reference to the `send` function which can keep the graph alive.\r\n    * Reasoning: The graph can be kept alive by keeping a reference to either tensor `C` or the `send` autograd function, as both of them hold a reference to the `add` autograd function. We choose to keep a reference to the `send` function instead of tensor `C`, because `C` as a non-leaf node produced by `add` is not needed in the backward pass. It should be freed as soon as possible. It is not memory efficient to hold C alive just because we want to have an entrance point to the autograd graph. \r\n* In the backward pass, how does `recv` on `worker0` find the correct `send` on `worker1` to talk to?\r\n    * This can be done by assigning a globally unique ID (**worker*****_id + local send/recv id***) for each `send` / `recv` function pair. \r\n* When can `worker1` delete its local autograd graph?\r\n    * `send` should have the same lifetime as its corresponding `recv` function. This can be done by sending a message from `worker0` to `worker1` when `recv` is destructed on `worker0`. The `recv` function is kept alive by the `loss` tensor. So, conceptually, the global autograd graph will be deleted when the final loss tensor is gone. \r\n\r\n### Hidden Autograd Path and Circular Dependency \r\n\r\nThings can become complicated when an autograd graph contains multiple send/recv pairs. Consider the following example.\r\n\r\n```python\r\n# all functions shoud be defined on all workers\r\ndef worker0_func(c2: Tensor) -> Tensor:\r\n    g = torch.rand(2, 2)\r\n    h = g + c2\r\n    return h\r\n\r\ndef worker1_func_top() -> Tensor:\r\n    a = torch.rand(2, 2)\r\n    b = torch.rand(2, 2)\r\n    c = a + b\r\n    return c\r\n\r\ndef worker1_func_bottom(c: Tensor, e1: Tensor) -> Tensor:\r\n    f = c + e1\r\n    return f\r\n\r\ndef worker2_func(c1: Tensor) -> Tensor:\r\n    d = torch.rand(2, 2)\r\n    e = c1 + d\r\n    return e\r\n\r\n# on Worker3\r\nc_ref = torch.remote(worker1_func_top, on=\"Worker1\")\r\nh1 = torch.rpc(worker0_func, c_ref, on=\"Worker0\")\r\ne_ref = torch.remote(worker2_func, c_ref, on=\"Worker2\")\r\nf1 = torch.rpc(worker1_funct_bottom, c_ref, e_ref, on=\"Worker1\")\r\ni = h1 + f1\r\ni.sum().backward()\r\n```\r\n\r\n![Screen Shot 2019-07-16 at 6 20 52 PM](https://user-images.githubusercontent.com/16999635/61566634-e94dc280-aa4a-11e9-9514-d8a51e54e4a2.png)\r\n\r\n\r\n\r\nThis example highlights two problems that we need to address:\r\n\r\n* **Hidden Autograd Path:** Existing local autograd engine starts from loss (or all outputs), and do a discovery/marking phase to identify all participating functions before executing the real autograd computation. So that all paths in the autograd graph are known upfront. However, we donâ€™t have this luxury in distributed autograd because some parts of the autograd graph reside on remote workers. For example, when grad arrives at `send5`, worker1 cannot tell whether `send3` will be in the backward pass if it only looks at local information. More specifically, `i.sum().backward()` will be the same as `f1.sum().backward()` from worker1â€™s perspective, but the former involves `send3` and the latter does not. \r\n    * To address this problem, we propose to record all globally upstream (upstream in the forward pass, downstream in the autograd graph) `send` / `recv` pairs in the forward pass, so that we know exactly which `send` / `recv` to wait for in the backward pass. \r\n* **Circular Dependency:** there are circular dependencies between worker1 and worker2, i.e., it is impossible to finish autograd computation on one worker before kicking off on another worker. One option is to start autograd computation on `worker1` first, and having an autograd thread blocking there waiting for grads for `send1`, but this is less ideal.\r\n    * To address this problem, we propose to only create the `send` autograd function and put it in the ready queue when the grad is received. Note that, when computing dependency count for `add1`, the autograd engine still takes `send1` into account, so that the engine will only start computing grads for add1 after both `add2` and `send1` finish. \r\n\r\n\r\nNote that we need to record information in the forward pass and do the discovery in the backward pass because we donâ€™t know which `send` function will be participating in the autograd computation. However, if the application can guarantee that all `send` functions will receive grad in the backward pass, we can skip all these complexity and have a more efficient version. Both scenarios are useful, so we propose to have two modes:\r\n\r\n* **Smart Mode** supports running backward on a subgraph of the global autograd graph, but there will be extra overhead in both forward and backward pass. \r\n* **Fast Mode** skips dependency recording in the forward pass and graph discovery in the backward pass, but the application needs to guarantee that *_all send autograd function will receive grad in the backward pass_*. \r\n\r\nThe two sections below describe the two algorithms in more details. \r\n\r\n### Distributed Autograd Algorithm Smart mode\r\n\r\n\r\n**Forward pass:** \r\n\r\nFor every `send`  **x**:\r\n\r\n1. Find `send` functions in **x**â€™s lineage, by:\r\n    1. Finds all locally reachable `recv` functions from `send` **x** in the autograd graph. In the example above, `send2` finds `recv1`, `send4` finds `recv3`, and `send5` finds `recv2`. \r\n    2. Use those found `recv` functions to find globally reachable `recv` functions in `send` **x**â€™s lineage. Note that this can be done, because in step 2 we send enough information from `send` to `recv`. In the example above `send4` knows `send3`, and `send5` knows `send1` and `send2`. \r\n2. Then, `send` **x** includes ids of its lineage `send` functions in the message. Intuitively, it means that if there is a grad received for `send` **x**, the backward pass must reach all `send` functions in its lineage as well. It helps a node to determine whether it should wait for a `send` grad.\r\n\r\n```python\r\n# pseudo code to demonstrate how send works in forward\r\ndef find_global_lineage(tensor):\r\n    # find local lineage\r\n    recvs = find_recvs(tensor.grad_fn)\r\n    dep_ids = {recv.id for recv in recvs}\r\n    # find global lineage\r\n    dep_ids.update({dep_id for recv in recvs for dep_id in recv.dep_ids})\r\n    return dep_ids\r\n\r\ndef send(func, tensors, on):\r\n    msg = Message(func)\r\n    for tensor in tensors:\r\n        lineage = find_global_lineage(tensor)\r\n        # connect send to autograd graph\r\n        send = SendFunc()\r\n        send.next = tensor.grad_fn\r\n        # remember the send by its id\r\n        RpcAgent.send_map[send.id] = send\r\n        # coalesce data\r\n        msg.data.append((tensor, send.id, lineage))\r\n    send_msg(msg, on)\r\n    \r\ndef recv(func, data, from):\r\n    tensors = []\r\n    for tensor, send_id, lineage in data:\r\n        # use send_id as recv_id, and remember global lineage\r\n        recv = RecvFunc(send_id, lineage)\r\n        tensor.grad_fn = recv\r\n        tensors.append(tensor)\r\n        \r\n    return func(tensors)\r\n```\r\n\r\n\r\n**Backward pass**: \r\n\r\nOn the node that calls `torch.distributed.backward`:\r\n\r\n1. Find all `send` functions in the lineage of the loss tensor. In the above example, it will be all 5 `send` functions. These ids will be propagated to the `recv` functions and will be passed to the counterpart `send` functions accordingly. \r\n    1. Optimizations can be added, e.g., drop unnecessary ids in backward pass to reduce message size. \r\n\r\nOn every node:\r\n\r\n1. Upon receiving the first message (be it a dedicated discovery message or grad of a send), record its `autograd_context_id`, and retrieve all participating `send` ids from the message. Compute dependency count from those `send` functions (and also from loss `grad_fn` if loss is on this node). Set dependency count for `send` functions as 1. If there is any autograd function has dependency count 0, put them into the ready queue. \r\n2. Upon receiving a `send` grad, decrement the dependency count of that `send` by 1, and add it to the ready queue. Note this is done on an `RpcAgent` thread, and some autograd engine thread will pick up the autograd function for execution. \r\n\r\n\r\n\r\n```python\r\n# pseudo code to demonstrate backward\r\ngraph_tasks = {}\r\ndef backward(loss):\r\n    global graph_tasks\r\n    \r\n    autograd_context_id = gen_autograd_id()\r\n    lineage = find_global_lineage(loss)\r\n    # these send will participate in the autograd pass\r\n    roots = local_sends.intersection(lineage)\r\n        \r\n    # propagate the autograd_id and deps info to all\r\n    # participating workers. This is non-blocking and can\r\n    # run concurrently with the real backward computation. \r\n    # This step is not absolutely necessary, but can help other\r\n    # workers to kick off autograd earlier.\r\n    disseminate(autograd_context_id, lineage)\r\n\r\n    # below is a handwaving impl to show how it works with local autograd engine\r\n    graph_task = GraphTask()\r\n    graph_tasks[autograd_context_id] = graph_task\r\n    roots.append(loss.grad_fn)\r\n    # setup dependency count properly\r\n    compute_dependencies(GraphRoot(roots), graph_task)\r\n    # insert the task to local engine ready queue. Only the FunctionTask\r\n    # for loss is inserted now, send FunctionTasks will be inserted later\r\n    # when their grad becomes available.\r\n    ready_queue.push_back(FunctionTask(graph_task, loss.grad_fn, ...))\r\n    return autograd_context_id\r\n    \r\n    \r\ndef on_grad_send(send_id, grad, autograd_id):\r\n    global graph_tasks\r\n    graph_task = graph_tasks[autograd_id]\r\n    send_func = RpcAgent.send_map[send_id]\r\n    ready_queue.push_back(FunctionTask(graph_task, send_func, grad))\r\n```\r\n\r\n\r\n\r\n### Distributed Autograd Algorithm Fast mode\r\n\r\nThe problem with the above approach is that including ids in  `send` / `recv` messages incurs overhead, especially when there are a lot of tensors communicated across multiple workers. And this discovery phase is only necessary when running autograd on subgraph. For example,  `f1.sum().loss()` requires the discovery phase to avoid waiting for `send3`, but it is easier for `i.sum().loss()` as all `send` are involved in the backward. So, we propose to have one additional mode for distributed autograd to bypass `send` / `recv` dependency discovery in both forward and backward **if all send for non-leaf or `requires_grad` tensors will receive grad in the backward pass**. The mode can be toggled when initializing RPC agents:\r\n\r\n\r\n```python\r\n# all_requires_grad (bool): If True, the application guarantees that all \r\n# send functions on non-leaf or requires_grad tensors will receive grad \r\n# in the backward pass. Hence, we can skip the distributed dependency \r\n# discovery algorithm (fast mode). If False, run smart mode, where \r\n# messages beween send/recv will contain dependency ids in both forward\r\n# and backward pass. (default False)\r\ntorch.distributed.init_rpc(name, backend=\"pg\", all_requires_grad=False)\r\n```\r\n\r\n\r\nInternally, `RpcAgent` will create a thread-local driver ID, where a driver is the worker that pieces together the autograd graph. In the above example, `Worker3` is the driver. In the forward pass, every `send` function originated from this driver will be tagged with its thread-local driver ID, and this applies to all downstream (upstream in the autograd graph) `send` functions as well. This can be done by either propagating this driver ID to RPC calls recursively, or do an active driver ID discovery by walking the autograd graph before sending a tensor. If this information is ambiguous, e.g., one `send` function traces back to two upstream (downstream in the autograd graph) `recv` functions from two different drivers, it will throw an error. In the backward pass, the thread-local driver id of the loss will be included in the entire autograd execution to identify participating `send` functions. Note that, in this mode, the application cannot keep two disjoint autograd graphs alive at the same time, as that would break the assumption that all send (originated from the driver) will receive grad in the backward pass.\r\n\r\n\r\n### Concurrent distributed Backward passes  \r\n\r\n```python\r\nA = torch.rand(2, 2)\r\nB = torch.rand(2, 2)\r\n    \r\n# on all workers\r\ndef add() -> Tensor:\r\n    global A, B\r\n    return A + B\r\n    \r\n# on worker0\r\nC = torch.remote(add, on=\"worker2\").to_here()\r\nC.sum().backward()\r\n\r\n# on worker1\r\nC = torch.remote(add, on=\"worker2\").to_here()\r\nC.sum().backward()\r\n\r\n```\r\n\r\n\r\nIn the above example, there are two concurrent backward passes triggered by `worker0` and `worker1` respectively, and both will reach `worker2`. To avoid race, the distributed autograd engine will use the globally unique `autograd_context_id` to create a dedicated context on every participating worker. Later, pass this `autograd_context_id` to optimizer to apply gradients.  More concretely, this would work as follows:\r\n\r\n1. Compute all the leaf nodes in the autograd graph.\r\n2. As part of running distributed backwards, use the outputs parameter of the autograd engine to avoid executing `AccumulateGrad` for the leaf nodes we have and instead return the appropriate `output_edges` to execute for accumulating gradients.\r\n3. Store the `output_edges` with the `autograd_context_id`. This would ensure multiple backward passes won't accumulate gradients in the same context. \r\n4. This completes the backward pass and gradients are accumulated in the autograd engine per `autograd_context_id.`\r\n5. Now we run the optimizer on each of the worker nodes and pass the `autograd_context_id` to the optimizer.\r\n6. The optimizer applies all the gradients to the leaf nodes that we computed originally.\r\n7. The context and enclosing gradients should be destroyed when the `autograd_context_id` is destructed on the caller of `backward()`.\r\n\r\n\r\nSome pseudo-code to illustrate this:\r\n\r\n```python\r\noptimizer = dist.optimizer(model)\r\nloss = model(inputs)\r\nbw_ctx_id = dist.autograd.backward(loss, timeout=60) # timeout of 60s\r\noptimizer.step(bw_ctx_id)\r\n```\r\n\r\n\r\n\r\n\r\n## RRef \r\n\r\n(more details are described in #26759)\r\n\r\n`RRef` is an important concept for building a distributed autograd graph. Each `RRef` is owned by a single worker (i.e., owner) and can be used by multiple users. The owner stores the real data referenced by its `RRef`s, and keeps track of the global reference counts for its `RRef`s. Every `RRef` can be uniquely identified by a global id `ref_id`, which is assigned at the time it is first created either on a user or on the owner. \r\n\r\nThe owner only keeps one `RRef` instance for each data object, while users can fork as many `RRef` instances as necessary. All usage on the owner should retrieve the `RRef` instance using the globally unique `ref_id`. A fork of `RRef` will be created when it is used as an argument or return value in a RPC call, but users don't need to worry about forking/forwarding and reference counting (RC) `RRef`s. These will be handled transparently, and every fork will also have its own `fork_id`, which is guaranteed to be unique across all `RRef` instances for the same data object. \r\n\r\n`RRef` needs to support fast and scalable RPC. Hence, in the RC design, we avoid using any global master to keep `RRef` states. Besides, when worker X invokes RPC on worker Y, Y should be able to start immediately after receiving the RPC request, without waiting for any third-party owner Z (unless Y needs to pull real data from Z), even if neither X nor Y owns the `RRef`. We propose the following algorithm: \r\n\r\n\r\n1. If the owner is the RPC caller, the owner will update RC for the `RRef` accordingly. \r\n2. If the owner is the RPC callee, the owner will drop the new fork, and use the unique `RRef` id in the fork to access its singleton local `RRef` instance.\r\n3. If the RPC is between two users:\r\n    1. The caller sends an RPC message to the callee, and also notifies the owner on the new fork.\r\n    2. The owner, upon receiving the notification, updates its local RC and then tells the callee the new fork is now known by the owner.\r\n    3. The callee can starts executing the RPC as soon as it receives the RPC message from the caller, and does not need to wait for the message from the owner. However, it cannot delete its local `RRef` fork until owner's message arrives.\r\n\r\n### Reference Count\r\n\r\nThe right time to delete an `RRef` on owner is when there are no living forks on any user and Python GC also agrees to delete the `RRef` instance on the owner. The tricky part is to determine if there are any living forks.\r\n\r\nA user can get a fork in three situations:\r\n\r\n\r\n1. Receiving a fork from the owner.\r\n2. Receiving a fork from another user.\r\n3. Creating a new `RRef` fork owned by another worker.\r\n\r\n\r\n`#1` is the simplest case where the owner initiates the fork, and hence it can easily increase local RC. The only requirement is that any fork must notify the owner before destruction. Hence, we need the first guarantee:\r\n\r\n* G1. The owner will be notified when any fork is deleted.*\r\n\r\nNote that the notification might come delayed or out-of-order. \r\n\r\nWith `#2` and `#3`, it is possible that the owner only partially knows the `RRef` fork graph or not even knowing it at all. For example, the `RRef` could be constructed on a user, and before the owner receives the RPC call, the creator user might have already shared the `RRef` with other users, and those users could further share the `RRef`. One invariant is that the fork graph of any `RRef` is a tree rooted at the owner, because forking an `RRef` always creates a new `RRef` instance, and hence every `RRef` has a parent. One nasty detail is that when an `RRef` is created on a user, technically the owner is not its parent but we still  consider it that way and it does not break the argument below.\r\n\r\nThe owner's view on any node (fork) in the tree has three stages 1) **unknown** â†’  2) **known** â†’  3) **deleted**, and the owner's view on the entire tree keeps changing. The owner deletes its `RRef` instance when it thinks there is no living forks, i.e., all the forks could be either indeed deleted or unknown. Therefore, the dangerous case is when some forks are unknown and others are deleted. We only need a simple guarantee to prevent this situation:\r\n\r\n*G2. No fork x can be deleted on a user before the owner knows xâ€™s parent fork.\r\n* \r\nThis works because owner's view on x can only change from **known** to **deleted** when x's parent is **known** or **deleted**. If the parent is **known**, owner will not delete local `RRef`. If the parent is **deleted**, this rule recursively applies to the parent's parent, until it reaches the root (owner). To implement the guarantee, we only need to make the caller include its own `fork_id` when notifying the owner on a new fork.\r\n\r\nG1 and G2 guarantee correct RC, but does not prevent a user deleting before finishes its own prior RPC calls using that `RRef` fork. This should be OK, because when the caller deserializes the RPC message, it would hold a reference () to that `RRef`, preventing it from been deleted.\r\n\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nHaving `torch.from_numba(cuda_arr)` can be useful in many cases.  \r\n## Motivation\r\nWith the easy to use APIs in numba, I would like to do the preprocessing on gpu and then pass resultant gpu arrays to pytorch model. But currently, we have to convert these gpu arrays to numpy and then use the `torch.from_numpy()` API. \r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n```Python\r\nimport numba \r\nimport torch\r\ncuda_arr = numba.cuda.device_array(shape=(5,4))\r\n# here, we need to perform this unnecessary conversion from gpu->cpu\r\nnumpy_arr = cuda_arr.copy_to_host()\r\ngpu_tensor = torch.from_numpy(numpy_arr)\r\n```\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nAdding automatic tuning flags for `batch_size` and `num_workers` to `torch.utils.data.dataloader`\r\n\r\n## Motivation\r\n\r\nWe'd like to help users to get good performance (throughput) in their data loading jobs. This will allow users to focus on defining the logic for the model inference.\r\n\r\nToday, users have to tune batch_size and num_workers manually when they hit performance issues. Tuning requires some expert knowledge of PyTorch and systems design. Even with this knowledge, users may spend hours or days benchmarking, profiling, identifying bottlenecks, and tweaking parameters to improve performance.\r\n\r\nThis feature is especially relevant to IO bound pipelines, commonly found in inference workloads, where images are not cached in memory and must be loaded from disk. Optimal `batch_size` and `num_workers` configs can improve an imgs/sec metric by as much as 10x.\r\n\r\n## Pitch\r\n\r\nJust as Tensorflow has introduced the `tf.data.experimental.AUTOTUNE` feature for `batch_size`, `prefetch_size`, and `num_parallel_calls` in datasets, PyTorch should include a similar feature for torch.utils.data.dataloader.\r\n\r\n## Alternatives\r\n\r\nIf autotuning flags are not introduced, users can guess at the optimal configs, or hyperparameter tune these configs before running full training / inference jobs on big data.\r\n\r\nIn practice, setting `batch_size` to be the largest possible without running into OOM errors works well, as does setting `num_workers` equal to the number of cores on the machine. However, these configs are not always optimal, and may not take advantage of features like hyperthreading.\r\n\r\nHyperparameter tuning can also work to discover optimal parameters, but lacks information about buffer sizes and has to deal with more noise and variance in imgs/sec result metrics. Additionally, extensive engineering is required to enable online tuning; otherwise, valid inference results may be discarded. \r\n\r\n## Additional context\r\n\r\nMy Questions\r\n* Are there any features like this in the PyTorch Pipeline?\r\n* Would this be useful to other folks in the contributor or user community of PyTorch?\r\n* Has anyone thought about autotuning before and/or have ideas/stubs of implementation?\r\n"},{"labels":[null,"enhancement",null,null],"text":"## ðŸš€ Feature\r\nImplement `torch.xlogy` which computes `x * log(y)` if `x != 0` and 0 if `x == 0`.\r\n\r\n## Motivation\r\nOften one wants to compute `x * log(x)` or `x * log(y)` but define the result to be zero if x is 0 (instead of NaN). The current alternatives is to use torch.where to mask out NaN values or to add a small epsilon to x, like we do in binary cross entropy loss.\r\n\r\n## Additional context\r\n\r\nThis is implemented in both SciPy and TensorFlow:\r\n\r\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.special.xlogy.html\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/xlogy\r\n\r\ncc @IssamLaradji"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nCurrently trying to call `torch.lu_solve` with a matrix that requires a gradient leads to the following error:\r\n\r\n```\r\n>>> x = torch.randn(10)\r\n>>> pivot = (torch.arange(10) + 1).int()\r\n>>> w = torch.randn(10, 10, requires_grad=True)\r\n>>> torch.lu_solve(x, w, pivot)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: the derivative for 'LU_data' is not implemented\r\n```\r\n\r\nIt would be good if `torch.lu_solve` could be differentiated with respect to the LU-factorized matrix.\r\n\r\n## Motivation\r\n\r\nMy model involves computing the inverse of a series of parametric transforms, in part involving linear transforms. Simply using `torch.inverse` proved to be too unstable with 16-bit precision even with small matrices, so I hoped `torch.lu_solve` would help here by directly parameterizing the linear transform as an LU-factorized matrix, but it turns out it doesn't support differentiating with respect to the matrix. Thus it seems my model can't be implemented in a stable way at all using PyTorch."},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\nWhy not implement some common metrics, to evaluate models, with strong GPU acceleration.\r\n\r\n## Motivation\r\n\r\nWhen I need to evaluate model accuracy (ex. measure r squared) I have to write it by hand, or use Numpy or other library (without GPU support). It'd be cleaner and simpler to have dedicated package with Pytonic API. It could also provide performance benefits because of low-level (c++) optimizations.  \r\n\r\n## Pitch\r\n\r\nI'd like to have some package ex: `torch.metrics` then I could do something like:\r\n\r\n```python\r\nfrom torch.metrics import RSquaredMetric\r\nmetric = RSquaredMetric()\r\nout = model(input)\r\nr_squared = metric(out, reference)\r\n```\r\nOr I could do:\r\n```python\r\nfrom torch.metrics import MSEMetric\r\nmetric = MSEMetric()\r\nout = model(input)\r\nmse = metric(out, reference)\r\n```\r\n\r\n## Alternatives\r\n\r\nFor now, I can use some Tensors modifications or use Numpy:\r\n\r\n```python\r\nref_val = reference_batch[key].cpu().detach().numpy()\r\ncor_matrix = np.corrcoef(val, ref_val, rowvar=False)\r\nn = cor_matrix.shape[0] // 2\r\nout_r2[key] = [cor_matrix[i][i + n] for i in range(n)]\r\n```\r\n\r\nIf you agree, I'd like to contribute.\n\ncc @ezyang @gchanan @zou3519 @albanD @mruberry"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nDistribute the hipification scripts in `tools/amd_build/pyHIPIFY` to access them for hipification of extensions such as torchvision.\r\n\r\n## Motivation\r\nIn #22091 , we aim to port the CUDAExtensions feature to support the HIP/ROCm stack. To make it easier for users, @ezyang proposed to add the hipification script to the distribution to get access to it from CUDAExtensions, thereby allowing us to hipify the extension sources (such as torchvision) on the fly as opposed to ahead of time.\r\n\r\n## Pitch\r\n\r\nDistribute the `pyHIPIFY` sources, make them accessible from CUDAExtensions.\r\n\r\n## Alternatives\r\n\r\nThe alternative is to hipify extension sources ahead of time, creating HIP and CUDA sources.\r\n\r\n## Additional context\r\n\r\nPrimary focus/target is to get torchvision including CUDA sources to work on ROCm.\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nThis is a description of several related features that are best considered together.\r\n\r\n1. Allow subclassing `Tensor` and propagating subclass instances correctly with `torch` functions, operators, using views/slices/etc.\r\n2. Support the NumPy array protocols\r\n3. Allow other libraries to reuse the PyTorch API via a similar method as NumPy uses\r\n\r\n\r\n## Motivation\r\n\r\nThis issue/document is motivated by the attempts in [PyTorch PR 17249](https://github.com/pytorch/pytorch/issues/17249) and follow-up PRs [18610](https://github.com/pytorch/pytorch/issues/18610), [22235](https://github.com/pytorch/pytorch/pull/22235) and [22247](https://github.com/pytorch/pytorch/issues/22247) to make `torch.Tensor` subclasses interact better with NumPy and Torch functions. Currently (June 2019), `Tensor` subclassing is not yet supported and while PyTorch in many cases follows the NumPy API, direct interoperability is limited (instead one needs to explicitly convert between `torch.Tensor` and `numpy.ndarray`).\r\n\r\n## Potential goals\r\n\r\nThese are _potential_ goals that have been collected from the above referenced PRs, other PyTorch issues (referenced in the relevant sections), as well as from discussions with mainly Edward Yang, and also other PyTorch and NumPy maintainers:\r\n\r\n1. Support subclassing `torch.Tensor` in Python\r\n2. Preserve `Tensor` subclasses when calling `torch` functions on them\r\n3. Preserve `Tensor` subclasses when calling `numpy` functions on them\r\n4. Use the NumPy API with PyTorch tensors (i.e. NumPy API calls dispatch to `torch` functions)\r\n5. Use the PyTorch API with `torch.Tensor`-like objects that are _not_ `Tensor` subclasses\r\n6. Reuse NumPy ufunc implementations directly from PyTorch\r\n7. Allow operations on mixed array types, e.g. `tensor + ndarray`\r\n\r\nImportant to keep in mind when implementing features that achieve any of the above goals:\r\n- The PyTorch team is [planning](https://github.com/pytorch/pytorch/issues/17249#issuecomment-505968610) more complex `Tensor` wrappers, an effort that should not be made significantly more difficult.\r\n- The PyTorch team may want to provide a (g)ufunc-like mechanism in PyTorch in the future. Also that should not be made unnecessarily complex.\r\n\r\n### Support subclassing `torch.Tensor` in Python\r\n\r\nNote that `Tensor` seems to have been designed with subclassing in mind, at least that's what the comments at https://pytorch.org/docs/stable/_modules/torch/tensor.html (_NB: If you subclass Tensor ..._) indicate. Support seems incomplete though. The most basic way of subclassing just adds some new attributes, e.g. to carry around specific metadata like \"this tensor represents voltages\".\r\n\r\n```\r\nclass AddedAttributeTensor(torch.Tensor):\r\n    data_info = 'voltage'\r\n\r\nt1 = torch.Tensor([1, 2])\r\nt2 = AddedAttributeTensor([3, 6])\r\n\r\nprint(\"Extra attribute of subclass: \", t2.extra_attr)\r\nprint(\"Tensor + subclass gives back a Tensor instance: \", t1 + t2)\r\nprint(\"Is subclass preserved for operators?  \", isinstance(t2 + t2, AddedAttributeTensor))\r\nprint(\"Does slicing preserve subclass?  \", isinstance(t2[:1], AddedAttributeTensor))\r\nprint(\"Does taking a view preserve subclass?  \", isinstance(t2.view((2, 1)), AddedAttributeTensor))\r\n```\r\n\r\nRunning this code shows that for a regular subclass, the subclass doesn't propagate (we always get a plain `Tensor` instance back:\r\n```\r\nExtra attribute of subclass:  voltage\r\nTensor + subclass gives back a Tensor instance:  tensor([4., 8.])\r\nIs subclass preserved for operators?   False\r\nDoes slicing preserve subclass?   False\r\nDoes taking a view preserve subclass?   False\r\n```\r\n\r\nThe [NumPy subclassing docs](https://www.numpy.org/devdocs/user/basics.subclassing.html#ndarrays-and-object-creation) discuss the ways in which new class instances can be created; the same will apply to `Tensor` instances. To deal with that, two methods are needed:\r\n1. A `__new__` method for initialization in case of an explicit constructor call.\r\n2. A method to deal with creation in other ways, like slicing or taking a view (which will bypass `__new__`).\r\n\r\nFor (2) NumPy uses `__array_finalize__`, however in [gh-22247](https://github.com/pytorch/pytorch/pull/22247) the claim was that this method is very expensive (because it gets called too often - this doesn't seem the case though, see the _\"Performance considerations\"_ section further down). Instead it introduced a `THPVariable_result_ptype`, which achieves the same thing (although it was confusingly mixed up with an incorrect use of `__array_ufunc__` there).\r\n\r\nAlso note that `torch.Tensor._make_subclass` already exists (defined in `torch/csrc/autograd/python_variable.cpp`, according to the comment specifically for use with `torch.nn.Parameter`). It's unclear whether that or the code in [gh-22235](https://github.com/pytorch/pytorch/pull/22235) works for views and slicing; it's not tested.\r\n\r\n### Preserve `Tensor` subclasses when calling `torch` functions on them\r\n\r\n_Note that this was the goal of [gh-22235](https://github.com/pytorch/pytorch/pull/22235), which is useful as a reference._\r\n\r\nFor `Tensor` subclasses that _do not_ implement `__torch_function__` (assuming that gets implemented, see goal 5), this will work if the `__array_finalize__` equivalent gets implemented (see previous section). For subclasses that _do_ implement `__torch_function__`, all `torch` functions get overridden by the subclass, so it has more control over this (although in many cases it will still make use of the `__array_finalize__` equivalent).\r\n\r\n### Preserve `Tensor` subclasses when calling `numpy` functions on them\r\n\r\n_Note that this was the goal of [gh-22247](https://github.com/pytorch/pytorch/pull/22247), which is useful as a reference._\r\n\r\nThis should be done via implementation of `__array_ufunc__` and `__array_function__` on the `Tensor` class. At that point, all the NumPy functions that have a PyTorch equivalent will work (including subclass propagation if that's implemented for `torch` functions and operators), and other NumPy functions will error.\r\n\r\n### Use the NumPy API with PyTorch tensors (i.e. NumPy API calls dispatch to `torch` functions)\r\n\r\nNumPy provides two protocols that ndarray-like objects (like `torch.Tensor`) can implement to make NumPy API calls dispatch to their own implementations. Those protocols are  `__array_ufunc__` (available since NumPy 1.13) and `__array_function__` (available since NumPy 1.17 by default; in 1.16 one can enable it via an environment variable). These two protocols work in the same way:\r\n\r\n1. Pass `Tensor` instance to a numpy function (e.g. `numpy.abs`)\r\n2. NumPy detects the presence of `Tensor.__array_ufunc__` (or `Tensor.__array_function__`) and delegates execution to it.\r\n3. The `Tensor.__array_ufunc__` implementation then can forward that function call to the right implementation (`torch.abs` or `Tensor.abs`).\r\n\r\nThe main benefit of implementing these functions is that users can prototype new code with NumPy, or reuse their existing code, and that code will then work unchanged when passing in PyTorch tensors (even if they live on a GPU). For more context, see e.g. [NEP 18](https://www.numpy.org/neps/nep-0018-array-function-protocol.html). Also note that CuPy, Dask and pydata/sparse already implement these protocols.\r\n\r\nNote that there's a related discussion at [gh-2228](https://github.com/pytorch/pytorch/issues/2228), \"PyTorch with NumPy syntax?\", with a fairly detailed plan to provide a new `torch.np` API. That is very much related. That plan does seem less desirable than using `__array_function__` - why create a whole new API in a `torch.np` submodule when it's now possible to use the NumPy API itself?\r\n\r\nThere may be a **backwards compatibility** issue here. Because `torch.Tensor` already implements\r\n`__array__` and `__array_wrap__`, many (but not all) NumPy functions will already work with Tensor:\r\n```\r\nIn [1]: import torch                                                                           \r\n\r\nIn [2]: t = torch.Tensor([1, -2])                                                              \r\n\r\nIn [3]: np.abs(t)                                                                              \r\nOut[3]: tensor([1., 2.])\r\n\r\nIn [4]: np.sin(t)                                                                              \r\nOut[4]: tensor([ 0.8415, -0.9093])\r\n\r\nIn [5]: np.dot(t, t)                                                                           \r\nOut[5]: 5.0\r\n\r\nIn [6]: torch.dot(t, t)  # would be called if t had __array_function__                         \r\nOut[6]: tensor(5.)\r\n\r\nIn [7]: np.mean(t)  # not all functions work ....                                              \r\n...\r\nTypeError: mean() missing 3 required positional argument: \"dim\", \"keepdim\", \"dtype\"\r\n```\r\nSo here the return from `np.dot(t, t)` would change from `5.0` to `tensor(5.)`. For functions in NumPy that _don't_ have a PyTorch equivalent, the PyTorch `__array_function__` implementation should explicitly convert to ndarray with `np.asarray` and then call the NumPy functions (this preserves current behavior). Returning `NotImplemented` will cause NumPy to raise a `TypeError` while those functions worked previously via `__array__`.\r\nLikely this is not a major issue (Dask, CuPy and pydata/sparse all didn't consider this problematic), but it's good to explicitly think about this. The [Partial implementation of NumPy's API](https://www.numpy.org/neps/nep-0018-array-function-protocol.html#partial-implementation-of-numpy-s-api) section of NEP 18 provides a detailed discussion on this point.\r\n\r\n\r\nA **note of caution** is probably warranted here: while `__array_ufunc__` has been around for over 2 years and has generally worked very well, `__array_function__` (which does work very similarly but has to deal with more flexible function signatures) is brand new. An alternative discussed in NEP 18 is to use [multiple dispatch](https://www.numpy.org/neps/nep-0018-array-function-protocol.html#multiple-dispatch). That would be a more comprehensive solution (one can override anything, see e.g. [uarray](http://uarray.readthedocs.io/)), however it's a more invasive change with likely larger overhead (3-5 function calls rather than 1). Adding a protocol now would not preclude adding a multiple dispatch layer on top later. If the larger overhead is acceptable though, the PyTorch team could also decide that a more complete multiple dispatch layer (perhaps in a separate namespace or project) would be the better solution.\r\n\r\n### Use the PyTorch API with `torch.Tensor`-like objects that are _not_ `Tensor` subclasses\r\n\r\nThis would allow users to write their own tensor implementations and have users use the familiar PyTorch API with it. It can be implemented with a `__torch_function__` protocol, which would work analogously to the NumPy `__array_function__` protocol.\r\n\r\nProviding such a `__torch_function__` protocol will also help `Tensor` subclasses to modify the behavior of individual `torch` functions, while forwarding directly to the `torch` functions that that subclass does not want to modify (for an example of how this can work see the `__array_ufunc__` section of the [NumPy subclassing docs](https://www.numpy.org/devdocs/user/basics.subclassing.html#array-ufunc-for-ufuncs)).\r\n\r\nIn [issue 17268](https://github.com/pytorch/pytorch/issues/17268), \"Generic object to tensor dispatching\", there's both a proposal to API to register tensor conversion functions and a response from the Pyro developers that they'd prefer support for `__array_function__`.\r\n\r\nAs an alternative, the `__array_function__` protocol could be used directly on `torch` functions. The way to reuse `__array_function__` would be to decorate functions in PyTorch with `@array_function_dispatch` (from `numpy.core.overrides`, which is currently still private). The upsides of that are that the mechanism exists already, and is already supported by other array libraries. The potential downsides are that:\r\n- the mechanism is still very new and marked in the NumPy docs as \"may still change\" (although changes other than bug fixes are unlikely). Therefore, vendoring the decorator would be the way to go.\r\n- it puts a more stringent requirement on functions in the `torch` namespace to be compatible in signature with the `numpy` ones - this is desirable, but may not be achievable due to backwards compatibility reasons.\r\n- it's still not clear if `__array_function__` is actually supposed to be used like this (likely yes, but still under discussion in [NumPy gh-13872](https://github.com/numpy/numpy/issues/13872))\r\n\r\nIn summary: it's probably better to go with `__torch_function__` that works the same way as `__array_function__` but has its own \"domain\". That requires other libraries that want to reuse the PyTorch API to implement `__torch_function__` to explicitly opt in.\r\n\r\n### Reuse NumPy ufunc implementations directly from PyTorch\r\n\r\nWhat the rationale for this goal would be is a little unclear. The number of NumPy functions that are ufuncs and are not already covered by equivalent PyTorch functionality is not that large (see a list of NumPy ufuncs [here](https://docs.scipy.org/doc/numpy/reference/ufuncs.html#available-ufuncs)). The implementation of this feature in [gh-22247](https://github.com/pytorch/pytorch/pull/22247) was partially motivated by the goal of using `Tensor` subclasses with NumPy functions; that is best done differently though.\r\n\r\nIn case users want to use NumPy functions (not just ufuncs) with `Tensor`s today, this either may already work (it does for many functions) or can be done with explicit conversion:\r\n```\r\nx = tensor.numpy()  # to a numpy array (no-copy if tensor lives in CPU memory)\r\ny = np.somefunction(x)  # use with NumPy\r\ntensor2 = torch.from_numpy(y)  # convert back to a tensor\r\n```\r\nAdding extra design complexity to avoid these explicit casts does not seem worthwhile. In case there are NumPy functions that are popular, adding those functions to PyTorch itself seems like a better option, especially because that would work for tensors that live in GPU memory as well and be more performant.\r\n\r\nThe explicit casts would be a little more cumbersome for subclasses, but that's probably not a good enough reason to add design complexity.\r\n\r\n### Allow operations on mixed array types\r\n\r\nIn [gh-22247](https://github.com/pytorch/pytorch/pull/22247#issuecomment-506123115) it was suggested that a good goal could be to make operators on mixed array/tensor types work better. Currently `torch.Tensor + numpy.ndarray` will call the PyTorch implementation of the `+` operator, while `numpy.ndarray + torch.Tensor` will call the NumPy implementation of `+`. This is simply the way Python operator support works, and is unaffected by any of the array protocols like `__array_function__`. The general advice should be \"don't do that\" - it's better to be explicit there and convert both left-hand and right-hand side of any expression to either `Tensor`s or `ndarray`s.\r\n\r\n## Performance considerations\r\n\r\nThe extra overhead of the `__array_function__` and `__array_ufunc__` protocols is that of a single Python function call. Typically that is 300-400 ns (see e.g. https://github.com/numpy/numpy/pull/12830#issuecomment-506893356 for benchmarks). Adding a `__torch_function__` protocol should give a similar extra overhead for calling `torch.somefunction` _if_ a new check needs to be added. However it's likely (says @ezyang) that a check and fast path for `torch.Tensor` input already exists - in that case there will be no extra overhead for `Tensor` input (and 300 ns for non-`Tensor` input seems less of an issue). _needs investigating_\r\n\r\nFor comparison, the current overhead a NumPy ufunc including `__array_ufunc__` is of the same order (~400 ns) while the overhead of `torch` functions is significantly larger, ~3 us:\r\n```\r\nIn [1]: import torch                                                                           \r\nt\r\nIn [2]: t = torch.Tensor([1, -2])                                                              \r\n\r\nIn [3]: %timeit torch.abs(t)                                                                   \r\n3.2 Âµs Â± 30.5 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\r\n\r\nIn [4]: x = t.numpy()                                                                          \r\n\r\nIn [5]: %timeit np.abs(x)                                                                      \r\n392 ns Â± 5.83 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)\r\n```\r\nThat implies that this overhead should be acceptable.\r\n\r\n\r\nIn the PRs that triggered this subclassing discussion, it was said that `__array_finalize__` has too much overhead so cannot be used (hence the alternative in gh-22247). To check this, let's implement a similar small array (taken from [here](https://www.numpy.org/devdocs/user/basics.subclassing.html#simple-example-adding-an-extra-attribute-to-ndarray), see that description for more extensive comments):\r\n```\r\nclass InfoArray(np.ndarray):\r\n    def __new__(subtype, shape, dtype=float, buffer=None, offset=0,\r\n                strides=None, order=None, info=None):\r\n        # Create the ndarray instance of our type, given the usual\r\n        # ndarray input arguments.  This will call the standard\r\n        # ndarray constructor, but return an object of our type.\r\n        # It also triggers a call to InfoArray.__array_finalize__\r\n        obj = super(InfoArray, subtype).__new__(subtype, shape, dtype,\r\n                                                buffer, offset, strides,\r\n                                                order)\r\n        obj.info = info\r\n        return obj\r\n\r\n    def __array_finalize__(self, obj):\r\n        # ``self`` is a new object resulting from\r\n        # ndarray.__new__(InfoArray, ...), therefore it only has\r\n        # attributes that the ndarray.__new__ constructor gave it -\r\n        # i.e. those of a standard ndarray.\r\n        \r\n        if obj is None: return\r\n        # Note that it is here, rather than in the __new__ method,\r\n        # that we set the default value for 'info', because this\r\n        # method sees all creation of default objects\r\n        self.info = getattr(obj, 'info', None)\r\n\r\nn = 3\r\nx = np.arange(n)\r\ni = InfoArray(shape=(n,), dtype=np.int64, buffer=x)\r\n```\r\nNow to test the performance:\r\n```\r\nIn [2]: %timeit x + x                                                                          \r\n436 ns Â± 0.616 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [3]: %timeit x + i                                                                          \r\n1.43 Âµs Â± 10.8 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [4]: %timeit i + i                                                                          \r\n2.4 Âµs Â± 20.3 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\r\n```\r\nAnd for `n = 30000`:\r\n```\r\nIn [6]: %timeit x + x                                                                          \r\n10.9 Âµs Â± 27.4 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\r\n\r\nIn [7]: %timeit x + i                                                                          \r\n12.5 Âµs Â± 29.9 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\r\n\r\nIn [8]: %timeit i + i                                                                          \r\n13.8 Âµs Â± 439 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\r\n```\r\nSo the extra overhead of `__array_finalize__` is ~2-3 us when implemented in pure Python (and a subclass author could decide to implement the method in C if that's a problem). There does seem to be a small design issue in NumPy, because `x + i` and `i + i` both require a single new subclass instance to be created, hence for operations of the same type `i + i` should not be more expensive (but this is a minor detail).\r\n\r\n## Some comments on the NumPy & PyTorch APIs\r\n\r\n- It's not desirable to copy all of the NumPy API; that API is way too large and many functions are of limited interest or have better alternatives.\r\n- See [RNumPy](https://github.com/Quansight-Labs/rnumpy) for a work-in-progress attempt to define a sensible subset of the full NumPy API that other array libraries can target.\r\n- The PyTorch maintainers have expressed interest/willingness in adding functions to the `torch` namespace. Focusing first on matching the signatures of functions in the `torch` and methods in the `torch.Tensor` namespace would be nice. Right now the functions that are there often have different signatures (e.g. compare `torch.sum` and `Tensor.sum`).\r\n\r\n## Pitch / Possible plan forward\r\n\r\nImplement the following (these don't depend on each other, no order implied):\r\n\r\n1. `__array_ufunc__` and `__array_function__` (small backwards compat impact, no performance impact)\r\n2. `__torch_function__` (no backwards compat impact, likely no performance impact for `Tensor` input and 300-400 ns for non-`Tensor` input, needs investigating)\r\n3. A subclass finalization method (as in [gh-22235](https://github.com/pytorch/pytorch/pull/22235) or `__array_finalize__`) (no backwards compat impact, 300 ns - 3 us performance impact for subclasses only)\r\n\r\nThis would close:\r\n\r\n- [gh-2228](https://github.com/pytorch/pytorch/issues/2228), \"PyTorch with numpy syntax?\"\r\n- [gh-20073](https://github.com/pytorch/pytorch/pull/20073), \"numpy arg translation proof of concept\"\r\n- [gh-17249](https://github.com/pytorch/pytorch/issues/17249): \"Proposal: Add `__tensor_wrap__` method similar to numpy `__array_wrap__`\"\r\n- [gh-22235](https://github.com/pytorch/pytorch/pull/22235): \"ptype propagation on torch functions\"\r\n- [gh-22247](https://github.com/pytorch/pytorch/pull/22247): \"ptype propagation on numpy functions\"\r\n\r\n"},{"labels":["enhancement",null,null],"text":"In #22036 we added sparse allreduce for ProcessGroupGloo. It works for sparse CUDA tensors, but doesn't leverage InfiniBand like NCCL does. Therefore, we should have a sparse allreduce implementation for ProcessGroupNCCL as well."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nAdd `key_padding_mask` as an argument to the `Transformer/TransformerEncoder/TransformerDecoder` `forward` methods.\r\n\r\n## Motivation\r\n\r\nThe current implementation of the Transformer only allows the use of the `attn_mask` parameter of the `MultiheadAttention` module. I think this can only be applied to a batch as a whole, not per input in the batch. I think it would be useful to allow the use of the `key_padding_mask` parameter for sequences with padded values.\r\n\r\n## Pitch\r\n\r\nI want the Transformer not to pay attention to padding elements in a sequence.\r\n\r\n## Alternatives\r\n\r\nModify TransformerEncoderLayer\r\n\r\n```\r\nclass TransformerEncoderLayer\r\n    ...\r\n    def forward(self, src, src_attn_mask=None, src_padding_mask=None):\r\n        src2 = self.self_attn(src, src, src, attn_mask=src_attn_mask, key_padding_mask=src_padding_mask)[0]\r\n    ...\r\n```\r\n\r\nRepeat for all relevant modules (`Transformer*`)\r\n\r\nI know this can be achieved using custom `Transformer/Encoder/Decoder` modules, but my proposal may be a common issue which can be easily included.\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nWeight decay is used very often. A common strategy is to implicitly use the learning rate scheduler todo so, or to simply shrinking the weights at the end of each iteration by a constant multiplicative factor. However, one could expect to use strategies different from this. In that case, we could have weight schedulers that modify the weights using a syntax and grammar similar to learning rate schedulers already available. \r\n\r\n## Motivation\r\n\r\nThe AdamW code from #21250 does not include the weight scheduler, as mentioned [here](https://github.com/pytorch/pytorch/pull/4429#issuecomment-508951336).\r\n\r\nThere are also currently a few issues and pull requests about weight schedulers, see below.\r\n\r\n## Alternatives\r\n\r\n* #4429 suggests modifying the optimizer logic to accept a new parameter `weight_decay` specifying the constant multiplicative factor to use.\r\n* #3740, #21250, #22163 introduce variations on Adam and other optimizers with a corresponding built-in weight decay. #3790 is requesting some of these to be supported.\r\n* We could instead have a new \"weight_decay_type\" option to those optimizers to switch between common strategies."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nHow gradient tensors are batched together in bucket prior to reducing them across processes is done completely statically today. A user specifies a lower bound on the bucket size and parameters are bucketed accordingly. We can use profile data of the backwards pass to find a bucket assignment that better aligns with the point in time gradients are ready.\r\n\r\n## Motivation\r\n\r\nIn some vision models the last layers have many weights (big fully connected layers) and the first layers have only a few weights (small convolutional layers) The bulk of gradients are computed early during the backwards pass. Conversely the gradient tensors for the first layers are computed last and are relatively small. If the final bucket to reduce is 25MB and we wait for the final 1KB worth of gradients before we start, we may achieve better end to end latency if we instead start reduction sooner, without the final 1KB, and run a separate reduction for the final piece of data. We have hooks in `c10d::Reducer`, as well as the autograd profiler, both of which can be used to generate timing data for the gradient of every parameter. Depending on this timing and the size of the gradients, we can create a bucketing assignment that is a better match for the execution order of autograd and doesn't have a big \"latency bubble\" after autograd has finished. \r\n\r\n## Pitch\r\n\r\n* Evaluate applicability of the autograd profiler here. E.g. can we get a mapping of the indices in `model.parameters()` to some relative timestamp their gradient was computed? If not, how easy would it be to add it? (If the autograd profiler cannot be used we can look into adding timing to `c10d::Reducer` directly.)\r\n* Generate a new bucketing assignment based on relative time per parameter as well as the size of the corresponding gradient.\r\n* Distribute the newly computed bucketing order to every participating process. This is done by nvidia/apex as well (cc @mcarilli) where the parameter order is distributed to all processes. If there is mismatch between the order that the model layers/parameters are defined and the order they are used, any approach that closely resembles the autograd order will already be an improved over the naive strategy of simply using definition order.  \r\n\r\n## Additional context\r\n\r\nStretch goal: generate a number of bucketing assignments and then empirically figure out which one is best for the set of machines the model is running on."},{"labels":["enhancement",null,null,null,null,null],"text":"## ðŸš€ Feature\r\n\r\nImplicitly calculate the scatter/covariance matrix of the im2col matrix.\r\n\r\n```python\r\ndef F(I):\r\n    \"\"\"\r\n    Please help us to do the following implicitly:\r\n    \"\"\"\r\n    X= im2col(I)\r\n    Cov=X@X.t()/N\r\n    return Cov\r\n```\r\n\r\n## Motivation\r\n\r\nNeural networks are learning from a convolved world! Machine learning is analogous to human learning with blurred/nearsighted vision. Therefore, it is hard. But things are much easier if the machines wear glasses.\r\n\r\nhttps://arxiv.org/abs/1905.11926\r\n\r\nRecently we notice that the convolutional networks converge much faster and better by applying a real 'deconvolution'. For example, on the well known CIFAR-10 dataset, with deconv, 20-epoch training achieves the same performance that used to require 100-300 epochs. Tested on over 10 networks on multiple datasets, our improvements over batch norm is similar to batch norm over vanilla training.\r\n\r\nThere is one step that hinders the speedup. When calculating the kernel for deconvolution, we need to  calculate the autocorrelation of the pixels. Currently, we resort to the unfold function which uses excessive memory and more time. The training could be much better and easier if this is done implicitly.\r\n\r\n## Pitch\r\n\r\nWe want this simple modification to be included in pytorch so that the future training of CNNs are more straightforward. \r\n\r\n## Alternatives\r\n\r\nhttps://github.com/yechengxi/deconvolution/blob/2aa372fd232ddad08082cb8a1f8ab46b16b28a5b/models/deconv.py#L303\r\n\r\nCurrently the calculation is implemented in three lines:\r\n\r\n```\r\nX = torch.nn.functional.unfold(x, self.kernel_size,self.dilation,self.padding,self.stride).transpose(1, 2).contiguous()\r\nX=X.view(-1,X.shape[-1])\r\nCov = X.t() @ X / X.shape[0]\r\n```\r\n\r\nThe world is likely to be very different if these three lines are optimized in the library.\r\n\r\n\r\ncc @fmassa"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nHopefully, `torch.nn.DataParallel` can support data parallel on multiple heterogeneous GPU.\r\n\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nI'm currently working on a big model and would like to use `torch.nn.DataParallel` to speed up training. However, I use two different GPU on my computer, i.e, GTX 1060 and GTX 1080, when I train the model, 1060 becomes a bottleneck and slows down the whole training process.\r\n\r\n\r\n## Pitch\r\nIt will be great that `nn.Dataparallel` can automatically assign tasks according to computation power or support manually specifying mini-batch size on each GPU.\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\n\r\nOffer the option to set box constraints for optimizers.\r\n\r\n## Motivation\r\n\r\nIn some applications of reinforcement learning, falling outside of a region correspond to inadmissible actions.\r\n\r\n## Alternatives\r\n\r\nOne can omit the constraints, and hope that the optimal parameters found happen to be within the constraints, without any guarantees.\r\n\r\n## Additional context\r\n\r\nThis was suggested in #938 for torch.optim.lbfgs.\r\n\r\nCC @bamos"},{"labels":["enhancement",null,null],"text":"Currently `~mask` is possible, but not inplace, if I understand well. \r\n\r\nNumPy equivalent would be https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.invert.html"},{"labels":["enhancement",null,null],"text":"## ðŸ› Bug\r\n\r\nPytorch is slower on windows than on linux\r\n\r\n## To Reproduce\r\n\r\nI used this: https://github.com/ultralytics/yolov3\r\n\r\nTry it on Linux, then try it on windows. It's about 2-5 times slower. \r\n\r\n## Expected behavior\r\n\r\nPerformance should be exactly the same. \r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Microsoft Windows Server 2019 Datacenter Evaluation\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce GTX 1080\r\n\r\nNvidia driver version: 430.86\r\ncuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] numpydoc==0.9.1\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.3.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] libblas                   3.8.0                    10_mkl    conda-forge\r\n[conda] libcblas                  3.8.0                    10_mkl    conda-forge\r\n[conda] mkl                       2019.3                      203\r\n[conda] mkl-service               2.0.2            py36hfa6e2cd_0    conda-forge\r\n[conda] mkl_fft                   1.0.13           py36hfa6e2cd_1    conda-forge\r\n[conda] mkl_random                1.0.4            py36h830ac7b_0    conda-forge\r\n[conda] pytorch                   1.1.0           py3.6_cuda100_cudnn7_1    pytorch\r\n[conda] torchvision               0.3.0              py36_cu100_1    pytorch\r\n\r\n## Additional context\r\n\r\nTensorflow has this issue too. It's my understanding that windows has a buffer of some sort that batches calls to the gpu. Now you'd imagine at the lower level, that's hopefully getting bypassed, but who knows. \r\nhttps://github.com/tensorflow/tensorflow/issues/29874\r\nIf you have a card that supports TCC, you should enable it and see if it makes a difference "},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nIn torch.distributed/c10d we should have some sort of ProcessGroup proxy that creates a fingerprint of the collective to be run, the input tensors (device type, data type, shape, etc), and executes an allgather on that fingerprint to verify the call is valid.\r\n\r\n## Motivation\r\n\r\nCurrently a user can call different collectives, or the same collective with different tensor shapes, and cause a crash that reports a daunting error message. If we provide a way to debug where this desynchronization is happening then we'll be able to better help out users who experience this and perhaps even make it entirely self-debuggable. For example, if we perform an allgather on the collective fingerprint and allgather it, every process can raise an exception and log their stack. This alone is a direct view into *where* the desynchronization is happening.\r\n\r\n## Pitch\r\n\r\nA ProcessGroup wrapper that takes 2 process groups for initialization: a Gloo process group that can be used to share the fingerprints, and a process group to call into if the fingerprints match. Calls to the underlying process group should be fully serialized, since this will be a tool for debugging only. We can catch and re-raise exceptions from process groups if they still fail and add some message about this debugging facility. Users can enable this by calling `init_process_group` with `debug=True` (for example).\r\n\r\n## Alternatives\r\n\r\nWe could choose to do this entirely in Python, since it doesn't need to be fast, but then again it wouldn't be applicable for C++ users. I'd rather have the implementation in C++ so it can be used everywhere.\r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nThe feature I want is to be able to check if the gradient was actually computed or not for a specific parameter in the optimizer, instead of checking if the gradient is None (as done currently).\r\n\r\n## Motivation\r\nThe motivation in my case comes from trying to implement some code that is doing multi-task learning. On each training iteration I'm getting a batch from a single task and compute gradients for that task specific head (and shared backbone for all tasks), while heads for other tasks are not supposed to be updated. This is indeed happening on the first iteration, but after we computed gradients for each task at least one time, each one of them will be updated on *every* iteration, even when no gradients were computed for that task specific head. The reason for that is because most optimizers will only ignore weights if their gradient is None, and if not, they will proceed updating them. Thus, after the gradients were computed ones, they will never be None again (since we set them to zero in optimizer.zero_grad) and on each iteration, the current momentum and weight decay will be applied to the weights for which we didn't really compute the gradients. \r\n\r\nIn my opinion this is very dangerous behavior because this is likely not what the user expects to happen, but the model could still work fine and train to some non-trivial accuracy even though each task weights receive additional weight decay and momentum updates. While it might not matter much in some cases, when the tasks are trained for different number of iterations (e.g. one task has 10 times more updates than the other), this might significantly harm performance. And there might be other cases besides multi-task learning, when not every weight is supposed to be updated on each iteration, where this behavior of the optimizer can cause some unexpected results without users even noticing this. \r\n\r\n## Pitch\r\n\r\nI am not sure how gradient flow is implemented internally in PyTorch, but conceptually it should be possible (and probably not too complicated) to have a way to check if the gradient was actually computed for each weight after the last call to .backward on the loss. And then this could be used to check if the optimizer should update weights or not, instead of checking for None. \r\n\r\n## Alternatives\r\n\r\nAs an alternative solution, I can explicitly set gradients for weights that are not supposed to be updated to None on each iteration. However, it seems like that will introduce additional computational overhead on each iteration, because new memory will have to be allocated for the gradient storage. \r\n\r\nAnother solution could be to have separate optimizers for each task. However, in this case the optimizer buffers for shared backbone weights will be separate. This means that, for example, momentum will be accumulated separately for each task gradients and not merged together and this might also lead to suboptimal performance. I could also have a separate optimizer for shared weights, but this becomes more complicated when certain tasks share additional weights between each other (for example, part of the head could be shared between two tasks and not shared with other tasks, while backbone is shared across all tasks). \r\n\r\nAnother solution could be to set learning rate to zero on each iteration for the parameters that should not be updated. However, in this case, their momentum buffers will still get updated and this will result in different performance.\r\n\r\nThe solution I think I will end-up doing is to modify the optimizer code to explicitly pass weights that should not be updated on the current iteration. This is also not ideal, because it is cumbersome to check which weights should and should not be updated on the current iteration and I have to look at the names of the weights for this. And I also have to add this new logic for each optimizer I plan to use for my models.\r\n\r\nLet me know if there is some other, easy to implement solution for this problem. However, even if that is the case, I think it might still be important to implement it by default in PyTorch or at least mention this in the documentation, or raise some kind of warning for this behavior. Because this problem will not lead to an immediate error and thus it is easy to overlook this and get unexpected results that could potentially harm performance of the models quite significantly.\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nNumpy has a function, `np.nanmean()`, that excludes NaN values when computing the mean. I'd like \r\n\r\n## Motivation\r\nSuppose I want to compute MSE over two vectors, one of which has NaN values. Right now, there's no easy way of doing this.\r\n\r\n## Hope\r\n`torch.nanmean(x)` returns the mean of all non-nan values of `x`."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nFor distributions created by calling `torch.distributions.Independent`, support `covariance_matrix`\r\n\r\n## Motivation\r\n\r\nThis will make it easier to treat multivariate distributions in a uniform way. Of course, this covariance matrix will just be a diagonal of the variance of (batched) distribution that was used to create the Independent distribution.\r\n\r\n## Pitch\r\n\r\nI think the implementation is just something like the following. (I guess this only works when `reinterpreted_batch_ndims==1`.)\r\n\r\n```python\r\n@lazy_property\r\ndef covariance_matrix(self):\r\n    return tt.diag(self.base_dist.variance))\r\n```"},{"labels":["enhancement",null,null,null],"text":"A generic pytorch transformer layer \r\n\r\n## ðŸš€ Feature\r\ninput: \r\n\r\na 2-dim Tensor [x_{00}, x_{01},  ... x_{0m_1}, x_{10}, ... x_{1m_1}, ... x_{km_k}]; each x is a vector.\r\n\r\nand either (or both, but they should match):\r\n* a 1-dim Tensor of offsets [o_0, ..., o_k ]\r\n* or a list of square 2-dim Tensors [ A_{m_0}, .... A_{m_k}] of attention masks\r\n\r\n\r\noutput: \r\na 2-dim Tensor [h_{00}, h_{01},  ... h_{0m_1}, h_{10}, ... h_{1m_1}, ... h_{km_k}]\r\n\r\nThe inputs follow the arrangement of nn.EmbeddingBag.  Each offset corresponds to the start of a set of inputs x (and the whole input is a set of these sets).    If attention masks are input, they are pointwise multiplied by the attention inner products before the softmax.\r\n\r\n\r\n## Motivation\r\n\r\nThis is a generic transformer formulation, and can be used for text, vision, graphs, etc.\r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nMake it possible to specify the axis on which to perform Batch Normalization.\r\n\r\n## Motivation\r\nRight now batch normalization is geared towards convolution layers which have typically a  layout of NxCxWxH (in the context of 2D batch normalization). But there are other scenarios possible (for example to apply BN to a RNN) where the features are located at a different axis. The current solution doesn't cater for those scenarios.\r\n\r\n## Pitch\r\nAdding the axis makes batch normalization more generic applicable to a wide variety of use cases and is inline with other ML frameworks. Default could still be axis=1 so NxCxWxH is handled as is currently the case but other cases are now also supported.\r\n\r\n## Alternatives\r\nThe only workaround right now it to shift the different axis before the BN and then move them back again after. Not the most elegant solution.\r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n```python\r\ntorch.disable_broadcast()\r\n# raise errors instead of doing broadcast underneath.\r\n\r\n```\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nBroadcast is very convenient and efficient in many scenarios, but it also introduces extra careless bugs which are hard to locate.\r\nIt should be great if I can disable the broadcast mechanism (or force the Tensor size check) at debugging time.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nAdd static const variable that means end of the tensor.\r\n\r\n## Motivation\r\nReference slice function's definition\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/Tensor.h#L527\r\n\r\nIn c++ slicing the whole tensor with step 2\r\n\r\n```\r\ntensor.slice(/*dim=*/0, /*start=*/0, /*end=*/9223372036854775807, /*step=*/2);\r\n```\r\n\r\nIf torch::end exists and is set 9223372036854775807.\r\n\r\n```\r\ntensor.slice(/*dim=*/0, /*start=*/0, /*end=*/torch::end, /*step=*/2);\r\n```\r\n## Pitch\r\n\r\nImprove readability and keep clear intention."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nImplement `torch.where(condition)` as an alias for a NumPy-like nonzero. Currently, we only support `torch.where(condition, x, y)` with the second and third arguments.\r\n\r\n## Motivation\r\nNumPy supports numpy.where(condition) without the x and y arguments. It's behaves like numpy.nonzero.\r\n\r\nhttps://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html\r\n\r\n## Pitch\r\nMaybe @alexpeys will stop bugging me if you implement this\r\n\r\n## Alternatives\r\nMove @alexpeys's desk farther away\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nImplement torch.fill_diagonal based on numpy.fill_diagonal\r\n\r\nSee https://docs.scipy.org/doc/numpy/reference/generated/numpy.fill_diagonal.html\r\n\r\n## Motivation\r\nAlex asked me how to do this today.\r\n\r\n## Alternatives\r\nThe following works but isn't obvious and may not work for non-strided Tensors:\r\n\r\n```python\r\ntorch.diagonal(x).fill_(value)\r\n```"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nA module that allows for batched 2d convolutions on sequence data. Essentially, a module that takes as input a tensor of dimensions (batch_size, sequence, channel, H, W) and returns a tensor of dimensions (batch_size,sequence,channel,H',W').\r\nNote: This function is **NOT** conv3d. The convolution is 2d, and is applied individually to each element of the sequence. \r\n## Motivation\r\nUsing conv2d on video data has its uses such as detecting objects in each frame. A batched implementation of this will considerably speed up processing for such applications. Current alternative is to loop the application of conv2d for the length of the sequence.\r\n\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nImporting ONNX models into Pytorch.\r\n## Motivation\r\n\r\nAlmost all other frameworks already support this. Importing ONNX models into Pytorch makes Pytorch much more flexible.\r\n\r\n## Pitch\r\n\r\nIn `torch.onnx`, a function should be created to take the ONNX model and outputs a Pytorch model. "},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nAdd a transform function in distributions that only transforms diagonal elements of matrix valued random variables, similar to the TransformDiagonal bijector in TensorFlow Probability.\r\n\r\n## Motivation\r\nImportant for reparametrizing random objects defined on the space of covariance matrices (e.g. Inverse Wishart distribution) to unconstrained space (first, take Cholesky, then transform the diagonal only as proposed here with log transform). \r\n\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nA dataloader and dataset that operate at the batch level, rather than the item level, pulling batches from contiguous blocks of memory and avoiding random access patterns in the dataloader.\r\n\r\n## Motivation\r\nLoading data item by item and coallating into a batch is very inefficient, particularly in the case of tabular or text data where the items are small.  This is compounded further when you want to use large batch sizes.  By pre shuffling the data each epoch (when required) we can grab each batch as a single read from contiguous memory.  This much faster and scales better with batch size, removing the necessity of multiprocessing, which adds complexity in the form of bus errors when not enough shared memory is available (https://github.com/pytorch/pytorch/issues/5040), CUDA init issues when forking (https://github.com/pytorch/pytorch/issues/4377), etc.  This forking issue was one of my original motivations as it solves the issue of using the dataloader in conjunction with RAPIDS or any other code that calls CUDA before the dataloader workers are forked.  It should also solve the issue on windows with the speed of dataloaders, at least for tabular and text data,  (https://github.com/pytorch/pytorch/issues/12831) as spawning is not necessary.  \r\n\r\nUsing the proposed method results in better GPU utilization, and better throughput when training in the tests on tabular data that I've run.  With no multiprocessing I've measured a 5-15% improvement* in throughput over an 8 worker vanilla dataloader (more were tried but it maxed out at 8).  I've also been able to increase batch sizes for tabular data into the 800K+ range with no loss of accuracy and get a 2x performance improvement over the best multiprocessor dataloader I could run without running into bus error issues that cropped up with large batch sizes.\r\n\r\n*depends on tensor and batch size\r\n\r\n## Pitch\r\n\r\nI've created source for a batch dataloader and batch dataset modelled after their vanilla counterparts and would love to see it integrated into the PyTorch repo.  Usage is similar, and I've tried to stick to the pytorch variable naming and formatting.\r\n\r\nCode can be found here: https://github.com/rapidsai/dataloaders/tree/master/pytorch/batch_dataloader\r\n\r\nIt should hopefully be ready to go; I've tested it with both base pytorch and with ignite, but more eyes on it would definitely be beneficial, particularly in use cases beyond tabular like text or small images.  It should be applicable to anyone who isn't doing large images or a lot of image augmentation.  It's undergone an internal (NVidia) review of @ptrblck who was immensely helpful in refining it and @ngimel who reviewed the codebase and had helpful suggestions regarding memory pinning.  \r\n\r\nI'm happy to work with the team to create test cases similar to those for dataset and dataloader and would love feedback on it.\r\n\r\n## Alternatives\r\n\r\nOne possible solution to the CUDA Init before fork issue is to spawn, however as seen in windows this is significantly slower and I had trouble getting it working.   \r\n\r\n## Additional context\r\n\r\nI'm also working on versions of this that work with larger than CPU memory datasets and on a version that works in GPU memory doing a 0-copy transform of a rapids cudf dataframe via dlpack."},{"labels":["enhancement",null,null,null],"text":"[`distributed_c10d.py`](https://github.com/pytorch/pytorch/blob/master/torch/distributed/distributed_c10d.py) provides APIs to reduce a single tensor per process, or multiple tensors per process where each of them needs to reside on a different device. It will be useful (e.g., for model averaging) to support reducing a list of tensors per process where all of them are on the same device. The implementation should apply bucketing under the hood to get better performance. "},{"labels":["enhancement",null,null],"text":"I would like to request one \"conditional overall argmax()\" function. Like the example below:\r\n\r\nlet's say we have a tensor Q with shape (4,4,4):\r\n\r\n![Snip20190609_3](https://user-images.githubusercontent.com/22250129/59160183-7a6f6800-8aa1-11e9-8707-2195076ba132.png)\r\n\r\nthen, given a **conditional index vector, [0,1,:]**, we pick out the corresponding part by **Q[0,1,:]**. If we do the argmax() now, it returns \"**tensor(2)**\". However, I hope there is one \"overall argmax()\" can return \"tensor(6)\" instead, which is the corresponding index under the overall flatten Q.view(-1)[6]:\r\n\r\n![Snip20190609_4](https://user-images.githubusercontent.com/22250129/59160352-e7cfc880-8aa2-11e9-9510-67484134418f.png)\r\n\r\nThe function should work for **any arbitrary conditional index vector**. \r\n\r\nAlso, this function should be able to receive any number of Q and the corresponding conditional vector, and then return the corresponding conditional overall argmax().\r\n\r\nFor example, if we have 1000 Q with shape (1000, 4, 4, 4) and 1000 condition index vector, the function should return me a tensor with shape (1000, 1) which is the conditional overall argmax() for each Q.\r\n\r\nThis function will be very helpful in Reinforcement Learning for the Multi-agent case when we learn a joint-Q-value and select the max one conditioned on certain agent's action.\r\n\r\nThank you!"},{"labels":["enhancement",null,null,null,null],"text":"PR #20983 contains a port of CUDA Blas functions gemm and gemv to ATen native.\r\n\r\nThere are several other Blas functions in `THCBlas.{h,cu}` that need to be ported.\r\n\r\nAlso, the duplication of `adjustLdLevel2`, `adjustLdLevel3`, `convertTransToCublasOperation`, `THCublasCheck` needs to be resolved by deleting these once all `THCBlas.cu` functions use Blas functions from `at::cuda::blas`."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nPlease implement the cdf for beta distribution.\r\n\r\n## Motivation\r\nThe cdf of beta is used quite frequently but it's not implemented.\r\n\r\n## Code to reproduce the problem:\r\n~~~~\r\nfrom torch.distributions.beta import Beta\r\nm = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\r\nm.cdf(0.0029)\r\n~~~~\r\n\r\n## Stack Trace\r\n```\r\nFile \"<stdin>\", line 1, in <module>\r\n  File \"/home/.virtualenvs/test/lib/python3.6/site-packages/torch/distributions/distribution.py\", line 133, in cdf\r\n    raise NotImplementedError\r\nNotImplementedError\r\n```"},{"labels":[null,"enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\nSupport the argument format `torch.einsum(op0, sublist0, op1, sublist1, ..., [sublistout])`. It would allow the users to use more than 26 tensors (i.e. the number of lower case letters) in Einstein Summation.\r\n\r\n## Motivation\r\n\r\nThis feature is already available in [Numpy](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html).\r\n\r\n## Pitch\r\n\r\nPerform the operation below in PyTorch:\r\n\r\n```python\r\nimport numpy as np\r\n\r\nx = np.random.rand(6).reshape(2, 3)\r\ny = np.random.rand(6).reshape(2, 3)\r\nz = np.random.rand(6).reshape(2, 3)\r\nw = np.random.rand(6).reshape(2, 3)\r\nr1 = np.einsum(\r\n            x, [0, 1], \r\n            y, [0, 2], \r\n            z, [0, 3], \r\n            w, [0, 4], \r\n            [1, 2, 3, 4]\r\n     ) # Not currently supported by PyTorch\r\nr2 = np.einsum('ni, nj, nk, nl -> ijkl', x, y, z, w) # Supported by PyTorch\r\nprint((r1 == r2).all())  # Prints True\r\n```\r\n\r\n## Additional context\r\n\r\nI actually don't know many contexts where one really needs to call Einstein Summation on more than 26 tensors. However, for some use cases one could gain some readability if you call Einstein Summation with, say, 5 or 6 tensors. "},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nAn implementation of group-equivariant convolutions defined by Cohen et. al in the paper 'Group equivariant convolutions' - in ICML 2016.\r\nSpecifically, an implementation of the group-convolution over the group of translations and rotations of multiples of 90 degrees at any centre, and the same group extended by reflections.  \r\n\r\n## Motivation\r\nGroup-equivariant convolutions (g-cnns) extend the equivariance of convolutions to beyond translations. Cohen et. al argue in their paper that the parameter-sharing in g-cnns allow for more efficient use of parameters.\r\nFurther, limited experiments conducted by Adam Bielski in his github repository https://github.com/adambielski/pytorch-gconv-experiments, suggest that g-cnns improve upon the performance of regular cnn networks in at least some cases.\r\nI'm not sure if the existing pytorch implementations are efficient as they take considerably more time to run than nn.Conv2d. \r\n\r\nTo sum up, it would be awesome if you guys could create nn.GConv2d() for group-equivariant convolutions on at least the two groups mentioned before.\r\n\r\n## Additional context\r\nTS cohen's original implementation of group-equivariant convolutional networks: https://github.com/tscohen/GrouPy"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n## Motivation\r\n\r\nI used something like `torch.mean(x, dim=(1,2,3))` for reduction. \r\nI get this in the export:\r\n\r\n```python\r\n/usr/local/lib/python2.7/dist-packages/torch/onnx/symbolic.py in symbolic(g, self, dim, keepdim)\r\n    332         else:\r\n    333             # dim-reduce path\r\n--> 334             dim, keepdim = _get_const(dim, 'i', 'dim'), _get_const(keepdim, 'i', 'keepdim')\r\n    335             return g.op(onnx_op_name, self, axes_i=[dim], keepdims_i=keepdim)\r\n\r\n```\r\n\r\n[multidim operators](https://github.com/pytorch/pytorch/issues/9703) for `mean` and `sum` are available since 1.0, and for `std`/`var` since 1.1\r\n\r\n## Pitch\r\n\r\nSupport having `dim=(1,2,3)` for `torch.mean`, `torch.std`, ...\r\n\r\n## Alternatives\r\n\r\nI use this for alternative: `x.view(n_b, -1).mean(1).view(n_b, 1, 1, 1)`\r\n## Additional context\r\n\r\nInitially reported the issue [here](https://github.com/microsoft/onnxruntime/issues/1084)\r\nAdded [this issue](https://github.com/onnx/onnx/issues/2045) for ONNX to support `std`/`var`, but still `mean`  with multidim also should be supported"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nneed support for matrix multiply of int matrix in cuda ( you can do it in cpu ). \r\n\r\n## Motivation\r\n\r\nA work of mine need achieve matrix multiply of 2 huge int matrix in cuda. if I change it to float, the memory of my gpu can't afford it (rtx titan, 24G).\r\n\r\nRight now, you can't achieve matrix multiply of  2 int matrix, it will raise error:\r\n\r\nRuntimeError: addmm for CUDA tensors only supports floating-point types. Try converting the tensors with .float()"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nI would like to have a decorator like\r\n\r\n```python\r\n    @onnx_convert('MeanVarianceNormalization')\r\n    def forward(self, x):\r\n        ....\r\n```\r\n\r\nTo return correct `g.op(MeanVarianceNormalization)` to the tracer.\r\n\r\n\r\n## Motivation\r\n\r\nI have this `MVN` implementation:\r\n\r\n```python\r\nclass MVN(nn.Module):\r\n    \"\"\"Mean-Variance Normalization (MVN) Layer\r\n    \"\"\"\r\n    def __init__(self, normalize_variance=True, across_channels=False, eps=None):\r\n        super(MVN, self).__init__()\r\n        self.normalize_variance = normalize_variance\r\n        self.across_channels = across_channels\r\n        if eps is None:\r\n            eps = 1e-9\r\n        self.eps = eps\r\n\r\n    def extra_repr(self):\r\n        \"\"\"Extra information\r\n        \"\"\"\r\n        return \"eps={}{}{}\".format(\r\n                self.eps,\r\n                \", normalize_variance\" if self.normalize_variance else \"\",\r\n               \", across_channels\" if self.across_channels else \"\",\r\n        )\r\n\r\n    def forward(self, x):\r\n        std = None\r\n        shape = x.data.shape\r\n        n_b = shape[0]\r\n        if self.across_channels:\r\n            if shape[0] == 1:\r\n                # single batch normalization reduction\r\n                mean = x.mean()\r\n                if self.normalize_variance:\r\n                    std = x.std()\r\n            else:\r\n                mean = x.view(n_b, -1).mean(1).view(n_b, 1, 1, 1)\r\n                if self.normalize_variance:\r\n                    std = x.view(n_b, -1).std(1).view(n_b, 1, 1, 1)\r\n        else:\r\n            n_c = shape[1]\r\n            mean = std = x.view(n_b, n_c, -1).mean(2).view(n_b, n_c, 1, 1)\r\n            if self.normalize_variance:\r\n                std = x.view(n_b, n_c, -1).std(2).view(n_b, n_c, 1, 1)\r\n        if std is not None:\r\n            return (x - mean) / (std + self.eps)\r\n        return x - mean\r\n```\r\n\r\nI cannot export it because [std is not supported in ONNX](https://github.com/onnx/onnx/issues/2045) but ONNX already has [MeanVarianceNormalization](https://github.com/onnx/onnx/blob/master/docs/Operators.md#MeanVarianceNormalization) that I would like to be used with this `nn.Module`. \r\n\r\n## Pitch\r\n\r\nInstead of having an exporter that knows every detail about everything, let the `nn.Module` (or `torch.autograd.Function`) implementer provide a function that returns the right export ops.\r\n\r\n## Alternatives\r\n\r\n1. Modify symbolic.py (not a good idea because it may change in the next update)\r\n2. monkey-patch symbolic.py, this file already monke-patches using `import torch.onnx.utils` so I hope this works\r\n3. provide decorator as `Functional` level instead of `nn.Module`, a little more cumbersome in simple cases but not bad\r\n\r\n## Additional context\r\n\r\nI also need this functionality for custom `nn.Modules` some of which using `CUDAExtension` or `CppExtension`, I would like to use this functionality to automatically export ONNX from full model that includes these custom layer/ops\r\n\r\nI first reported this [here](https://github.com/microsoft/onnxruntime/issues/1087) but it is really related to PyTorch's ONNX exporter.\r\n"},{"labels":["enhancement",null,null,null],"text":"At Facebook we are building a data reading framework for PyTorch which can efficiently read from data stores like Hive, MySQL, our internal blob store and any other tabular data sources. The framework allows for specifying complex input pipelines to read from different sources. For example if you have a table which stores handles for images, you can write SQL like code to read from the table, apply filters to select certain handles and then retrieve those handles from another data source with a few lines of code.\r\n\r\nIn addition to this, the framework supports running user-defined transforms which can either be pure python (ex: [torchvision.transforms](https://pytorch.org/docs/stable/torchvision/transforms.html)) or torchscript code. This framework can also be used with the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) package to distribute the data across multiple nodes for training. The input pipeline that the user specifies can be defined once, serialized as a plan and run on multiple remote machines if required.\r\n\r\nThe framework builds upon the OSS dataloader and dataset framework. In particular it uses [IterableDataset](https://github.com/pytorch/pytorch/pull/19228) to provide a stream based interface for data retrieved from input pipelines.\r\n\r\nSample code to illustrate what reading and pre-processing images would look like:\r\n\r\n```\r\n# Hive table has columns handle and partition_no. The partition column \r\n# for the table is partition_no\r\ndf = data.data_warehouse(\"mynamespace\", \"mytable\")\r\n\r\n# Filter to partition the data across multiple workers.\r\npartition_filter = \"hash(partition_no) % {0} = {1}\".format(worker_info.num_workers, worker_id)\r\ndf = df.filter(partition_filter)\r\n\r\n# Fetch the handle from a blobstore\r\ndf = df.map([\"fetch_handle(handle) as img\"])\r\n\r\n# Rebatch the data\r\ndf = df.rebatch(batch_size=16)\r\n\r\n# transform_image is a user supplied function to run image transforms.\r\nds = MyDataset(df=df, transforms=transform_image)\r\ndl = torch.utils.data.DataLoader(ds)\r\n\r\nfor batch in dl:\r\n    pass\r\n```\r\nWe are evaluating whether it makes sense to open source this framework. For OSS users, this framework might be useful for training jobs which store large amount of data in Hive or S3 (images). Although, we would love to hear from the community whether this would be useful and also some use cases that might benefit from a framework like this.\r\n\r\n@dzhulgakov @jspisak @aartibasant @apaszke @SsnL "},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nI propose a `stabilize_grad` flag for the `Tensor.unfold` method.\r\n\r\n## Motivation\r\n\r\nImagine you want to calculate a loss between different patches of two images. Consider the following snippet:\r\n```python\r\nheight = 8\r\nwidth = 8\r\nsize = 3\r\nstep = 2\r\n\r\ninput_unstabilized = torch.ones(1, 1, height, width).requires_grad_(True)\r\ntarget = torch.zeros(1, 1, height, width).detach()\r\n\r\ninput_unstabilized_patches = input_unstabilized\r\ntarget_patches = target\r\nfor dim in range(2, 4):\r\n    input_unstabilized_patches = input_unstabilized_patches.unfold(dim, size, step)\r\n\r\n    target_patches = target_patches.unfold(dim, size, step)\r\n\r\nloss = torch.sum((input_unstabilized_patches - target_patches) ** 2.0)\r\nloss.backward()\r\nprint(input_unstabilized.grad[0, 0, :, :])\r\n```\r\nThis results in:\r\n```\r\ntensor([[2., 2., 4., 2., 4., 2., 2., 0.],\r\n        [2., 2., 4., 2., 4., 2., 2., 0.],\r\n        [4., 4., 8., 4., 8., 4., 4., 0.],\r\n        [2., 2., 4., 2., 4., 2., 2., 0.],\r\n        [4., 4., 8., 4., 8., 4., 4., 0.],\r\n        [2., 2., 4., 2., 4., 2., 2., 0.],\r\n        [2., 2., 4., 2., 4., 2., 2., 0.],\r\n        [0., 0., 0., 0., 0., 0., 0., 0.]])\r\n```\r\nAs you can see, some elements receive a higher gradient update, simply because they appear in multiple patches. This effect might lead to unwanted results.\r\n\r\n## Pitch\r\n\r\nWith `stabilize_grad=True` the `unfold` method should track how often each element appears in a patch and in the backward pass average the gradient with this number.\r\n\r\n## Alternatives\r\n\r\nRight now I achieve this functionality as follows:\r\n```python\r\nclass _UnfoldGradStabilizer(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, input, dim, size, step):\r\n        ctx.needs_stabilizing = step < size\r\n        if ctx.needs_stabilizing:\r\n            stabilizer = torch.zeros_like(input)\r\n            item = [slice(None) for _ in range(input.dim())]\r\n            for idx in range(0, stabilizer.size()[dim] - size, step):\r\n                item[dim] = slice(idx, idx + size)\r\n                stabilizer[item].add_(1.0)\r\n\r\n            # clamping to avoid zero division\r\n            ctx.save_for_backward(torch.clamp(stabilizer, min=1.0))\r\n        return input\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        if ctx.needs_stabilizing:\r\n            stabilizer, = ctx.saved_tensors\r\n            grad_input = grad_output / stabilizer\r\n        else:\r\n            grad_input = grad_output.clone()\r\n        return grad_input, None, None, None\r\n\r\n\r\nunfold_grad_stabilizer = _UnfoldGradStabilizer.apply\r\n\r\n\r\ndef stable_unfold(input, dim, size, step):\r\n    return unfold_grad_stabilizer(input, dim, size, step).unfold(dim, size, step)\r\n\r\n\r\ninput_stabilized = torch.ones(1, 1, height, width).requires_grad_(True)\r\n\r\ninput_stabilized_patches = input_stabilized\r\nfor dim in range(2, 4):\r\n    input_stabilized_patches = stable_unfold(input_stabilized_patches, dim, size, step)\r\n\r\nloss = torch.sum((input_stabilized_patches - target_patches) ** 2.0)\r\nloss.backward()\r\nprint(input_stabilized.grad[0, 0, :, :])\r\n```\r\nThis results in\r\n```\r\ntensor([[2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [0., 0., 0., 0., 0., 0., 0., 0.]])\r\n```\r\n\r\nI have not yet timed itin a larger context, but I suspect the creation of the `stabilizer` with a `for` loop in every forward pass might be slow. If you can think of a better way, please let me know.\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nAdd `nn.Module.activation` field to hold the last activation (or tuple, etc) returned from forward().\r\n\r\n```py\r\nwith torch.capture_activations():\r\n    x = net(imgs)\r\n\r\n    # last activation from net.conv layer\r\n    loss = cross_entropy(x,y) + L2(net.conv.activation) \r\n\r\n    for (name, activation) in net.named_activations():\r\n        writer.add_histogram('activation_'+name, activation)\r\n\r\n# net.conv.activation is released\r\n\r\n# capture only desired activations to save memory\r\nwith torch.capture_activations(pattern='net/batch_norm_*'):  \r\n    ....\r\n```\r\n\r\n## Motivation\r\n\r\n- Visualizing activation histogram in tensorboard is not easy.\r\n- Capturing intermediate activations from a network for regularization loss is not easy.\r\n\r\n## Pitch\r\n\r\nWeight tensors can be enumerated and put into tensorboard like the following:\r\n\r\n```py\r\nfor (name, param) in model.named_parameters():\r\n    writer.add_histogram('activation_' + name, param, global_step=global_step)\r\n```\r\n\r\nHowever it is not easy to enumerate activations. One needs to insert `writer.add_histogram` in every `forward()` function. For example, capturing intermediate activations of `nn.Sequential` is impossible, and capturing from nn.Module is cumborsome.\r\n\r\n```py\r\ndef forward(self, x):\r\n    self.memory = []\r\n    x = self.layer1(x)\r\n    self.memory.append(x)\r\n    x = self.layer2(x)\r\n    self.memory.append(x)\r\n    ...\r\n    return x\r\n```\r\n\r\n## Alternatives\r\n\r\nAdd a forward hook with pattern filter. It does not hold the tensor and saves memory for some cases.\r\n"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n## Motivation\r\n\r\nSay for example you have a batch of data but want to take a different action (e.g. run a different model) based on the label index, or based on the length of the input, etc. You want to subdivide the batch according to that value, and in some cases you need to reconstruct the outputs back into the shape of the original tensor. What is needed is a function that groups the rows of the tensor based on the value of the groupby field.\r\n\r\nI've ended up needing to implement this multiple times across projects. \r\n\r\n## Pitch\r\n\r\n```\r\ngroups, values, inverse_map = T.groupby(F[, dim=0])\r\n```\r\n\r\nBasically T is an `Nx...` tensor and `F` is a size `N` tensor (assuming dim=0). This function returns a list `groups` of length equal to the number of unique values in F. Each group contains all the rows of `T` for that unique element. `values` is just a tensor containing the unique values corresponding to each group, and `inverse_map` is a list of tensors telling you where all the rows came from.\r\n\r\n## Alternatives\r\n\r\nThis can be implemented in about 10 lines in pytorch with something like:\r\n```\r\nsorted_vals, order = F.sort(0)\r\ndelta = F[1:] - F[:-1]\r\ncutpoints = delta.nonzero()[0].tolist()\r\nres, inverse_map = [], [], torch.zeros(len(cutpoints))\r\nfor start, end, i in zip([0] + cutpoints, cutpoints + [len(F)], range(len(cutpoints) + 1)):\r\n    res.append(T[order[start:end]])\r\n    inverse_map.append(order[start:end])\r\n    values[i] = F[start]\r\n\r\nreturn res, values, inverse_map\r\n```\r\n\r\nThis could be done faster and in a more standard way as part of Pytorch.\r\n\r\n## Additional context\r\n\r\ncc @colesbury @lerks "},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nI need to install `pytorch` through `pypi` (https://pypi.org/project/pytorch/) but it says:\r\n\r\n> You tried to install â€œpytorchâ€. The package named for PyTorch is â€œtorch\"\r\n\r\nWhile trying to install `torch` and `torchvision` through `pypi`, I found that there is no source distribution available for both, there are only wheels. In order to include any library, we require only source distribution available on `pypi` and due to unavailability of the same, it is not part of officially supported libraries within our company due to which we are not able to use `pytorch`.\r\n\r\n\r\n## Motivation\r\nNot able to build any `pytorch` model. Reason: Unavailability of source distribution of `torch` and `torchvision` on `pypi` and unsuccessful installation of `pytorch`\r\n\r\nQuestion: What is the purpose of having `pytorch` source distribution if it can't be installed via `pip`? (shows above error)\r\n\r\n## Pitch\r\n1. Availability of source distributions of `torch` and `torchvision` on `pypi`\r\n2. Alternatively successful installation of `pytorch` (since it's source distribution is available) on `pypi`.\r\n\r\n## Alternatives\r\nNone\r\n\r\n## Additional context\r\nNone\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nA class based dataset sampler for class incremental and continual learning research.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nFor research in class incremental learning domain, we need datasets to be split up on the basis of classes so that after training on one subset of classes, others can be introduced and the model can be made to generalize on that too in an architectural fashion or using generalization techniques like Elastic Weight Consolidation. Pytorch is strong enough with dynamic graphs but currently it lacks a sampler which can return examples from specific classes.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nI would like to add a sampler similar to others but which takes in the class labels as argument and returns examples from those classes. A random flag can be added to the arguments to randomize the order of samples in the subset. The implementation would be similar to that of SubsetRandomSampler in torch.utils.data but instead of indices, it would take in the class labels as argument.\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\nPlease feel free to suggest any alternative implementations. :)"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Tensor and nn.Module Pruning\r\nTensor method and/or `nn` util to sparsify tensors and/or model, according to various pruning techniques in the literature. \r\n\r\n## Motivation\r\nState-of-the-art deep learning techniques rely on over-parametrized models that are hard to deploy. On the contrary, biological neural networks are known to use efficient sparse connectivity. It's important to identify best techniques to compress models by reducing the number of parameters in them, in order to reduce memory, battery, and hardware consumption without sacrificing accuracy, deploy lightweight models on device, and guarantee privacy with private on-device computation. On the research front, pruning is used to investigate the differences in learning dynamics of over-parametrized and under-parametrized networks, to study the role of lucky sparse subnetworks and initializations (\"lottery tickets\" [[1]](https://arxiv.org/abs/1803.03635)), as a destructive neural architecture search technique, and others.\r\nGoal of this feature: harmonizing pruning practices by providing a standard interface in PyTorch.\r\nTarget audience: researchers, engineering and product teams.\r\n\r\n## Pitch\r\nMinimalist API, with deeper flexibility for power-users. \r\n\r\nAt the tensor level, this could look as follows:\r\n```python\r\nt = torch.randn(3, 2, 4)\r\n# e.g.: randomly mask 6 entries\r\npruned_t = t.prune(method='random', amount=6)\r\n# e.g.: prune bottom 80% of entries by absolute value\r\npruned_t = t.prune(method='L1', amount=0.8)\r\n# e.g.: prune 50% of channels along the last dimension by L1 norm\r\npruned_t = t.prune(method='L1Structured', amount=0.5)\r\n# e.g.: prune 2 channels along the 0th dimension by L2 norm\r\npruned_t = t.prune(method='L2Structured', amount=2, axis=0)\r\n# e.g.: prune 1 channel along the last dimension by L0 norm\r\npruned_t = t.prune(method='LnStructured', n=0, amount=1)\r\n```\r\nA not-in-place `.prune` method will return a `torch.Tensor` of the same type and size as the one it acts on.\r\nIn-place pruning supported via `t.prune_(...)`.\r\n\r\nAt the model level, this will require a bit of thinking but should follow similar API patterns. This is important because not all pruning methods make sense on all parameters in a model (pruning conv kernels != pruning biases != pruning RNNs != pruning in the presence of batch norm, etc.). \r\nFirst, we should have a sensible, well-documented default behavior for the average-user's API, where a call to `net.prune(method='L1', amount=0.8)` defaults to pruning PyTorch \"prepackaged\" modules (such as linear, conv, and recurrent layers) in some sensible, expected way. \r\nMost power users though would probably want to prune custom layers, or prune different layer types or layers at different depths using different pruning methods or pruning method parameters. This could be specified via a dictionary, which maps parameter names (contained in `net.state_dict().keys()`) to a pruning method and its parameters: \r\n```python\r\n{\r\n    'features.0.weight' : L1PruningMethod(amount=0.8),\r\n    ...\r\n}\r\n```\r\n\r\nSimilar to the tensor operations, model-level pruning could return a copy of the model, or act on the model in place.\r\n\r\nPruning methods can be used during or post- training; this implementation will be training-loop-agnostic: the user will have to take care of writing their own training loop to decide when to prune and what to do with the pruned object (re-initialize and retrain, finetune, etc.).\r\n\r\n## Alternatives\r\nDepending on where this will live within the codebase, `t.prune(method=method, **kwargs)` could also look like: `torch.nn.utils.pruning_function(t, **kwargs)` or `torch.nn.utils.pruning_function(module=module, name=param_name, **kwargs)`. I personally would prefer the first option because the kwargs are parameters of the pruning method itself, while `t` is the tensor it acts on (some pruning methods will also have to take in some data `X` or `(X, y)` when they're applied), but the last option is more in line with how, say, `weight_norm` is implemented. Perhaps, for Module-level application, following the [example of the `weight_norm` implementation](https://pytorch.org/docs/stable/_modules/torch/nn/utils/weight_norm.html) using hooks will make this more PyTorch-y, but I don't know if we want to sacrifice the ability to act directly on a tensor that is not part of a Module. Would that go into `torch.nn.functional`? Open to suggestions here.\r\n"},{"labels":["enhancement",null,null,null],"text":"Currently trying to use distributed on MacOS crashes because torch.distributed namespace is empty\r\nI vaguely recall it working a year ago.\r\n\r\nThis is useful for quick sanity checks on my MacBook before deploying to cluster.\r\n\r\n```\r\nPython 3.7.3 (default, Mar 27 2019, 16:54:48) \r\n[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n>>> print(torch.version.__version__)\r\n1.1.0\r\n>>> import torch.distributed as dist\r\n>>> dist.init_process_group\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'torch.distributed' has no attribute 'init_process_group'\r\n```"},{"labels":["enhancement",null,null],"text":"Quite a few times (e.g. https://github.com/pytorch/pytorch/issues/20301) some examples from docs became obsolete and ran unnoticed.\r\n\r\nProposal: ability to mark some examples in docs as tests and run them on CI and check execution for errors, warnings etc."},{"labels":["enhancement",null,null],"text":"I got an error ```Can only index with tensors that are scalars (zero-dim) (operator [] at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\ATen/TensorOperators.h:62)(no backtrace available)```\r\nwhile the python one ran normal:\r\n```python\r\ntorch.Size([2, 2, 1])\r\ntensor([[[[[1],\r\n           [1]]],\r\n\r\n         [[[1],\r\n           [1]]]],\r\n\r\n        [[[[1],\r\n           [1]]],\r\n\r\n         [[[1],\r\n           [1]]]]])\r\n```\r\ncode is shown below\r\n```c++\r\n#include <torch/torch.h>\r\n#include <iostream>\r\nusing namespace std;\r\nint main()\r\n{\r\n\ttry\r\n\t{\r\n\t\ttorch::Tensor data = torch::eye(3);\r\n\t\ttorch::Tensor index = torch::eye(3);\r\n\t\tdata[index];\r\n\t}\r\n\tcatch (const std::exception&e)\r\n\t{\r\n\t\tcout << e.what();\r\n\t}\r\n\tgetchar();\r\n\treturn 0;\r\n}\r\n```\r\n\r\n```python \r\nimport torch\r\nimport torch.nn as nn\r\ndata=torch.tensor([[[1],[1]],[[1],[1]]]);\r\nindex=torch.tensor([[[1],[1]],[[1],[1]]]);\r\nprint(data.shape)\r\nprint(data[index])\r\n```\r\n\r\nis there anything i didn't notice about the C++ frontend?or the interface is a little different from the python one."},{"labels":["enhancement",null,null,null],"text":"## ðŸ› Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nSparse tensors can't be used in DataLoader running many workers\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n```python\r\nimport torch.utils.data as D\r\nfrom scipy.sparse import csr_matrix\r\nimport numpy as np\r\nimport torch\r\n\r\nrow  = np.array([0, 3, 0, 7])\r\ncol  = np.array([0, 3, 1, 9])\r\ndata = np.array([4, 5, 7, 9])\r\n# using CSR matrix as can be row indexed\r\nsparse = csr_matrix((data, (row, col)), shape=(8, 10))\r\n\r\nclass Dataset(D.Dataset):\r\n    \r\n    def __init__(self, sparse):\r\n        self.data = sparse\r\n\r\n    def __len__(self):\r\n        return self.data.shape[0]\r\n    \r\n    def __getitem__(self, index):\r\n        # convert it to COO so to get the atributes to create a sparse tensor\r\n        data = self.data[index].tocoo()\r\n        i = torch.LongTensor(np.vstack((data.row, data.col)))\r\n        v = torch.FloatTensor(data.data)\r\n        data = torch.sparse.FloatTensor(i, v, torch.Size(data.shape))\r\n        return data\r\n\r\nd = Dataset(sparse)\r\n\r\nloader = torch.utils.data.DataLoader(d, \r\n                                     batch_size=2,\r\n                                     num_workers=2)\r\n\r\nfor i in loader:\r\n    print(i)\r\n```\r\n\r\n```\r\nRuntimeError: Traceback (most recent call last):\r\n  File \"/Users/efelix/miniconda3/envs/release3.7/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\r\n    samples = collate_fn([dataset[i] for i in batch_indices])\r\n  File \"/Users/efelix/miniconda3/envs/release3.7/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 41, in default_collate\r\n    storage = batch[0].storage()._new_shared(numel)\r\nRuntimeError: sparse tensors do not have storage\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe same one that I have when not setting n_workers parameter.\r\n\r\n```python\r\nloader = torch.utils.data.DataLoader(d, \r\n                                     batch_size=2)\r\nfor i in loader:\r\n    print(i)\r\n```\r\n\r\n```\r\ntensor(indices=tensor([[0, 0],\r\n                       [0, 0],\r\n                       [0, 1]]),\r\n       values=tensor([4., 7.]),\r\n       size=(2, 1, 10), nnz=2, layout=torch.sparse_coo)\r\ntensor(indices=tensor([[1],\r\n                       [0],\r\n                       [3]]),\r\n       values=tensor([5.]),\r\n       size=(2, 1, 10), nnz=1, layout=torch.sparse_coo)\r\ntensor(indices=tensor([], size=(3, 0)),\r\n       values=tensor([], size=(0,)),\r\n       size=(2, 1, 10), nnz=0, layout=torch.sparse_coo)\r\ntensor(indices=tensor([[1],\r\n                       [0],\r\n                       [9]]),\r\n       values=tensor([9.]),\r\n       size=(2, 1, 10), nnz=1, layout=torch.sparse_coo)\r\n```\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.4\r\nGCC version: Could not collect\r\nCMake version: version 3.14.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl-service               1.1.2            py36hfbe908c_5  \r\n[conda] mkl_fft                   1.0.10           py36h5e564d8_0  \r\n[conda] mkl_random                1.0.2            py36h27c97d8_0  \r\n[conda] pytorch                   1.1.0                   py3.6_0    pytorch\r\n[conda] torchvision               0.2.2.post3              pypi_0    pypi\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"},{"labels":["enhancement",null,null],"text":"Useful in finetuning, gan training, etc. :)"},{"labels":["enhancement",null,null,null,null,null],"text":"## ðŸš€ Feature\r\nAdd cumulative maximum for tensors, where each element along an axis is the max of it and everything before it.\r\n\r\n## Motivation\r\nThis is useful for evaluating mAP by creating precision-recall curves, where precision should be monotonically decreasing with respect to recall. More info: https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173\r\n\r\n## Pitch\r\nTorch has .cumsum and .cumprod so I suggest .cummax to be added.\r\n\r\n## Alternatives\r\nNumpy supports this with `np.maximum.accumulate()`, so we have to convert to numpy then back.\r\n"},{"labels":["enhancement",null,null],"text":"A common pattern people use in `__init__` of a `nn.Module` is to build a `list` first and then feed it into `nn.Sequential` because `Sequential` doesn't support many handy methods existing on `list`. E.g., \r\n\r\n```py\r\ndef __init__(self, logres):\r\n    super().__init__():\r\n    layers = [\r\n          nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\r\n          nn.ReLU(inplace=True),\r\n          nn.MaxPool2d(kernel_size=3, stride=2),\r\n    ]\r\n\r\n    for _ in range(logres):\r\n         layers.extend([\r\n              nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\r\n              nn.ReLU(inplace=True),\r\n              nn.MaxPool2d(kernel_size=3, stride=2),\r\n        ])\r\n    self.layers = nn.Sequential(*layers)\r\n```\r\n\r\nThis is totally unnecessary if we can just provide `.append` and `.extend` on `Sequential`."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nTensorflow has a noise_shape keyword for tf.nn.dropout, which specifies which dimensions should have dropout masks calculated independently and which dimensions should have shared dropout masks. PyTorch should have a similar feature too.\r\n\r\n## Motivation\r\n\r\nThis is a very useful feature to have (for example, if we process the same example multiple times, we may want to tie dropout masks for each time we see that example), and while easy to implement independently, would be good to have common functionality for.\r\n\r\n## Pitch\r\n\r\nInclude extra kwarg to torch.nn.Dropout called ``noise_shape``, with the same functionality as tf.nn.dropout.\r\n\r\n## Alternatives\r\n\r\nFairly easy to implement independently, but would save hassle for many people if there was a core implementation.\r\n"},{"labels":["enhancement",null,null,null],"text":"## Issue\r\nCurrently `accscalar_t` for `float` is `double` on the CPU:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/AccumulateType.h#L34\r\n\r\nI suggest to have a small discussion whether it'd be better to switch to float.\r\n\r\n## Motivation\r\n\r\nThis is prompted by @wanchaol asking about a [`UndefinedBehaviorSanitizer: float-cast-overflow`](https://gist.github.com/wanchaol/aceb0a1e8d3a93c8853689730b0f709f) error, but I think there are three possible reasons to consider changing this:\r\n- Consistency with GPU,\r\n- support platforms on which float is faster (e.g. arm32)\r\n- get rid of UB.\r\n\r\nI seem to recall @apaszke preferring the current behaviour a year ago or so back (in the context of #6855, which always dispatched a double auxilliary function on CPU or so).\r\n\r\n## Pitch\r\n\r\nSwitch to `float` generally.\r\n\r\n## Alternatives\r\n\r\nSwitch to `float` only for specific platforms (e.g. arm32),  somehow get rid of the UB warning.\r\n\r\n## Additional context\r\n\r\nWe (e.g. @ljk53 and me) might be interested in changing this for Android specifically if we don't generally.\r\n"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nSequential sampler based on **indices**, not on data\r\n\r\n## Motivation\r\nIt might be really helpful to have samplers that sample data based on **indexes without randomness** (In short, no-randomness version of `SubsetRandomSampler`), so that it make it possible to generate `Dataloader` for train, valid and test dataset like below:\r\n```\r\ntrain_loader = DataLoader(train_dataset, batch_size, sampler=SubsetRandomSampler(train_indexes_list))\r\nvalid_loader = DataLoader(test_dataset, batch_size, sampler=SequentialIndicesSampler(valid_indexes_list))\r\ntest_loader = DataLoader(test_dataset, batch_size, sampler=SequentialIndicesSampler(test_indexes_list))\r\n```\r\n(`len(train_indexes_list) : len(valid_indexes_list) : (len(test_indexes_list) could be 6:2:2`)\r\n\r\n\r\nCode suggestion:\r\n\r\n```\r\nclass SequentialIndicesSampler(Sampler):\r\n    def __init__(self, indices):\r\n        self.indices = indices\r\n\r\n    def __iter__(self):\r\n        return iter(self.indices)\r\n\r\n    def __len__(self):\r\n        return len(self.indices)\r\n```"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n&emsp; DataLoader for large corpus file, supporting multi-process and multi-thread and memory optimization \r\n\r\n## Motivation\r\n&emsp; Thanks a lot for your pytorch framework, I've benefited a lot in work by using it. But I also got some little confusions with regard to data loading and processing. \r\n&emsp; In NLP cases, suppose I have a very large corpus file with labeled data \"corpus.csv\", which is too large to load it into memery once time, or even it is infinite large. I want to train my model on this dataset from top to bottom by mini batches. That means in each batch, I yield certain text lines from file begining to file ending to train my model (cannot support shuffle). I can do this with python generator easily. However, I don't know how to write subclass of  `torch.utils.data.Dataset` and use `torch.utils.data.DataLoader` to customize my own dataset on \"corpus.csv\". Since \"corpus.csv\" cannot be loaded into memory totally, I can not write `__getitem__` and `__len__` attribute. \r\n&emsp; I would be grateful if you could develop a module that satisfy the cases above ? Since merely by python generator, it is too hard for me to write `DataLoader` supporting multi-process and multi-thread loading and memory optimization.\r\n\r\n## Alternatives\r\n&emsp; By multi processing, one can be training the model on this batch and loading the next batch at the same time.\r\n\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nPyTorch supports MaxPooling, AveragePooling layers and also supports MaxUnpooling layer. However AverageUnpooling layer is currently not supported in PyTorch.\r\n\r\n## Motivation\r\nAs a researcher I find that having AverageUnpooling layer as part of PyTorch will be very useful for following reasons:\r\n1. Common Use-case: CNN autoencoder with average pooling. Other use-cases include image reconstruction.\r\n2. The average unpooling layer is also used (although not as extensively as MaxUnpooling) in research and I strongly feel that this layer should exist in PyTorch.\r\n3. This will also help address average unpooling queries, such as one posted here in [discuss.pytorch.org](https://discuss.pytorch.org/t/the-most-recommended-way-to-perform-average-unpooling-in-pytorch/10705)\r\n\r\n## Pitch\r\n\r\n**Implementing AverageUnpooling layer** similar to https://github.com/HyeonwooNoh/caffe/blob/master/src/caffe/layers/unpooling_layer.cpp\r\n\r\nEDIT: PyTorch's interpolation suits my requirements. Hence closed the issue."},{"labels":["enhancement",null,null,null],"text":"```c++\r\n  std::stringstream ss;\r\n  torch::save(at::ones({2, 2}), ss);\r\n```\r\nThe code above will hit the following error:\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  Tensor that was converted to Variable was not actually a Variable (Variable at /data/users/shenli/pytorch/torch/csrc/autograd/variable.h:143)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x45 (0x7f926dbfd085 in /data/users/shenli/pytorch/build/lib/libc10.so)\r\nframe #1: torch::serialize::OutputArchive::write(std::string const&, at::Tensor const&, bool) + 0xdc (0x7f92864c0eec in /data/users/shenli/pytorch/build/lib/libtorch.so.1)\r\nframe #2: void torch::save<at::Tensor, std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >&>(at::Tensor const&, std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >&) + 0x46 (0x407ac6 in ./build/bin/ServerTest)\r\nframe #3: main + 0xd3 (0x404c33 in ./build/bin/ServerTest)\r\nframe #4: __libc_start_main + 0xf5 (0x7f926d829445 in /lib64/libc.so.6)\r\nframe #5: ./build/bin/ServerTest() [0x405f97]\r\n```\r\n\r\n@yf225 will this be solved after tensor-variable merge?"},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\nA new weight initialisation when using dropout with ReLU activation.\r\n\r\n## Motivation\r\nIn a recent NeurIPS paper ([Pretorius et al. 2018](https://arxiv.org/pdf/1811.00293.pdf)), we derived a new weight initialisation strategy for deep ReLU networks that use dropout. This new initialisation ensures stable variance propagation for ReLU activations travelling through a dropout layer. The go-to initialisation for dense ReLU layers is the He initialisation, but in the above paper, we showed that if a dropout layer is applied after a dense ReLU layer the variances are longer preserved in the forward pass. This can lead to unstable signal propagation in deep ReLU networks that use dropout. We fix this with the new initialisation and thought it might be useful to the community if there was a PyTorch implementation.  \r\n\r\n## Pitch\r\n\r\nIn short, the initialisation samples from a normal distribution with a `gain = \\sqrt{2*(1-p)}`, where `p is the probability of an element to be zeroed`. To implement the initialisation in PyTorch, I was thinking perhaps something along these lines of code below could work? The first block of code could be added to the `calculate_gain` function. The second is then an implementation of the initialisation.\r\n\r\n```python\r\nelif nonlinearity == 'dropout_relu':\r\n        if param is None:\r\n            raise ValueError(\"dropout_rate set to {}. It is recommended to set the value equal to the dropout rate in the subsequent dropout layer\".format(param))\r\n        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\r\n            # True/False are instances of int, hence check above\r\n            if p < 0. or p > 1.:\r\n                raise ValueError(\"dropout probability has to be between 0 and 1, \"\r\n                         \"but got {}\".format(p))\r\n            else:\r\n                dropout_rate = param\r\n        else:\r\n            raise ValueError(\"dropout_rate {} not a valid number\".format(param))\r\n        return math.sqrt(2.0 * (1-dropout_rate))\r\n```\r\n\r\nBelow is the code implementing the dropout initialisation. It is important that it be applied to the ReLU layer that comes before the dropout layer.\r\n\r\n```python\r\ndef dropout_(tensor, p=0, mode='fan_in', nonlinearity='dropout_relu'):\r\n    r\"\"\"Fills the input `Tensor` with values according to the method\r\n    described in \"Critical initialisation for deep signal propagation in\r\nnoisy rectifier neural networks\" - Pretorius, A. et al. (2018). The resulting tensor will have values sampled from\r\n    :math:`\\mathcal{N}(0, \\text{std})` where\r\n\r\n    .. math::\r\n        \\text{std} = \\sqrt{\\frac{2 \\times (1-p)}{\\text{fan\\_in}}}\r\n\r\n    Args:\r\n        tensor: an n-dimensional `torch.Tensor`\r\n        p: probability of an element to be zeroed. This should be set equal to the dropout rate p in the subsequent dropout layer. \r\n        mode: either 'fan_in' (default) or 'fan_out'. Choosing `fan_in`\r\n            preserves the magnitude of the variance of the weights in the\r\n            forward pass. Choosing `fan_out` preserves the magnitudes in the\r\n            backwards pass.\r\n        nonlinearity: the non-linear function (`nn.functional` name),\r\n            recommended to use only with 'relu'.\r\n\r\n    Examples:\r\n        >>> w = torch.empty(3, 5)\r\n        >>> nn.init.dropout_(w, p=0.6, mode='fan_in', nonlinearity='relu')\r\n    \"\"\"\r\n    fan = _calculate_correct_fan(tensor, mode)\r\n    gain = calculate_gain(nonlinearity, p)\r\n    std = gain / math.sqrt(fan)\r\n    with torch.no_grad():\r\n        return tensor.normal_(0, std)\r\n```\r\n\r\n## Alternatives\r\n\r\nPlease feel free to suggest any alternative implementations. :) \r\n\r\n\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nCurrently it only synchronizes the current device. It would be great if it can  \r\n\r\n+ take in a device object and automatically synchronize that device. \r\n"},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\nThe context manager `torch.autograd.profiler.profile()` enables profiling by calling `torch.autograd._enable_profiler()` and `torch.autograd._disable_profiler()`.\r\n\r\nEnabling/disabling this profiling on the fly should be fully supported. Right now exporting chrome traces as json breaks when calling _enable and _disable directly while training happens in another thread. (The `pop()` call in https://github.com/pytorch/pytorch/blob/master/torch/autograd/profiler.py#L465 can then happen on an empty list.)\r\n\r\nWhile we're at it, building the Chrome tracing json file in Python has significant overhead and it seems reasonable to move that code into C++.\r\n\r\nIt also seems reasonable to export a gzipped json file, as Chrome can deal with those directly.\r\n\r\n(It might also be worth investigating whether running an http server alongside PyTorch which displays tracing information on demand is something we could support. Other popular ML frameworks do that.)\r\n\r\n## Motivation\r\n\r\nIn situations in which (1) there's many op executions and (2) the profiled code needs a certain burn-in time, the json file resulting from calling `export_chrome_trace()` can get many GB large. Building these large json files in Python takes a long time, and Chrome runs OOM when you try to load them.\r\n\r\nManually enabling/disabling profiling while other threads run PyTorch ops also breaks building the json traces as described above.\r\n\r\n## Pitch\r\n\r\n```python\r\ntorch.autograd.profiler.enable()\r\ntime.sleep(10)\r\nprof = torch.autograd.profiler.disable()\r\nprof.export_chrome_trace(\"tracefile.json.gz\")  # Not in Python; fast.  \r\n```\r\n\r\n## Alternatives\r\n\r\nNone what. so. ever.\r\n"},{"labels":["enhancement",null],"text":"I use .cpu().numpy() very frequently to debug. Its awesome!\r\nI think it would be even more awesome if it was shorter for example : tensor.np() or to_np() or something like that.\r\nObviously, numpy assumes cpu so there's really no point in writing both of those functions. and \"np\" is very well recognized as shorthand for numpy.\r\n\r\nThanks and please."},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nAdd stable distribution in torch.distributions\r\n## Motivation\r\nI would like to get a pdf of stable distribution on GPU. So far, I have to transfer to Numpy and use Scipy to do that and transfer back to tensor on GPU. This process becomes a bottleneck of my code.\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\nAdd stable distribution in torch.distributions\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\nNo\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\nN/A\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nThere is no Poisson NLL loss in libtorch. \r\n\r\n## Motivation\r\n\r\nNeeded when running models with Poisson NLL loss using libtorch. \r\n\r\n## Pitch\r\n\r\nI can work on it if someone advice me on which files to touch. \r\nWhere should the implementation go? In `aten\\native\\Loss.cpp` plus a binding in `native_functions.yaml`? Where should tests go?\r\n\r\n## Alternatives\r\n\r\n## Additional context\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nSupport .tobytes() method on torch.Tensor\r\n\r\n## Motivation\r\n\r\nIf I need to extract the raw bytes from a Tensor, I need to convert to numpy first and then use `tobytes`. It would be nice to have a tobytes() method for the Tensors themselves. For example, if I have a jpg image stored in a ByteTensor and I would like to open it using PIL, I need to do something like this currently:\r\n\r\n```\r\ndata = tensor.numpy().tobytes()\r\nimg = Image.open(BytesIO(data), mode='r')\r\n```\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nIn issue  #1909 which got resolved by #18001 implement Cycling learning. Lets enhance it or create a new Lr_scheduler to include one cycle policy\r\n\r\n## Motivation\r\nOne cycle policy has proposed as a new enhancement to Cyclic Learning by same author https://arxiv.org/abs/1803.09820.  With one cycle policy we will have only 2 step size and it will have triangular cycle. This can now currently be done passing parameter to cyclic learning. However as this become popular day by day it is worthwhile to create it as a standalone function. Please share your opinion. "},{"labels":["enhancement",null,null,null],"text":"## ðŸ› Bug\r\nI tried to use multi-gpu capability in `C++`, but when I write `torch::nn::parallel::data_parallel`, I am getting `No member named 'parallel' in namespace 'torch::nn'` error. \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nJust add `torch::nn::parallel::data_parallel` in code with torch.h included. \r\n\r\n## Expected behavior\r\nIt should at least know `parallel` namespace. \r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Red Hat Enterprise Linux Server 7.6 (Maipo)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCMake version: version 3.12.0\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 7.5.17\r\nGPU models and configuration:\r\nGPU 0: Tesla P100-PCIE-16GB\r\nGPU 1: Tesla P40\r\n\r\nNvidia driver version: 384.81\r\ncuDNN version: /usr/lib64/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] tinynumpy==1.2.1\r\n[pip] torch==1.0.1.post2\r\n[pip] torchvision==0.2.1\r\n[conda] Could not collect\r\n\r\n```\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\nIs `torch::nn::parallel::data_parallel` functional now? \n\ncc @yf225 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar"},{"labels":["enhancement",null,null,null],"text":"numpy and matlab both have a trapz (and a simps) function, which makes it convenient to integrate y values over some x values. It seems pytorch is lacking this convenient function (even though it has other functions like cumsum). It would be great if we could have an implementation of trapz or simps similar to [numpy's](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.trapz.html). \r\n\r\nOne simple motivation is that pytorch is desirable as a significantly faster alternative to numpy, and trapz is a standard and necessary feature in that library.\r\n\r\nOne main concrete example of its use is for [KDE](https://en.wikipedia.org/wiki/Kernel_density_estimation) when using a non-standard distribution. \r\n\r\ntrapz (and simps) should also have a very simple implementation as well. For the 1D case, it's as simple as:\r\n```python\r\ndef trapz(y, x=None, dx=1.0):\r\n  y_avg = (y[1:] + y[:-1])/2\r\n  if not (x is None):\r\n    dx = x[1:] - x[:-1]\r\n  return sum(y_avg*dx)\r\n```"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nneed this numpy feature for easy plotting with matplotlib: https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.ndim.html\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n## Motivation\r\nmatplotlibs .plot uses .ndim when passing a ndarray\r\n\r\nThe missing attribute on tensor can be circumvented by the following:\r\n#This monkey-patch is there to be able to plot tensors\r\ntorch.Tensor.ndim = property(lambda x: len(x.shape))\r\n\r\n"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\n\r\nUsers are running into issues where they are tracing a non-forward method on a module, and getting errors because the weights used in the trace are either being considered constants, or they are autograd recording tensors and the tracer refuses to handle them. The trace API already understands how to trace a forward method correctly capturing weights. It would be easy to extend the API so that in the general case you can trace multiple methods of a module to create a single  ScriptModule with  multiple methods.\r\n\r\n```py\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv = nn.Conv2d(3, 3, 3)\r\n\r\n    def forward(self, x):\r\n        return self.conv(x)\r\n\r\n    def weighted_kernel_sum(self, weight):  # I want to trace this thing\r\n        return (weight * self.conv.weight).sum()\r\n\r\nn = Net()\r\ntraced_forward = torch.jit.trace(n, example_forward_input)\r\n\r\ntraced_weight_kernel_sum = torch.jit.trace(n.weighted_kernel_sum, example_weight)\r\n# current: error constants are requiring gradients, or the weights are captured as constants\r\n\r\n# proposed generic API:\r\nfully_traced = torch.jit.trace(n, { 'forward' : example_forward_input, 'weighted_kernel_sum': example_weight})\r\n# fully_traced has both forward and weighted_kernel_sum present\r\n\r\n# syntax sugar for the old behavior:\r\nm = torch.jit.trace(n, example_input) # --> torch.jit.trace(n, {'foward': example_input})\r\n#syntax sugar for tracing a single method:\r\nm = torch.jit.trace(n.weighted_kernel_sum, example_weight # --> torch.jit.trace(n.weighted_kernel_sum.__self__, {'weighted_kernel_sum': n.weighted_kernel_sum.__name__}) \r\n\r\n```"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nFor custom C classes or for complex modules in JIT, there needs to be ability to supply customer serialization code.\r\n\r\n## Motivation\r\n\r\nWe want to support custom opaque representations with the ability to serialize them. This is pretty important for opaque optimized layouts in inference (e.g. when using fbgemm), currently we use a hack with pack/unpack: https://github.com/pytorch/pytorch/blob/master/torch/jit/quantized.py#L27\r\n\r\nThis functionality will also be useful for custom C class bindings which would need to be serialized as well.\r\n\r\n## Pitch\r\n\r\nProposal is to implement get_state/set_state similarly how they are handled in pickle: one can return any value (`IValue`) that will be serialized and later passed to `__set_state__` for deserialization.\r\n\r\n```python\r\nclass MyLayerPacked(torch.nn.Module):\r\n    # same as before\r\n    def __init__(self, original: MyLayer):\r\n        self.packed_weight = self.__pack__(original.weight) # ByteTensor with pointer\r\n    def forward(self, x):\r\n        return torch.my_mm_fast(self.packed_weight, x)\r\n    # it has to be script method to work in C++\r\n    @torch.jit.script_method\r\n    def __getstate__(self):\r\n        return {'weight': torch.unpack_from_my_mm(self.packed_weight)}\r\n    def __setstate__(self, state):\r\n        # attribute mutability is required here\r\n        self.packed_weight = torch.pack_for_my_mm(state['weight'])\r\n```\r\n\r\n## Alternatives\r\n\r\nInstead of following Pickle style we can define some other interface for it"},{"labels":["enhancement",null,null,null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nI am building an application where there's basically a producer and a consumer of `std::tuple<torch::Tensor, torch::Tensor>`s.\r\n\r\nThe producer has to run a neural network to produce the tensors, and the consumer has to train from the output of the producer, occasionally checkpoint to file, which the producer then occasionally reads and updates its own network with the new one. (I'm implementing Alpha Zero)\r\n\r\nThe only problem is, it seems like pytorch is not thread safe in this regard. Even when I assign two separate (but identical) networks to producer and consumer, I get an error when I run the two concurrently.\r\n\r\nTherefore, I'm trying to separate the two as processes. I'm trying to use a message queue so that the producer can send its output to the consumer. However, now I don't know how to serialize `torch::Tensor`s. I think it should be doable with `torch::serialize::InputArchive` and `torch::serialize::OutputArchive`, but the C++ API reference wasn't too kind for me to understand.\r\n\r\nSince I wasn't too sure if this is doable or not, I decided to submit as a feature request.\r\n"},{"labels":["enhancement",null,null,null,null,null],"text":"Efficient usage of parallelism is important for achieving high performance in CPU tasks. Currently there're three main use cases/sources of parallelism in PT:\r\n1. Operator implementations (intra-op parallelism) - using multiple parallel tasks to execute an operator/function;\r\n2. TorchScript JIT interpreter - explicit fork/wait calls used in JIT programs;\r\n3. Autograd engine - uses multiple CPU threads pinned to devices (one for CPU and one per each GPU device) to run backward pass.\r\n\r\nThat's in addition to other mechanisms, such as multiprocessing used by eager mode training and low-level mechanisms (e.g. vectorization, not discussed here). This issue focuses mainly on server CPU inference and to a lesser extent on autograd engine parallelism.\r\n\r\nAs of now, the implementations of intra- and inter-op parallelism are based on:\r\n\r\n - For intra-op (within an op) we typically use OpenMP - either explicitly (through pragmas) or by using MKL/MKL-DNN that use OpenMP\r\n - For inter-op parallelism we either explicitly use a global (per process) thread pool (in JIT interpreter) or a set of threads pinned to devices (in autograd engine).\r\n\r\nThe main goal of the proposed parallelism work is to unify the usage of parallelism in PT behind a simple interface and abstract away from specific parallelization libraries. This will allow us to switch between and experiment with different parallel implementations, including:\r\n1. OpenMP-based (for operator implementations only);\r\n2. TBB-based;\r\n3. Native thread pool based.\r\n\r\nMore specifically, the proposed work has the following steps:\r\nIntra-op parallelism:\r\n- Update torch.get_num_threads/set_num_threads to use Parallel interface\r\n   - Redirect management of number of threads (intra-op) to Parallel.h impl\r\n   - On the details of handling of OMP/MKL threads see https://github.com/pytorch/pytorch/issues/19001\r\n- Port exisiting intra-op use cases to Parallel.h\r\n   - TH\r\n   - THNN\r\n   - ATen/native\r\n- Split Parallel.h into interface and impl parts (_openmp.cpp)\r\n- Cmake scaffolding to select Parallel.h backend\r\n- Add native backend for Parallel.h\r\n   - Using a separate native thread pool\r\n- Add TBB submodule into PT\r\n- Add Cmake TBB option that enables TBB, disables OpenMP, links with MKL-TBB and MKLDNN-TBB\r\n   - Users reported perf regressions when linking with TBB (https://github.com/pytorch/pytorch/issues/7903). In case of TBB we might need to turn off OpenMP usage and link with TBB versions of MKL, MKLDNN\r\n - Add TBB backend for Parallel.h\r\n\t\t\t\r\nInter-op parallelism:\r\n - Move Future and task launching interface into Parallel.h\r\n - Scaffolding to set number of inter-op threads, switch between single vs separate inter-/intra-op thread pools usage\r\n - Use Parallel.h from JIT interpreter to launch tasks\r\n - Update Autograd Engine to use Parallel.h\r\n   - Needs some discussion about pinning of threads to specific GPUs\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nA simple method for transforming a PIL images directly into torch tensor.\r\n\r\n## Motivation\r\nIt's frustrating to use transforms for a simple conversion between a PIL image and torch tensors and in the same time it's very easy to get tensor from numpy through ```torch.from_numpy()``` \r\n\r\n## Pitch\r\n\r\nGiving it a PIL images of any type and it returns a torch tensor\r\n\r\n## Alternatives\r\nI have considered two:\r\n1. Transforms\r\n2. convert it first to numpy then to torch\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nWish that pytorch can adjust its computation according to GPU memory upper bound.\r\n\r\n## Motivation\r\n\r\nIt is frustrating when I have several machines that holds GPUs of different memory sizes. Usually, I would like to implement model in a time efficient way while this will cause high GPU memory usage. However, in this way, some machines with lower GPU memory capacity will be unable to run the models because of OOM exception. In such case, it is obvious that there exist some method which can slow down the computation while lower the GPU memory usage requirement. So I just hope that pytorch can have such a feature.\r\n\r\n## Possible Solution\r\n\r\nSome global coordinators can be implemented with data structures like queue to ensure that the GPU memory usage will not exceed some upper limit at any time, which will automatically make balance between time and memory.\r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"To support further adoption of PyTorch in production environments we'd want to make sure that PyTorch supports a number of tools and options available regarding performance monitoring and logging, including integration with third-party logging and profiling tools, as well as support for pluggable debugging routines.\r\n\r\nMore precisely, there're several important use cases that we'd want to cover:\r\n\r\n- Fleet-wide on-demand and continuous statistics gathering and monitoring.\r\nBased on experience we had while running Caffe2 on thousands of hosts at FB we know it is extremely important to have a robust and easily pluggable monitoring solution, this greatly helps with debugging as well as analysis of existing utilization patterns across thousands of concurrent training and inference jobs.\r\n- Integration with debugging tools\r\n- A developer tool to locally profile a specific code on-demand, including gathering operator runtimes and timelines.\r\n\r\nExisting profiler tool is well optimized for the latter use case, the tool is fast and adds minimum overhead. However the tool also has a number of limitations - it supports only a given set of statistics and it is not possible to enable it on an already running code, as the current profiler is not thread safe and adding thread-safety would incur an additional overhead. Different customers might also want to use their existing (and potentially proprietary) libraries to collect system stats and not all of these stats are suitable to be included into the general tool and some of them might slow down the profiling considerably  (e.g. operator input analysis).\r\n\r\nA typical and well-tested way to implement a solution for the use cases above is to add an  _observer_ interface to the operator calls. Observers provide an easy mechanism to 'plug-in' an external code (callbacks) to be executed for certain events. For example, in Caffe2 observers are called on nets and operators at the start and end of execution. In addition to that, observers can unify existing parts of PyTorch such as profiling and Autograd tracing.\r\nThis series of PRs implements the operator callbacks for PT:\r\n\r\n- https://github.com/pytorch/pytorch/pull/17844 - generalizes `RecordFunction` - a PyTorch guard object used for profiling - to support callbacks. RecordFunction becomes a generalized solution to insert callback code, that can be used not only for the profiling. For example, one important use case is that we can further extend the RecordFunction to support (autograd) tracer as a callback. This way we merge profiling and autograd tracing into one conceptually simple construct that's easy to understand, instead of relying on autogeneration of RecordFunction and tracing code. We've benchmarked the code to make sure that this change does not slow down existing profiling tool;\r\n- https://github.com/pytorch/pytorch/pull/18717 - optionally adds support for including operator's inputs into the information passed to RecordFunction. Having operator inputs is very helpful for debugging as well as collecting of detailed stats to be logged.\r\n- as the next step we plan to add a tool to inspect the types and shapes of the inputs used while running the user code that is based on changes in https://github.com/pytorch/pytorch/pull/17844 and https://github.com/pytorch/pytorch/pull/18717\r\n\r\n"},{"labels":["enhancement",null,null,null,null],"text":"Today, autograd allocates a fixed set of threads, one for CPU, and one per CUDA device, and processes work for all backwards invocations (even if they are happening in parallel, or reentrantly) on these threads. This maintains the \"no-reentrant apply invariant\" that a single function's apply never gets entered concurrently; this allows us to implement AccumulateGrad without locks.\r\n\r\nThe purpose of this issue is to propose a new design for autograd, based on a pool of threads, and a per-device set of locks which protect the work queue for backwards function on that device, and help us maintain the no-reentrant apply invariant.\r\n\r\n1. We maintain a pool of unallocated threads, waiting for work to do. There is a lock per device.\r\n2. When work for device X enters the autograd scheduler, it is placed on its respective work queue. If there is no worker for that device, we wake up a thread from the pool, which takes out the lock for the device and then begins processing work on the pool in a loop.\r\n3. When there is no more work to be done, the thread gives up the lock and returns to the pool.\r\n4. When a reentrant call to backwards occurs on a worker device (or really, any blocking operation), the worker thread gives up its lock and wakes up a thread from the unallocated pool to take it. It then does the blocking operation. After it finishes the blocking operation, it attempts to take back the lock (so that it can finish processing the backwards function).\r\n\r\nThis redesign would fix #6959, because reentrant calls wouldn't recursively process the worker queue; instead, such processing would happen in another thread. It also has good performance characteristics, as without reentrant backwards calls, you use exactly the same number of threads that you used previously. Unlike the generator solution proposed in https://github.com/pytorch/pytorch/pull/18568#issuecomment-479315379 it is backwards compatible with current autograd Function syntax.\r\n\r\nFrom a scheduling perspective, because worker threads are greedy (they will do as much as work as possible before exiting), this design will favor executing all available work, before returning the lock to threads which are waiting to reacquire the lock to finish up reentrant backwards calls. You could solve this by adding a preemption mechanism, so that a thread that wants to return can preempt the existing worker.\r\n\r\ncc @apaszke @shubhtuls @sublee @albanD "},{"labels":["enhancement",null,null,null],"text":""},{"labels":["enhancement",null],"text":"This should work\r\n\r\n```python\r\n@torch.jit.script\r\ndef fn():\r\n    a_dict = {'a': 1, 'b': 2, 'c': 3}\r\n    sum = 0\r\n    for key in a_dict:\r\n      sum += a_dict[key]\r\n    return sum\r\n```\r\n\r\n\r\n\r\nWe also should start out with a conservative approach to [iterator invalidation](https://en.cppreference.com/w/cpp/container/unordered_map) and disallow editing the map during iteration."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nAllow tracing of models which output tuples with some attributes equal to `None`\r\n\r\n## Motivation\r\nIn big complex models, the forward pass should be able to output different information, depending on whether the model is being trained or runs in inference (test) mode. In test mode (the one which usually gets traced) some training outputs can be possibly missing. For example, in RL actor-critic methods, during training I want to visualize/plot the output of the critic. However, at test time, I want to be able to perform inference as fast as possible, which requires running the forward pass only of the actor but not of the critic, and return only the actor output.\r\n\r\n## Pitch\r\n```python\r\nclass ActorCriticModule(torch.nn.Module):\r\n  def __init__(self):\r\n    self.actor = torch.nn.Sequential(...)\r\n    self.critic = torch.nn.Sequential(...)\r\n\r\n  def forward(self, x, test_mode: bool) -> tuple:\r\n    if test_mode:\r\n      critic_out  = None\r\n    else:\r\n      critic_out = self.critic(x)\r\n    actor_out = self.actor(x)\r\n    return actor_out, critic_out\r\n\r\nmodule = ActorCriticModule()\r\ntorch.jit.trace(module, (torch.zeros(), True))\r\n```\r\n\r\nAt the moment, `torch.jit.trace(module, (torch.zeros(), True))` will fail, because `critic_out` will be `None` and tracing will fail.\r\n\r\n## Alternatives\r\nReturn tuples of different size all the time. It's too scary to image the ugliness of the resulting code, especially with large models\r\n\r\n## Additional context\r\nApplicable to any deeplearning scenario.\n\ncc @suo"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nThis is a rather difficult project, but I wanted to put down the idea anyway, maybe someone is bold enough to take it up.\r\n\r\n`cuFFT` allows callbacks that would allow for some interesting speedups for the STFT. The callback for loading a window could handle:\r\n* optional conversion from int to float\r\n* padding frames that exceed the border\r\n* multiplication with the window function\r\n\r\nThe callback for writing the result could handle:\r\n* optional computation of magnitudes from real and imaginary part\r\n\r\n## Motivation\r\n\r\nThe current implementation of `torch.stft` does an explicit copy of all the signal if it must be padded, and it does an explicit expansion of all the signal if a window function must be applied. This can eat up lots of memory and time.\r\nSimilarly, many use cases will start with 16-bit signals that need to be converted to float before, and are only interested in magnitudes rather than the complex results.\r\n\r\n## Pitch\r\n\r\nThe interface of `torch.stft` would need to be adapted to allow a dtype conversion, and to specify whether to compute the complex result or only the magnitudes. When possible, the implementation would use cuFFT callbacks to perform the operations.\r\n\r\n## Alternatives\r\n\r\nTo reduce the impact of padding, if the signals are large enough to be worth it (or the user requests it), they could be cut into three parts: the beginning that needs to be padded, the main part that doesn't, and the end that needs to be padded. The results would be written into the correct locations in a preallocated buffer.\r\n\r\n## Additional context\r\n\r\nPointers to cuFFT callbacks:\r\n* https://devblogs.nvidia.com/cuda-pro-tip-use-cufft-callbacks-custom-data-processing/\r\n* https://stackoverflow.com/questions/39535141/why-does-cufft-performance-suffer-with-overlapping-inputs"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nThese can be mostly autogenerated. \r\ncf https://github.com/pytorch/pytorch/issues/16996."},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nAdd a PyTorch implementation of layer-wise adaptive rate scaling (LARS) from the paper \"[Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888)\" by You, Gitman, and Ginsburg. Namely, include the implementation found [here](https://github.com/noahgolmant/pytorch-lars) in the `torch.optim.optimizers` submodule.\r\n\r\n## Motivation\r\n\r\nLARS is one of the most popular optimization algorithms for large-batch training. This feature will expose the algorithm through the high-level optimizer interface. There are currently no other implementations of this algorithm available in widely used frameworks.\r\n\r\n## Pitch\r\n\r\nI have already written/tested a PyTorch optimizer for LARS in [this repo](https://github.com/noahgolmant/pytorch-lars). Changes would be minimal (I just have to make the imports relative at the top of the file).\r\n\r\n## Alternatives\r\n\r\nThe only existing code for this is in caffe, and it looks like it is not exposed at a higher level within the solver framework. I also do not know whether or not that code has been tested.\r\n\r\n## Additional context\r\n\r\nNotably, I was unable to reproduce their results (namely reducing the generalization gap) on CIFAR-10 with ResNet. I would welcome additional tests to see if this implementation is able to replicate their performance on larger datasets like ImageNet.\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature [FR] Unserialize autograd\r\n\r\nbackward() function is serialized, per device, so also on CPU. A multithreaded learning (e.g. with separate models, trainers) is not possible, no matter how many cores the box has.\r\nMy models are small, so GPUs are not that useful, but I would like to train a number of them concurrently, e.g. by manipulating a subset of data presented to each of them. \r\nAlso, evolutionary algorithms/NNs hybrids would benefit.\r\nMultithreading was supposed to be a major use case for C++/Libtorch, but is rather crippled with backward() serialized.\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ tl;dr\r\nAttached is a proposal for graph mode quantization in pytorch (model_quantizer) that provides end to end post training quantization support for both mobile and server backends. Model quantization supports fp32 and int8 precisions as a starting point and will expand to support other precision types based on customer needs. Details can be found in the attached pdf doc:\r\n\r\n[Model Quantization for Pytorch.pdf](https://github.com/pytorch/pytorch/files/2994860/Model.Quantization.for.Pytorch.pdf)\r\n\r\ncc @soumith, @gchanan, @raghuramank100 \r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Context Manager that disables training mode with in a nn.Module.\r\nWhen running inference on validation datasets, you need to put some modules in evaluation mode and its desirable to stop gradient calculation. Currently `with torch.no_grad():` can be used to disable gradient calculation temporarily while batchnorm and drop out layers have to be set to evaluation mode using `model.train(True)` followed by a `model.train(False)` once evaluation is complete. Currently this syntax is used, \r\n\r\n```python\r\nmodel.train(False)\r\nwith torch.no_grad():\r\n  outputs = model(inputs)\r\n  model.train(True)\r\n  return outputs\r\n```\r\n. This is a potential source of bugs, as people will sometimes instead use `model.eval()` and forget to set the model back to training mode at the end of inference. Suggest having the following syntax, \r\n\r\n```python\r\nwith torch.no_train(model):\r\n  with torch.no_grad():\r\n    outputs = model(inputs):\r\n    return outputs\r\n```\r\n. This syntax would stop this type of bug from occurring. \r\n"},{"labels":["enhancement",null,null,null,null,null],"text":"## ðŸš€ Feature\r\nUpdate weight initialisations to current best practices.\r\n\r\n## Motivation\r\nThe current weight initialisations for a lot of modules (e.g. `nn.Linear`) may be ad-hoc/carried over from Torch7, and hence may not reflect what is considered to be the best practice now. At least they are now documented (https://github.com/pytorch/pytorch/pull/9038), but it would be better to pick something sensible now and document it (as opposed to e.g. what happened with `nn.Conv2d`: https://twitter.com/jeremyphoward/status/1107869607677681664).\r\n\r\n## Pitch\r\nUpdate the weight initialisations for modules to the following (using `nn.init` where applicable):\r\n\r\n- `Linear`: ? weight, zero bias\r\n- `Bilinear`: ? weight, zero bias\r\n- `Embedding`: ? weight\r\n- `EmbeddingBag`: ? weight\r\n- `Conv{Transpose}{1,2,3}d`: ? weight, zero bias\r\n- `RNN{Cell}`: ? weight_ih, ? weight_hh, zero bias_ih, zero bias_hh\r\n- `LSTM{Cell}`: ? weight_ih, ? weight_hh, zero bias_ih, zero bias_hh (though perhaps +1 forget gate bias)\r\n- `GRU{Cell}`: ? weight_ih, ? weight_hh, zero bias_ih, zero bias_hh\r\n- `BatchNorm{1,2,3}d`: ones weight, zero bias\r\n- `GroupNorm`: ones weight, zero bias\r\n- `InstanceNorm{1,2,3}d`: ones weight, zero bias\r\n- `LayerNorm`: ones weight, zero bias\r\n\r\nThe current proposal is to have new weight initialisations as default, acknowledge that this is a breaking change, but have a (global?) flag to revert to the original initialisations in order to preserve backwards compatibility if necessary.\n\ncc @ezyang @gchanan @zou3519 @SsnL"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nIt would be useful to have support for mixture models in torch.distributions e.g. Gaussian Mixture models. \r\n\r\n## Motivation\r\n\r\nCurrently, there are no easy way of defining mixture models/distributions in pytorch. I propose an re implementation of tensorflows api for doing this\r\n\r\n```import tensorflow as tf\r\nimport tensorflow_probability as tfp\r\ntfd = tfp.distributions\r\n\r\nloc = tf.get_variable(name=\"loc\", shape=[mixture_components, latent_size])\r\nraw_scale_diag = tf.get_variable(name=\"scale\", shape=[mixture_components, latent_size])\r\nmixture_logits = tf.get_variable(name=\"mixture_logits\", shape=[mixture_components])\r\nmix = tfd.Categorical(logits=mixture_logits)\r\ncomp = tfd.MultivariateNormalDiag(loc=loc, scale_diag=tf.nn.softplus(raw_scale_diag))\r\ngmm =  tfd.MixtureSameFamily(components_distribution=comp,\r\n                             mixture_distribution=mix,\r\n                             name=\"prior\")\r\n# Usefull methods\r\ngmm.mean()\r\ngmm.sample(...)\r\ngmm.log_prob(...)\r\n```\r\n\r\nI personally use this for variational autoencoders, to be able to work with more expressive priors than the standard gaussian prior.\r\n\r\n## Pitch\r\n\r\nRe implementation of the [MixtureSameFamily](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily) or [Mixture](tfp.distributions.Mixture) class from tensorflow. Personally, I consider the MixtureSameFamily class to be the most important of the two, because mixtures of different distributions is rarely considered in practice. \r\n\r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"Dataloader call __getitem__ as many times as indices in the current batch. \r\nIn case datasets support a list of indices in one call, or a native python slice object, add a __getbatch__ (optional) to pytorch dataset class. \r\nIf dataloader sees the dataset has such a method implemented, make it fetch the batch corresping to the list of indices with one __getbatch__ call rather than many __getitem__ calls.\r\nThis saves the trouble of calling __getitem__ many times in cases where for example a disk file has to be opened then closed for every call and the underlying data structure supports a slicing or lists of indices for example hdf5 files.\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\n`nn.Module.register_buffer()` should get a keyword argument `persistent=True`. If set to `False`, the buffer will not be included in the output of `state_dict()`, and not loaded in `_load_state_dict()`.\r\n\r\n## Motivation\r\n\r\nI repeatedly come across cases where I want to precompute a tensor at construction time that is then used in every `forward()` call. Let's take a Discrete Cosine Transform module as an example. It can be computed by generating a DCT matrix and applying a matrix product. Assuming the input size is fixed at construction time, it would be wasteful to recompute the matrix in every forward call.\r\nI currently have three options: Making it an `nn.Parameter`, registering it as a buffer, or directly storing it as an attribute.\r\nThe first two cause it to be included in `state_dict`. It would be wasteful to store the matrix in every model, and it would lock me in to that implementation -- if I decide to implement the DCT differently, I will have to implement a state dict hook that discards the matrix when loading older models.\r\nThe third one does not include it in `state_dict`, but also does not convert it when calling `.cuda()`, `double()` etc.. This can be fixed by overriding `_apply()`, but I don't want to do this in every Module and it would cause the model to not work with `data_parallel()` (which explicitly copies only the parameters and buffers).\r\n\r\n## Pitch\r\n\r\nWith `register_buffer(name, tensor, persistent=False)`, I would want a buffer to be registered that is not stored and restored, but otherwise treated as any other buffer. The docstring of `register_buffer` already speaks of \"persistent buffers\", so it seems sensible to also allow \"non-persistent buffers\". From what I understand, this would only require changes in `state_dict()` and `_load_state_dict()`, as well as a way to track which buffers are non-persistent.\r\n\r\n## Alternatives\r\n\r\nMy use case is about what would be a constant in graph-based frameworks: Something that can be computed once and reused, something that is independent of the input to `forward()`. It would be possible to have a `nn.Constant` wrapper class similar to `nn.Parameter`, that is registered when assigning to a `Module` attribute, and included whenever a model is moved or replicated. But my impression is that there are several places in Pytorch that assume a model only has parameters and buffers, and would need updating to know about constants. Furthermore, it's not important that the value is constant, so that's a too narrow concept.\r\n\r\nThere once was a proposal for \"calculated parameters\" (https://github.com/pytorch/pytorch/issues/7313#issuecomment-388490440) that would also fit my use case, but would cause more overhead for me, the humble developer -- I would need to implement a Module that computes the DCT matrix. All I want is a way to store Tensors as Module attributes that are not included in the state dict, but moved across devices just like parameters or buffers."},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nSupport batch dot product\r\n\r\n## Motivation\r\nCommonly used operation\r\n\r\n## Alternatives\r\n\r\nCurrently, we can do this with bmm. But I think it makes sense to just support batch dot product since it is very commonly used"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\n\r\nMake serialization part of the torchscript and compilable. Use case looks like this:\r\n```\r\nclass ModelThatNeedsDebugging(jit.ScriptModule)\r\n  def __init__(self)\r\n      ///\r\n  def forward(self, x):\r\n         y =  self.conv(x)\r\n         y.save(\"./attention_input.pt\")\r\n         output = self.attention(y)\r\n         output.save(\"./attention_feature_map.pt\")\r\n         return output\r\n```\r\nThis way it allows me to conveniently debug the model inside a production workflow\r\n```\r\nauto module = torch::jit::load(\"./ModelThatNeedsDebugging.pt\");\r\nmodule->forward(input);\r\n```\r\n## Motivation\r\n\r\nA common way of debugging neural networks is to visualize the intermediate feature maps. This is super easy to do in Python. However, if you wanted to debug on the production side. There is no way of doing so.\r\n\r\nBy allowing serialization to be compilable. It allows us to serialize and debug intermediate tensors.\r\n"},{"labels":["enhancement"],"text":"## ðŸš€ Feature\r\nEnable PyTorch to compute convolutional features given:\r\n- A collection of batches of images of shape (b_i, b_j, c, h, w)\r\n- A batch of kernel weights of shape (b_i, out_features, in_features, kernel_height, kernel_width)\r\n\r\nEach batch in a collection is convolved *only* with its respective parameters. This can be currently achieved with the existing PyTorch operations but is rather slow compared to what a CUDA implementation could do.\r\n\r\n## Motivation\r\nMeta-learning is slowly becoming a very popular framework for tackling problems. In almost every instance of meta-learning, there exists an inner loop optimization process, where a network, is updated, multiple times, given a task. There exists a batch of tasks, so currently, one has to process each task sequentially. This is necessary because the weights of a network change within the inner loop optimization process, thus doing this in parallel would require a convolution operator that can receive a batch of parameter kernels to be convolved with a batch of tasks.\r\n\r\nFurthermore, genetic algorithms, and RL, often require similar operations, where each individual in a population changes it's inner state with each update, hence, requiring a similar mechanism.\r\n\r\nFinally, in cases where Hyper Networks are used, if one chose to have a sample conditional weight matrix for a particular operation, then a Batch Convolutional operator would be necessary.\r\n\r\nI think it's in PyTorch's best interests to have such a feature available, to get ahead of the curve.\r\n\r\n## Pitch\r\n\r\nI have already implemented a variant of what I am proposing. When I am using a large collection size, then my method out performs sequential convolutions by a factor of 4x. However, when my collection size is close to 1, hence, effectively being a normal convolutional layer, my method is 20x slower than the default CUDA-based convolutional implementation. I need help by someone who can either write such CUDA-based implementations, or point me to the right direction so I can do it.\r\n\r\nFind my existing implementation on this attached \r\n\r\n```\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass BatchConvLayer(nn.Module):\r\n    def __init__(self, input_shape, kernel_size, stride, padding, dilation, groups):\r\n        super(BatchConvLayer, self).__init__()\r\n        self.input_shape = input_shape\r\n        self.kernel_size = kernel_size\r\n        self.stride = stride\r\n        self.padding = padding\r\n        self.dilation = dilation\r\n        self.groups = groups\r\n        self.build_block()\r\n\r\n    def image_to_patch_coordinates(self, input_image, patch_size, stride, dilation):\r\n        patches = []\r\n\r\n        for i in range(0, input_image.shape[2] - (patch_size[0] - 1), stride):\r\n            for j in range(0, input_image.shape[3] - (patch_size[1] - 1), stride):\r\n                x_range = (patch_size[0] + ((patch_size[0] - 1) * (dilation - 1)))\r\n                y_range = (patch_size[1] + ((patch_size[1] - 1) * (dilation - 1)))\r\n                #print(x_range, y_range)\r\n                if i + x_range <= input_image.shape[2] and j + y_range <= input_image.shape[3]:\r\n                    coord = torch.Tensor([x * input_image.shape[2] + y for x in range(i, i + x_range, dilation) for y in\r\n                                      range(j, j + y_range, dilation)])\r\n\r\n\r\n                patches.append(coord)\r\n\r\n        indexes_of_patches = torch.stack(patches, dim=0).long()\r\n        return indexes_of_patches\r\n\r\n    def image_to_vectors(self, input_image, indexes_of_patches):\r\n        p, num_pixels = indexes_of_patches.shape\r\n        indexes_of_patches = indexes_of_patches.view(p * num_pixels)\r\n\r\n\r\n\r\n        out = input_image.view(input_image.shape[0], input_image.shape[1], -1)\r\n        #print(out.shape)\r\n        out = torch.index_select(out, dim=2,\r\n                                 index=indexes_of_patches.to(input_image.device))\r\n        out = out.view(out.shape[0], out.shape[1], p, num_pixels)\r\n\r\n        return out\r\n\r\n    def build_block(self):\r\n        x = torch.zeros(self.input_shape)\r\n\r\n        if type(self.kernel_size) is int:\r\n            self.kernel_size = (self.kernel_size, self.kernel_size)\r\n\r\n        out = x.view(-1, x.shape[2], x.shape[3], x.shape[4])\r\n\r\n\r\n        if self.padding > 0:\r\n            out = F.pad(out, pad=[self.padding, self.padding, self.padding, self.padding], mode='constant', value=0)\r\n\r\n        self.indexes_of_patches = self.image_to_patch_coordinates(input_image=out, patch_size=self.kernel_size, stride=self.stride, dilation=self.dilation).to(x.device)\r\n\r\n        x_vectors = self.image_to_vectors(out, indexes_of_patches=self.indexes_of_patches)\r\n        self.spatial_shape = None\r\n        print('block built', x_vectors.shape)\r\n\r\n    def forward(self, x, weights, biases=None):\r\n\r\n        # assert x.shape[0] == weights.shape[0], \\\r\n        #     \"The batch size of the input images needs to be equal to the batch size of the params\"\r\n        # if biases is not None:\r\n        #     assert weights.shape[0] == biases.shape[0], \\\r\n        #     \"The batch size of the weight parameters needs to be equal to the batch size of the bias parameters\"\r\n        weights = weights.permute([0, 2, 3, 4, 1]).view(weights.shape[0], -1, weights.shape[1])\r\n\r\n        b_out, b_in = x.shape[:2]\r\n        out = x.view(-1, x.shape[2], x.shape[3], x.shape[4])\r\n\r\n        if self.padding > 0:\r\n            out = F.pad(out, pad=[self.padding, self.padding, self.padding, self.padding], mode='constant', value=0)\r\n\r\n\r\n        out = self.image_to_vectors(out, indexes_of_patches=self.indexes_of_patches)\r\n\r\n        out = torch.cat(out.unbind(1), 2)\r\n        out = out.view(b_out, b_in * out.shape[1], out.shape[2])\r\n\r\n        out = torch.bmm(out, weights)\r\n\r\n\r\n        if biases is not None:\r\n            out = out + biases.unsequeeze(1)\r\n\r\n        out = out.view(b_out, b_in, -1, out.shape[-1])\r\n\r\n        if self.spatial_shape is None:\r\n            self.spatial_shape = int(np.floor(np.sqrt(out.shape[2])))\r\n\r\n        out = out.view(b_out, b_in, self.spatial_shape, self.spatial_shape, out.shape[-1])\r\n\r\n        out = out.permute([0, 1, 4, 2, 3])\r\n\r\n        return out\r\n\r\n```"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nI want to add batch preconditioned conjugate gradient (including its gradient) to the torch api.\r\n\r\n## Motivation\r\n\r\nThis feature exists in as scipy, as scipy.linalg.cg. I'd like a torch equivalent that can handle batches.\r\n\r\n## Pitch\r\n\r\nAdd a torch function cg(A, B) that returns X^(-1) B by running CG in parallel across the columns of B.\r\n\r\n## Alternatives\r\n\r\nN/A\r\n\r\n## Additional context\r\n\r\nI've implemented the cg algorithm as a pytorch c++ extension, as well as its gradients (with respect to A and B as sparse tensors) and it seems to work. It doesn't seem like it would be too hard to pull this into pytorch, but I'm not sure what the right steps are.\r\n\r\nSee www.github.com/sbarratt/torch_cg\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nChange `sample_shape` argument in `sample` (`torch/distributions/distributions.py`) to `*sizes` for consistency with `torch.rand`.\r\n\r\n## Motivation\r\n\r\nWithin `distributions`, the `sample` attribute takes `sample_shape` as a tuple. Whereas in `torch.rand` and `torch.randn` samples are generated by passing `*sizes` is passed as variable length arguments. This creates inconsistency and seems easy to change!\r\n\r\n## Pitch\r\n\r\nThe change should be as simple as...\r\n```python\r\n def sample(self, *sample_shape):\r\n    pass\r\n```\r\n\r\nBecause in most of the distributions I explored (Normal, Poisson, Exponential family) the `sample` attributes makes a call to `_extend_shape` which will convert tuples to `torch.Size`.\r\n\r\n```python\r\ndef _extended_shape(self, sample_shape=torch.Size()):\r\n\r\n        if not isinstance(sample_shape, torch.Size): \r\n            sample_shape = torch.Size(sample_shape) # <-- Right here!\r\n        return sample_shape + self._batch_shape + self._event_shape\r\n```\r\n\r\nSo really only one line of code needs to be changed?\r\n\r\n## Alternatives\r\n\r\nKeep it as is, and very little will change. This would be a small quality of life improvement, and I recommend it only because it seems easy to implement.\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nSupport pytorch from PyPy -- a fast, compliant alternative implementation of the Python language (http://pypy.org)\r\n\r\n## Motivation\r\n\r\nWhile pytorch itself probably won't benefit much from PyPy JIT, but often pytorch is used as a part of larger application, where using PyPy can have speed benefits, e.g. for evaluation, data generation, or other activities.\r\n\r\n## Pitch\r\n\r\n(A clear and concise description of what you want to happen). I can imagine several stages:\r\n\r\n- PyPy support works by compiling from source\r\n- PyPy is integrated into CI\r\n- PyPy wheels are built and released\r\n\r\n## Additional context\r\n\r\nRight now it seems that PyPy support, at least for commonly used stuff, is not far from reality, for example this branch https://github.com/lopuhin/pytorch/tree/pypy3.6 works in PyPy 3.6 7.0.0 for a moderately complex application (didn't try running tests yet).\r\n\r\nHere is a summary of fixes in that branch:\r\n\r\n1. working around a compilation issue in the JIT: https://github.com/pytorch/pytorch/commit/0481da14052c09605b38940e08d8cae275c6a8cd - this is definitely a hack to get the compilation working, a proper implementation for PyPy needs to be in place.\r\n2. **[merged]** `PySlice_Unpack` is not yet available in PyPy 3.6 -https://github.com/pytorch/pytorch/commit/752e204c90a782432eed77018e4fe3ad5fa4dbf2 - probably it's ok to merge this. With two above fixes, pytorch compiles on PyPy 3.6. PR https://github.com/pytorch/pytorch/pull/17836\r\n3. **[merged in https://github.com/pybind/pybind11/pull/2146 ]** Applying https://github.com/pybind/pybind11/pull/1494 to pybind11 - https://github.com/pytorch/pytorch/commit/912b0b2a77b3ea0a5bac9fbe3ac5d577c86c6d7. Unfortunately, while the fix seems correct, and pybind11 has partial support and CI for PyPy, more work needs to be done to properly integrate it into pybind11, because other pybind11 tests are failing on a more recent PyPy, see https://github.com/pybind/pybind11/pull/1494 and https://github.com/pybind/pybind11/pull/1720\r\n4. **[merged]** work around PyPy cpyext issue - https://github.com/pytorch/pytorch/commit/00941d8ae4e28bd03245a04c5096297c35a3128d - see https://bitbucket.org/pypy/pypy/issues/2968/segfault-calling-cpyext_tp_new_tuple (thanks @rlamy for the fix!), it seems that it is fine to merge this. PR https://github.com/pytorch/pytorch/pull/17837\r\n\r\nThanks to @xuhdev for an already merged PyPy compilation fix https://github.com/pytorch/pytorch/pull/11857\r\n\r\nI submitted PRs for 2 and 4. If anyone can pick other items, that would be great.\r\n\r\nWhile these are quite small fixes, still properly integrating them is not a small task, not speaking of setting up CI and supporting it.\r\n\r\n\r\n\r\n\r\ncc @ezyang"},{"labels":["enhancement",null,null],"text":"There are separate implementations for the distributed data parallelism for CUDA and CPU tensors.\r\n\r\nThe CUDA one has evolved quite a bit and includes:\r\n* Dynamic bucketing of gradient tensors to reduce the number of reduction calls\r\n* Start reduction when a bucket is full (i.e. all gradients participating have been computed)\r\n* Handling of fp32 and fp16 gradients\r\n* Handling of multiple model copies across devices\r\n* Buffer broadcast\r\n\r\nThe CPU one has not evolved and does the following:\r\n1. Wait for all gradients to be computed\r\n2. Flatten all gradients into one big tensor\r\n3. Allreduce\r\n4. Unflatten\r\n\r\nIdeally, we turn the CUDA approach into a device agnostic one, such that we can remove the CPU specific implementation. Parts of the CUDA approach are written in C++ (see #12852, #12954, #13496, #13607) and these can be made device agnostic as well. Both the bucketing and autograd hooks are now defined in Python land but can be turned into C++ code as well.\r\n\r\nSeparately, @mcarilli and team made really nice improvements to standard DDP in [Apex](https://github.com/NVIDIA/apex). The ideal bucketing order is discovered dynamically such that it reflects the **execution order** instead of **definition order**, which can cause significant speedups if these orderings are mismatched (e.g. layers that are used last are defined first).\r\n\r\nGoals:\r\n* Use the same DDP implementation regardless of backend\r\n* Enable use of DDP for models defined with the C++ frontend\r\n* Discover ideal bucketing order dynamically\r\n\r\nLet's use this as a starting point for discussion.\r\n\r\ncc @mcarilli @mrshenli "},{"labels":["enhancement",null],"text":"# What is jax (just in case for other reader)\r\n[jax](https://github.com/google/jax) is full numpy acceleration and autodiff with functional neural networks, and is essentially autograd 2.0. (from [Italian Association for Machine Learning)](https://iaml.it/blog/jax-intro-english)\r\n\r\n# Feature Request\r\nIs it possible to incorporate jax into implementation of Pytorch? It looks so promising to speed up machine learning.  \r\nI would like to know if there is any plan about jax. Thanks."},{"labels":["enhancement",null,null,null,null],"text":"## ðŸ› Bug\r\nHi,\r\nI am running an OMP loop in a libtorch program (Windows 10). \r\n```\r\n#pragma omp parallel for\r\n      for (int inet=0; inet<NUM_OF_NETS; inet++) { \r\n        auto& nnStack=*nnStack_arr[inet];\r\n        auto& trainer=*trainers_arr[inet];\r\n        ...\r\n       totalLoss_t.backward();\r\n       ...\r\n}\r\n```\r\nSo, separate nets, trainers  prepared earlier. All other objects are locally created within the loop.\r\nThings go as expected, i.e. multithreaded, until I hit \r\ntotalLoss_t.backward();\r\nwhere things appear serialized (on 12 core machine 8% CPU). totalLoss_t is also a local object.\r\nOnce the call is done, the threads appear to start working in parallel again. \r\nDo I miss some configuration call?\r\nTried both debug and release builds, the same.\r\n\r\n - PyTorch: 1.0, compiled from latest\r\n - OS: Windows 10\r\n - CUDA/cuDNN version: CPU build\r\n\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n\r\nIt would be great if you could implement [Adaptive Input Representations for Neural Language Modeling](https://arxiv.org/abs/1809.10853). This is, essentially, the same trick that PyTorch currently uses for adaptive softmax outputs, but applied to the input embeddings as well. In addition, it would be helpful to provide optional support for adaptive input and output weight tying.\r\n\r\n## Motivation\r\n\r\nPyTorch has already implemented adaptive representations for output. The paper shows that these representations are also useful for input. They allow faster training, and provide better accuracy. In addition, in some situations the paper shows it can be helpful to use weight-tying, which would allow this feature to be integrated nicely with the existing adaptive output functionality.\r\n\r\nAt the time the paper was written, adaptive input representations were the state of the art in language modeling.\r\n\r\n## Alternatives\r\n\r\nAn alternative would be for this functionality to be moved in to higher level libs like fastai, but that would mean to get weight tying to work, either the existing pytorch adaptive output would have to be recreated in that library, or else that library would have to rely on using the internal data structures of the pytorch module - neither of these would be ideal.\n\ncc @albanD @mruberry"},{"labels":["enhancement",null],"text":"Global Second Order Pooling is claimed to be better than Global Average Pooling, which is used in many networks. Producing covariance matrices as image representations, it has achieved state-of-the-art results in a variety of vision tasks. Few examples are as follows:\r\n\r\nhttp://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Towards_Faster_Training_CVPR_2018_paper.pdf\r\nhttp://openaccess.thecvf.com/content_ICCV_2017/papers/Li_Is_Second-Order_Information_ICCV_2017_paper.pdf\r\nhttps://vision.cornell.edu/se3/wp-content/uploads/2017/04/cui2017cvpr.pdf\r\nhttps://arxiv.org/pdf/1804.00428.pdf\r\nhttps://arxiv.org/abs/1611.02155\r\n\r\nMore importantly, in this (https://arxiv.org/pdf/1811.12006.pdf) paper, they also claim it to be better than several state-of-the-art spatial and channel attention modules (Squeeze-Excite, Gather-Excite, CBAM, etc). Some implementations are here. I believe that it would be great to have Global Second Order Pooling in the PyTorch framework with a reliable and fast implementation of the functionality required.\r\n\r\nhttps://github.com/jiangtaoxie/fast-MPN-COV/blob/master/src/representation/MPNCOV.py"},{"labels":["enhancement"],"text":"## ðŸš€ Feature\r\nCurrently, an indexing of torch.HalfTensor like half_tensor[indices] gives this runtime error.\r\nRuntimeError: \"index\" not implemented for 'torch.HalfTensor'. So, requesting for an indexing method. \r\n\r\n## Motivation\r\nIt would be great to be able to index half tensors. My current use case for this is while doing large scale similarity metric computations on embeddings on the GPU where I figured I could use float16 to reduce memory usage.\r\n\r\n## Pitch\r\nIndexing method added to HalfTensor objects.\r\n\r\n## Alternatives\r\nFor my use-case the alternative is to chunk up tensors appropriately to do similarity computations. While its works just as well, I can imagine other use cases where indexing on HalfTensor objects are needed.\r\n"},{"labels":["enhancement"],"text":"## ðŸš€ Feature\r\nCurrently Conv2d layers supports int or tuple however tuple is only allowed to be 2-tuple. So this only supports symmetrical padding case. This should be 4-tuple to allow for asymmetrical padding. This will be required anytime we are trying to mix and match odd and even sized inputs and kernels.\r\n\r\n## Pitch and Motivation\r\n\r\nOne of the first issue users coming from TensorFlow encounter is lack of feature `padding='SAME'`. This feature has allowed users to quickly prototype without having to dive into convolutional arithmetic for network design. There have been [some reservations](https://github.com/pytorch/pytorch/issues/3867) for adding this feature despite of strong demand from users. I think the least we can probably do is above simple change that hopefully won't cause any other issues.\r\n\r\n## Alternatives\r\n\r\nCurrent alternative for users is to spend time in learning and extracting formulas for convolutional arithmetic, computing neccessory padding and use either `F.pad` or padding layers (which are unfortunately not easily discoverable). \r\n\r\nAnother alternative is to discover various threads like [1](https://discuss.pytorch.org/t/padding-for-convolutions/5881/5), [2](https://discuss.pytorch.org/t/utility-function-for-calculating-the-shape-of-a-conv-output/11173/2), [3](https://github.com/pytorch/pytorch/issues/3867) and piece togather some workaround proposed from various answers. "},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nIt would be nice to replicate numpy's RandomState API, which creates an object containing the state of a random number generator.  The object has methods for generating random numbers.  i.e.\r\n\r\n```\r\nrng = torch.RandomState(seed=1234)\r\ndata = rng.randn(3, 4)  # (create a 3x4 random matrix from a standard normal distribution)\r\n```\r\n\r\nCurrently, you can actually do this, though the feature is [undocumented](https://pytorch.org/docs/stable/torch.html?highlight=rand#torch.rand) and not quite as nice as numpy's way:\r\n\r\n```\r\nrng = torch.Generator()\r\nrng.manual_seed(1234)\r\ndata = torch.randn(3, 4, generator=rng) \r\n```\r\n\r\n## Motivation\r\n\r\nPytorch is currently a bit awkward when it comes to random seeds, because the only officially supported option is to globally set random seeds.  For reproducibility and model-comparison, it sometimes makes sense to have separate random number generators for, e.g. initial-parameters and stochastic-inference.\r\n\r\ne.g. \r\n\r\n```\r\nmodel = create_mlp_with_dropout(\r\n    params = initialize_mlp_params(\r\n        layer_sizes = [784, 500, 500, 10], \r\n        rng = torch.RandomState(param_seed)\r\n        ),\r\n    rng = torch.RandomState(inference_seed)  # random seed for inference\r\n    )\r\nfor x, y in sample_minibatches(data, rng=torch.RandomState(data_seed)): \r\n    model.train(x, y)\r\n```\r\n\r\nOf course you could do this by manually reseeding the global random number generator, passing a random number generator object is nicer because it makes the dependence on the seed explicit.\r\n\r\n## Alternatives\r\n\r\nYou could also just document the current solution, though numpy's seems more elegant.  \r\n\r\nAnother alternative would be just to add a context manager `with torch.use_random_state(generator=rng): ...`, which sets the global generator to `rng` on entrance and reverts to the previous on exit.  However it still seems more pythonic to pass the generator as an optional argument than be messing around with global variables.  \r\n\r\n## Additional context\r\n\r\nThis arose from a discussion [here](https://discuss.pytorch.org/t/is-there-a-randomstate-equivalent-in-pytorch-for-local-random-generator-seeding/37131/4)\r\n"},{"labels":["enhancement",null,null],"text":"This will be useful for external contributors who currently rely on closing and re-opening a PR to re-run all CI jobs.\r\n\r\ncc @kostmo @ezyang"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nRuntimeError: hardshrink is not implemented for type torch.sparse.FloatTensor\r\n\r\n## Motivation\r\n\r\nWe are experimenting with sparse neural networks. We want to model the connection between neurons with sparse matrices. At times, if the weights are too small we want to prune connections, i.e. set the element in the sparse matrix to zero via a hardshrink.\r\n\n\ncc @vincentqb"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nImplement `numpy.random.choice` equivalent.\r\n\r\n## Motivation\r\nIn some cases, it is useful to get random samples from a torch Tensor efficiently. For instance, during the training of some Region-Based Detectors, it is necessary to control the proportion of positive and negative regions of interest (RoIs) over mini-batches. \r\n\r\nHere is a workaround adopted in maskrcnn-benchmark: https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/modeling/balanced_positive_negative_sampler.py#L49-L50, which can be inefficient if `positive.numel()` is big and `num_pos` is small, for instance.\r\n\r\n## Pitch\r\n\r\nImplement `torch.random.choice` to have an equivalent behavior to the numpy implementation.\r\n\r\n"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nForward attribute access on a `DataParallel` object to underlying `module` attribute using Python descriptors `__get__` `__set__`.\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nCurrently, when you wrap your model using `DataParallel` class, you would need to change all attribute access to your original model from `model.attr` to `model.module.attr` as suggested here: https://discuss.pytorch.org/t/how-to-reach-model-attributes-wrapped-by-nn-dataparallel/1373\r\n\r\nThis might result in a lot changes in your code when you want to extend your model to multiple GPUs, rather than just adding `model = nn.DataParallel(model)` line, which is a super quick way of adding multi-GPU support.\r\n\r\nTo overcome this difficulty, we can forward access to properties not belonging to `DataParallel` object toward it's underlying `module` attribute, and we can fail it module object also does not have such an attribute.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nBe able to continue using `model.custom_attr` after I wrap `model` with `DataParallel` using `model = nn.DataParallel(model)`.\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered if any. -->\r\n\r\nConvert all attribute access to your model from `model.custom_attr` to `model.module.custom_attr`.\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\r\nIf core team is OK with this approach, I can prepare a PR with this feature."},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nThe ability to pickle.load a Python object containing a torch cuda tensor on a CPU only machine.\r\n\r\n## Motivation\r\n\r\nCurrently, trying to do this gives `RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location='cpu' to map your storages to the CPU.`  Even though you are loading with `pickle.load`, not `torch.load`.  \r\n\r\nWhen the \"loading\" code is pytorch agnostic (exists in a repo that does not use pytorch), you can't just change a `pickle.load(f)` into a `torch.load(f, map_location='cpu')`.  This may be the case when the saved data takes a particular structure and loading/unloading is handled by some code that does not depend on pytorch.\r\n\r\n## Pitch\r\n\r\nA context manager could take care of this:\r\n\r\n```\r\nwith torch.loading_context(map_location='cpu'):\r\n    obj = pickle.load(f)  # In my case this call is buried deeper in torch-agnostic code\r\n```\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nThe function `torch.split` partitions a tensor into chunks of equal size (except last) if called with an `int` parameter:\r\n```python\r\ntorch.split(torch.arange(10), split_size_or_sections=3)\r\n(\r\n    tensor([0, 1, 2]), \r\n    tensor([3, 4, 5]), \r\n    tensor([6, 7, 8]), \r\n    tensor([9])\r\n)\r\n```\r\nor chucks of specific sizes if called with a list of ints:\r\n```python\r\ntorch.split(torch.arange(10), split_size_or_sections=[7, 3])                                                                                           \r\n(\r\n    tensor([0, 1, 2, 3, 4, 5, 6]), \r\n    tensor([7, 8, 9])\r\n)\r\n```\r\nThis second behavior is delegated to the (undocumented) function `torch.split_with_sizes` which only accepts a list of integers as parameter. I would like to be able to call `torch.split` and `torch.split_with_sizes` using a `LongTensor` instead of a list of ints.\r\n\r\n## Motivation\r\nIn the project I am working on, the sizes of the split are part of the data itself, and it makes sense to me to store them as tensors rather than simple lists. With the current behavior I find myself writing this quite often:\r\n```python\r\nfeatures: torch.Tensor\r\nsplits: torch.LongTensor\r\ntorch.split(features, splits.tolist())\r\n```\r\nI am afraid that the conversion to a native list might be inefficient and, in general, error-prone.\r\n\r\n## Changes\r\n- The signature of `torch.split` should become:\r\n  `torch.split(tensor: torch.Tensor, split_size_or_sections: Union[int, Sequence[int], torch.LongTensor], dim: int=0)`\r\n- The signature of `torch.split_with_sizes` should become:\r\n  `torch.split(tensor: torch.Tensor, split_sizes: Union[Sequence[int], torch.LongTensor], dim: int=0)`\r\n- (Optionally) I think `split_with_sizes` should have a docstring so that one can easily call it instead of calling `torch.split` which in turn delegates to `torch.split_with_sizes`"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nMy model takes a path to a model and converts it to pytorch.\r\nThere are some variations on this model with different parameter shapes, and I would like to support all of them. It does this already, and they are stored with `self.register_buffer('name', value)`.\r\n\r\nBut I can't save and load the model with `model.load_state_dict(torch.load(PATH))`. The keys need to be present in self._buffers or self._parameters for this to work, and they need to match in shape, but None values are ignored.\r\n\r\n## Motivation\r\nIt is already possible to call `self.register_buffer('name', None)`. This is an undocumented feature, and the null-value is simply added to self._buffers, but then ignored everywhere. (if ... is not None)\r\n\r\nIf I could load the model only knowing the names of the parameters, I could make it work with the two models I need interchangeably, and with all the features offered by pytorch serialization.\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\nMy proposal is that `self.register_buffer('name', None)` should prevent the \"Unexpected key(s) in state_dict\" error and then allow any shape of tensor to be loaded into this buffer with `load_state_dict`.\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\nIt seems quite straightforward to implement in `_load_from_state_dict` of module.py.  Since the value None is already present in self._buffers. It is just ignored.\r\nHere's a link: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py\r\n\r\nIt could be an issue if anyone uses None-valued buffers or parameters and then `load_state_dict` with `strict=True`, so you could add an additional argument to this function for this new functionality (completely safe) or simply only allow it when `strict=False`.\r\n\r\nThe values that I store with self.register_buffer are constant throughout.\r\nThe models I am working with are SMPL ( http://smpl.is.tue.mpg.de/ )\r\nand SMIL ( https://www.iosb.fraunhofer.de/servlet/is/82920/ )\r\n\r\nThis request may conflict with https://github.com/pytorch/pytorch/issues/8104, if it is implemented.\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nI want to be able to save a model with sparse tensors.\r\n\r\n## Motivation\r\nI have a module where one of the parameters may be sparse. (It is an option)\r\nAll of the parameters are stored with register_buffer, so they are present in the state dict. But when one of them are sparse it cannot be saved to disk.\r\nIt throws the error `RuntimeError: sparse tensors do not have storage` for example simply on `torch.save(torch.sparse.FloatTensor(), 'sparse.pt')`.\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\nIt would be nice if the sparse matrices worked interchangeably with the dense counterparts.\r\n`torch.save(torch.sparse.FloatTensor(), 'sparse.pt')` should just store the tensor.\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\nAlternatively a quick fix would be to simply convert the tensor to dense and then store that.\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n(Approximate) Earth Mover's Distance Loss Function\r\n\r\n## Motivation\r\nThis is a common function used in deep learning papers for point cloud generation and distribution matching.\r\n\r\n## Pitch\r\nThis ought to be added to the loss function library as it is only increasing in usage in deep learning literature.\r\n\r\n## Additional context\r\nThere are a few open source implementations out there already. I have updated [this older one](https://github.com/fxia22/pointGAN/tree/74b6c432c5eaa1e0a833e755f450df2ee2c5488e/emd) for PyTorch 1.0 according to the CUDA extension tutorial. While it generally works, there are some bugs though that go beyond my expertise (or understanding) when it comes to larger point sets. These bugs were extant before I started toying with it. My version [can be found here](https://github.com/meder411/PyTorch-EMDLoss)."},{"labels":["enhancement",null,null,null],"text":"We generated a type stub for `torch` module in , which is good enough to get autocomplete working in PyCharm. However, the type stub is not that well tested for actually, you know, accurately reflecting the types of our functions (we have light testing, but it's not enough) and it's unlikely that it's actually good enough to actually use mypy to typecheck PyTorch-using code. It seems like this is something that people might be interested in, so I'm creating this issue to track. Please emoji if this is something that you would use.\n\ncc @ezyang @gchanan @zou3519"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nRequested by @gkioxari, it would bring `unique` api closer to numpy equivalent. Cuda implementation is not hard, but I haven't looked at CPU. \r\nMay be #15804 scope should be widened to also include this. \r\ncc @mruberry @rgommers @ptrblck, @gkioxari, @jcjohnson  \r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n\r\nHaving multiple resettable torch.cuda.max_memory_allocated() counters\r\n\r\n## Motivation\r\n\r\nWith the help of torch.cuda's `reset_max_memory_allocated` and `max_memory_allocated` one can now measure peak memory usage. Which is very helpful.\r\n\r\nNow, there is a need for identical functionality, but supported in multiple and concurrent scopes, where scopes can overlap or be nested, and as such one global counter is not enough.\r\n\r\nScenario 1: an application relying on the normal functioning (pre-reset implementation) max_memory_allocated or max_memory_cached could now malfunction if some other application resets either or both (action a a distance).\r\n\r\nScenario 2: two profilers measuring different scopes. Say one measuring at a function level, another at a wider or narrower scope. Since there is only one counter they will be resetting each otherâ€™s measurements. pythonâ€™s tracemalloc has the same issue, since it doesnâ€™t create an object instance for the counter.\r\n\r\nThe 2nd scenario is not hypothetical, itâ€™s actually a need I have right now as I have different profilers measuring different scopes. There are in different applications so they canâ€™t really communicate with each other to keep each other in sync with reset calls. e.g. I have one profiler running on the train loop epoch-level, another on the jupyter cell-level, yet another on larger parts of the notebook. And unfortunately, my current peak measuring thread approach is clearly failing to catch all peaks, so itâ€™d be extremely helpful to be able to switch to use max_memory_allocated yet different instances of it in different scopes.\r\n\r\n## Pitch\r\n\r\nSo I need to be able to do something like:\r\n```\r\nmax_obj1 = MaxMemoryAllocated()\r\n# run some code 1\r\nfor epoch in epochs:\r\n    max_obj2 = MaxMemoryAllocated()\r\n    # run epoch code\r\n    peak_epoch = max_obj2.peak()\r\n# run some code ...\r\npeak = max_obj1.peak()\r\n```\r\nOf course, those would be unrelated applications, this code sample is just demonstrating how their execution will overlap and why the current implementation is insufficient.\r\n\r\n## Alternatives\r\n\r\nCurrently, I spawn a thread per counter that measures peak memory usage via nvml (reading directly nvidia stats). Which doesn't give correct measurements, since it's easy for a thread to miss a peak.\r\n\r\n## Additional context\r\n\r\nDiscussion thread: https://discuss.pytorch.org/t/measuring-peak-memory-usage-tracemalloc-for-pytorch/34067/19\r\n\r\nThank you.\r\n"},{"labels":["enhancement",null,null],"text":"## ðŸš€ Feature\r\nExactly as introduced in Microsoft paper : https://arxiv.org/pdf/1703.06211.pdf\r\n\r\n## Motivation\r\nThis operator has great advantages and abilities, and his popularity is increasing.\r\n\r\nI don't think that any further explanation is needed.\r\n\r\nBTW, other frameworks already have this operator, and I'm not talking about private people githubs.."},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nCurrently after an epoch is ended dataloader spawns a new process to read data. This means if processes have cached some internal state (db connection, indexing, ...) they will be lost and the new process will have the overhead of creating the connection or indexing.\r\n\r\n## Motivation\r\n\r\nI have a custom dataset (that indexes data for fast retrieval) and only part of the dataset is used in training. If I have a training with longer epochs, training will be faster than shorter epochs because when an epoch ends a new process will have to re-read and index the custom dataset.\r\n\r\n## Pitch\r\n\r\nA flag to tell dataloader to re-use the worker processes. If the process pool has the re-use option ([Loky from joblib](https://github.com/tomMoral/loky) does) that would be enough.\r\n\r\n## Alternatives\r\n\r\nDocumentation should specify this issue. Option to use [Loky](https://github.com/tomMoral/loky) for all multi-processing backend will be also great.\r\n\r\n## Additional context\r\n\r\nStarted a discussion [here](https://discuss.pytorch.org/t/dataloader-re-use-worker-processes/34071)\r\n\n\ncc @ezyang @gchanan @SsnL"},{"labels":["enhancement",null],"text":"I think it is very useful if we can have feature enhancement that we can compute the entropy of a tensor, the similar way that we can do it for mean, std, etc\r\n\r\n\r\nin more details:\r\nIt would be super useful to have a function that compute the entropy of a tensor. well, saying that, it is good if that function can compute the entropy in different ways depending on our desire; meaning that we can define a dimension and compute the entropy based on that, e.g. computiong the entropy of a tensor channel wise, or etc\r\n\r\nThis request was first made [Here](https://github.com/pytorch/vision/issues/662#issuecomment-442036469) and then [here](https://github.com/pytorch/contrib/issues/16), but I am told that it will be more useful to open it here"},{"labels":["enhancement",null,null,null],"text":"Hi,\r\nis there any plan to implement the inverse short-time fourier transform (ISTFT)?  I saw that STFT is already implemented, but I cannot find the inverse transform. This would be a very important feature for people working on audio and speech.\r\n\r\nThank you!\r\n\r\nMirco"},{"labels":["enhancement",null,null,null],"text":"## ðŸ› Bug\r\n\r\nReparameterized gradient computation fails when dirichlet (hence beta) parameters are dispached on GPU.\r\n\r\nthis works:\r\n```\r\na,b = torch.ones(3,4,1,5,requires_grad=True),torch.ones(3,4,1,5,requires_grad=True)\r\ns = torch.distributions.beta.Beta(a,b).rsample(torch.Size((10,)))\r\ntorch.sum(s).backward()\r\na.grad\r\n```\r\n\r\nbut this doesn't\r\n```\r\na,b = torch.ones(3,4,1,5,requires_grad=True,device='cuda'),torch.ones(3,4,1,5,requires_grad=True,device='cuda')\r\ns = torch.distributions.beta.Beta(a,b).rsample(torch.Size((10,)))\r\ntorch.sum(s).backward()\r\na.grad\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: TITAN X (Pascal)\r\n...\r\n\r\nNvidia driver version: 384.130\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl  \r\n[conda] magma-cuda92              2.4.0                         1    pytorch\r\n[conda] mkl                       2018.0.3                      1  \r\n[conda] mkl-include               2019.1                      144  \r\n[conda] mkl_fft                   1.0.6            py37h7dd41cf_0  \r\n[conda] mkl_random                1.0.1            py37h4414c95_1\r\nIn [ ]:"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\n\r\nI would request that `torch.set_printoptions` add a flag so that scientific notation can be enabled or disabled.\r\n\r\n## Motivation\r\n\r\nPyTorch printing has a limitation: the end user cannot force scientific mode printing to be enabled or disabled. (Currently, `sci_mode` is chosen based on logic in [_tensor_str.py](https://github.com/pytorch/pytorch/blob/master/torch/_tensor_str.py#L69).)\r\n\r\nThis would give end-users more control, and it would be an easy change.\r\n\r\n## Pitch\r\n\r\nOne possible implementation would involve adding a named argument `sci_mode` to `set_printoptions`. Perhaps:\r\n\r\n* `sci_mode=None` would allow `sci_mode` to be determined by the current logic (the current default)\r\n* `sci_mode=True` would force `sci_mode` to be True\r\n* `sci_mode=False` would force `sci_mode` to be False\r\n\r\nThis would be easy to implement: based on the argument, `sci_mode` in class `_Formatter` could be unaffected or forced to be `True` or `False`.\r\n\r\n## Alternatives\r\n\r\nI'm open to other implementations that would satisfy the same problem.\r\n\r\nFor example, Numpy's `suppress` argument to its `set_printoption` affects the printing of scientific notation.  (Note: In my opinion, Numpy's approach (with `suppress`) is a bit harder to reason about than my proposal, above. My proposal is orthogonal to the other feature flags. I often consider orthogonality to be a desirable property when it comes to configuration.)\r\n\r\n## Additional context\r\n\r\n* https://en.wikipedia.org/wiki/Orthogonality_(programming)\r\n\r\n## Related Issues\r\n\r\n* https://github.com/pytorch/pytorch/issues/13446 "},{"labels":["enhancement",null,null,null,null,null],"text":"## ðŸš€ Feature\r\nWe should have tests in C++ for the Vec256 classes.\r\n\r\n## Motivation\r\nCurrently, the Vec256 code is only tested indirectly through operators that use it. This has led to insufficient test coverage and a number of recent bugs in Vec256 and the operators that use it. We should add sufficient test infrastructure so that we can easily add test cases for new functions on Vec256. It's important that the tests cover the different data types (float, double, int32, int64, etc.) and instruction sets (generic, avx, avx2).\r\n\r\nThis will require changes to CMake since the current test cases don't enable the AVX/AVX2 instruction sets.\n\ncc @VitalyFedyunin"},{"labels":["enhancement",null,null],"text":"A few projects now implement the `__cuda_array_interface__` protocol, which was originally designed by the Numba developers to enable clean operation on arrays managed by other projects.  I believe that today both PyTorch and CuPy have implemented this protocol, and that Numba is the only consumer.\r\n\r\nHowever, libraries like PyTorch and CuPy could also identify this protocol on input objects and use it within their functions.  Probably the first use case would be to allow users to easily convert between different GPU array libraries.  For example, ideally the following would work.\r\n\r\n```python\r\nimport cupy\r\nx = cupy.random.random((1000, 1000))\r\n\r\nimport torch\r\nt = torch.tensor(x)\r\n```\r\n\r\nIdeally the check within `torch.tensor` would be something like the following:\r\n\r\n```python\r\nif hasattr(x, '__cuda_array_interface__'):\r\n    ...\r\n```\r\n\r\nrather than the following:\r\n\r\n```python\r\nif isinstance(x, cupy.ndarray):\r\n    ...\r\n```\r\n\r\nwhich would be useful for support of future GPU libraries.\r\n\r\nxref https://github.com/pytorch/pytorch/issues/11914"},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\nWe should add a global variable to force PyTorch to use bitwise deterministic algorithms. Soumith suggests adding the flag to a `torch.experimental` sub-package because we're not sure about some of the details.\r\n\r\n## Motivation\r\nBitwise determinism between runs is sometimes useful for debugging. However, it's difficult to write efficient deterministic algorithms for some operations.\r\n\r\n## Pitch\r\nWhen `torch.experimental.deterministic` is `False` (the default), PyTorch should use the fastest algorithm available for a given operation. When `torch.experimental.deterministic` is `True`, PyTorch should only use deterministic algorithms. PyTorch should issue a warning if we don't have a deterministic algorithm available for a given operation and `torch.experimental.deterministic` is `True`.\r\n\r\n## cuDNN\r\nWe already have a `torch.backends.cudnn.deterministic` flag to control cuDNN algorithm choices. We should keep this flag for now and restrict cuDNN to deterministic algos if either `torch.backends.cudnn.deterministic` or `torch.experimental.deterministic` is True.\r\n\r\n## Non-goals\r\nWe only aim for bitwise determinism between runs on machines with the same architecture and configuration. For example, even when `torch.experimental.deterministic` is True we do **not** aim for bitwise determinism when any of the following varies:\r\n\r\n* PyTorch version\r\n* CPU architecture (e.g. x86 with AVX vs. ARM)\r\n* GPU architecture (e.g. AMD vs. NVIDIA or P100 vs. V100)\r\n* Library dependencies (e.g. OpenBLAS vs. MKL)\r\n* Number of OpenMP threads\r\n\r\n## Implementation suggestions\r\nI suggest adding this feature in two steps. The first step is to add the `torch.backends.cudnn.deterministic` flag and add warnings to any non-deterministic operations. The second step is to add deterministic implementations for the non-deterministic operations.\r\n\r\nThere is a partial list of non-deterministic operations in the [PyTorch docs](https://pytorch.org/docs/stable/notes/randomness.html?highlight=determinism).\r\n\r\n## Open questions\r\nHow should `torch.experimental.deterministic` interact with the RNG seed? Should it set a default seed if no manual seed has been set? Should it issue a warning if no manual seed has been set?\n\ncc @ezyang @gchanan @zou3519"},{"labels":["enhancement",null,null,null],"text":"Now we've already had [F.pdist](https://pytorch.org/docs/stable/nn.html?highlight=pdist#torch.nn.functional.pdist), which computes pairwise distances between each pair in **a single set** of vectors.\r\n\r\nHowever, in retrieval problems, we often need to compute the pairwise distances between each pair consisting one sample from a probe/query set and another sample from a gallery/database set, in order to evaluate the performances of a retrieval model. \r\n\r\nSpecifically, if the database tensor has size N-by-D and the query M-by-D, the tensor returned by a *cdist* function should have size N-by-M where the (i, j)-th element is the distance between the i-th sample from the database and the j-th sample from the query.\r\n\r\nCurrently, a plausible way (ok, I use this method because I had no idea about GPU programming to achieve better performance) to do this evaluation is:\r\n1. transform the tensor to a numpy array: query_np = query.cpu().numpy(), database_np = database.cpu().numpy()\r\n2. using [cdist](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html) provided by scipy: dist_matrix = cdist(query_np, database_np)\r\n\r\nwhich is really far from elegent and could not utilize GPU power and thus inefficient. So, could we develop some efficient cdist function which can use GPU, and hopefully act like [cdist in scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html)?"},{"labels":["enhancement",null,null,null,null,null],"text":"## ðŸš€ Feature\r\n\r\nWhen x is a tensor of shape (N, D) and idx is a tensor of indices of shape K, the backward pass of x[idx] is much slower than the equivalent operation implemented using gather. Here is a benchmarking script:\r\n\r\nhttps://gist.github.com/jcjohnson/b03a0275e64681bb7587bbc7399a645a\r\n\r\nOn a P100 GPU with PyTorch 1.0 stable, across a variety of problem shapes I get the following results:\r\n\r\n```\r\nForward gather speedup:\r\n  Min:  0.7549055905220289\r\n  Max:  5.590410529614541\r\n  Mean:  0.9328673787035276\r\n  Median:  0.880012936610608\r\nBackward gather speedup:\r\n  Min:  1.6313537996980372\r\n  Max:  23.95120218579235\r\n  Mean:  3.340551245050125\r\n  Median:  1.8802246977054176\r\n```\r\n\r\nBasically this says that on the forward pass index is sometimes faster and gather is sometimes faster. However on the backward pass, gather is always faster than integer indexing.\r\n\r\nThis is surprising, and suggests that although the two operations perform the same computation their implementations have very different performance characteristics. Integer indexing is much more intuitive than gather, so I suspect that many users are unknowingly leaving a lot of performance on the table by choosing integer indexing over gather. In one of my own applications, replacing integer indexing with gather resulted in a more than 2x speedup on my overall training iteration times!\r\n\r\nWould it be possible to somehow unify the implementation of the two operations, or otherwise ensure that integer indexing always performs at least as well as gather?\r\n"},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\nI would like to add the function and `torch.nn.Module` for sparsemax as described in https://arxiv.org/abs/1602.02068\r\n\r\n## Motivation\r\n\r\nIt is a function that I implemented in plain pytorch and I would like to share it with the community and enrich the library.\r\n\r\n## Pitch\r\n\r\nI have already have a piece of code that does that with both forward and backward. It should work for any batch dimensions and it applies sparsemax on the dimension `dim`. However, I think it should be revisited as the standard required by the library.\r\n\r\n```\r\nimport torch\r\n\r\nclass SparsemaxFunction(torch.autograd.Function):\r\n\r\n    @staticmethod\r\n    def forward(self, z, dim=-1):\r\n        z = z.transpose(dim, -1)\r\n        sorted_z, _ = z.sort(-1, descending=True)\r\n        k_z = (1 + torch.arange(1., 1 + z.shape[-1]) * sorted_z > sorted_z.cumsum(-1)).sum(-1, keepdim=True) + 1\r\n        mask = (k_z >= torch.arange(1, z.shape[-1] + 1).repeat(z.shape[:-1] + (1, )))\r\n        t_z = ((mask.float() * sorted_z).sum(-1, keepdim=True) - 1) / k_z.float()\r\n        out = torch.relu(z - t_z)\r\n        self.save_for_backward(out, torch.tensor(dim))\r\n        return out.transpose(dim, -1)\r\n\r\n    @staticmethod\r\n    def backward(self, grad_output):\r\n        dim = self.saved_tensors[1]\r\n        s = (self.saved_tensors[0] > 0).float()\r\n        v = (s * grad_output).sum(dim, keepdim=True) / s.sum(dim, keepdim=True)\r\n        grad = s * (grad_output - v)\r\n        return grad\r\n    \r\nsparsemax = SparsemaxFunction.apply\r\n    \r\nclass Sparsemax(torch.nn.Module):\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\n        \r\n    def forward(self, z):\r\n        return SparsemaxFunction.apply(z)\r\n```\r\n\r\n## Alternatives\r\n\r\nThis function might be implemented directly in C++ since it is quite simple.\r\n\n\ncc @ezyang @gchanan @zou3519"},{"labels":["enhancement",null,null],"text":"Once facebookincubator/gloo#152 is fixed we can remove copies added in #14688.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski"},{"labels":["enhancement",null,null],"text":"This will avoid the type of confusion in this comment: https://github.com/pytorch/pytorch/issues/14536#issuecomment-443257710.\r\n\r\nList replication means the tensors refer to the same underlying storage. I can't think of a case where this would be expected behavior, so we can check and throw if two output tensors point to the same memory for these operations."},{"labels":["enhancement",null,null],"text":"In `torch.distributed` you can initialize a global process group using the `init_process_group` function. This function takes a backend argument and will initialize **either** the Gloo, NCCL, or MPI backend. You're at a loss if you want to use both Gloo and NCCL through the `torch.distributed` frontend. The only way to do this today is by manually creating ProcessGroup instances and using them directly.\r\n\r\nFor the case where you're dealing with both CPU and CUDA tensors and want to use Gloo for the CPU ones and NCCL2 for the CUDA ones, we should consider supporting a mode where we don't initialize a single backend, but multiple at the same time. We then use a process group dispatcher class that forwards the calls to the appropriate process group depending on the device type of the tensor arguments. This class should be a subclass of the ProcessGroup base class in C++ such that we can use it both from the Python side as well as from the C++ side.\r\n\r\nHave to figure out what is the right way to expose this to the `init_process_group` function.\r\n\r\ncc @teng-li "},{"labels":["enhancement",null],"text":"This is not supported because of the lack of unbound buffer support in the gloo ibverbs transport.\r\n\r\nSupport is not scheduled yet, but creating issue to track."},{"labels":["enhancement",null],"text":"Also see #14297."},{"labels":["enhancement",null],"text":"Also see #14297."},{"labels":["enhancement",null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nImplementation of RePr as a toggleable option for convolutional layers.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nRePr appears to promote a good improvement to generalization speed for general use.\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\r\nArxiv Link: [RePr: Improved Training of Convolutional Filters](https://arxiv.org/pdf/1811.07275.pdf)\r\n"},{"labels":["enhancement",null,null],"text":"It seems there are several people working on batch mode linear algebra routines, i.e. #11796 and #14071 are active. \r\n\r\nAny plans for adding a batch mode SVD? This would be useful for certain implementations of group equivariant networks. \r\n\r\nI'm not completely familiar with the PyTorch codebase, but if I'm not mistaken the usual backend used for linear algebra computations on the GPU is MAGMA. I don't think MAGMA implements a batch SVD operation, but [cuSolver](https://docs.nvidia.com/cuda/cusolver/index.html) does for small matrices (max 32x32). For larger matrices we can just fall back to the current approach. \r\n\r\nIf no one else is planning on working on this I can take a look at it. The correct way to do this would be to model something like #9949, right? \r\n\r\nI realize several others have made similar suggestions: #10172, #4689. Those issues don't seem active however. "},{"labels":["enhancement",null],"text":"See https://github.com/pytorch/pytorch/pull/14039#issuecomment-439219408"},{"labels":["enhancement",null],"text":"Comment by @SsnL in https://github.com/pytorch/pytorch/issues/13750#issuecomment-437242684, and discussion following that comment.\r\n\r\nThe FileStore will delete the underlying file if all participating processes terminate gracefully. If they don't there will be a file hanging around. If you try to reuse that file, the process group that uses it will get to see old values, and likely hang or crash. The technique showed by @SsnL fixes this by ensuring that a file gets truncated when it is first opened."},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ High-level changes:\r\n1. **IMPORTANT**: Both `Variable` and `Variable::Impl` are removed, and `at::Tensor` is always the tensor that's passed around in PyTorch, and it can record autograd history when its autograd metadata (`AutogradMeta`) is not null.\r\n2. **IMPORTANT**: Autograd-related function implementations in Variable will be moved to VariableType.\r\n3. Autograd metadata now lives in an `AutogradMeta` struct that `TensorImpl` has a pointer to, and the `AutogradMeta` is *only* populated when the `at::Tensor` requires gradient.\r\n4. We decide whether to dispatch to VariableType / non-VariableType functions using the `at::AutoNonVariableTypeMode` in appropriate places internally. (We only dispatch to VariableType functions if we need profiling/JIT-tracing/autograd)\r\n5. Common Tensor functions (e.g. `numel()`Â /Â `sizes()`Â /Â `dim()`) are de-virtualized in TensorImpl and have their runtime reduced by 43%-86%.\r\n6. `tensor.is_variable()` and `options.is_variable()` always return true, because every `at::Tensor` is a variable (and can record autograd history when its `AutogradMeta` is not null). (We keep `options.is_variable(...)` for backward compatibility, and raise warning if it's set to false.)\r\n7. API behavior change: changing shape/storage on `tensor.data` in Python or `tensor.data()` in C++ will no longer update `tensor`.\r\n\r\n## Pitch\r\n\r\nCurrently, the distinction between `at::Tensor` and `Variable` (subclass of `at::Tensor` that contains autograd metadata and functions) creates unnecessary cognitive overhead for PyTorch core development. We want to remove this distinction and make it possible to use `at::Tensor` everywhere in PyTorch. After merging `Variable` into `at::Tensor`, here are the common end-user APIs:\r\n\r\n- **When C++ user wants to create a non-history-recording `at::Tensor` from another `at::Tensor`:**\r\nCurrent API (unchanged):\r\n```cpp\r\nauto t = torch::ones({2, 2}, torch::requires_grad()); // t is recording history\r\nauto t_detached = t.detach() // t_detached is the non-history-recording version of t\r\n```\r\nWhen the user calls `t.detach()`, we do the following under the hood:\r\n1. We do the shallow copy of `t`'s TensorImpl, which copies the storage pointer and all other TensorImpl fields (e.g. `size` / `stride`).\r\n    - Note that subclasses of TensorImpl (e.g. `SparseTensorImpl`) need to know how to make a shallow copy of themselves, and we dispatch this operation to each TensorImpl subclass' own `shallow_copy_and_detach()` function (by making the `shallow_copy_and_detach()` function virtual in TensorImpl and overriding it in TensorImpl subclasses).\r\n2. We set the `AutogradMeta` pointer to NULL, to indicate that it doesn't need to record history.\r\n3. We return an at::Tensor that wraps the new TensorImpl.\r\n\r\n<br />\r\n\r\n- **When C++ user wants to enable/disable history-recording for an `at::Tensor`:**\r\nProposed API:\r\n```cpp\r\nauto t = torch::ones({2, 2});  // t is not recording history (this already works)\r\nt.requires_grad_(true);  // t is recording history now (new API)\r\nt.requires_grad_(false); // t is not recording history anymore (new API)\r\n```\r\nWhen the user calls `t.requires_grad_(true)`, we do the following under the hood:\r\n1. We initialize a struct called `AutogradMeta`, which stores autograd-specific fields (such as `grad_`/`grad_fn_`/`grad_accumulator_`).\r\n2. We assign the struct to the `AutogradMeta` pointer in `t`'s TensorImpl.\r\n\r\nWhen the user calls `t.requires_grad_(false)`, we do the following under the hood:\r\n1. We set the `AutogradMeta` pointer in `t`'s TensorImpl to NULL.\r\n\r\n<br />\r\n\r\n- **When C++ user wants to call non-Variable operations on an `at::Tensor` when dispatching through `type()`**\r\nProposed API:\r\n```cpp\r\n{\r\n  auto t_type = t.type();  // `t_type` is a Variable type if `t` contains AutogradMeta\r\n}\r\n{\r\n  at::AutoNonVariableTypeMode grad_mode(false);  // thread-local guard (new API)\r\n  auto non_var_type = t.type();  // \"non_var_type\" is a non-Variable type\r\n}\r\n{\r\n  at::AutoNonVariableTypeMode grad_mode(true);  // thread-local guard (new API)\r\n  auto var_type = t.type();  // \"var_type\" is a Variable type\r\n}\r\n```\r\nUnder the hood, `type()` checks whether the `at::AutoNonVariableTypeMode` thread-local guard is enabled when determining the type of the variable.\r\n\r\n<br />\r\n\r\n- **When C++ user wants to change content of an `at::Tensor` that has AutogradMeta, without affecting the tensor's `grad_fn` or `version_counter_`**\r\nProposed behavior:\r\n```cpp\r\nauto t = torch::ones({2, 2});\r\nt.requires_grad_(true);\r\nAT_ASSERT(t.current_version() == 0);\r\nt.data().add_(1);  // This is consistent with Python `.data` behavior: changing `.data` of a tensor in Python doesn't affect the tensor's `grad_fn` or `version_counter_`\r\nAT_ASSERT(t.current_version() == 0);\r\n```\r\n\r\n\r\n## Motivation\r\n\r\n- **Overly Complex OOP design**: Currently the distinction between `Variable` and `Tensor` is hard to grasp: `Variable::Impl` is a subclass of TensorImpl, but it also has an `at::Tensor` data member which internally wraps another TensorImpl. This co-existence of \"is-a\" and \"has-a\" relationship makes the code complicated and adds cognitive overhead.  In particular, it's difficult to track which functions we have overridden in `Variable::Impl`, and which functions are applicable to Tensor vs. Variable (e.g. `is_wrapped_number()` is only valid on Tensor, not Variable) (for more context, also see note: [We regret making Variable hold a Tensor](https://github.com/pytorch/pytorch/blob/b6a8c45f57b65d11894c4a6e5a3267708ecec1c5/c10/core/TensorImpl.h#L470-L489)). Ideally, we want to use the same tensor type everywhere in PyTorch code.\r\n\r\n- **Unused data members in `Variable::Impl` take up cache/memory space**: Since `Variable::Impl` is a subclass of TensorImpl, it contains all of the data members that a normal TensorImpl would have (such as `sizes_` / `strides_` / etc.). However, the `Variable::Impl` functions always call into the underlying `at::Tensor` and ignores the rest of the fields, which causes a lot of wasted cache/memory space.\r\n\r\n- **Virtual functions are slow**: We care about how much time it takes to execute common Tensor functions such as `numel()` / `sizes()` / `dim()`. Currently, these functions are `virtual` in TensorImpl, so that `Variable::Impl` (a subclass of TensorImpl) can override them and dispatch those calls to the `Variable::Impl`'s underlying `at::Tensor`. Virtual function calls are slow because they involve an extra vtable lookup. Specifically, we did the following comparison on the most common Tensor functions (all timings are in ns):\r\n\r\nBenchmark | Time (no flush) | Time (flush L1) | Time (flush L1+L2) | Time (flush L1+L2+L3)\r\n-- | -- | -- | -- | --\r\nTensor.dim() - non-virtual | 1.3 | 3.33 | 7.6 | 58\r\nVariable.dim() - virtual | 4.5 | 24.4 | 52 | 173.67\r\n**Runtime Savings** | **-71.11111%** | **-86.35246%** | **-85.38462%** | **-66.60333%**\r\nÂ  | Â  | Â  | Â  | Â \r\nTensor.numel() - non-virtual | 22.6 | 63.89 | 109.22 | 294.5\r\nVariable.numel() - virtual | 80.33 | 133.1 | 192 | 810.9\r\n**Runtime Savings** | **-71.86605%** | **-51.9985%** | **-43.11458%** | **-63.68233%**\r\nÂ  | Â  | Â  | Â  | Â \r\nTensor.size(0) - non-virtual | 30.4 | 60.1 | 100.44 | 384.3\r\nVariable.size(0) - virtual | 75.4 | 127.67 | 203.8 | 875.9\r\n**Runtime Savings** | **-59.6817%** | **-52.92551%** | **-50.71639%** | **-56.12513%**\r\nÂ  | Â  | Â  | Â  | Â \r\nTensor.sizes() - non-virtual | 2 | 4.25 | 13.25 | 67.6\r\nVariable.sizes() - virtual | 5.2 | 28.44 | 62.1 | 254.78\r\n**Runtime Savings** | **-61.53846%** | **-85.05626%** | **-78.66345%** | **-73.46731%**\r\nÂ  | Â  | Â  | Â  | Â \r\nTensor.resize_({0}) no-op - non-virtual | 23.11 | 86.44 | 105.44 | 332.33\r\nVariable.resize_({0}) no-op - virtual | 168.4 | 254.22 | 348.56 | 890.9\r\n**Runtime Savings** | **-86.27672%** | **-65.99795%** | **-69.74983%** | **-62.69727%**\r\nÂ  | Â  | Â  | Â  | Â \r\nTensor.resize_({64, 2048}) no-op - non-virtual | 33.4 | 102.56 | 129.56 | 407.22\r\nVariable.resize_({64, 2048}) no-op - virtual | 193 | 278.1 | 364.9 | 936.6\r\n**Runtime Savings** | **-82.6943%** | **-63.12118%** | **-64.49438%** | **-56.52146%**\r\n\r\n> Benchmarked commit: https://github.com/pytorch/pytorch/commit/f000101b8139378f342b175c11072a925c9d7c7a\r\n> Benchmark script: https://github.com/yf225/benchmark/blob/tensor_functions/timing/cpp2/benchmarks/aten_overheads.cpp\r\n> Non-virtual code: https://github.com/pytorch/pytorch/compare/master...yf225:nonvirtual_tensorimpl\r\n> Virtual code: https://github.com/pytorch/pytorch/compare/master...yf225:virtual_tensorimpl\r\n\r\nBased on our current implementation, the runtime difference for `dim()`, `numel()`, `size()`, `sizes()`, and no-op `resize()` comes from the virtual function call overhead and the `at::Tensor` data member indirection in `Variable::Impl`. If we de-virtualize those functions, we would be able to cut the runtime by **43%-86%** on the most common Tensor functions.\r\n\r\n\r\n\r\n## Breaking changes\r\n\r\nNote that this change will break the current API in the following way:\r\n\r\nIn the old world, whenever we want to create a `Variable` that shares the same data with another `Variable`, we simply do `auto var_new = make_variable(var.data())` or `auto var_new = var.detach()`, and any shape / data / storage pointer changes to `var_new` will be reflected in `var` automatically, because internally they share the same underlying `at::Tensor`.\r\n\r\nHowever, in the new world, there is no concept of the \"underlying `at::Tensor`\" of a Variable, since the Variable itself is the Tensor. When we want to create an `at::Tensor` that shares the same data with another `at::Tensor`, we can still call `auto t_new = t.detach()`, but in this case, only the tensor storage data is shared (via ref-counted pointer) between `t_new` and `t`, but not the tensor size/stride information (they are copied by value). In other words, changing anything (e.g. size / stride / storage_ptr ) in the detached Tensor (`t_new`) that are not bits inside tensor storage won't update the original Tensor (`t`), and we should no longer expect those data to be shared.\r\n\r\nThis has implications for Python call sites that do\r\n```python\r\ntensor.data.in_place_operation_()\r\n```\r\nor\r\n```python\r\ntensor_detached = tensor.detach()\r\ntensor_detached.in_place_operation_()\r\n```\r\nIf `in_place_operation_()` only updates the data inside the tensor (such as `zeros_()`), such operation will still work properly; if the in-place operation changes the size, stride or the storage pointer inside the TensorImpl (e.g. `resize_` / `resize_as_` / `set_` / `transpose_`), such operation on `tensor.data` or `tensor_detached` will no longer update the `tensor`. We will address this inconsistency in the following ways:\r\n\r\n1. Add an `allow_tensor_metadata_change_` flag to `TensorImpl` to disallow size/stride/storage_ptr changes from in-place operations such as `resize_` / `resize_as_` / `set_` / `transpose_`, and set this flag to true when people call `tensor.data` in Python.\r\n2. Write text in the docs to actively discourage changing the shape or storage of `tensor_detached` and expecting `tensor` to also be updated.\r\n\r\n\r\n\r\n## Finished changes\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/13827)\r\n1. Add a flag to `TensorImpl` to disallow size/stride/storage_ptr changes from in-place operations such as `resize_` / `resize_as_` / `set_` / `transpose_`, and set this flag to true when people call `tensor.data` in Python.\r\n2. Write text in the docs to actively discourage changing the shape or storage of `tensor_detached` and expecting `tensor` to also be updated.\r\n3. Move `Variable::Impl` data members into TensorImpl as `AutogradMeta` struct\r\n4. Change `Variable::Impl` functions to use data members in `AutogradMeta` struct\r\n5. Add `shallow_copy()` function to each subclass of TensorImpl\r\n6. Do shallow copy when the user calls `make_variable(tensor)` / `variable.detach()` (Reason: now that autograd metadata lives in TensorImpl, in order to create a new history for for the Variable returned from `variable.detach()` we not only need to create a new AutogradMeta struct, but we also need to create a new TensorImpl object that stores pointer to the new AutogradMeta struct (which we obtain by shallow-copying the original TensorImpl). Otherwise, changing history of the detached Variable will also change the history of the original Variable, which is not the correct behavior.)\r\n7. Add `AutogradMetaInterface` class, and make `AutogradMeta` a subclass of it, so that we can make `autograd_meta_` a unique_ptr in TensorImpl\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/15487)\r\n1. Move `set_requires_grad()` / `requires_grad()` / `grad()` from `Variable::Impl` to `AutogradMeta`\r\n2. Move `Variable::Impl` functions such as `backward()` / `rebase_history()` / `grad_accumulator()` / `grad_fn()` out of `Variable::Impl` and into `AutogradMeta`.\r\n3. Note: we need to make these changes so that we can remove `Variable::Impl` class in the next PR.\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/15939)\r\n1. Add thread-local guard (`at::AutoNonVariableTypeMode`) to make sure that in VariableType.cpp the operations on baseType still dispatch to non-Variable type, even if the parameters are now Variables\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/16305)\r\n1. Make `gesv_out` return the original input tensor instead of a new tensor (currently by copying the result tensor into the original input tensor, because a true in-place `gesv` is more difficult to implement. NOTE: also open an issue for this).\r\n2. In VariableType.cpp, after each in-place function on the \"unpacked\" tensor, check pointer address equality for storage in the original input variable's TensorImpl (check this for all arguments in `unpacked_args`)\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/16325)\r\n1. Remove `.type()` calls as much as possible, to reduce the need of using the `at::AutoNonVariableTypeMode` guard\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/16596)\r\n1. Make JIT attributes `t_` and `ts_` store Variable instead of Tensor (and in `t_` and `ts_` use sites, don't wrap the tensor into Variable again) (global search `make_variable(` in jit/ to find places where we are doing double-wrapping for `t_` and `ts_` attributes)\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/17031)\r\n1. `tril_` and `triu_` should not change the input tensor's TensorImpl pointer\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/18225)\r\n1. Move `pyobj_` to TensorImpl itself, because we always need to be able to convert to and from the Python representation.\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/18223)\r\n1. Move `version_counter_` to storage or TensorImpl, because we may capture non-requires-grad variables inside an autograd function, and we need a working version counter in these cases.\r\n2. We should not share version counter in `shallow_copy_and_detach()`, because a pure Tensor doesn't have concept of version counter, and it's managed by autograd instead.\r\n3. We should preserve the API semantics of `tensor.data` in Python, and allow it as an escape route for in-place operations without bumping version counter.\r\n\r\n- [x] PR: https://github.com/pytorch/pytorch/pull/19139\r\n1. `tensor.is_variable()` should check whether the TensorImpl has AutogradMeta. `is_variable_` should be removed.\r\n\r\n- [x] PR: Fix version counter sharing in Variable.set_data(...) https://github.com/pytorch/pytorch/pull/20391\r\n\r\n- [x] PR: Move at::NonVariableTypeMode to TensorImpl, and check it in TensorImpl is_variable() https://github.com/pytorch/pytorch/pull/20392\r\n\r\n- [x] PR: Require passing version_counter and allow_tensor_metadata_change to shallow_copy_and_detach(): https://github.com/pytorch/pytorch/pull/20496\r\n\r\n- [x] PR: Shallow-copy `indices` and `values` in sparse tensor constructor https://github.com/pytorch/pytorch/pull/20330\r\n\r\n- [x] PR: Remove Variable::Impl (https://github.com/pytorch/pytorch/pull/17072)\r\n1. Remove the `at::Tensor` data member (`data_`) from `Variable::Impl`\r\n2. In Variable construction and in `Variable.set_data()`, copy all data from `data.impl` to the variable's TensorImpl.\r\n3. Make `Variable.data()` the same semantics as `tensor.data` in Python. Notice breakage in any `Variable.data()` call sites\r\n1. Remove the `Variable::Impl` class and the `DifferentiableViewImpl` class\r\n2. Remove mentions of `Variable::Impl` and `DifferentiableViewImpl`\r\n3. Fix comments in `[Tensor versus Variable in C++]`, `[We regret making Variable hold a Tensor]`, `[ Autograd View Variables ]`. Go through all comments in variable.h and variable.cpp and fix any inconsistency.\r\n4. **NOTE**: we don't need to add `SparseVariableImpl` that handles how to copy `SparseTensorImpl`, because `SparseTensorImpl` already implements the `shallow_copy_and_detach()` function that Variable factory functions can call.\r\n3. In places where we need to ensure the tensor is not requiring gradient, we should check `!requires_grad() || at::NonVariableTypeMode::is_enabled()`, instead of `!requires_grad() || !at::GradMode::is_enabled()`, because we don't want to move `at::GradMode` to ATen.\r\n\r\n## Changes remaining:\r\n\r\n- [x] Make AutogradMeta optional, so that Variable and Tensor become the same. (Tracking issue: https://github.com/pytorch/pytorch/issues/23032)\r\n\r\n- [ ] Miscellaneous cleanup\r\n1. Remove `unpack()` in VariableType*.cpp.\r\n2. Clean up the `unpack_args` logic in gen_variable_type.py, since we are not doing unpack anymore.\r\n3. Fix comments for `use_derived` in gen_variable_type.py\r\n4. Remove `requires_tensor: True` in native_functions.yaml. Figure out how to fix _dimV, _dimS case (`torch.randn(2, 3)._dimV()` shouldn't hit that error)\r\n\r\n- [ ] TensorImpl de-virtualization (tracking issue: https://github.com/pytorch/pytorch/issues/22815)\r\n\r\n- [ ] Sparse invariant fix (tracking issue: https://github.com/pytorch/pytorch/issues/22778)\r\n\r\n- [ ] Remove `tensor_data()` API (@yf225 is working on it)\r\n\r\n- [ ] Python / C++ Tensor API parity (@yf225 is working on it)\r\n1. Any Python Tensor API should also work on C++ Tensor, without explicit casting to Variable\r\n\r\n- [ ] C++ API doc fix: (@yf225 is working on it)\r\n1. Remove https://pytorch.org/cppdocs/#aten section, and replace all `at::Tensor` with `torch::Tensor`, and remove/fix all mentions of ATen in cpp docs and tutorials."},{"labels":["enhancement",null,null,null],"text":"Google Brain open-sourced a simple impl based on FFT: https://github.com/brain-research/conv-sv/blob/master/conv2d_singular_values.py ([paper](https://arxiv.org/abs/1805.10408))\r\n\r\nIt could be a nice addition to SpectralNorm or for debugging in general\n\ncc @vishwakftw @SsnL @jianyuh"},{"labels":["enhancement",null,null],"text":"I realized that PyTorch does partly prevent operator overloading by raising a `TypeError` rather than returning `NotImplemented`. Therefore Python never checks object methods implementing operations with reflected operands, e. g. `__rmul__`:\r\n\r\n```\r\nIn[1]: import torch\r\nIn[2]: class Two:\r\n           def __mul__(self, other):\r\n               return other * 2\r\n           def __rmul__(self, other):\r\n               return self * other\r\n           \r\nIn[3]: two = Two()\r\nIn[4]: two * 3\r\nOut[4]: 6\r\nIn[5]: 3 * two\r\nOut[5]: 6\r\nIn[6]: two * torch.tensor(3)\r\nOut[6]: tensor(6)\r\nIn[7]: torch.tensor(3) * two\r\nTraceback (most recent call last):\r\n  File \"/path/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-8-e59deefe7a13>\", line 1, in <module>\r\n    torch.tensor(3) * two\r\nTypeError: mul() received an invalid combination of arguments - got (Two), but expected one of:\r\n * (Tensor other)\r\n      didn't match because some of the arguments have invalid types: (!Two!)\r\n * (float other)\r\n      didn't match because some of the arguments have invalid types: (!Two!)\r\n```\r\n\r\nIs this behavior intended (e. g. for performance reasons)? I would prefer PyTorch returning `NotImplemented` (which might result in raising a `TypeError`) rather than directly raising a `TypeError`."},{"labels":["enhancement",null,null,null,null],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nSo far, it is possible to get a second order gradient by extracting the first-order gradient as a differentiable tensor:\r\nobjective = loss(module, target)\r\ngradients = torch.autograd.grad(objective, module.parameters, create_graph=True)\r\n\r\nHowever, as far as I understand, it is not possible to differentiate through module updates (using optimizers). For example, let say that Z is an independent parameter I want to optimize, and I have a module M with parameter P (P is not Z). I update P as follow:\r\ncost1 = loss( f(P,Z), target1)\r\ncost1.backward()\r\nM_optimizer.step() # P(Z) = P + g1(Z)\r\ncost2 = loss( f(P,Z), target2)\r\ncost2.backward()\r\nM_optimizer.step() # P(Z) = P + g1(Z) + g2(Z)\r\n\r\nAnd now, I want to update Z in order to maximize a meta-objective meta_loss( P(Z), meta-target ). \r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nThis king of meta-update is common in meta-learning approaches, such as MAML (https://arxiv.org/abs/1703.03400). As mentioned in this post (https://discuss.pytorch.org/t/pytorch-implementation-of-maml-that-works-with-module-style-networks/26278) MAML implementations are limited to functional-based structure and gradient descent are done by hand. For more complex models (CNN + LSTM) a user would have to re-implement everything by hands.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nAn optimizer that would change a module's parameters with a differentiable operation, e.g. that would first extract the gradient as a differentiable tensor, and sum the parameter with this differentiable gradient.\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\nAt least (if the solution above is not possible), the possibility to sum a module's parameter with a differentiable tensor (and then the gradient descent can be easily implemented by hands).\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["enhancement",null],"text":"## ðŸš€ Feature\r\nThe current implementation of `torch.unique` does not have a `return_counts` argument, which returns the counts of all unique occurrences in the tensor.\r\n\r\n## Motivation\r\n\r\nThe feature request is related to some users missing this option, e.g. [this thread](https://discuss.pytorch.org/t/how-to-efficiently-perform-averaging-across-predefined-groups-in-a-tensor/27091/2?u=ptrblck). A current workaround is to use something like this:\r\n```\r\nx = torch.tensor([1, 2, 2, 2, 3])\r\nx_unique = x.unique(sorted=True)\r\nx_unique_count = torch.stack([(x==x_u).sum() for x_u in x_unique])\r\n```\r\n\r\nAlso, numpy supports this option, which should be a nice to have for new users. [np.unique](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.unique.html)\r\nAs this might be added as the last argument, no breaking changes would occur.\r\n\r\n## Pitch\r\n\r\nI could add this option to the current `torch.unique` implementation as I've recently added the `return_inverse` option and am still familiar with the code.\r\n\r\n## Alternatives\r\n\r\nAn alternative would be to use the hacky code from above.\r\n\r\nLet me know, if that sound good to you!\r\nAlso a bit unrelated, but we could also add the `return_index` option together to match the numpy function as close as possible, but that's probably another feature request.\r\n\r\nBest,\r\nptrblck\r\n"},{"labels":["enhancement"],"text":"## ðŸš€ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nThe `dim` parameter for `torch.sum` and e.g. `torch.max` do not have the same set of supported input types.  From https://pytorch.org/docs/master/torch.html#reduction-ops :\r\n\r\n> torch.max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor)\r\n> Parameters: dim (int) â€“ the dimension to reduce\r\n\r\nvs.\r\n\r\n> torch.sum(input, dim, keepdim=False, dtype=None) -> Tensor\r\n> Parameters: dim (int or tuple of python:ints) â€“ the dimension or dimensions to reduce\r\n\r\nThis means that reducing a 5D tensor looks like one of the following:\r\n\r\n    t.sum([2,3,4])\r\n    t.view(t.size(0), t.size(1), -1).max(dim=2)[0]\r\n\r\nAnd it would be nice to have the same style available for all reduction operations.\r\n\r\n## Motivation\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nHaving to spell similar operations in very different ways results in less readable code. It's harder to verify visually that the two operations do the equivalent thing.\r\n\r\n## Pitch\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nI would like the multi-index `dim` support for `torch.sum` added to `torch.min`, `torch.max`, etc.\r\n\r\n## Alternatives\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\nThe status quo works for my use case, but it's unclear if that remains true for users that need the second item of the `torch.max` return tuple.\r\n"},{"labels":["enhancement",null,null,null,null],"text":"Now that we have support for tensors with zero in its size, I believe it would be very handy to have support for accepting batches of size 0 in `nn.functional` functions.\r\n\r\nA (non-exhaustive) list of functions that would be good supporting:\r\n- [x] `conv{1-2-3}d`\r\n- [x] `conv_transpose{1-2-3}d`\r\n- [x] `batch_norm`\r\n- [x] `interpolate`\r\n\r\nHandling the losses is a bit trickier, because it generally involves computing a `.mean()`, which results in `NaN` due to 0 / 0 division. I'd expect having a 0 loss for empty batches to make sense, but that's debatable so might be worth postponing this decision."},{"labels":["enhancement",null,null],"text":"## Issue description\r\n\r\nThere is currently no `Kumaraswamy` distribution in `torch.distributions`.\r\nIt would be nice to have! It is already available in Tensorflow Probability.\r\n\r\nI'm happy to contribute it.\n\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw"},{"labels":["enhancement",null],"text":"I tried searching the documentation, but besides sparse matrices (which in most cases would use _more_ space than a dense matrix), I didn't see any tensor types that would take advantage of the ability to save space with the knowledge that the tensor is triangular. This would also save time when performing `matmul()`, I'd imagine. For example, my use case is a symmetric matrix (a modification of the distance matrix (https://en.wikipedia.org/wiki/Distance_matrix), and I only need the upper half without the diagonal, so performing operations on the other elements would be worthless, and saving them would be a waste of space.\r\n\r\nCan such a type be implemented? Apologies if I missed something and it already exists."},{"labels":["enhancement",null,null,null],"text":"## Issue description\r\nThis issue came about when trying to find the cosine similarity between samples in two different tensors. To my surprise `F.cosine_similarity` performs cosine similarity between pairs of tensors with the same index across certain dimension. I was expecting something like:\r\n\r\nhttp://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\r\n\r\nthat is, a cosine similarity measure across all pairs of samples.\r\nI can't really see the use cases of the current implementation of `F.cosine_similarity`. Since it is the special case of getting the diagonal of what I describe or using `F.pairwise_distance` with an extra normalize parameters. Perhaps would be nice to know what are the use cases for the current implementation.\r\n\r\nIn order to mantain  compatibility, I suggest creating an `F.cosine_distance` function and layer similar to:\r\n\r\nhttps://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.cosine.html\r\n\r\nwhich operates in tensors, similar to the sklearn implementation\r\nhttp://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_distances.html\r\n\r\nI don't think this is difficult to implement btw\r\n\r\n## Code example\r\n```\r\ndef cosine_distance(x1, x2=None, eps=1e-8):\r\n    x2 = x1 if x2 is None else x2\r\n    w1 = x1.norm(p=2, dim=1, keepdim=True)\r\n    w2 = w1 if x2 is x1 else x2.norm(p=2, dim=1, keepdim=True)\r\n    return 1 - torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps)\r\n```\r\nExample:\r\n```\r\n>>> m1 = torch.tensor([[0, 1],\r\n                   [0, 1]], dtype=torch.float64)\r\n>>> cosine_distance(m1, m1)\r\ntensor([[0., 0.],\r\n        [0., 0.]], dtype=torch.float64)\r\n>>> cosine_distance(m1)\r\ntensor([[0., 0.],\r\n        [0., 0.]], dtype=torch.float64)\r\n>>> m2 = torch.tensor([[1, 0],\r\n                   [0, 1]], dtype=torch.float64) \r\n>>> cosine_distance(m1, m2)\r\ntensor([[1., 0.],\r\n        [1., 0.]], dtype=torch.float64)\r\n>>> m3 = torch.tensor([[0, 0],\r\n                   [0, 0]], dtype=torch.float64)\r\n>>> cosine_distance(m3)\r\ntensor([[1., 1.],\r\n        [1., 1.]], dtype=torch.float64)\r\n>>> cosine_distance(m3, m3)\r\ntensor([[1., 1.],\r\n        [1., 1.]], dtype=torch.float64)\r\n```\r\nCreation of the layer from here is straight forward. if you think this might be a good idea I would like to make a PR\n\ncc @albanD @mruberry"},{"labels":["enhancement",null,null],"text":"PyTorch version: 0.4.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 8.2.0\r\nCMake version: version 3.11.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n\r\nHello I'm running latest PyTorch on my laptop with AMD [A10-9600p](https://www.amd.com/en/support/apu/amd-series-processors/amd-a10-series-apu-for-laptops/7th-gen-a10-9600p-apu) and it's iGPU that I wish to use in my projects but am not sure if it works and if yes how to set it to use iGPU but have no CUDA support on both Linux(Arch with Antergos) and Win10 Pro Insider so it would be nice to have support for something that AMD supports.\r\n"},{"labels":["enhancement"],"text":"Currently, `torch.nn.functional.interpolate` has dedicated implementation for 1d, 2d and 3d data, as well as for nearest, bilinear and bicubic interpolation. The set of different kernels that are dispatched can be seen [here](https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L2050-L2079).\r\n\r\nThose implementation are all very similar one to the other, and there is a lot of code duplication in there.\r\nCompare for example [nearest](https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialUpSamplingNearest.c#L31-L95) with [bilinear](https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialUpSamplingBilinear.c#L33-L106).\r\n\r\nI believe it is possible to refactor the underlying implementation so that we can have a single C++ / CUDA codepath, with minimal  code duplication.\r\n\r\nThis could be achieved in two independent steps (that could be done at the same time):\r\n- factor out the different kernel computations, so that we have a generic `filter` that is used to compute the interpolation, plus the size of the filter as a struct. For an example, [see how Pillow implements it](https://github.com/python-pillow/Pillow/blob/master/src/libImaging/Resample.c#L9-L83), and then the interpolation coefficients can be generically computed [as in here](https://github.com/python-pillow/Pillow/blob/master/src/libImaging/Resample.c#L236).\r\n- make the interpolate kernel separable (it is already separable in most cases). This means that we can have the user specify the dimensions he wants to interpolate as a list, and in the C++/CUDA code we have a loop over the dimensions. Something like `F.interpolate(image, dim=[-2, -1])` for spatial interpolation, or `F.interpolate(volume, dim=[-3, -2, -1])` for volumetric data. We can have reasonable defaults if `dim` is not specified (that depends on the shape of the input), to keep backwards compatibility.\r\n\r\nThe first point will allow us to fuse the `nearest` / `bilinear` / `bicubic` interpolation modes in a single file, while the second point will fuse `temporal` / `spatial` and `volumetric` into the same function."},{"labels":["enhancement",null],"text":"## feature request description\r\n\r\nThe NLP community is shifting from LSTMs to [Transformers](https://arxiv.org/abs/1706.03762) for a number of NLP tasks.\r\nThis would be great if we could have a packed standard Transformer implementation in the 'nn' package, i.e. a **nn.Transformer**, just like we have a nn.LSTM.\r\n\r\n## Code example\r\n\r\nfairseq-py has a tested implementation of the Transformer encoder and decoder (for LM and NMT) https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py. Wrapping this up in 'nn' should be the most straightforward approach to reimplementing this model.\r\n\r\nThanks!"},{"labels":["enhancement",null,null,null,null],"text":"## Issue description\r\n\r\nPyTorch currently does not QR decomposition with pivots, which is useful in the context of rank identification.\r\n\r\nThis can be done with the current setup using `SVD`, but is much less efficient than a QR decomposition.\r\n\r\nGiven the availability of `orgqr` and `geqrf` as direct calls, I assume that `torch.qr` is implemented using those. Support for [`dgeqp3`](http://www.netlib.org/lapack/explore-html/dd/d9a/group__double_g_ecomputational_ga1b0500f49e03d2771b797c6e88adabbb.html#ga1b0500f49e03d2771b797c6e88adabbb) (QR with pivots) would be a nice addition.\r\n\r\nScipy's implementation of [`sp.linalg.qr`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.qr.html) supports pivoting through the `pivot=True` argument and a call to [`dgeqp3`](http://www.netlib.org/lapack/explore-html/dd/d9a/group__double_g_ecomputational_ga1b0500f49e03d2771b797c6e88adabbb.html#ga1b0500f49e03d2771b797c6e88adabbb)\n\ncc @vishwakftw @SsnL @jianyuh"},{"labels":["enhancement",null,null,null],"text":"Hi,\r\n\r\nAs far as I can tell, PyTorch doesn't have inverses implemented for hyperbolic functions.  These are useful, e.g., for operations on hyperbolic space.  (A notable example being that the distance function on the hyperboloid model of hyperbolic space uses the inverse hyperbolic cosine.)  It's not that hard to implement these functions by hand, but inconvenient.\r\n\r\nThanks,\r\nShawn\n\ncc @VitalyFedyunin @ngimel @mruberry"},{"labels":["enhancement",null,null,null,null],"text":"Thanks for the awesome library! It would be great if PyTorch could support forward-mode automatic differentiation. The main use case is to compute a Jacobian-vector product. I tried using [this trick](https://j-towns.github.io/2017/06/12/A-new-trick.html) that simulates forward-mode autodiff by running reverse-mode twice, but it causes my GPU to run out of memory with AlexNet. [HIPS/autograd](https://github.com/HIPS/autograd) supports this operation, and it would be really nice if PyTorch could as well. Thanks!\n\ncc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen"},{"labels":["enhancement",null,null],"text":"**Proposed** - add `torch.geevj` and `torch.syevj` calls, which hook into cuSolver's jacobi-based eigensolvers. These functions could operate on matrices or batches of matrices (that are smaller than 32x32).\r\n\r\n**Long** - there are two issues:\r\n\r\n- For small matrices (e.g. <32x32 matrices), `eig` and `symeig` are much slower for cuda matrices than cpu matrices.\r\n- It would be useful to run `eig`/`symeig` on batches of (small) matrices.\r\n\r\n```\r\nIn [8]: %timeit ac.symeig()  # CUDA 32x32 matrix\r\n19.1 ms Â± 840 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [9]: %timeit a.symeig()  # CPU 32x32 matrix\r\n40.8 Âµs Â± 672 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)\r\n```\r\n\r\ncuSolver has Jacobi-based eigensolvers (`geevj` and `syevj`), and batched versions of these solvers as well. They are faster for small/medium sized matrices. The only limitation of these functions is that the batched version of `geevj` and `syevj` only operate on matrices smaller than 32x32.\r\n\r\nIt would be nice to have calls in torch (`torch.geevj` and `torch.syevj`) that hook into these functions.\r\n\r\ncc/ @jacobrgardner @Balandat @darbour"},{"labels":["enhancement",null,null],"text":"We are working on to increase supports for sparse tensor. Currently we have [summarized current state of sparse tensor](https://github.com/pytorch/pytorch/issues/9674) and listed out [sparse ops to support](https://github.com/pytorch/pytorch/issues/8853). We would like to collect sparse tensor use cases to facilitate the design decisions and prioritize TODO list according. It will be very helpful if you can post your use cases and desired sparse ops here or at the [PyTorch Forum](https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047). Thanks! \r\n\r\nI find these questions useful when writing use cases:\r\n- Where do I need sparse tensor? During training deep learning model?\r\n- Do I need autograd support for the sparse ops?\r\n\r\nA possible example will be:\r\n\r\nI am training model that has `mul(Sparse, Dense)` ops. I would like to have its forward and backward. I know there will be a dense gradient at the backward of `mul`, so here I am asking for a special kind of `mul` ops (called `sparse_mul`) that returns a sparse grad tensor and only keep the nnz's gradients.\r\n"},{"labels":["enhancement",null,null,null,null],"text":"Add matrix power as implemented by [numpy.linalg.matrix_power](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_power.html), matrix exponential as implemented by [scipy.linalg.expm](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.linalg.expm.html), and matrix logarithm as implemented by [scipy.linalg.logm](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.linalg.logm.html).\r\n\r\n\r\n- [x] matrix_power\r\n- [x] matrix_exp\r\n- [ ] matrix_log\r\n- [ ] matrix_sqrt\r\n\r\ncc @ezyang @gchanan @zou3519"},{"labels":["enhancement",null,null],"text":"\r\n## Issue description\r\nAre there any methods to get the conv module input params from the â€˜grad_fnâ€™ in the newest version of pytorch?\r\n\r\nin pytorch 0.2, we can get convolution parameters, such as kernel_size, padding, and stride from grad_fn, this way is very useful to convert the trained model to caffe prototxt.\r\nbut, in pytorch 0.4, the ThnnConv2DBackward and CudnnConvolutionBackward  do not support this way.\r\n\r\nfor example,in pytorch version 0.2,\r\n\r\n## Code example\r\nmodle = nn.Conv2d(in_channels=3 out_channels=32, kernel_size=3,stride=2, padding=1, dilation=1))\r\ny = modle(x)\r\n\r\nThen, we can get the input paramâ€™s values like kernel_sizeã€strideã€padding value use y.grad_fn.kernel_sizeï¼Œy.grad_fn.padding,â€¦â€¦\r\n\r\nbut now, in pytorch 0.4, it report that \r\n\"AttributeError: 'CudnnConvolutionBackward' object has no attribute 'kernel_size'\"\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["enhancement",null,null],"text":"Would like to see a checkpoint manager taking a module, dataloader, and optimizer at construction that would be able to:\r\n- Save the module\r\n- Save the optimizer (including state)\r\n- Save the dataloader state (be able to continue from the middle of an epoch)\r\n- Save RNG states (CPU, GPU, including data sampler)\r\n- Call every iteration and automatically checkpoint every X minutes (for preemptible instances)\r\n- Automatically keep best N checkpoints (Remove rest)\r\n- Save some kind of visualization state (like can be done with TensorBoard)"},{"labels":["enhancement",null,null,null],"text":"This proposes an algorithm for computing the result type of a mixed-type operation like `torch.add`. The function will be exposed to Python via `torch.result_type(*tensors)`. This proposal covers the default result type calculation; some operators may override the default behavior.\r\n\r\nThe similar NumPy function is [`numpy.result_type`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.result_type.html).\r\n\r\n**Wrapped numbers**\r\n\r\nA Tensor is a considered a \"wrapped number\" if it is auto-wrapped from a C++ or Python number type. Integer types are wrapped as 0-dim int64 tensors and floating-point types are wrapped as 0-dim double tensors. All wrapped numbers are 0-dim tensors, but not all 0-dim tensors are wrapped numbers. In general, wrapped numbers behave like normal 0-dim tensors, except they are handled specially in the `torch.result_type` calculation.\r\n\r\nFor example, in `tensor + 5` and `torch.add(tensor, 5)`, `5` gets wrapped as a 0-dim `torch.int64` (a \"wrapped number\").  However, `torch.add(tensor, torch.tensor(5))` does not have a wrapped number because `torch.tensor(5)` is an explicit construction. Wrapped number status does not propagate to returned tensors. Returned tensors are never considered wrapped numbers.\r\n\r\n**Result type calculation**\r\n\r\nEach operand has a category (integer or floating-point) and a priority:\r\n\r\n1) Tensors of dimension 1 or larger\r\n2) Tensors of dimension 0 that are not wrapped numbers\r\n3) Wrapped numbers\r\n\r\nBy default, only the highest priority operands participate in the type promotion logic. Lower priority operands participate if their category (e.g. floating-point) is of higher rank than any higher priority operands (e.g. integers).\r\n\r\nIn pseudo-code the result-type calculation is:\r\n\r\n```\r\ndef result_type(*args):\r\n  return promote_types(infer_scalar_type(arg) for arg in args if participates(arg, args))\r\n\r\ndef infer_scalar_type(arg):\r\n  if is_wrapped_number(arg):\r\n    return torch.get_default_dtype() if is_floating_point(arg) else torch.int64\r\n  else:\r\n    return arg.dtype\r\n\r\ndef participates(arg, args):\r\n  if priority(arg) >= max(priority(other) for other in args):\r\n    return True\r\n  if category(arg) > max(category(other) for other in args if priority(other) > priority(arg)):\r\n   return True\r\n  return False\r\n\r\ndef priority(arg):\r\n  if arg.dim() > 0: return 3\r\n  elif not is_wrapped_number(arg): return 2\r\n  else: return 1\r\n\r\ndef category(arg):\r\n  if is_floating_point(arg): return 2\r\n  else: return 1\r\n```\r\n\r\nExamples (assuming default float32 tensor dtype):\r\n\r\n```\r\nrandn(3, dtype=float32) * 5 -> float32\r\ntensor([0, 0, 1], dtype=uint8) + 1 -> uint8\r\ntensor([0, 0, 1], dtype=uint8) + 1000 -> uint8  # NOTE: integer overflow\r\ntensor([0, 0, 1], dtype=uint8) + 5.5 -> float32 (default tensor dtype)\r\ntensor([0, 0, 1], dtype=uint8) + tensor(5.5, dtype=double) -> double\r\n\r\nrandn(3, dtype=float32) + tensor(5.5, dtype=double) -> float32\r\ntensor(5.5, dtype=float16) + 2.2 -> float16\r\ntensor(5.5, dtype=float16) + 100000 -> float16 # NOTE: inf\r\ntensor(5.5, dtype=float16) + tensor(100000.0) -> float32 (default tensor dtype)\r\n```\r\n\r\nAppendix:\r\n\r\n**Why don't we use NumPy's behavior?**\r\n\r\nNumPy's result_type logic has two undesirable behaviors. The first is that requires examining the actual value of scalars (and 0-dim arrays). This would require a host-device synchronization for 0-dim CUDA tensors. The second is that it often up-promotes 0-dim arrays to float64 or int64. For example:\r\n\r\n```\r\ntype(np.array(4.0, dtype=np.float32) + 1) -> np.float64\r\ntype(np.array(0, dtype=np.uint8) + 1) -> np.int64\r\n```"},{"labels":["enhancement",null,null],"text":"**TL;DR:**  Implementing block-sparse operations for faster matrix-multiplication. \r\nIs this something worth adding to PyTorch?\r\n\r\nGoals:\r\n1. Faster matrix-multiplication by taking advantage of block-sparsity\r\n2. Improve running time of large-scale LSTMs - Word Language or Sentiment Analysis Models\r\n3. Based on https://blog.openai.com/block-sparse-gpu-kernels/\r\n\r\nNew Features:\r\n1. LinearBSMM Layer â€“ A plug & play replacement for the standard linear layer where the user specifies the block size and sparsity pattern\r\n2. Helper functions to generate different sparsity patterns â€“ Random, Barabasi Albert, Watts Strogatz\r\n\r\nExamples:\r\n**Original Linear Layer**\r\n```\r\nm = nn.Linear(20, 30)\r\ninput = torch.randn(128, 20)\r\noutput = m(input)\r\n```\r\n**Block Sparse Linear Layer**\r\n```\r\nD = 20\r\nN = 30\r\nm = linearBSMM.random(D, N, p=0.25, block_size=32)\r\ninput = torch.randn(128, D)\r\noutput = m(input)\r\n```\r\nOR\r\n```\r\nD = 20\r\nN = 30\r\nsparsity_pattern = linearBSMM.random(D, N, p=0.25, block_size=32)\r\nm = nn.LinearBSMM(D, N, sparsity_pattern)\r\ninput = torch.randn(128, D)\r\noutput = m(input)\r\n```"},{"labels":["enhancement",null,null,null],"text":"Hello there, \r\n\r\nfrom the doc of `nn.LSTM`, the forward method outputs `output, (h_n, c_n)`. \r\nCurrently `output` contains the \"output features (h_t) from the last layer of the LSTM, for each t\". I'd find it interesting if output also returned the cell state from the last layer for each t.\r\n\r\nIndeed, when trying to feed a LSTM with its predictions (for time series) you need to also provide the \"new initial\" hidden state, which is *not* (`h_n`, `c_n`).\r\n\r\nFor example, for a sequence of length n, if you want to predict values t+1 for each t, you just have to input a null hidden state and the LSTM will output the predicted values. However, if now you want to predict values t+2 from the previously predicted values, you need to input a non-zero hidden state, which is (`h_1`,` c_1`) (compared to `h_n`, `c_n`).\r\n\r\nYou can easily get `h_1` from `output`, but not `c_1`. Then you have to use LSTMCell which is much slower. Not that this is not a problem for RNN / GRU.\r\n\r\nI hope I'm making myself clear in the example and request.\r\n\r\nThanks\r\n\r\n"},{"labels":["enhancement",null,null,null,null,null,null],"text":"Facebook published reference code of fast NumPy randomized SVD (based on power iteration): https://github.com/facebook/fbpca/blob/master/fbpca.py#L1503\r\n\r\nOnly top eigenvectors are useful e.g. for spectral clustering, it would be nice to have it supported for CUDA."},{"labels":["enhancement",null,null],"text":"For some application, I need to get gradients for each elements of a sum.\r\nI am aware that this issue has already been raised previously, in various forms ([here](https://discuss.pytorch.org/t/gradient-w-r-t-each-sample/1433/2), [here](https://discuss.pytorch.org/t/efficient-per-example-gradient-computations/17204), [here](https://discuss.pytorch.org/t/quickly-get-individual-gradients-not-sum-of-gradients-of-all-network-outputs/8405) and possibly related to [here](https://github.com/pytorch/pytorch/issues/1407)) \r\nand has also been raised for other autodifferentiation libraries (some examples for TensorFlow: [here](https://github.com/tensorflow/tensorflow/issues/675), long discussion [here](https://github.com/tensorflow/tensorflow/issues/4897))\r\n\r\nWhile the feature does exists in that there is a way to get the desired output, \r\nI have not found an efficient way of computing it after investing a significant amount of time.\r\n\r\nI am bringing it up again, with some numbers showing the inefficiency of the existing solutions I have been able to find.\r\n\r\nIt is possible that the running time I observed is not an issue with the existing solutions but only with my implementation of them, \r\nor that my attempts have been misguided and that a simpler solution exists.\r\nIf there is a way to make that operation in an efficient way, I would love to know about it.\r\n\r\nI can spend more time working on this issue, but as of now I do not know of a way forward.\r\nThe following contains a description of the desired feature, existing \"workaround\" and some evaluation of their performance.\r\n\r\n---\r\n\r\n## Feature description:\r\n\r\nGiven an objective that is a sum of functions, `f(x) = f_1(x) + ... + f_N(x)`, \r\na _simple to use_ and _efficient_ way to compute the gradient with respect to `x` for each of the individual function `f_n`, i.e., getting\r\n`[âˆ‡f_1(x), ..., âˆ‡f_N(x)]`\r\n\r\nIf there already is a way to do so, an example would be very nice.\r\n\r\n(I understand that this would involve additional memory overhead if `N` is large, as pointed out [here](https://discuss.pytorch.org/t/quickly-get-individual-gradients-not-sum-of-gradients-of-all-network-outputs/8405/2) - my setting is not bound by memory but by time)\r\n\r\n## Use case:\r\n- Computing approximate second-order information (Generalized Gauss-Newton type of algorithm).\r\n- Computing gradient statistics such as the variance at a given point.\r\nSome papers based on those ideas: [here](https://pdfs.semanticscholar.org/42e2/1cd78f578fa6ce61b06b99848697da85ed76.pdf), [here](http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf)\r\n\r\n## Existing workarounds:\r\n- The `naive` implementation: do the forward pass in batch up to the last part of the function to minimize overhead, and then call backward on each individual `f_n(x)`.\r\n- Goodfellow showed how to recover the individual gradients from gradients with respect to the activation functions in the case of feed-forward neural networks [here](https://arxiv.org/abs/1510.01799). This requires an additional derivation and performs the linear parts of the transformation in Python instead of C, making it scale badly. \r\n- What I call \"Multiple Models\": Define copies of the parameters `x_1, ..., x_N` and compute `grad(f_1(x_1) + ... + f_N(x_N))`. This ensures that the gradients are not accumulated, however it scales poorly. Adapted from the original formulation [here](https://github.com/tensorflow/tensorflow/issues/4897#issuecomment-290997283)\r\n\r\n\r\n## Evaluation of existing workarounds\r\nI have tried to make the simplest example of a sufficiently complex problem where this becomes an issue.\r\nThe code to reproduce these is available [here](https://github.com/fKunstner/fast-individual-gradients-with-autodiff).\r\n\r\nRunning on a simple Multi-Layer Perceptron, I am comparing the running time of\r\n* `full`: computing the gradient of the full objective function\r\n* `naive`, `goodf`, `multi`: computing the gradient of the sum by first computing individual gradients with the methods described above, and then taking the sum[1].\r\n\r\nI am taking a sum over `N = 1000` elements, and tried different network configurations;\r\n```\r\n# D: Dimension of input and of each Layer\r\n# L: Number of hidden layers\r\n```\r\nFor a (relatively) wide and shallow network (`D, L = 500, 1`), I get the following running time\r\n```\r\nFull  :  0.04s\r\nnaive : 15.42s\r\ngoodf :  4.42s\r\nmulti :  1.89s\r\n```\r\nFor a narrower and deeper network (`D, L = 100, 10`) I get\r\n```\r\nFull  :  0.03s\r\nnaive : 11.40s\r\ngoodf :  1.70s\r\nmulti :  1.60s\r\n```\r\n\r\nWhile `goodf` and `multi` definitely are improvements on the naive method, they are still ~50 time slower than simply computing the sum. I would not expect any method that stores individual gradients to perform as well as a method that can simply throw them away, but they are essentially doing the same operations so it should be possible to do better.\r\nWith those numbers, training a small-ish neural network that would normally take ~10 minutes would take 10 hours, making experimentation very difficult.\r\n\r\nFrom the [information I have been able to find](https://discuss.pytorch.org/t/how-the-hook-works/2222/2), it seems difficult to get access to the un-aggregated gradients.\r\nIt is a bit frustrating to know that the backward pass on the function computes the sum in almost no time, and all that is needed for this feature is a way to intercept elements of this sum.\r\n\r\nA hook that would allow to store individual gradients on the fly in some list or tensor - maybe with the use of those hooks or by defining a new sum function that intercepts gradients - would be amazing, but I do not understand the codebase and/or AD to do it. \r\nSome pointers in that direction would already be a big help\r\n\r\n[1] _Note: I am not looking for an expensive way of computing the sum by computing individual gradients - I am taking the sum in the end so that all methods do the \"same thing\", but what is really needed is the intermediate matrix containing individual gradients_\r\n\r\n\r\n\r\n\r\n\n\ncc @ezyang @SsnL @albanD @zou3519"},{"labels":["enhancement",null,null],"text":"Right now [Upsample](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Upsample) in onnx supports fixed (calculated for specific input size) scale factors. Which are [calculated](https://github.com/pytorch/pytorch/blob/0a11018db690b02a52dd25b2470fdde794a8fac6/torch/onnx/symbolic.py#L498) during graph tracing based on current input and output sizes.\r\n\r\nAt the same time [F.upsample](https://pytorch.org/docs/master/nn.html?highlight=pad#torch.nn.functional.upsample) allows to specify exact output size, which is very convenient if we want to evaluate models with backward connections (like [Feature Pyramid Network](https://arxiv.org/pdf/1612.03144.pdf)) on inputs with varying sizes.\r\n\r\nCould we extend onnx Upsample operator to support desired output size (exactly like in F.upsample), taken as additional input to the operator?\r\n\r\nSimple example to check (this should export the graph with upsample taking its output size dynamically as h1.size()[-2:]):\r\n```\r\nclass SimpleNet(nn.Module):\r\n    def __init__(self):\r\n        super(SimpleNet, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1)\r\n        self.conv2 = nn.Conv2d(3, 8, kernel_size=3, stride=2, padding=1)\r\n        \r\n    def forward(self, x):\r\n        h1 = self.conv1(x)\r\n        h2 = F.upsample(x, size=h1.size()[-2:], mode='bilinear', align_corners=False)        \r\n        h = torch.cat([h1, h2], dim=1)\r\n        return h\r\n\r\n\r\nmodel = SimpleNet()\r\nsave_path = '/Users/aachigorin/temp/test_bilinear_onnx_export.txt'\r\nx = Variable(torch.randn(1, 3, 10, 10)).cpu()\r\ntorch_out = torch.onnx._export(model, x, save_path, export_params=True, verbose=True)\r\nprint('finished onnx export')\r\n\r\nmodel = onnx.load(save_path)\r\nprint('check = {}'.format(onnx.checker.check_model(model)))\r\nprint(onnx.helper.printable_graph(model.graph))\r\n```"},{"labels":["enhancement",null,null],"text":"## Issue description\r\nStreams in THC are thread local, so in backward, since autograd creates its own threads, kernels are always put on the default streams, and don't respect stream that was set previously. \r\n## Code example\r\n```\r\nimport torch\r\n\r\na=torch.Tensor(128,512).cuda().uniform_()\r\n\r\nmodel = torch.nn.Linear(512,512).cuda()\r\n\r\nfwd_stream = torch.cuda.Stream()\r\nbwd_stream = torch.cuda.Stream()\r\n\r\n\r\nfor i in range(3):\r\n   with torch.cuda.stream(fwd_stream):\r\n       out = model(a).sum()\r\n   with torch.cuda.stream(bwd_stream):\r\n       out.backward() #kernels are actually on the default stream\r\n   a.detach()\r\n```\r\n\r\nPytorch, current master\r\n"},{"labels":["enhancement",null,null],"text":"Given the rising support of low-precision networks (e.g., NVIDIA and Google) it would be nice to have support for INT8 model training and inference. \r\n\r\nhttps://www.tensorflow.org/performance/quantization#quantization_training_with_tensorflow\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov"},{"labels":["enhancement",null,null],"text":"The [PyTorch 0.4 Migration Guide](https://pytorch.org/2018/04/22/0_4_0-migration-guide.html), simplifies writing device-agnostic code as follows:\r\n\r\n```python\r\n# at beginning of the script\r\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n\r\n...\r\n\r\n# then whenever you get a new Tensor or Module\r\n# this won't copy if they are already on the desired device\r\ninput = data.to(device)\r\nmodel = MyModule(...).to(device)\r\n```\r\n\r\nHowever, this is still not clean.\r\n\r\nIdeally, we would like PyTorch to move everything over to the GPU, if it's available...\r\nmuch like TensorFlow.\r\n\r\nI tried setting the global tensor type to a cuda tensor using the ```torch.set_default_tensor_type()``` method.\r\n\r\nHowever, there are some fundamental problems with setting the default tensor type.\r\n\r\n* Dataloaders give normal (non-cuda) tensors by default. They have to be manually cast using the `Tensor.to()` method.\r\n\r\n* Many methods are simply not implemented for `torch.cuda.*Tensor`. Thus, setting the global tensor type to cuda fails.\r\n\r\n* Conversions to numpy using the `numpy()` method arenâ€™t available for cuda tensors. One has to go `x.cpu().numpy()`.\r\nAlthough this chain is agnostic, it defeats the purpose.\r\n\r\n<hr>\r\n\r\nI find that I use methods like `.to(device)` and `.cpu()` far too often in my projects.\r\n\r\nIn my view, _it makes the code more verbose than it needs to be_ and makes it just a little harder to read.\r\n\r\n**I think that there is room for a global `use_gpu` flag that can enable developers to run the entire subsequent code in the GPU, where required.**\r\n\r\nSpecifically, my request is the following:\r\n\r\n#### 1. Abolish need for the `.to(device)` suffix:\r\nCircumvent it by letting the developer set the device using a global method like `torch.set_default_device()` or a convinience method/flag like `use_gpu`\r\n\r\nThen, whenever, an error is encountered because a CUDA tensor is expected in place of a regular tensor or vice-versa, automatically cast the tensor to the expected device.\r\n\r\nAdditionally,\r\na. Move `nn.Module` automatically to the default device.\r\nb. Move the yield of `DataLoader`s to the default device.\r\n\r\nPrevent the need to manually cast to the default device.\r\n\r\n#### 2. Add the numpy() method to cuda tensors:\r\nThe existing way is to move the tensor to cpu first.\r\nThus, we have `x.cpu().numpy()`, which is agnostic but redundant.\r\n\r\n#### 3. Use GPU by default if available:\r\nPyTorch is built from the ground up with the Deep Learning community in mind.\r\n\r\nWith most Deep Learning done on GPUs, they be considered as the default device automatically.\r\n\r\n_Let PyTorch give first preference to the GPU._"},{"labels":["enhancement",null,null,null],"text":"Per comment from @apaszke in #7439, there is no need to reopen the underlying file for every operation. Instead we can keep the file open and only acquire/release the shared/exclusive lock. "},{"labels":["enhancement",null,null,null,null],"text":"## Issue description\r\n\r\nSparse tensors do not currently support indexing (neither `gather`, nor `select_index`). This could be handy for e.g. embedding categorical features with non-contiguous vocabulary. \r\n\r\nSidenote: a similar result can be achieved via bucketization (see #7284), without using sparse tensors.\r\n\r\n## Code example\r\n\r\n```\r\n>>> import numpy as np\r\n>>> import torch\r\n>>> vocab = [102, 104, 2, 103, 0, 101, 3]\r\n>>> lookup_table = torch.sparse.IntTensor(\r\n...     torch.tensor(np.atleast_2d(vocab)), \r\n...     torch.arange(len(vocab), dtype=torch.int), \r\n...     torch.Size((max(vocab) + 1, )))\r\n...\r\n>>> lookup_table[102]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: Sparse tensors do not have strides.\r\n```\r\n\r\n## System Info\r\n\r\n```\r\nPyTorch version: 0.5.0a0+3e785d5\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.3\r\nGCC version: Could not collect\r\nCMake version: version 3.9.4\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.2)\r\n[pip3] torch (0.5.0a0+3e785d5)\r\n[pip3] torchvision (0.2.1)\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torch                     0.5.0a0+3e785d5           <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n"},{"labels":["enhancement",null,null],"text":"I am trying to implement Dynamic Samplers in Pytorch. This seems impossible with the existing train_loader api's. As the weighted the samplers only allow weights to be set once. I was thinking that weights could change based on the loss value for each and individual data point. This can be done without train_loader API but I think this a feature that could be included in a future release. \r\n\r\nSomething as simple as Loss Based sampling can't be done right now.\r\n"},{"labels":[null,"enhancement",null,null,null],"text":"## Issue description\r\n\r\nBoth the 0.4.0 version and the `master` version are missing a bucketization operation, i.e. an operation which given a 1-D tensor of values, and another 1-D tensor of bucket boundaries, return a new tensor where each value is substituted for the index of the corresponding bucket.\r\n\r\nThe operation seems to be [available](https://github.com/pytorch/pytorch/blob/master/caffe2/operators/one_hot_ops.cc#L90) in Caffe2, but I'm not sure if the code can be reused in PyTorch.\r\n\r\nAlternatively, PyTorch could provide a variant of [`searchsorted`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.searchsorted.html) so that the users can easily write O(log n) bucketization.\r\n\r\n## Code example\r\n\r\nA trivial implementation might be something like:\r\n\r\n```python\r\ndef bucketize(tensor, bucket_boundaries):\r\n    result = torch.zeros_like(tensor, dtype=torch.int32)\r\n    for boundary in bucket_boundaries:\r\n        result += (tensor > boundary).int()\r\n    return result\r\n```\r\n\r\n```python\r\n>>> bucketize(torch.tensor([-3, 1, 2, 4]), bucket_boundaries=torch.tensor([2]))\r\ntensor([ 0,  0,  0,  1], dtype=torch.int32)\r\n```\r\n\r\nN.B. the handling of bucket endpoints could be implemented differently.\r\n\r\n## System Info\r\n\r\n```\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.3\r\nGCC version: Could not collect\r\nCMake version: version 3.8.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.2)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n\n\ncc @ezyang @gchanan @zou3519"},{"labels":["enhancement",null,null],"text":"## Issue description\r\n\r\nThere are a slew of new AI hardware accelerators available that can beat the GPUs handily either in terms of raw performance or performance per Watt. There are also many new efforts in analytic and sensor fusion hardware acceleration that are optimizing tensor processor for specific tasks using specific number systems, particularly for the embedded and robotics space. \r\n\r\nFor Pytorch to be future proof, it would be beneficial if Pytorch provides a more general purpose interface to connect to hardware acceleration alternatives.\r\n\r\nWe would like to collaborate with other researchers that are interested in AI research in Deep Reinforcement Learning that share our desire to connect Pytorch to optimized robotic brain and spine alternatives that look nothing like GPUs.\r\n\r\n\n\ncc @ezyang"},{"labels":["enhancement",null],"text":"Hey there pyTorch Community,\r\n\r\nI am currently working with RNNs and realized, that it would be great to have a norm argument for RNNCells which enables LayerNormalization for them. I implemented it for the LSTMCell and it should be straight forward to apply to the other ones as well.\r\nI think it would be worthwhile to implement it and I would be happy to do so, as it allows first time users to utilize a layer norm without going down to torch/nn/functions_/rnn.\r\nWhat do you think?\r\n\r\nCheers,\r\n\r\nJendrik"},{"labels":["enhancement",null,null],"text":"The Structural Similarity Index (SSIM) is generally considered to be a milestone in the recent\r\nhistory of Image Quality Assessment (IQA). \r\n\r\nIt would be nice to see in-build **SSIM/MS-SSIM** function in pytorch."},{"labels":[null,"enhancement"],"text":"It would be really useful to have unpooling layers in Caffe2, much like torch.nn.MaxUnpool2d etc in PyTorch.\r\n"},{"labels":["enhancement",null,null,null,null],"text":"In the realm of image processing, Circular Convolution is common used because it is suitable to do FFT. \r\nCircular Convolution means that firstly padding the tensor with circular boundary and then do the convolution. It works like _scipy.ndimage.filters.convolve(x,ker,mode='wrap')_ in Scipy or _imfilter(x,ker,'circular','conv')_ in Matlab. \r\nUp to now, Pytorch does not support this operation. In _torch.nn.functional.con3d_, only zero padding are supported.\r\nAn alternative solution is padding the tensor first. However, in _torch.nn.functional.pad_, only \"constant\", \"replicate\" and \"replicate\" modes are supported. \r\nPlease do consider this request as the circular convolution is a very basic operation in image processing and a fast implementation is needed. \r\nThank you!\r\n\r\n\r\n"},{"labels":["enhancement",null,null,null,null],"text":"I'm working on a model that requires computing what's currently implemented in `torch.bmm`, with the difference that the first argument is a batch of sparse matrices rather than dense matrices. Since `torch.bmm` has nice autodiff and gpu support, it's the closest operation I found in pytorch that fits my current needs and I'm converting my sparse tensors to dense tensors to use it, but it's not optimal and it'd be very useful to have a sparse x dense bmm.\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @vincentqb"},{"labels":["enhancement",null,null],"text":"Feature:\r\nMake all Tensors have an optional list of dimension names like ['B','C','H','W']\r\nThen the broadcasting in binary operations like\r\na + b\r\ncan be done according to the names (if they both have named dimensions). For example, if Tensor a has a list ['B','C','H','W'] and b has list ['C'] , then the expected result is efficiently equivalent to\r\na + b.view([1,-1,1,1])\r\n\r\nThis will make the broadcasting safe and very convenient to use.\r\nCurrently, because broadcasting is implicit, if something goes wrong it is a luck if a dimension mismatch happens somewhen later on or there is an out of memory error thrown.\r\n\r\nI think it could further ease the writing of a generic code that has to work with different number of spatial dimensions (e.g. fully connected layers and conv layers), debug problems, etc. Maybe the named dimensions method be also adopted in functions to allow for the syntax like\r\nlog_softmax(axis='C')\r\n\r\nWhy the usual axis number is not sufficient? The axes are hardwired to the dimensionality. For example, the affine weight in BatchNorm is 1D, and its dimension 0 is by default not matching to the dimension 0 in e.g. the input data. So there going to be lots of these .view() commands and if you expect the input with or without spatial dimensions it is going to be more pain.\r\n\r\nNow consider the code using named dimensions. It will pretty much work if you change the data layout (how it is linearized in the memory). Or if you increase the number of dimensions. For example, you want to extend to a siamese (or 4-stream) network processing, for which it is convenient to use 5D Tensors with dimensions  'instance', 'stream', 'channels','width','height'. Suddenly 'channels' becomes dimension number 2 instead of 1, but if dimensions are matched by names there is no problem.\r\n\r\n"},{"labels":["enhancement",null,null,null,null,null],"text":"The 2D grid interpolator function 'grid_sample' only does bilinear sampling from the source image at the moment. It would be useful to have 'nearest' and other samplings available. The function call does take 'mode' as a parameter but only bilinear sampling seems to have been implemented. It would especially be useful if we have to use 'grid_sample' for class label maps for segmentation, and the grid (source indices) is a Variable.\r\n\n\ncc @jlin27"},{"labels":["enhancement",null,null],"text":"In the release phase of the code of our article : Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge ( https://openreview.net/pdf?id=By4HsfWAZ ) we have implemented a Gaussian grid sampling scheme :  \r\nhttps://github.com/pajotarthur/pytorch/blob/master/aten/src/THNN/generic/SpatialGridSamplerGaussian.c\r\nhttps://github.com/pajotarthur/pytorch/blob/master/aten/src/THCUNN/SpatialGridSamplerGaussian.cu\r\n\r\nDoes it make sense to add it to the main pytorch repo ? \n\ncc @albanD @mruberry"},{"labels":["enhancement",null,null],"text":"The default is a dtype name, but we find the default via the Type; we should use dtypes because this won't work if the Types aren't compiled.\r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"I have a dtype, I want the byte-ified version of it (if it was a CPU float dtype, I want CPU byte dtype; if it was GPU float dtype, I want GPU byte dtype). In ATen I can use toScalarType. The new Python-side dtype doesn't have this.\r\n\r\nA generalized version of this ticket is to ensure API parity between ATen Type, and PyTorch dtype. (Maybe we should rename ATen type so that the names line up too.)\n\ncc @yf225"},{"labels":["enhancement",null,null],"text":"Linear algebra operations such as `svd()` and `symeig()` only support a single matrix input (e.g.,  a matrix of size `M x N`).   I request to extend these functions to support running these operations on batches in parallel on a single GPU machine.  That is, the input will take the form of `B x M x N` sized tensors and the function will calculate `B` operations in parallel on a GPU.\r\n\r\n**Simpler feature request**: Given that It seems that they support batch QR decomposition, can you implement the batch version of `torch.qr()`?\r\nhttp://icl.cs.utk.edu/projectsfiles/magma/doxygen/group__group__orthogonal__batched.html\r\n\r\nGiven batch QR, I can compute batch `torch.symeig()` using [the famous QR algorithm](https://en.wikipedia.org/wiki/QR_algorithm). \r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"Feature Request: Please consider adding a floating point operations calculator for computational graph operations. \r\n\r\nWe'd like to use it for the deep learning models.\n\ncc @ezyang @gchanan @zou3519 @VitalyFedyunin @ngimel"},{"labels":["enhancement"],"text":"PyTorch includes avx_mathfun.h, which includes an 8-way vectorized implementation of `exp` (exp256_ps). We'd like to use that to implement a vectorized tanh function (accessible from torch using `torch.tanh`). Currently the tanh function is not vectorized.\r\n\r\nTo get started, look into how other vectorized functions, like cadd, are implemented:\r\n\r\nActual implementation:\r\nhttps://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/vector/AVX2.c\r\nCode to figure out whether AVX2 is available dynamically and dispatch:\r\nhttps://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/generic/THVectorDispatch.c#L47\r\n\r\nCurrent place where tanh (non-vectorized) is created (you will need to modify this to do correct AVX2 dispatch, and may not be able to use the macro that currently defines it):\r\nhttps://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/generic/THTensorMath.c#L3081\r\n\r\n@tschrager\r\n\r\n\r\n"},{"labels":["enhancement",null,null,null],"text":"I think something similar to Tensorflow's extract_glimpse function would be useful. I'd like to extract a single patch from an image, but I'd like to do it to a batch of images at once with control over where the patch is centered on each image. This is useful in models that select individual patches of images/feature maps to focus on.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/image/extract_glimpse\r\n\r\nThere doesn't seem to be a way to do this in Pytorch without a loop which is a bit slow. Or is there?\n\ncc @fmassa @vfdev-5"},{"labels":["enhancement",null,null],"text":"When an input parameter ***clearly*** has zero gradient (not due to numerical coincidence), the current ATen guideline requires the backward function to return a zero tensor. Since there is no special treatment of zero tensors, autograd engine will still trace back the entire graph supporting that variable. This can happen easily when ATen functions have multiple outputs (e.g. many double backward functions). For example, \r\n\r\n```python\r\ni = deep_nn_1(...)\r\nw = deep_nn_2(...)\r\no = conv(i, w)\r\nloss = autograd.grad(o.sum(), i, create_graph=True).sum()\r\nloss.backward()\r\n```\r\n\r\nIn conv double backward, `gI` only depends on `gO` and `ggW`, and `gW` only depends on `gO` and `ggI`. In this case, `ggW` is zero when doing the double backward, which means that `gI` should be zero and `deep_nn_2` doesn't need to be traversed at all. However, ATen conv double backward still [would output a zero `gI`](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Convolution.cpp#L686-L688) even if it already has the check on `ggW.defined()` to compute `gI`. Then the autograd engine will go through `deep_nn_2` unnecessarily. \r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["enhancement"],"text":"PyTorch includes avx_mathfun.h, which includes an 8-way vectorized implementation of `exp` (exp256_ps). We'd like to use that to implement a vectorized sigmoid function (accessible from torch using `torch.sigmoid`). Currently the sigmoid function is not vectorized.\r\n\r\nTo get started, look into how other vectorized functions, like cadd, are implemented:\r\n\r\nActual implementation:\r\nhttps://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/vector/AVX2.c\r\nCode to figure out whether AVX2 is availiable dynamically and dispatch:\r\nhttps://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/generic/THVectorDispatch.c#L47\r\nCode that implements non-vectorized default:\r\nhttps://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/generic/THVectorDefault.c#L37\r\n\r\nCurrent place where sigmoid (non-vectorized) is created (you will need to modify this to do correct AVX2 dispatch):\r\nhttps://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/generic/THVectorDefault.c#L238\r\n\r\n@vedanuj\r\n\r\n\r\n\r\n"},{"labels":["enhancement",null,null],"text":"By default, sequences packed in `PackedSequence` will be sorted by length.\r\nHowever in some cases where a datum contains two sequences (**A - B**), the descendant order of **A** is not the order of **B**. This will course some mismatches. \r\nMaybe the correct way is:\r\n`sort A -> feed into RNN -> unsort A -> sort B -> feed into RNN -> unsort B `\r\nI am wondering if `PackedSequence` can support the **sort** and **unsort** operation automatically.\r\n\r\nBelow is the code I implemented for an RNN:\r\n```python\r\n    def forward(self, input_ids: List, input_lens: List, lens_sorted=True):\r\n        \"\"\"Sort by input lens\"\"\"\r\n        if lens_sorted:\r\n            sorted_word_ids, sorted_lens = torch.autograd.Variable(torch.LongTensor(input_ids)), input_lens\r\n        else:\r\n            sorted_lens, sorted_idx = torch.sort(torch.autograd.Variable(torch.LongTensor(input_lens)), 0, descending=True)\r\n            sorted_lens = sorted_lens.cpu().data.numpy().tolist()\r\n            sorted_word_ids = torch.index_select(torch.autograd.Variable(torch.LongTensor(input_ids)), dim=0, index=sorted_idx)\r\n            unsorted_idx = torch.zeros(sorted_idx.size()).long() \\\r\n                .scatter_(0, sorted_idx.cpu().data, torch.LongTensor(list(range(len(input_ids)))))\r\n\r\n        \"\"\"Calculating the hidden and outputs\"\"\"\r\n        # ......\r\n\r\n        \"\"\"Unsort by input lens\"\"\"\r\n        if lens_sorted:\r\n            unsorted_outputs, unsorted_hidden = outputs, hidden\r\n        else:\r\n            unsorted_outputs = torch.index_select(outputs, dim=0, index=torch.autograd.Variable(unsorted_idx))\r\n            unsorted_hidden = torch.index_select(hidden, dim=1, index=torch.autograd.Variable(unsorted_idx))\r\n\r\n        return unsorted_outputs, unsorted_hidden\r\n```\n\ncc @albanD @mruberry"},{"labels":["enhancement",null,null],"text":"Pytorch has written SSE, AVX and AVX2 [intrinstics](https://github.com/pytorch/pytorch/tree/master/aten/src/TH/vector) to vectorize operations on CPU.  Now  AVX-512 instruction sets are more and more widely introduced to Intel CPUs.  It is necessary to add AVX-512F intrinstics  to get better performance.\r\n"},{"labels":["enhancement",null,null,null,null],"text":"Hello everyone, \r\n\r\nThank you for your amazing work. \r\nI would be interested by the implementation of the first derivative of the potri or/and potrs operators. I can use matrix inversion instead, but it comes with a performance drop cost.\r\n\r\nThank you again, \r\nBest, \r\n\r\nAlex"},{"labels":["enhancement",null,null,null],"text":"Recent papers have proposed SGD variants based on stochastic gradient MCMC algorithms that can complete with SOTA optimizers like Adam. Would anyone be interested in an implementation of Santa ([Chen et al. 2016](http://people.ee.duke.edu/~lcarin/Santa_aistats16.pdf)) and relativistic stochastic gradient descent ([Lu, Perrone et al. 2017](http://proceedings.mlr.press/v54/lu17b/lu17b.pdf))?\n\ncc @vincentqb"},{"labels":["enhancement",null,null,null],"text":"Are there any implementation that realize the hadamard product between sparse and dense tensor in pytorch? It's very weird that we can get the hadamard product between sparse and sparse tensor by sparse_a * sparse_b, while we  can't get the hadamard product between sparse and dense tensor by any func.\r\n  \n\ncc @vincentqb @aocsa"},{"labels":["enhancement",null,null],"text":"hi,\r\nI encountered a problem when I was using ONNX to export a model in which max_pool3d was used, but ceil_mode was False.\r\nThe pytorch I am using was downloaded today(27 Dec 2017).\r\nCould anyone help me to solve it?\r\nThanks a lot!!!!!\r\n\r\nUserWarning: ONNX export failed on max_pool3d because torch.onnx.symbolic.max_pool3d does not exist\r\n  .format(op_name, op_name))\r\nRuntimeError: ONNX export failed: Couldn't export operator max_pool3d\r\n\r\n\n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof"},{"labels":["enhancement",null,null,null],"text":"My team is trying to use `torch.autograd.grad(...)` with torch optimizers, but optimizers only consume `.grad` properties, not arbitrary lists of gradients. To work around this, we manually set `.grad` properties from stuff we compute with `grad(...)`.\r\n\r\nCould optimizers add first-class support for lists of gradients? @soumith [suggests](https://discuss.pytorch.org/t/using-optimizers-with-torch-autograd-grad/11078) that\r\n>  optimizers should take an optional `grads` argument, either in step or at construction time\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @vincentqb"},{"labels":["enhancement",null,null,null,null,null],"text":"It would be great if there was a way to name tensor dimensions and use the names in all of the tensor methods that take dim as an argument. \r\n\r\n```python\r\nimages = torch.Tensor(data, dims=('batch', 'channel', 'height', 'width'))\r\nmore_images = torch.Tensor(data, dims=('batch', 'channel', 'height', 'width'))\r\n\r\ntorch.cat((images, more_images), dim='batch')\r\n```\r\n\r\nThis could obviously be expanded to support axis objects that could support a basic algebra to allow using dimensions in functions like `view()`.\r\n\r\nAnother somewhat related feature would be a way to add type annotations as proposed in [this google doc](https://docs.google.com/document/d/1vpMse4c6DrWH5rq2tQSx3qwP_m_0lyn-Ij4WHqQqRHY/edit#heading=h.rkj7d39awayl):\r\n\r\n```python\r\nBatch = DimensionVar('Batch')\r\nRow = DimensionVar('Row')\r\nColumn = DimensionVar('Column')\r\nChannel = DimensionVar('Channel', size=3)\r\n\r\nRGBImage = NDArray[Batch, Row, Column, Channel]\r\nGrayscaleImage = NDArray[Batch, Row, Column]\r\n\r\ndef rgb_to_grayscale(array: RGBImage) -> GrayscaleImage:\r\n  return numpy.mean(array, axis=RGBImage.axis(Channel))\r\n```\r\n\r\n- [Datarray: numpy with axis names](https://github.com/BIDS/datarray)\r\n- [python mailing list discussion](https://mail.python.org/pipermail/numpy-discussion/2017-November/077429.html)\r\n- [Google doc proposing array shape annotations](https://docs.google.com/document/d/1vpMse4c6DrWH5rq2tQSx3qwP_m_0lyn-Ij4WHqQqRHY/edit#heading=h.rkj7d39awayl)\r\n- [Python PEP 472 proposing named indexing]( https://www.python.org/dev/peps/pep-0472/)\n\ncc @ezyang @gchanan @zou3519"},{"labels":["enhancement",null,null,null,null],"text":"https://github.com/pytorch/pytorch/pull/3683 splits off in-place functions to use the underscore suffix, like the functions on the `Tensor`.\r\n\r\nWe should consider deprecating the inplace argument completely and require people to either call, e.g.:\r\n\r\n```\r\nF.relu(input)  # never in-place\r\nF.relu_(input)  # always in-place\r\n```\n\ncc @albanD @mruberry @ezyang @SsnL @gchanan"},{"labels":["enhancement",null,null,null],"text":"It would be very convenient to have an option to set the convolution algorithm preference to prefer memory over speed when enabling the cuDNN benchmark. For a lot of applications memory efficient convolutions are preferred to the fastest approach. So having a way to set this would be welcome.\r\n\r\n`torch.backends.cudnn.benchmark = True`\r\n\r\nThe [cudnnConvolutionFwdPreference](https://docs.rs/cudnn-sys/0.0.3/cudnn_sys/enum.cudnnConvolutionFwdPreference_t.html) during the benchmark knows several settings: \r\n* CUDNN_CONVOLUTION_FWD_NO_WORKSPACE\t\t\r\n* CUDNN_CONVOLUTION_FWD_PREFER_FASTEST\t\t\r\n* CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\r\n\r\nYet, in the current [implementation](https://github.com/pytorch/pytorch/blob/8fbe003d4ed946804d67a6d3bcd84eb6c3df9a4a/torch/csrc/cudnn/Conv.cpp) this appears to be fixed to: CUDNN_CONVOLUTION_FWD_PREFER_FASTEST\r\n\r\nThis is of course similar for:\r\n* cudnnConvolutionBwdDataPreference_t\r\n* cudnnConvolutionBwdFilterPreference_t \r\n* cudnnConvolutionFwdPreference_t\r\n\r\n\r\nMy initial proposal would be to include a property to the `torch.backends.cudnn` which can be set to pick a different preference, i.e.:\r\n`torch.backends.cudnn.benchmark_memory_limit = None` (FASTEST / default value)\r\n`torch.backends.cudnn.benchmark_memory_limit = 0` (NO_WORKSPACE)\r\n`torch.backends.cudnn.benchmark_memory_limit = 12` (SPECIFY_WORKSPACE_LIMIT)\r\n\r\nThe numbers could represent GPU memory in GB/MB or even bytes.\r\n\r\n\r\n\r\n"},{"labels":["enhancement",null,null,null,null],"text":"It would be useful for torch.distributed.send and .recv to be able to send arbitrary objects. I have two requests:\r\n\r\n1. One version of send and recv that does not copy to tensor, but instead returns a new tensor. This way, we can send tensors of arbitrary sizes, useful for many reinforcement learning settings\r\n2. Ability to send arbitrary Python objects, preferrably over pickle streams. mpi4py supports this mode, and it is sometimes very useful to prototype with despite its inefficiencies.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar"},{"labels":["enhancement"],"text":"Can you add please gradient calculation for eigenvector decomposition, please? Eigenvector decomposition is very important in deep learning. "},{"labels":["enhancement",null],"text":"In production environment, deep learning models have to be deployed in applications developed in languages beyond python. ONNX is never a viable solution for complex projects in which there are many custom ops/extensions. It is terrible to develop and maintain the same custom ops/extensions for both pytorch and caffe2. c/c++ interfaces for model deployment is needed, even if it may require running python interpreter in c/c++.\r\n\r\nAnother feature request is better support for custom op. In tensorflow, custom ops can be built into a .so with the toolchain of the user's choice. The custom ops can be used in training after loading the .so file in python. The ffi way provided by pytorch is too limited and is not suitable for big c/c++ projects."},{"labels":["enhancement",null,null,null,null],"text":"I believe that should be useful to have a function similar to https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.in1d.html, that compares a tensor element-wise with a list of possible values. (I'm using it to filter labels/classes in some classifiers).\r\n\r\nExpected behavior:\r\n\r\n```\r\n>>> a = torch.LongTensor([[1,2,3],[1,1,2],[3,5,1]])\r\n>>> a\r\n 1  2  3\r\n 1  1  2\r\n 3  5  1\r\n[torch.LongTensor of size 3x3]\r\n>>> a.in(torch.LongTensor([1, 2, 5]))\r\n 1 1 0\r\n 1 1 1\r\n 0 1 1\r\n```\r\n\r\nNow it's possible to implement it by iterating over the filter, and storing the results in a tensor with the OR operator. But a faster implementation is possible with TH/THC\r\n\r\nMy current implementation:\r\n\r\n```\r\n@utils.tensorfy(0, 1, tensor_klass=torch.LongTensor)\r\ndef filter_labels(y, labels):\r\n    \"\"\"Utility used to create a mask to filter values in a tensor.\r\n\r\n    Args:\r\n        y (list, torch.Tensor): tensor where each element is a numeric integer\r\n            representing a label.\r\n        labels (list, torch.Tensor): filter used to generate the mask. For each\r\n            value in ``y`` its mask will be \"1\" if its value is in ``labels``,\r\n            \"0\" otherwise\".\r\n\r\n    Shape:\r\n        y: can have any shape. Usually will be :math:`(N, S)` or :math:`(S)`,\r\n            containing `batch X samples` or just a list of `samples`.\r\n        labels: a flatten list, or a 1D LongTensor.\r\n\r\n    Returns:\r\n        mask (torch.ByteTensor): a binary mask, with \"1\" with the respective value from ``y`` is\r\n        in the ``labels`` filter.\r\n\r\n    Example::\r\n\r\n        >>> a = torch.LongTensor([[1,2,3],[1,1,2],[3,5,1]])\r\n        >>> a\r\n         1  2  3\r\n         1  1  2\r\n         3  5  1\r\n        [torch.LongTensor of size 3x3]\r\n        >>> classification.filter_labels(a, [1, 2, 5])\r\n         1  1  0\r\n         1  1  1\r\n         0  1  1\r\n        [torch.ByteTensor of size 3x3]\r\n        >>> classification.filter_labels(a, torch.LongTensor([1]))\r\n         1  0  0\r\n         1  1  0\r\n         0  0  1\r\n        [torch.ByteTensor of size 3x3]\r\n    \"\"\"\r\n    mapping = torch.zeros(y.size()).byte()\r\n\r\n    for label in labels:\r\n        mapping = mapping | y.eq(label)\r\n\r\n    return mapping\r\n\r\n```\n\ncc @mruberry @rgommers"},{"labels":["enhancement",null,null,null,null],"text":"Right now they're legacy functions and don't support double backprop\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @mruberry"},{"labels":["enhancement",null,null],"text":"Right now `detach_()` only modifies the current Variable, without modifying the graph in any way, which doesn't stop backprop for its previous uses. This behavior is sometimes wanted when e.g. doing BPTT and reusing some of the steps (say you do BPTT over 20 steps with stride of 2 steps).\r\n\r\nCurrent behavior:\r\n```python\r\na = Variable(torch.randn(2, 2), requires_grad=True)\r\nb = a * 2\r\nc = b * 2\r\nb.detach_()\r\nc.sum().backward()\r\nassert a.grad is not None\r\n```\r\n\r\nProposed fix:\r\n```python\r\na = Variable(torch.randn(2, 2), requires_grad=True)\r\nb = a * 2\r\nc = b * 2\r\nb.detach_(with_uses=True)\r\nc.sum().backward()\r\nassert a.grad is None\r\n```\r\n\r\nImplementation is not straightforward at this point and some parts still need to be worked out.\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["enhancement",null,null],"text":"Is it possible to implement a simple backtracking for the `ReduceLROnPlateau` module?  \r\nThat is, store the best model coefficients and reload it upon rate reduction.\r\n\r\nIn my experiments, this helps speed up learning, though it might be expensive for very large models.\n\ncc @vincentqb"},{"labels":[null,"enhancement",null],"text":"I use blstm-crf in advance_tutorialï¼Œ but it runs very slow, can you add crf layer in pytorchï¼Ÿ"},{"labels":["enhancement",null,null,null],"text":"```\r\nx = torch.sparse.FloatTensor(5, 5)\r\ny = torch.FloatTensor(5, 5)\r\n\r\ntorch.mm(x, y) # works\r\n\r\nxx = torch.autograd.Variable(x)\r\nxy = torch.autograd.Variable(y)\r\n\r\ntorch.mm(x, y) # fails\r\n```\r\n\r\nError Message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"sparse_dense_mm.py\", line 22, in <module>\r\n    AX = torch.mm(A, X) # This doesn't work\r\n  File \"/home/killeent/github/pytorch/torch/autograd/variable.py\", line 580, in mm\r\n    return Addmm.apply(output, self, matrix, 0, 1, True)\r\n  File \"/home/killeent/github/pytorch/torch/autograd/_functions/blas.py\", line 26, in forward\r\n    matrix1, matrix2, out=output)\r\nTypeError: Type torch.sparse.FloatTensor doesn't implement stateless method addmm\r\n```\r\n\r\nI'm not sure what the correct semantics are for dense/sparse, so I may be wrong about this being something we should support. If so, feel free to close :)\n\ncc @vincentqb"},{"labels":["enhancement",null,null,null],"text":"MSRA's deformable Convolution Net needed~\r\ni found that it had been implement in pytorch at :https://github.com/oeway/pytorch-deform-conv\r\nis there any possiblity add this module into core of pytorch?"},{"labels":["enhancement",null,null,null,null],"text":"## Versions\r\n```\r\nUbuntu 16.04\r\nPython 3.6.2 \r\npytorch 0.1.12_2\r\n```\r\n## Issue description\r\n\r\n```python\r\nimport torch\r\nimport torch.multiprocessing as mp\r\nimport torch.functional as f\r\nimport threading\r\nimport numpy as np\r\nfrom timeit import timeit\r\n\r\ndef build(cuda=False):\r\n    nn = torch.nn.Sequential(\r\n        torch.nn.Linear(1024, 1024),\r\n        torch.nn.Linear(1024, 1)\r\n    )\r\n\r\n    return nn.cuda() if cuda else nn\r\n\r\ndef train(nn, X, y, epoch=100):\r\n    X = torch.autograd.Variable(X)\r\n    y = torch.autograd.Variable(y)\r\n    optim = torch.optim.SGD(nn.parameters(), lr=0.1)\r\n    for i in range(epoch):\r\n        yhat = nn(X)\r\n        loss = ((yhat - y) ** 2).mean()\r\n        loss.backward()\r\n        optim.step()\r\n\r\ndef data(cuda=False):\r\n    X = torch.Tensor(np.random.randn(10, 1024))\r\n    y = torch.Tensor(np.random.randn(10, 1))\r\n    return (X.cuda(), y.cuda()) if cuda else (X, y)\r\n\r\ndef cpu_run(i=None):\r\n    nn = build(cuda=False)\r\n    d = data(cuda=False)\r\n    train(nn, *d)\r\n\r\ndef seq_cpu_run():\r\n    for i in range(5):\r\n        cpu_run()\r\n\r\ndef multiprocess_cpu_run():\r\n    pool = torch.multiprocessing.Pool(processes=1)\r\n    result = pool.map(cpu_run, [() for i in range(1)])\r\n    pool.close()\r\n    pool.join()\r\n    return result\r\n\r\nif __name__ == \"__main__\":\r\n    print(timeit(seq_cpu_run, number=1)) # 1\r\n    print(timeit(multiprocess_cpu_run, number=1))  # 2\r\n``` \r\n\r\n#1 run okay alone.\r\n#2 run okay alone.\r\n#2 then #1 runs okay.\r\n#1 then #2 never terminate.\r\n\r\nwhere\r\n#1 = seq_cpu_run, #2 = multiprocess_cpu_run\r\n"},{"labels":["enhancement",null,null,null,null],"text":"Hi all,\r\n\r\nSo there're some great things about PyTorch, but one of the not-great things is that it uses a mostly-but-not-fully different API than the one used for numpy, theano, and tensorflow.  I find myself having to consult a [lookup-table](https://github.com/torch/torch7/wiki/Torch-for-Numpy-users) when I want to run familiar commands.\r\n\r\nAre there any plans to make a numpytorch, where the API matches numpy's as closely as possible?\r\n\r\nAn advantage would be that many functions written for numpy could also be used by pytorch.  It would also be nice to be able to call matplotlib functions (plot, imshow, etc) directly with torch variables without having to do any hacking of matplotlib.\n\ncc @ezyang @gchanan @zou3519"},{"labels":["enhancement",null,null],"text":"If you call:\r\n```\r\ntorch.Storage(512).pin_memory()\r\n```\r\npinned memory will be allocated, and then have your uninitialized tensor copied into it, which is unnecessary.\r\n\r\nIt's possible to to directly allocate pinned memory using:\r\n\r\n```\r\ntorch.Storage(512, allocator=torch.cuda._host_allocator())\r\n```\r\n\r\nBut it would be nice if there was a stable method to call that did this, maybe similar to [`Storage.from_file()`](http://pytorch.org/docs/master/storage.html#torch.FloatStorage.from_file). Maybe `Storage.pinned()`? Or `Storage(512, pinned=True)`?\r\n\r\nI'd be happy to implement this if I'm told what the API should look like."},{"labels":["enhancement",null,null],"text":"An implementation of [Noisy Networks for Exploration](https://arxiv.org/abs/1706.10295).  I've written up a solution at [this gist](https://gist.github.com/jvmancuso/6f088cf8b08f0ce2347cc2db4d3351d2).  I only implemented it for linear layers because the authors used it that way in the paper.  I'll submit a pull request pending approval.\r\n\r\nEDIT: Realized I need to add some details around resetting the noise. Hoping this gets some attention before I implement that."},{"labels":["enhancement",null,null],"text":"`model.summary` in keras gives a very fine visualization of your model and it's very convenient when it comes to debugging the network. Can we try to implement something like it in PyTorch?"},{"labels":["enhancement",null,null,null],"text":"Hello, I would really like the functionality to use a different activation function on the output of the RNNs. I have found ReLU more useful in classification models because, for instance, tanh output from an LSTM makes it easy for a subsequent softmax-linear layer to produce values near .999. Just throwing the idea out there incase someone wants to include that in an upcoming release.\n\ncc @zou3519"},{"labels":["enhancement",null,null,null],"text":"Hi! Small list of simple features that I used on a day to day basis both in numpy and theano and would be helpful to have now. They're all super dumb, but writing hack-arounds or debugging a few extra things is sometimes annoying, and they should be super easy to add.\r\n\r\n- Add .mean() for ByteTensors so we can calculate accs easily.\r\n```\r\nx = torch.Tensor(3).normal_()\r\n(x > 0).mean() # Throws error\r\n\r\ny = np.random.randn(3)\r\n(y > 0).mean() # Works ok\r\n```\r\n\r\n- It's weird that .random_() defaults to garbage and not U[0,1] or something like that. Seems very error prone (I myself had to debug this ~ 10 times). Maybe a warning at least? \r\n```\r\nx = torch.Tensor(2,3)\r\nx.random_() # Returns random ints from 0 to max_int?\r\n\r\ny = np.random.random((2,3)) # Returns U[0,1] on each entry\r\n```\r\n\r\n- Having something like ones_like and zeros_like would be ideal! I used to use this all the time for masks and other things. Also the hack arounds are very prone to do a bad use of memory, as opposed to the expand_as [esp when the programmer is bad such as myself :) ].\r\n```\r\ny = np.random.rand(3)\r\nz = np.ones_like(y)\r\n```\r\n\r\n- It's very annoying that torch.arange returns a FloatTensor and index_select requires a LongTensor. I guess when advanced indexing is in this won't matter :)\r\n\r\n- Edit: Adding that np.arange works just by specifying the end (as range) and torch.arange doesn't. Also the numpy version has dtypo int64 while the torch one is a FloatTensor, though similar things like torch.randperm give LongTensor (this might be useful, idk).\r\n```\r\ny = np.arange(10) # [0, ..., 9]\r\nx = torch.arange(10) # Hell unleashes\r\n```\r\n\r\nCheers!"},{"labels":["enhancement",null,null],"text":"Does PyTorch provide a [factorized output layer](https://arxiv.org/pdf/1412.7091.pdf) in `nn` module? It computes the gradient much faster in sparse output space. This will benefit a lot in NLP experiments.\r\n\r\nI can propose such a layer if there isn't one."},{"labels":["enhancement",null],"text":"It should calculate topk for each dimension in parallel."},{"labels":["enhancement",null],"text":"The part of `btriunpack` that extract pivots has been causing some unexpected performance bottlenecks in qpth. Here's a newer version I've tried that uses gather/scatter operations across a batched vector instead of row interchanges on a batched matrix. I think it's a step towards a better method but the current form is just as slow. I want to do what the LAPACK LASWP function provides but with a batch so maybe we could use some knowledge from those implementations, like [this one in OpenBLAS](https://github.com/xianyi/OpenBLAS/blob/develop/lapack/laswp/generic/laswp_k_1.c).\r\n\r\n## Slightly improved pivot matrix extraction but still slow version using gather/scatter\r\n\r\n```Python\r\nPidx = type(LU_data)(range(sz)).repeat(nBatch, 1).long()\r\n\r\nfor i in range(sz):\r\n    k = LU_pivots[:, i] - 1\r\n    t = Pidx[:, i].clone()\r\n    Pidx[:, i] = torch.gather(Pidx, 1, k.unsqueeze(1).long())\r\n    Pidx.scatter_(1, k.unsqueeze(1).long(), t.unsqueeze(1))\r\n\r\nP = type(LU_data)(nBatch, sz, sz).zero_()\r\nfor i in range(nBatch):\r\n    P[i].scatter_(0, Pidx[i].unsqueeze(0), 1.0)\r\n```"},{"labels":["enhancement",null,null],"text":"Any plans to have this implemented as a basic module? Moreover, it would be really nice if pytorch have the rnn package in torch wrapped"},{"labels":["enhancement",null,null],"text":"In high memory pressure situations, the following is a common occurrence:\r\n\r\n1. create model\r\n2. read state_dict from checkpoint file (loads on GPU)\r\n3. model.load_state_dict(s)\r\n\r\nBecause of memory pressure, a common workaround is to first do:\r\n\r\n```\r\ns = torch.load('my_file.pt', map_location=lambda storage, loc: storage)\r\n```\r\n\r\nAnd then load `s` into `model`.\r\n\r\nThis is a very common scenario that we should be able to avoid, and this scenario might have some pitfalls: what happens on part-GPU part-CPU models, what happens on multi-GPU models...\r\n\r\nif load_state_dict took a filename directly, it can delete it's existing parameter storages and set them to the new one on the fly, thereby requiring no extra memory.\r\n\r\n"},{"labels":["enhancement",null,null],"text":"Currently the PixelShuffle module only implements scaling by an upscaling factor > 1. However an equivalent operation that performs downscaling would involve only a small tweak and is used in some models, e.g. YOLOv2. This would also bring feature parity with Tensorflow which has both `depth_to_space` and `space_to_depth` as equivalent operations. \r\n\r\nThis would involve the following change to pixel_shuffle:\r\n\r\n```\r\ndef pixel_shuffle(input, scale_factor):\r\n    batch_size, in_channels, in_height, in_width = input.size()\r\n\r\n    out_channels = channels // (scale_factor * scale_factor)\r\n    out_height = in_height * scale_factor\r\n    out_width = in_width * scale_factor\r\n\r\n    if scale_factor >= 1:\r\n        input_view = input.contiguous().view(\r\n            batch_size, channels, upscale_factor, upscale_factor,\r\n            in_height, in_width)\r\n        shuffle_out = input_view.permute(0, 1, 4, 2, 5, 3).contiguous()\r\n    else:\r\n        block_size = 1 / scale_factor\r\n        input_view = input.contiguous().view(\r\n            batch_size, channels, out_height, block_size,\r\n            out_width, block_size)\r\n        shuffle_out = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()\r\n\r\n    return shuffle_out.view(batch_size, out_channels, out_height, out_width)\r\n```\r\nAlso it doesn't look like pixel_shuffle currently has any error checking to see if the scale factor is valid. `out_channels`, `out_height`, and `out_width` should all be integers. "},{"labels":["enhancement",null],"text":"[https://arxiv.org/abs/1705.07860](https://arxiv.org/abs/1705.07860)\r\nI would LOVE to see this in PyTorch."},{"labels":["enhancement",null,null],"text":"This functionality is useful for multiple cases, e.g. to efficiently implement optimizer's parameters update."},{"labels":["enhancement",null],"text":"Right not it's not mentioned anywhere in the docs. Also, random functions don't have a generator argument in their signature."},{"labels":["enhancement",null,null,null,null,null,null,null],"text":"I recently discovered the PyTorch Tensor and am very excited about its GPU support. However, it seems to support no interpolation algorithms as MATLAB / Numpy does. My application requires a pre-processing step using linear interpolation of the input data. It is OK now that this pre-processing is done in CPU using _scipy.interpolate_, but it will be nicer if there is something inside PyTorch Tensor that supports doing that operation inside GPU since they will be load into GPU eventually. \r\n\r\nSince I am pretty new to PyTorch, I may be wrong that interpolation is an existing support operator (I did check the doc and found nothing about it). If not, do you guys think it is a good operation to be done in GPU? Is it something that you guys would like to  **include in PyTorch**. If so, I can open a PR and start to working on it.\n\ncc @ezyang @gchanan @zou3519"},{"labels":["enhancement",null,null],"text":"Would be cool to peek into the state of the caching allocator on things like:\r\n- [x] Total cached memory\r\n- [x] Total currently used memory, referenced by Tensors\r\n- [x] Forced free of unused segments\r\n\r\n- [ ] Tracing of memory allocations (along with some measure of fragmentation) and deallocations. Would be useful for custom anasysis scripts and for understanding a reason of OOM (fragmentation or actual lack of memory)\r\n- [ ] Stats about currently existing tensors (if possible, otherwise with a full trace one implement this post-hoc): type, sizes, gpu device. if we had a way to dump timestamp of allocation, would be cool too (would allow to track sort of reliably memory leaks)\r\n- [ ] Dump information of all existing tensors / storages with refcounts, so that an easy vis of fragmentation can be done (hopefully with annotation of what required them for backward)\r\n\r\nWith caching allocaotr it's hard to understand sometimes what's happening with memory since after some big allocations / deallocations memory on nvidia-smi always stays high and doesn't reflect actual usage."},{"labels":["enhancement",null,null],"text":"Second order information has been proven to be effective for visual recognition, for example, adding a co-variance layer after the last conv could perform better within several fine-grained image classification task. Matrix backpropagation is one of key fundamental building block to introduce such layers which could perform better than such networks as bilinear according to some benchmarks. As a result, it's very useful to have such operators included into Pytorch.\r\n\r\n[1] C. Ionescu, O. Vantzos, and C. Sminchisescu. Matrix back- propagation for deep networks with structured layers. In ICCV, 2015\r\n[2] C. Ionescu, O. Vantzos, and C. Sminchisescu. Training deep networks with structured layers by matrix backpropagation. arXiv, abs/1509.07838, 2015\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["enhancement",null,null],"text":"Motivating example is returning bounding box annotation for images along with an image. An annotation list can contain variable number of boxes depending on an image, and padding them to a single length (and storing that length) may be nasty and unnecessarily complex.\r\n\r\n```python\r\nimport torch.utils.data\r\nloader = torch.utils.data.DataLoader(dataset = [(torch.zeros(3, 128, 128), torch.zeros(i, 4)) for i in range(1, 3)], batch_size = 2)\r\nfor batch in loader:\r\n    print(batch)\r\n```\r\n\r\nCurrently this blows with a message below because collate wants to `torch.stack` batch elements, regardless if they have same size:\r\n```\r\nFile \"...torch/utils/data/dataloader.py\", line 188, in __next__\r\n    batch = self.collate_fn([self.dataset[i] for i in indices])\r\n  File \".../torch/utils/data/dataloader.py\", line 110, in default_collate\r\n    return [default_collate(samples) for samples in transposed]\r\n  File \".../torch/utils/data/dataloader.py\", line 92, in default_collate\r\n    return torch.stack(batch, 0, out=out)\r\n  File \".../torch/functional.py\", line 56, in stack\r\n    inputs = [t.unsqueeze(dim) for t in sequence]\r\nRuntimeError: cannot unsqueeze empty tensor at .../torch/lib/TH/generic/THTensor.c:530\r\n```\r\n\r\nReturning a list instead of variable-sized tensor doesn't work either. Providing a custom collate isn't very nice either, since most of the default behavior needs to be copied, and the default collate doesn't allow hooks.\r\n\r\nA solution would be either adding an easy way to extend the default collate, or changing the first collate's branch to something like:\r\n\r\n```python\r\nif all(map(torch.is_tensor, batch)) and any([tensor.size() != batch[0].size() for tensor in batch]):\r\n     return batch\r\n```\r\n\r\nAs a workaround, I'm currently monkey-patching the default collate like this:\r\n```python\r\ncollate_old = torch.utils.data.dataloader.default_collate\r\ntorch.utils.data.dataloader.default_collate = lambda batch: batch if all(map(torch.is_tensor, batch)) and any([tensor.size() != batch[0].size() for tensor in batch]) else collate_old(batch)\r\n```\n\ncc @SsnL"},{"labels":["enhancement",null,null],"text":"Much like #1257, I'd like to allow nearest neighbor upsampling to be non-uniform.  From what I can tell, this means going down into cuda/c level code.\r\n\r\nIt seems like [this](https://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/generic/SpatialUpSamplingNearest.cu) is the relevant file, in particular lines [19-20](https://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/generic/SpatialUpSamplingNearest.cu#L19-L20), [54-55](https://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/generic/SpatialUpSamplingNearest.cu#L54-L55), and [](url) all use a single integer `scale_factor` parameter in simple ways, but also, [165-166](https://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/generic/SpatialUpSamplingNearest.cu#L165-L166) is a cuda kernel call (I *think*).   The idea would be to change this to a pair of ints, and do the tuple casting up in the python layers.\r\n\r\n@apaszke @soumith  any pointers about what I'd need to change or thoughts in general?  also pinging @lantiga, who's been working on another upsampling PR, and has done work at the cuda/c level.\r\n\r\nFWIW and for anyone that finds this via search I'll point out that I've build a manual solution for my needs in a `forward()` call within my network.  Simply make a new `Variable` that's the size needed and manually fill the array with data from the smaller variable."},{"labels":["enhancement",null,null],"text":"on backward avg pool only uses shape of the saved tensor, not the actual tensor, we should thus only hold shape, but this needs modifications in THNN and THCUNN.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/pooling.py#L316"},{"labels":["enhancement",null,null,null],"text":"Currently, sparse_mask takes a sparse tensor, throws out the values, and uses just the indices to mask a dense tensor. This is not a very good API; for one, it causes trouble when the mask tensor is of different type of the dense tensor you're masking (this shouldn't cause any problems, but right now the dispatcher chokes, because it requires them to be the same.)\r\n\r\nSo let's introduce a new function: `sparse_select`, which takes JUST the indexing mask, and produces a sparse vector of just those indices selected.\r\n\r\nThere is one minor downside to doing things this way, which is that we lose information about whether or not the indices are coalesced or not.\r\n\r\n----\r\n\r\n**Old description.** Recently I got this error:\r\n\r\n```\r\n  File \"/Users/ezyang/Dev/pytorch/torch/optim/sgd.py\", line 93, in step\r\n    d_s = (state['step'] - state['last_update']._sparse_mask(d_p.long())).type_as(p.data)\r\nTypeError: _sparse_mask received an invalid combination of arguments - got (torch.sparse.LongTensor), but expected (torch.sparse.IntTensor mask)\r\n```\r\n\r\nThat's a bit silly: in the end you're only ever going to use the indices of the sparse mask tensor, so you don't really care about what the values in the mask tensor.\r\n\r\nUnfortunately, making sparse_mask polymorphic over the type of the mask tensor seems a bit fiddly to do (one existing function which is similarly polymorphic is `copy`, but the way it is polymorphic is by generating a copy of the function for each type.\r\n\r\nAnother strategy would be to introduce a variant of sparse_mask which takes only the index tensor, rather than the entire sparse tensor. This would be similar to index_select, but index_select indexes along a dimension, whereas with sparse_mask we have the coordinates for all dimensions.\r\n\r\nAdditionally, to a certain degree you can work around this problem:\r\n\r\n1. If you don't care about efficiency, you can cast the sparse mask to the required type to make the dispatcher stop complaining. If you do care about efficiency, you could probably make a fake value tensor to get it the right type.\r\n\r\n2. If your update is purely additive, you can use `spadd` to apply your sparse vector of additions to the new tensor. (For example, this is true for adagrad.) If you don't care about efficiency, you can manually difference out the original value so that you have an additive update.\r\n\r\n3. If your sparse tensors are 1D, index_select and related functions can serve your needs.\n\ncc @vincentqb"},{"labels":["enhancement",null,null],"text":"There is currently no good way to set the GPU id for an optimizer state. This is particuarly relevant in the use case that model training stops, and needs to be restarted on a different gpu.\r\n\r\nI'm currently working around that problem with the following:\r\n```\r\ndef set_gpu_recursive(var, gpu_id):\r\n    for key in var:\r\n        if isinstance(var[key], dict):\r\n            var[key] = set_gpu_recursive(var[key], gpu_id)\r\n        else:\r\n            try:\r\n                var[key] = var[key].cuda(gpu_id)\r\n            except:\r\n                pass\r\n    return var\r\n\r\nopt.load_state_dict(torch.load(opt_save_path))\r\nopt.state = set_gpu_recursive(opt.state, gpu_id)\r\n```\r\n"},{"labels":["enhancement",null,null],"text":"There isn't a standard loss function implementing this, even though it's pretty common. I am perfectly willing to implement it myself, if nobody else feels like it. It shouldn't be terribly complicated. I would structure it something like:\r\n```python\r\nclass NCELoss(torch.nn.modules.loss._Loss):\r\n    r\"\"\"Noise contrastive estimation loss function.\r\n    Args:\r\n        num_classes: int number of classes for the output layer\r\n        num_sampled: int number of samples to extract from noise distribution\r\n        noise_sampler: () -> int function\r\n            Function to generate k class labels according to noise distribution.\r\n            By default, noise will be assumed log uniform unigram.\r\n\r\n    Shape:\r\n        - Input: :math:`(N, C)` where `C = num_classes`\r\n        - Target: :math:`(N)` where each value is `0 <= targets[i] <= C-1`\r\n    \"\"\"\r\n```\r\nThe API for tensorflow also uses a parameter `subtract_log_q` so that by setting it to `False` you can switch to a negative sampling objective from nce. Is this worth doing?\r\n\r\nIs this something that already exists? Is this something worth implementing?"},{"labels":["enhancement",null,null,null],"text":"The torch.nn.optim docs promised me conjugate gradient descent (mentions it in the closure section), but it's not there. No references to the algorithm in the code base either :(. Not yet ported from Lua?\n\ncc @vincentqb"},{"labels":["enhancement",null],"text":"There are some models which use \"crop layers\" (e.g. [U-Net](https://arxiv.org/abs/1505.04597), [ParseNet](https://arxiv.org/abs/1506.04579)) so I think it wouldn't be bad to have a `CenterCropNd` layer.\r\n\r\nAs `F.pad` supports negative padding we just need to calculate padding offsets on top.\r\n\r\n```python\r\nimport torch\r\n\r\nfrom torch.nn import functional as F\r\nfrom torch.autograd import Variable\r\n\r\ndef center_crop(x, height, width):\r\n    crop_h = torch.FloatTensor([x.size()[2]]).sub(height).div(-2)\r\n    crop_w = torch.FloatTensor([x.size()[3]]).sub(width).div(-2)\r\n\r\n    return F.pad(x, [\r\n        crop_w.ceil().int()[0], crop_w.floor().int()[0],\r\n        crop_h.ceil().int()[0], crop_h.floor().int()[0],\r\n    ])\r\n\r\nvariable = Variable(torch.randn(1, 3, 60, 40))\r\n\r\nprint(center_crop(variable, 20, 20).size())\r\nprint(center_crop(variable, 20, 40).size())\r\n```\r\n\r\nI can send a PR on interest."},{"labels":["enhancement",null,null],"text":"Hi all,\r\n\r\nI implemented a paper \"Improving Stochastic Gradient Descent with Feedback\" as called [Eve](https://arxiv.org/abs/1611.01505).\r\nEve is a modified version of Adam, and outperforms other SGD algorithms on some benchmark tasks including image classification.\r\nPlease, give me any advice and code review.\r\n\r\nThe code is uploaded in this [gist](https://gist.github.com/snowyday/19b959b268d3af7785b2dd0e2f37f6bb), and below:\r\n```python\r\nimport math\r\n# from .optimizer import Optimizer\r\nfrom torch.optim import Optimizer\r\n\r\n\r\nclass Eve(Optimizer):\r\n    \"\"\"Implements Eve (Adam with feedback) algorithm.\r\n    \r\n    It has been proposed in `Improving Stochastic Gradient Descent with Feedback, `_.\r\n    \r\n    Arguments:\r\n        params (iterable): iterable of parameters to optimize or dicts defining\r\n            parameter groups\r\n        lr (float, optional): learning rate (default: 1e-2)\r\n        betas (Tuple[float, float, float], optional): coefficients used for computing\r\n            running averages of gradient and its square (default: (0.9, 0.999, 0.999))\r\n        thr ((Tuple[float, float], optional): lower and upper threshold for relative change \r\n            (default: (0.1, 10))\r\n        eps (float, optional): term added to the denominator to improve\r\n            numerical stability (default: 1e-8)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n        \r\n    .. _Eve\\: Improving Stochastic Gradient Descent with Feedback\r\n        https://arxiv.org/abs/1611.01505\r\n    \"\"\"\r\n\r\n    def __init__(self, params, lr=1e-2, betas=(0.9, 0.999, 0.999), eps=1e-8, thr=(0.1, 10), weight_decay=0):\r\n        defaults = dict(lr=lr, betas=betas, eps=eps, thr=thr, weight_decay=weight_decay)\r\n        super(Eve, self).__init__(params, defaults)\r\n\r\n    def step(self, closure=None):\r\n        \"\"\"Performs a single optimization step.\r\n        \r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        \"\"\"\r\n\r\n        if closure is not None:\r\n            loss = closure()\r\n            loss_val = loss.data[0]\r\n        else:\r\n            raise ValueError(\"Eve requires a value of the loss function.\")\r\n\r\n        for group in self.param_groups:\r\n            for p in group['params']:\r\n\r\n                grad = p.grad.data\r\n                state = self.state[id(p)]\r\n\r\n                # State initialization\r\n                if len(state) == 0:\r\n                    state['step'] = 0\r\n                    # Exponential moving average of gradient values\r\n                    state['exp_avg'] = grad.new().resize_as_(grad).zero_()\r\n                    # Exponential moving average of squared gradient values\r\n                    state['exp_avg_sq'] = grad.new().resize_as_(grad).zero_()\r\n                    # Previous loss value\r\n                    state['loss_hat_prev'] = loss_val\r\n                    # Feed-back from the loss function\r\n                    state['decay_rate'] = 1\r\n\r\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\r\n                beta1, beta2, beta3 = group['betas']\r\n                thl, thu = group['thr']\r\n                loss_hat_prev = state['loss_hat_prev']\r\n\r\n                state['step'] += 1\r\n\r\n                if group['weight_decay'] != 0:\r\n                    grad = grad.add(group['weight_decay'], p.data)\r\n\r\n                # Decay the first and second moment running average coefficient\r\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n\r\n                bias_correction1 = 1 - beta1 ** state['step']\r\n                bias_correction2 = 1 - beta2 ** state['step']\r\n                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\r\n\r\n                if state['step'] > 1:\r\n                    if loss_val >= loss_hat_prev:\r\n                        lower_bound = thl + 1\r\n                        upper_bound = thu + 1\r\n                    else:\r\n                        lower_bound = 1 / (thu + 1)\r\n                        upper_bound = 1 / (thl + 1)\r\n\r\n                    clip = min(max(lower_bound, loss_val / loss_hat_prev), upper_bound)\r\n                    loss_hat = clip * loss_hat_prev\r\n                    relative_change = abs(loss_hat - loss_hat_prev) / min(loss_hat, loss_hat_prev)\r\n                    state['decay_rate'] = beta3 * state['decay_rate'] + (1 - beta3) * relative_change\r\n                    state['loss_hat_prev'] = loss_hat\r\n\r\n                denom = exp_avg_sq.sqrt().mul_(state['decay_rate']).add_(group['eps'])\r\n\r\n                p.data.addcdiv_(-step_size, exp_avg, denom)\r\n\r\n        return loss\r\n```"},{"labels":["enhancement",null,null,null],"text":"I believe I've stumbled upon a slight whoops in nn.CrossEntropyLoss(). If the criterion is called with (a, y) where a in (N, C) and y in (N) such that some yi > C, I get the internal error message below (took a while to parse)... seems like this could use a wrapper. A simple note following the internal error would suffice--how about: \"Ensure the class dimension of the predictions matches the class dimension of the targets\" ?  \r\n\r\nTHCudaCheck FAIL file=/py/conda-bld/pytorch_1490903321756/work/torch/lib/THC/generic/THCTensorCopy.c line=65 error=59 : device-side assert triggered\r\n\r\nSystem: Ubuntu 16.06, Python 3.6 (conda install)."},{"labels":["enhancement",null,null,null],"text":"Now that sparse tensors are mostly working (https://github.com/pytorch/pytorch/pull/1147), it would be awesome if all the optimizers worked with sparse tensors. This requires some cleverness to do amortized updates to the parameters. For example, for weight decay, you would do something like (pseudocode):\r\n\r\n```\r\n# apply weight decay\r\n# dp is sparse\r\nn_t++\r\nfor i in dp.indices():\r\n  p[i] *= (1 - lr * wd)^(n_t - n[i])\r\n  n[i] = n_t\r\n```\r\nNote this isn't exactly equivalent (to be equivalent you'd need to apply the weight decay before the forward pass, not after backwards), but it's a good approximation. You can do the same thing for momentum. \r\n\r\nI'm guessing the same thing works for Adam/Adamax as well but I haven't worked through the equations. https://arxiv.org/pdf/1412.6980.pdf\r\n\r\n@ezyang expressed interest in working on this."},{"labels":["enhancement",null,null],"text":"Hey all,\r\n\r\nI implemented bare bones versions of `np.corrcoef` and `scipy.stats.pearsonr`. These are two functions I use all the time, so I often have to convert back and forth to numpy for this. It'd be nice to have these incorporated, although I only see the need for the forward pass.\r\n\r\nClearly, they need to be in a more suitable format (inheriting from Function?) and need some tests, so any advice & code review there is appreciated. The functions are found below and in this [gist](https://gist.github.com/ncullen93/58e71c4303b89e420bd8e0b0aa54bf48)\r\n\r\n```python\r\ndef pearsonr(x, y):\r\n    \"\"\"\r\n    Mimics `scipy.stats.pearsonr`\r\n\r\n    Arguments\r\n    ---------\r\n    x : 1D torch.Tensor\r\n    y : 1D torch.Tensor\r\n\r\n    Returns\r\n    -------\r\n    r_val : float\r\n        pearsonr correlation coefficient between x and y\r\n    \r\n    Scipy docs ref:\r\n        https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\r\n    \r\n    Scipy code ref:\r\n        https://github.com/scipy/scipy/blob/v0.19.0/scipy/stats/stats.py#L2975-L3033\r\n    Example:\r\n        >>> x = np.random.randn(100)\r\n        >>> y = np.random.randn(100)\r\n        >>> sp_corr = scipy.stats.pearsonr(x, y)[0]\r\n        >>> th_corr = pearsonr(torch.from_numpy(x), torch.from_numpy(y))\r\n        >>> np.allclose(sp_corr, th_corr)\r\n    \"\"\"\r\n    mean_x = torch.mean(x)\r\n    mean_y = torch.mean(y)\r\n    xm = x.sub(mean_x)\r\n    ym = y.sub(mean_y)\r\n    r_num = xm.dot(ym)\r\n    r_den = torch.norm(xm, 2) * torch.norm(ym, 2)\r\n    r_val = r_num / r_den\r\n    return r_val\r\n\r\ndef corrcoef(x):\r\n    \"\"\"\r\n    Mimics `np.corrcoef`\r\n\r\n    Arguments\r\n    ---------\r\n    x : 2D torch.Tensor\r\n    \r\n    Returns\r\n    -------\r\n    c : torch.Tensor\r\n        if x.size() = (5, 100), then return val will be of size (5,5)\r\n\r\n    Numpy docs ref:\r\n        https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html\r\n    Numpy code ref: \r\n        https://github.com/numpy/numpy/blob/v1.12.0/numpy/lib/function_base.py#L2933-L3013\r\n\r\n    Example:\r\n        >>> x = np.random.randn(5,120)\r\n        # result is a (5,5) matrix of correlations between rows\r\n        >>> np_corr = np.corrcoef(x)\r\n        >>> th_corr = corrcoef(torch.from_numpy(x))\r\n        >>> np.allclose(np_corr, th_corr.numpy())\r\n        # [out]: True\r\n    \"\"\"\r\n    # calculate covariance matrix of rows\r\n    mean_x = torch.mean(x, 1)\r\n    xm = x.sub(mean_x.expand_as(x))\r\n    c = xm.mm(xm.t())\r\n    c = c / (x.size(1) - 1)\r\n\r\n    # normalize covariance matrix\r\n    d = torch.diag(c)\r\n    stddev = torch.pow(d, 0.5)\r\n    c = c.div(stddev.expand_as(c))\r\n    c = c.div(stddev.expand_as(c).t())\r\n\r\n    # clamp between -1 and 1\r\n    # probably not necessary but numpy does it\r\n    c = torch.clamp(c, -1.0, 1.0)\r\n\r\n    return c\r\n```"},{"labels":[null,"enhancement",null],"text":"As far as I know, when we use cudnn on convolution operations, there exists an option to specify whether an input data is in NCHW format or in NHWC format.\r\nIt seems that currently PyTorch only supports NCHW format, thus one has to apply transpose operation and then make the results contiguous explicitly.\r\nI think it would be useful if I can directly use NHWC format for convolutional operations."},{"labels":["enhancement",null,null],"text":"The following works : \r\n\r\n```\r\nimport numpy as np\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nx = torch.from_numpy(np.array([1.,2.,3.]))\r\nx = Variable(x, requires_grad=True)\r\ny = x * 2\r\ngradients = torch.FloatTensor([1, 1, 1])\r\ny.backward(gradients)\r\n```\r\nHowever the following yields an error (notice I have `y = x * x` instead of `y = x * 2`) : \r\n\r\n```\r\nimport numpy as np\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nx = torch.from_numpy(np.array([1.,2.,3.]))\r\nx = Variable(x, requires_grad=True)\r\ny = x * x\r\ngradients = torch.FloatTensor([1, 1, 1])\r\ny.backward(gradients)\r\n```\r\nsaying : \r\n\r\n```\r\nTypeError: mul received an invalid combination of arguments - got (torch.DoubleTensor), but expected one of:\r\n * (float value)\r\n * (torch.FloatTensor other)\r\n\r\n```\r\nThis can be fixed by using `x = torch.from_numpy(np.array([1.,2.,3.],dtype=np.float32))` instead of `x = torch.from_numpy(np.array([1.,2.,3.]))`. But why does the first example not require this change, whereas the second example requires it?\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["enhancement",null,null,null,null,null],"text":"These should significantly speed up [qpth](https://github.com/locuslab/qpth) in some cases when sparse matrices can be used. Here is a [cusolver reference](http://docs.nvidia.com/cuda/cusolver/#cusolver-lt-t-gt-csrqrbatched) on these functions. This issue depends on the other core sparse functions being added (I think at least PR #1147, please add other dependencies here so I'll know when I can start) so I won't immediately start trying to add these. Also please post a reference if a good cusolver wrapper usage example gets added that we can use as a reference for wrapping these QR functions.\r\n\r\nFollowing the naming conventions of the batched LU factorizations and solves of `btrifact` and `btrisolve`, I propose `bqrfact` and `bqrsolve` for the names and for now we can just implement them for sparse tensors.\r\n\r\n\\cc @vincentqb @vishwakftw @SsnL @jianyuh @zkolter"},{"labels":["enhancement",null,null,null],"text":"Sometimes it is useful to disable the activation or use a different activation function than `tanh`. For example, when implementing this [paper](https://arxiv.org/abs/1506.04878), it is needed to disable output non-linearity. Similar options are provided in [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell)."},{"labels":["enhancement",null,null,null],"text":"In [PyTorch ImageNet example](https://github.com/pytorch/examples/blob/master/imagenet/main.py#L266-L270), the approach to anneal LR is to change internal parameters, which is defined in [here](https://github.com/pytorch/pytorch/blob/master/torch/optim/optimizer.py#L28). An API to handle hyper parameter state might be better.\r\n\r\nPossible solutions might be\r\n1. A member function to directly change internal members\r\n2. A annealing scheme function `(epoch) -> {lr, wd, ...}` as an argument when initializing the opimizers\n\ncc @vincentqb"},{"labels":["enhancement",null,null,null],"text":"With current design of `dataloader`, if we set `num_workers=8`, 8 batches of data will be prepared in advance, each worker works on one batch. This is not very efficient especially when one batch is large/expensive to process; and normally we only need 1-2 batches ahead of time . Why didn't we go with the design where all workers work on one batch at a time? \n\ncc @SsnL"},{"labels":["enhancement",null,null],"text":"Bilinear is available but there doesn't appear to be bicubic."},{"labels":["enhancement",null,null,null,null],"text":"Here's a semi-minimal repro script:\r\n\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport time\r\n\r\nmr = Variable(torch.cuda.FloatTensor(8192, 9, 4), requires_grad=True)\r\nlu = Variable(torch.cuda.FloatTensor(8192, 4, 1), requires_grad=True)\r\ns = time.time()\r\nfor i in range(300):\r\n    u = torch.bmm(mr, lu)\r\n    u.sum().backward()\r\nprint(time.time() - s)\r\n```\r\n\r\nRun this with `CUDA_LAUNCH_BLOCKING=1`, it takes 0.6s on Kepler and 3.0s on Maxwell. If you run in nvprof you'll see that Kepler routes `cublasSgemmBatched` to these faster kernels:\r\n\r\n```\r\n==3865486== Profiling result:\r\nTime(%)      Time     Calls       Avg       Min       Max  Name\r\n 53.35%  76.054ms       600  126.76us  122.82us  130.88us  void fermiPlusSgemmLDS64_batched<bool=0, bool=0, int=4, int=4, int=4, int=3, int=3, bool=1, bool=0>(float**, float**, float**, float*, float const *, float const *, int, int, int, int, int, int, __int64, __int64, __int64, float const *, float const *, float, float, int)\r\n 26.38%  37.610ms       300  125.37us  124.93us  127.26us  void fermiPlusSgemmLDS64_batched<bool=1, bool=0, int=4, int=4, int=4, int=3, int=3, bool=1, bool=0>(float**, float**, float**, float*, float const *, float const *, int, int, int, int, int, int, __int64, __int64, __int64, float const *, float const *, float, float, int)\r\n```\r\n\r\nwhereas Maxwell uses these slow Maxwell-specific kernels:\r\n\r\n```\r\n==1234091== Profiling result:\r\nTime(%)      Time     Calls       Avg       Min       Max  Name\r\n 67.67%  1.66077s       600  2.7679ms  2.5584ms  3.4175ms  maxwell_sgemmBatched_128x128_raggedMn_nn\r\n 31.57%  774.71ms       300  2.5824ms  2.5525ms  3.1564ms  maxwell_sgemmBatched_128x128_raggedMn_tn\r\n```\r\n\r\nNot sure how to proceed. @colesbury @soumith "},{"labels":["enhancement",null,null,null],"text":"Unlike NLLLoss, BCELoss doesn't accept LongTensor targets\r\n\r\n(Reported by Marc'Aurelio)"},{"labels":["enhancement",null,null],"text":"New description from @ezyang:\r\n\r\nWork is in progress at https://github.com/Roger-luo/pytorch-complex\r\n\r\n## Organizational principles\r\n\r\n* Complex tensor support is important to PyTorch, and we will accept patches to core which add small amounts of code to make adding complex support.\r\n* Adding complex involves writing a lot of new kernels and code: we'd like this code to initially live out of repo, so it is easier for people to iterate quickly on them without having to go through the PyTorch main code review process. We will *NOT* commit to reviewing large new kernels in the short term, but eventually we would like all the kernels to come back to PyTorch.\r\n* The external library will be buildable separately from PyTorch, so you will be able to maintain it as a separate repository without having to merge with PyTorch (and deal with loads of merge conflicts).\r\n    * PyTorch may occasionally make breaking changes in C++ API; if you bring these to our attention we will do our utmost to help solve these problems.\r\n* The hooks needed for this will NOT ship with PyTorch 1.0, but they will ship with a released version of PyTorch in the not too distant future.\r\n\r\n## How will I work on complex kernels?\r\n\r\nHere is what the workflow will look like in the steady state.\r\n\r\n**PyTorch will natively contain APIs for referring to the complex dtype, but they won't do anything by default.** PyTorch defines torch.complex64 and torch.complex128 referring to complex tensors. However, if you try to construct a tensor this way, by default, PyTorch will error:\r\n\r\n```\r\n>>> torch.zeros({2,2}, dtype=torch.complex64)\r\nRuntimeError: complex64 not supported by PyTorch\r\n```\r\n\r\n@ezyang provided a patch which adds these dtypes to PyTorch. https://github.com/pytorch/pytorch/pull/11173\r\n\r\nIn the mid-term, we will merge support for basic functionality (like allocating a tensor of zeros) to be supported by PyTorch natively. A reasonable proxy for what support is â€œbasicâ€ is PyTorch's native support for CPU half tensors (which are extremely impoverished).\r\n\r\n**PyTorch publishes an interface for registering an implementation of complex tensors.** The implementation inherits from the TypeDefault class (https://github.com/pytorch/pytorch/pull/11013) and will override methods on this class to define implementations of functions for which we have complex implementations. It will look something like this:\r\n\r\n```\r\nstruct CPUComplexFloatType final : public TypeDefault {\r\n  virtual Tensor add(const Tensor & self, const Tensor & other, Scalar alpha=1) const override {\r\n    // Your implementation of add for complex tensors\r\n  }\r\n  // ...\r\n}\r\n```\r\n\r\nThis class will override exactly the types which are supported for complex; all other implementations are provided by TypeDefault and will error by default.\r\n\r\nThere will be a canonical listing of methods supported on Type (the overall interface) as an autogenerated file that is checked into the PyTorch source repository; we'll communicate API changes by diffs to this file. In general, the methods are in one-to-one correspondence with their corresponding names in the PyTorch frontend.\r\n\r\nIn general, when you use an operation which you haven't implemented yet, \r\n\r\n**WARNING:** We intend to refactor Type away into a new system that also supports open registration of new operations (this obviously doesn't work if you have a single superclass that defines all the methods you might possibly want to support). Thus, try not to get too tied to the particular implementation strategy of writing Type as a subclass.\r\n\r\n**To publish new, complex only operations, you will use the C++ extension API.** The C++ extension API is documented at https://pytorch.org/tutorials/advanced/cpp_extension.html Essentially, you can write a C++ function like:\r\n\r\n```\r\nat::Tensor imag(at::Tensor z) {\r\n  ...\r\n}\r\n```\r\n\r\nAnd then the C++ extension API will generate a Python binding so that you invoke this function from Python.\r\n\r\n**Some operations will be â€œeasyâ€ to integrate into PyTorch as it exists today.** For example, for implementation of binary operations, it probably makes more sense to extend add_kernel in BinaryOpsKernel.cpp so that it dispatches over complex types (and then you get it for free, because std::complex implements addition). As long as these patches are small and self-contained, we promise to merge them on a timely basis.\r\n\r\nIt should ALWAYS be possible to unblock, by just writing an override on Type instead of using existing infrastructure, and doing liberal copy pasting. But let's avoid it when it's easy!\r\n\r\n**Autograd.** As long as you're working on operations which already have derivative formulas defined for them, you will â€œautomaticallyâ€ get autograd support, as long as you implement complex support for all the constituent functions which are invoked in the backwards implementation from derivatives.yaml.\r\n\r\nIn some cases, we may need to adjust autograd formulas so that they work for complex numbers; e.g., the gradient of 'abs' isn't 'grad . self.sign()'. In these cases, all we need to do is upstream fix of changing the autograd formula of 'abs' to 'abs_backward', which is a function that can be overridden.\r\n\r\nFor general complex valued back propagation, there are some references:\r\n\r\n1. *Akiraâ€™s â€œComplex Valued Neural Networksâ€.*\r\n2. https://giggleliu.github.io/2018/02/01/complex_bp.html\r\n\r\nGenerally, we won't need to modify the autograd since in most cases we only calculate the derivatives of a real-valued function (the loss). \r\n\r\n## Work plan\r\n\r\nMany of the necessary pieces are in place today, but they are not put together in an end-to-end way. Here is what needs to be done.\r\n\r\n- [X] Codemod TH to not ifdef real https://github.com/pytorch/pytorch/pull/11163\r\n- [X] Built-in support for torch.complex64 and torch.complex128 dtypes. https://github.com/pytorch/pytorch/pull/11173\r\n- [X] An interface for registering CPUComplexType, etc., so that this implementation is invoked when you request a complex tensor with dtype=torch.complex64 or do an operation on complex tensors. \r\n- [X] Land https://github.com/pytorch/pytorch/pull/11013\r\n- [X] An end-to-end example, including working build system, of a separately compileable C++ program that links against libtorch and uses the aforementioned interface to implement complex tensor allocation.\r\n\r\nShort term integration plan. These operations are â€œeasyâ€ to implement, and so we should mainline them in PyTorch as soon as possible.\r\n\r\n- [X] Basic tensor factories: torch.empty, torch.zeros, torch.ones\r\n- [ ] CPU binary operations: add, sub, mul, div #11641\r\n- [ ] FFT\r\n- [ ] ???\r\n\r\nKernel implementation:\r\n\r\nTODO: Generate a list based on https://github.com/Roger-luo/TH/blob/master/ChangeLog.md\r\n\r\nOther complex related tasks:\r\n\r\n- [ ] Figure out the type promotion rules for complex tensors, and implement it in promoteTypes #11641\r\n\r\n## Historical issue content\r\n\r\nOriginal comment from @PhilippPelz \r\n\r\nI was wondering if there is interest in incorporating complex tensors into pytorch.\r\nFor CPU support there is ztorch and I have written z-cutorch ( https://github.com/PhilippPelz/z-cutorch ) a while ago. It is a fork off cutorch before the refactoring for CudaHalfTensor (don't have the hardware yet). \r\nIf it's not too much work, I would like to slowly integrate it with pytorch. I am using matplotlib for plotting via fb.ptyhon and it turns out a huge pain every time I reinstall my system (compiling all the dependencies), plus it seems pytorch will work under Windows soon, which one of my experiment PCs runs on.\r\nI would also need complex gradients, so I would sooner or later touch autograd as well. \r\nWhile tf supports complex tensors per se, it seems many ops don't support it yet (https://github.com/tensorflow/tensorflow/issues/2255), plus it seems a bit heavyweight for my purposes.\r\n\r\nMaybe someone could say a few words how and where to start with this, if it's a welcome idea.\r\n"},{"labels":["enhancement",null],"text":"Currently base RNN class/functions are hard to extend. If someone would like to extend LSTM with new features to pytorch they would have to modify:\r\nAutogradRNN ([nn/_functions/rnn.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L192-L234))\r\nStackedRNN ([nn/_functions/rnn.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L50-L88))\r\nRNNBase ([nn/modules/rnn.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py#L9-L54))\r\nFurthermore, the default RNN implementation is restrictive, enforcing every stacked RNN layer to be exactly the same. \r\n\r\nI was thinking it may be worthwhile to instead have an RNN driver abstract RNN base class.\r\n\r\nRNN Driver would be similar in function to recurrent, stackedRNN, and AutogradRNN functions but would be dependent on abstract RNN class but independent of specific RNN definitions.\r\n\r\nThe code would look something like...\r\n\r\n```py\r\nclass RNNDriver(nn.Module):        \r\n    #constructor could either take an RNN or list of RNN layers\r\n    def __init__(self, inputRNN, num_layers=1):\r\n        if not isinstance(inputRNN, list):\r\n            self.rnns = [inputRNN]\r\n            for i in range(num_layers-1):\r\n                self.rnns.append(inputRNN.clone())\r\n        else:\r\n            assert len(inputRNN) == num_layers, \"RNN list length must be equal to num_layers\"\r\n            self.rnns=inputRNN\r\n    #Parameters call to group parameters of all layers\r\n    def parameters(self):\r\n        memo = set()\r\n        for rnn in self.rnns:\r\n            for p in rnn.parameters(memo):\r\n                yield p\r\n                \r\n    def forward(self, input, train=True, batch_first=False, dropout=0, bidirectional=False):\r\n        ...\r\n\r\n    def initHidden(self, bsz):\r\n        for rnn in self.rnns:\r\n            rnn.initHidden(bsz)\r\n\r\n    def resetHidden(self, bsz):\r\n        for rnn in self.rnns:\r\n            rnn.resetHidden(bsz)\r\n\r\n    def initInference(self, bsz):    \r\n        for rnn in self.rnns:\r\n            rnn.initInference(bsz)\r\n\r\nclass RNNBase(nn.Module):\r\n#Base initialization could be for a simple RNN layer or could be empty\r\n    def __init__(self, input_size, hidden_size):\r\n        super(RNNBase, self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.input_size = input_size\r\n\r\n        self.w_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))\r\n        self.b_ih = nn.Parameter(torch.Tensor(hidden_size))\r\n        self.w_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\r\n\r\n        self.hidden = None\r\n        self.reset_parameters()\r\n\r\n    def reset_parameters(self, feature_size):\r\n            stdv = 1.0 / math.sqrt(feature_size)\r\n            for weight in self.parameters():\r\n                weight.data.uniform_(-stdv, stdv)\r\n\r\n    def initHidden(self, bsz):\r\n        #Create hidden variable(s)\r\n        raise NotImplementedError\r\n\r\n    def resetHidden(self, bsz):\r\n        #Re-wrap hidden variables\r\n        raise NotImplementedError\r\n\r\n    def initInference(self, bsz):\r\n        #Re-wrap hidden in a variable with volatile=True\r\n        raise NotImplementedError\r\n\r\n    def forward(self, input):\r\n        #Implement RNN layer and return output\r\n        raise NotImplementedError\r\n```"},{"labels":["enhancement",null,null,null],"text":"Hi,\r\n\r\nI'm trying to create a module that, given an n-dimensional `LongTensor` returns an n+2 dimensional `FloatTensor` arranged correspondingly from slices of a 3-dimensional `FloatTensor`.  In theory this can be accomplished with `nn.Embedding` plus a reshape, except that I need the matrix corresponding to the `padding_idx` to be the identity matrix instead of a zero matrix.  This could be accomplished by allowing the user to specify what default vector is returned corresponding to `padding_idx`.  Would that be possible?\r\n\r\nThanks,\r\nShawn"},{"labels":["enhancement",null,null,null],"text":"It's a bit difficult to write a SkipGram word2vec model without these functions.\r\n\r\nNot entirely sure, but the Chainer implementations for [NegativeSampling](https://github.com/pfnet/chainer/blob/master/chainer/links/loss/negative_sampling.py) and [HierarchicalSoftmax](https://github.com/pfnet/chainer/blob/master/chainer/links/loss/hierarchical_softmax.py) could be easily ported to pytorch.\r\n\r\nI think this could be a useful addition to pytorch. Also, if you need some help in this, I might be able to find some time and submit a PR."},{"labels":["enhancement",null,null],"text":"cuDNN supports skipping the matrix multiplication with the incoming input via the `CUDNN_SKIP_INPUT` flag on the first recurrent layer. This was useful for passing in a pre-batchnormed input to the RNN.\r\n\r\nCurrently it's hardcoded to linear [here](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L157) and I assume this is due to being able to seamlessly move between torch RNNs and cudnn RNNs.\r\n\r\nWould I be correct in thinking that just [these](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L10-L47) cells need to be updated to take into consideration the flag? If so I'll get to work!"},{"labels":["enhancement",null],"text":"From this [paper](http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf). Peephole connections seem to help in learning precise timings of events."},{"labels":["enhancement",null],"text":"[Zoneout](https://arxiv.org/abs/1606.01305) is one method to perform recurrent dropout on the hidden state of an RNN and has been shown to work quite well on a variety of cases.\r\n\r\nImplementing a zoneout LSTM, such as in [Chainer's implementation](https://github.com/pfnet/chainer/blob/master/chainer/links/connection/zoneoutlstm.py), requires reasonably trivial modifications to implement and would be a good early project for someone new to contributing to PyTorch. If no-one has started by the point I have free time I'll look at doing it myself :)\r\n\r\nNote: other forms of recurrent dropout are also worth looking at but I feel zoneout is likely the most broadly applicable version."},{"labels":["enhancement",null,null,null],"text":"The Lua version of Torch implements a [Locally Connected Layer](https://github.com/torch/nn/blob/master/doc/convolution.md#spatialconvolutio) (Convolution without shared weights). It would be good to have an implementation of this available on the python API version as well."},{"labels":["enhancement",null,null,null,null],"text":"High Priority:\r\n\r\n* [x] `repeat`\r\n* [x] `split`\r\n* [x] `std`\r\n* [x] `var`\r\n* [x] `renorm`\r\n* [x] `eq`\r\n* [x] `ge`\r\n* [x] `gt`\r\n* [x] `le`\r\n* [x] `lt`\r\n* [x] `ne`\r\n\r\nMid Priority:\r\n* [x] `cross`\r\n* [x] `cumsum`\r\n* [x] `trace`\r\n* [x] `unfold`\r\n* [x] `cumprod`\r\n* [x] `atan2`\r\n* [ ] `__and__`\r\n* [ ] `__bool__`\r\n* [ ] `__iand__`\r\n* [ ] `__ior__`\r\n* [ ] `__ixor__`\r\n* [ ] `__mod__`\r\n* [ ] `__nonzero__`\r\n* [ ] `__or__`\r\n* [ ] `__xor__`\r\n* [ ] `cauchy`\r\n* [ ] `exponential`\r\n* [ ] `fill`\r\n* [ ] `geometric`\r\n* [ ] `log_normal`\r\n* [ ] `normal`\r\n* [ ] `ones`\r\n* [ ] `random`\r\n* [ ] `uniform`\r\n* [ ] `zero`\r\n* [ ] `zeros`\r\n\r\nWontfix (as in very very low priority, but PRs are welcome!):\r\n* [x] `gesv`\r\n* [x] `inverse`\r\n* [x] `potrf`\r\n* [x] `svd`\r\n* [x] `trtrs`\r\n* [ ] `orgqr`\r\n* [ ] `ormqr`\r\n* [ ] `potri`\r\n* [x] `potrs`\r\n* [x] `pstrf` [NOW DEFUNCT]\r\n* [x] `qr`\r\n* [x] `symeig`\r\n* [ ] `eig`\r\n* [ ] `gels`\r\n* [ ] `geqrf`\r\n"},{"labels":["enhancement",null,null,null],"text":"Because this:\r\n```python\r\nactions = model(input).max(1).indices\r\n```\r\nlooks much better than this:\r\n```python\r\nactions = model(input).max(1)[1]\r\n```"},{"labels":["enhancement",null,null],"text":"Would it be possible to feed the embedding layer with IntTensors rather than LongTensors only?\n"},{"labels":["enhancement",null,null,null,null],"text":"Would be useful to have stats like BLAS, CUDA archs, compile options etc at runtime, similar to `numpy.__config__.show()`\n"},{"labels":["enhancement",null,null,null,null],"text":"These will serve to bisect:\r\n- [ ] MNIST\r\n- [ ] PTB Language model\r\n- [ ] small token parser\r\n\r\nsort of like smoke tests.\r\n"},{"labels":["enhancement",null,null],"text":"This is needed for the advanced user, where one might need to choose a specific backend, or a particular algorithm / mode in cudnn.\n"},{"labels":["enhancement",null,null,null],"text":"Requests from M'aurelio and Sumit Chopra.\n\nRNN Example with:\n- [ ] DataParallel Multi-GPU\n- [ ] ModelParallel Multi-GPU and part CPU - part GPU\n- [ ] PipelineParallel\n- [ ] RNN + Reinforce / Mixer ( https://github.com/facebookresearch/mixer )\n"},{"labels":["enhancement",null,null],"text":"E.g. it would be nice if this worked:\n\n```\na = torch.IntTensor(10)\nb = torch.LongTensor(10)\na.eq(b)\n```\n\nSame with add, subtract, etc.\n"}]