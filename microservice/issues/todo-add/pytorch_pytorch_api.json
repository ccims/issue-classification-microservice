[{"labels":[null,"api",null],"text":"I need to use U-Net in Qt5. Before it, I tried to make experiment in cmake. The result was perfect.\r\nHowever, I integrated the code into a Qt5 application to load the same model,  it reported an error. \r\n\r\nThe CMakeList.txt file without Q5:\r\n```\r\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\r\nproject(custom_ops)\r\nfind_package(Torch REQUIRED)\r\nset(TORCH_DIRS0 /home/SCHUSE/software/libtorch/include)\r\nset(TORCH_DIRS1 /home/SCHUSE/software/libtorch/include/csrc/api/include)\r\nfind_package( OpenCV REQUIRED )\r\ninclude_directories( ${OpenCV_INCLUDE_DIRS} )\r\ninclude_directories( ${TORCH_DIRS0} ${TORCH_DIRS1} )\r\nadd_executable(unet-app unet-app.cpp)\r\ntarget_link_libraries(unet-app ${TORCH_LIBRARIES} ${OpenCV_LIBS})\r\nset_property(TARGET unet-app PROPERTY CXX_STANDARD 11)\r\n```\r\n\r\nHere, its the code of loading model:\r\n```\r\nstd::shared_ptr<torch::jit::script::Module> module = torch::jit::load(argv[1]);\r\n```\r\nIN THIS WAY, THE MODEL WORKS PERFECTLY!!!!!!!!!!!!!!!!!!\r\n\r\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\r\nFor the Qt5 application, this application uses gstreamer, opencv and libtorch. The CMakeList.txt file with Qt5:\r\n```\r\nproject(multi_camera)\r\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\r\nset(CMAKE_CXX_STANDARD 11)\r\nfind_package(PkgConfig REQUIRED)\r\nfind_package(Torch REQUIRED)\r\nfind_package(OpenCV REQUIRED )\r\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\")\r\n# Enter the directory of the tiscamera repository here:\r\nset(TISCAMERA_DIR /home/yaok/software/camera/tiscamera) \r\nset(TORCH_DIRS0 /home/SCHUSE/software/libtorch/include)\r\nset(TORCH_DIRS1 /home/SCHUSE/software/libtorch/include/csrc/api/include)\r\nset(CUDA_DIR /usr/local/cuda/include )\r\nset(CMAKE_BUILD_TYPE Debug)\r\npkg_check_modules(GSTREAMER REQUIRED gstreamer-1.0 gstreamer-app-1.0 gstreamer-video-1.0)\r\npkg_check_modules(TCAMLIB tcam)\r\n\r\nset(CMAKE_AUTOMOC ON)\r\nset(CMAKE_AUTOUIC ON)\r\nfind_package(Qt5 COMPONENTS Widgets Core Xml)\r\nif (Qt5Widgets_FOUND)\r\n    if (Qt5Widgets_VERSION VERSION_LESS 5.5)\r\n        message(FATAL_ERROR \"Minimum supported Qt5 version is 5.5\" ${Qt5_DIR} ${QT_QMAKE_EXECUTABLE})\r\n    endif()\r\nelse()\r\n    message(SEND_ERROR \"The Qt5Widgets library could not be found!\")\r\nendif(Qt5Widgets_FOUND)\r\n\r\ninclude_directories( ${CMAKE_CURRENT_BINARY_DIR} ${TISCAMERA_DIR}/examples/cpp/common  ${GSTREAMER_INCLUDE_DIRS} ${TCAM_INCLUDE_DIRS} ${OpenCV_INCLUDE_DIRS} ${CUDA_DIR})\r\ninclude_directories( ${TORCH_DIRS0} ${TORCH_DIRS1} )\r\n\r\nadd_definitions(${GSTREAMER_CFLAGS_OTHER})  \r\nadd_executable(multi_camera main.cpp mainwindow.cpp qcameraform.cpp tcamcamera.cpp cpropertiesdialog.cpp qdetthread.cpp qrevthread.cpp qsavethread.cpp segdetector.cpp  )\r\ntarget_link_libraries(multi_camera ${TCAMLIB_LIBRARIES} ${GSTREAMER_LIBRARIES} Qt5::Widgets Qt5::Core Qt5::Xml ${TORCH_LIBRARIES} ${OpenCV_LIBS})\r\nset_property(TARGET multi_camera PROPERTY CXX_STANDARD 11)\r\n```\r\nThe c++ code in QT5:\r\n```\r\nstd::shared_ptr<torch::jit::script::Module> module = torch::jit::load(model_name);\r\n```\r\n\r\nTHE ERROR INFORMATION IS :\r\n```\r\nstart load model: /home/SCHUSE/Downloads/torch_unet/aunet_model.pt\r\nterminate called after throwing an instance of 'std::runtime_error'\r\n  what():  expected ] but found 'number' here:\r\n  _195 = _193.bias\r\n  _196 = getattr(_188, \"4\")\r\n  weight32 = _196.weight\r\n  bias32 = _196.bias\r\n  running_mean32 = _196.running_mean\r\n  running_var32 = _196.running_var\r\n  _197 = self.Conv_Seg\r\n  _198 = _197.weight\r\n  _199 = _197.bias\r\n  input0 = torch._convolution(input, _3, _4, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True)\r\n                                                 ~ <--- HERE\r\n  input1 = torch.batch_norm(input0, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)\r\n  input2 = torch.relu_(input1)\r\n  input3 = torch._convolution(input2, _7, _8, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True)\r\n  x = torch.batch_norm(input3, weight0, bias0, running_mean0, running_var0, False, 0.10000000000000001, 1.0000000000000001e-05, True)\r\n  identity = torch._convolution(input, _11, _12, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True)\r\n  input4 = torch.add_(x, identity, alpha=1)\r\n  input5 = torch.relu_(input4)\r\n  input6 = torch.max_pool2d(input5, [2, 2], [2, 2], [0, 0], [1, 1], False)\r\n  input7 = torch._convolution(input6, _16, _17, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True)\r\nAborted (core dumped)\r\n```\r\nThe environment is:\r\n```\r\n-- Caffe2: CUDA detected: 10.0\r\n-- Caffe2: CUDA nvcc is: /usr/local/cuda-10.0/bin/nvcc\r\n-- Caffe2: CUDA toolkit directory: /usr/local/cuda-10.0\r\n-- Caffe2: Header version is: 10.0\r\n-- Found cuDNN: v7.4.2  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\r\n-- Autodetected CUDA architecture(s):  6.1\r\n-- Added CUDA NVCC flags for: -gencode;arch=compute_61,code=sm_61\r\n-- Pytorch: 1.1.0\r\n-- libtorch:  libtorch-shared-with-deps-1.1.0\r\n```\r\nActually, I load the same model and I make sure that the path is correct. \r\nNow, I cannot localize the problem and do not know the reason for that error. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n\ncc @malfet @seemethere @walterddr @yf225 @glaringlee"},{"labels":["api",null,null],"text":"## üìö Documentation\r\n\r\nFor some C++ functions (for example, torch::load), the [function documentation](https://pytorch.org/cppdocs/api/function_namespacetorch_1a4b369494adfb10b9a005aeb0bb6207cb.html?highlight=load) page shows a warning:  \r\n\r\n```\r\n\"doxygenfunction: Unable to resolve multiple matches for function ‚Äútorch::load‚Äù with arguments (std::vector<torch::Tensor>&, LoadFromArgs&&‚Ä¶) in doxygen xml output for project ‚ÄúPyTorch‚Äù from directory: /var/lib/jenkins/workspace/docs/cpp/build/xml.\"\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/39305301/92850084-9ab57180-f41e-11ea-8e73-47015c71c87e.png)\r\n\n\ncc @yf225 @glaringlee @jlin27"},{"labels":["api",null,null],"text":"#39273 updated Python's circular padding, correcting several errors in the previous implementation. PyTorch has two circular padding implementations, however, one in Python and the other in C++:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/7b547f086fbef74dd2e9c07f4392da3a2d3266b4/torch/nn/functional.py#L3831\r\n\r\nhttps://github.com/pytorch/pytorch/blob/46447045ea450069ab9a9cbbf71e86110013fb0b/torch/csrc/api/include/torch/nn/functional/padding.h#L13\r\n\r\nIt seems like we'd prefer to only have one implementation, although the author of this PR expressed that he was not familiar with C++. \n\ncc @yf225 @glaringlee @albanD @mruberry"},{"labels":["api",null,null,null],"text":"## üöÄ Feature\r\nCreate a specialized schema type that can represent None, int, or tuple of ints. Could be named `Dims`, `DimList`, `ReductionDim`, or something else.\r\n\r\n## Motivation\r\n\r\nCurrently, the only single schema type that can do this is `int[1]?`, which is an `optional<IntArrayRef>` in C++. This type seems to almost work correctly, except that if the function is called from C++ with an integer, the `optional<IntArrayRef>` is filled with a garbage value. The value must be wrapped with `IntArrayRef({<value>})` to get correct behavior:\r\n\r\n```\r\nvoid func(optional<IntArrayRef> opt_dims) {\r\n  if (opt_dims.has_value()) {\r\n    for (auto dim : opt_dims.value()) {\r\n      std::cout << dim << \" \" << std::endl;\r\n    }\r\n  }\r\n  std::cout << std::endl;\r\n}\r\n\r\nfunc(23); // This prints out a garbage value\r\nfunc({23}); // This also prints out a garbage value\r\nfunc(IntArrayRef({23})); // This correctly prints out \"23\"\r\n```\r\n\r\nAfter `int[n]?` was introduced, I tried to use it to fix #29137, but the issue with `optional<IntArrayRef>` silently breaks any C++ calls that provide an integer as the dim argument. The C++ API of `at::sum` cannot easily be changed to prevent callers from providing an int--for one, that would be a BC break, but also, there does not seem to be any way to detect whether the `optional<IntArrayRef>` argument was initialized in this faulty way.\r\n\r\nMore details starting here: https://github.com/pytorch/pytorch/pull/43982#issuecomment-689173522\r\nAnd an older discussion starting here: https://github.com/pytorch/pytorch/pull/30822#issuecomment-571818073\r\n\r\n## Pitch\r\n\r\n@zdevito wrote a good pitch here: https://github.com/pytorch/pytorch/pull/30822#issuecomment-572783469\r\n\r\nIf we had a dedicated class to represent an optional list of dimensions, we would avoid having to use `optional<IntArrayRef>`. This dimension list class would have constructors that accept the following:\r\n* An integer\r\n* A list of integers\r\n* A representation of `None` (perhaps c10::nullopt)\r\n\r\n## Alternatives\r\n\r\n\r\n\r\n## Additional context\r\n\r\nI think it's important to consider how this new dimension list class would interact with the `Dimname[n]` schema type, which accepts `None` as a valid value. Currently, if a function has one overload where an argument type is `Dimname[1]`, and another overload where that argument is `int[1]?`, this creates an ambiguous situation since both overloads would accept `None`. Some additional info: https://github.com/pytorch/pytorch/pull/43982#discussion_r481426382\r\n\r\nThis ambiguous situation compiles without error. Apparently codegen will arbitrarily choose which C++ function gets called (perhaps based on the alphabetical order of the schema overload names). To avoid the ambiguity, we could potentially have something like a `DimnameListNotNone` schema type that does everything that `Dimname[1]` does, except that it does not accept `None`. Perhaps there are other possible solutions though.\r\n\r\n@t-vi suggested trying to split `Dimname[1]` into `Dimname?[]` and `Dimname[]`, and I haven't tried that yet. Still, it would not be ideal to have to have two separate overloads for the dimname list case.\r\n\r\n\r\ncc @yf225 @glaringlee @ezyang @bhosmer @smessmer @ljk53 @mruberry @rgommers "},{"labels":[null,"api",null],"text":"## üöÄ Feature\r\nI want to be able to determine the version of libtorch in my c++ binary in cases where I don't have access to the python library.\r\n\r\nSomething like:\r\n```\r\n#define TORCH_VERSION_MAJOR=1\r\n#define TORCH_VERSION_MINOR=6\r\n#define TORCH_VERSION_PATCH=0\r\n```\r\n\r\n## Motivation\r\n\r\nThis is useful for \r\n- Turning off torch features depending on the version of torch we are running\r\n- Debugging which version of torch is installed in production where different versions may be running.\r\n\r\n## Pitch\r\n\r\n1) I'm a developer trying to upgrade the API, but a backwards incompatible API change requires me to refactor code before upgrading.  I need to roll this out so I want to only enable the new features if I build my application for the new version of torch.\r\n2) I'm a site reliability engineer and I am rolling out a the server deploy with a new version of libtorch.  There is a bug that is claimed to be fixed in the newer version of torch and I want to verify that the new version was deployed when I validate this issue is resolved.\r\n\r\n## Alternatives\r\n\r\nCurrently I am pulling the version from the strings in libtorch_cpu.so, but this didn't get updated in v1.6.0.  This is an unreliable hack solution that I would like to remove.\r\n```\r\nstrings libtorch_cpu.so | egrep -A1 pytorch_version\r\npytorch_version\r\n1.5.0\r\n```\r\n\r\npytorch has the version number baked into the pip install and in the torch.__version__.  For my purposes I don't have access to either of these as we are not using python in our production environment and the shared libraries are built separate from the ones in the pip wheel.\r\n\r\n## Additional context\r\n\r\nI recently upgraded to libtorch version 1.6.0 to resolve a segfault on destruction of `DeviceThreadHandlePool` which seemed related to: https://github.com/pytorch/pytorch/pull/36416.  After upgrading I am still seeing some segfaults on destruction, but was worried that I might not have deployed the new version of libtorch.   By looking at other strings in that library I believe I am at v1.6.0, but I wasted some time on the red herring of the 1.5.0 version in the strings of libtorch_cpu.so and would like to avoid that in the future.\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null,null],"text":"## ‚ùì Questions and Help\r\n\r\nMy work host have all instruction sets, but service hosts are not support service host, such as 'AVX' and 'SSE4', so i want to disable the instruction set before build libtorch. How can I do it?\n\ncc @malfet @seemethere @walterddr @yf225 @glaringlee @jlin27"},{"labels":["api",null,null],"text":"There are 9 boolean flags within TensorImpl.cpp which exceeds 8bytes already.\r\nThis made the size of TensorImpl 1 word bigger than before which is not necessary. (introduced in #33033). \r\n\r\nThe purpose of this  issue is to move all the boolean flags into a 8bytes uint as bitfields, so the size of TensorImpl reduced 1 word. \r\nAnd we will also enhence the note of TensorImpl, making it more clear on the process of modifying TensorImpl.\r\n\r\ncc @yf225 @glaringlee @ezyang @bhosmer @smessmer @ljk53"},{"labels":[null,"api",null],"text":"pr #41911 adds the _fft_ namespace, which steps on the c++ _torch::fft()_ function.\r\n\r\nwhat is the intended workaround?\r\n\r\ncall _at::fft_ ?\r\n\r\nthanks\n\ncc @yf225 @glaringlee"},{"labels":["api",null],"text":"https://github.com/pytorch/pytorch/#from-source indicates indicates that a C++14 compiler is required.  However, extensive use of enable_if_t (and similar) indicates that at least C++17 is required.\n\ncc @yf225 @glaringlee"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\nTrying convert a RefArray into a tensor like this  :\r\n \r\n```cpp\r\nauto tmp = torch::ones({2,3});\r\nauto shapes = torch::tensor({tmp.sizes()});\r\n```\r\nresults in this exception message : \r\n```\r\nfalse INTERNAL ASSERT FAILED at \"D:\\\\Codes\\\\cpp\\\\port\\\\LibtorchPort\\\\Dependencies\\\\libtorch-debug-latest\\\\libtorch\\\\include\\\\torch\\\\csrc\\\\api\\\\include\\\\torch\\\\detail\\\\TensorDataContainer.h\":299, please report a bug to PyTorch. TensorDataContainer is already a Tensor type, `fill_tensor` should not be called\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun this : \r\n \r\n```cpp\r\nauto tmp = torch::ones({2,3});\r\nauto shapes = torch::tensor({tmp.sizes()});\r\n```\r\n## Expected behavior\r\n\r\nShouldnt happen (since it has a `vec()` member which can be automatically used in this case!) or at least should issue a better error message. \r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6\r\n - OS (e.g., Linux): Windows 10\r\n - How you installed PyTorch (`conda`, `pip`, source): pip for python, downloaded the prebuilt libraries for libtorch\r\n - Build command you used (if compiling from source): - \r\n - Python version: 3.7\r\n - CUDA/cuDNN version: - \r\n - GPU models and configuration: - \r\n - Any other relevant information: I use cpu-mode\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"## üöÄ Feature\r\nCurrently the lack of such function is really felt. for example, when displaying the results on screen, it comes up a lot that you face something like : \r\n```\r\noffsets.shape: [1, 4, 46, 85]\r\nprobs.shape: [46, 85]\r\noffsets: (1,1,.,.) =\r\n 0.01 *\r\n  0.1006  1.2322\r\n  -2.9587 -2.2280\r\n\r\n(1,2,.,.) =\r\n 0.01 *\r\n  1.3772  1.3971\r\n  -1.2813 -0.8563\r\n\r\n(1,3,.,.) =\r\n 0.01 *\r\n  6.2367  9.2561\r\n   3.5719  5.4744\r\n\r\n(1,4,.,.) =\r\n  0.2901  0.2963\r\n  0.2618  0.2771\r\n[ CPUFloatType{1,4,2,2} ]\r\nprobs: 0.0001 *\r\n 1.4593  1.0351\r\n  6.6782  4.9104\r\n[ CPUFloatType{2,2} ]\r\n```\r\nwhere some tensors dims do not show  their finalized output, instead, it seems, a scaler is shown in the output instead which makes it really hard to read, specially when you are porting a Python module into C++ and you want to compare the intermediate outputs for example. \r\nApart from this, specifying the precision, threshold, edgeitems, etc can be very benificial and make life much easier! \r\n\r\n## Motivation\r\n\r\nDuring porting a Python project into C++ for production, I faced this issue, and it creeps up a lot and makes reading and comparing values between Python and C++ very hard. \r\n\r\n## Pitch\r\n\r\nPlease kindly implement set_printoptions in libtorch as well.\r\n\r\n## Alternatives\r\n\r\nThe only alternative is to alter torch code which is a very bad idea! I know no other way around this! \r\n\r\n\r\n\n\ncc @yf225 @glaringlee"},{"labels":["api",null,null,null],"text":"## üêõ Bug\r\n\r\nAs the title suggested, the `right` argument in `torch.bucketize` works opposite to the description of the documentation,\r\nas well as `numpy.digitize`.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the example in the docstring, get the same results as in the docstring\r\n```python\r\n>>> boundaries = torch.tensor([1, 3, 5, 7, 9])\r\n>>> v = torch.tensor([[3, 6, 9], [3, 6, 9]])\r\n>>> torch.bucketize(v, boundaries)\r\ntensor([[1, 3, 4],\r\n        [1, 3, 4]])\r\n>>> torch.bucketize(v, boundaries, right=True)\r\ntensor([[2, 3, 5],\r\n        [2, 3, 5]])\r\n```\r\n2. Run it with `numpy.digitize`\r\n```python\r\n>>> boundaries = numpy.array([1, 3, 5, 7, 9])\r\n>>> v = numpy.array([[3, 6, 9], [3, 6, 9]])\r\n>>> numpy.digitize(v, boundaries)\r\narray([[2, 3, 5],\r\n       [2, 3, 5]])\r\n>>> numpy.digitize(v, boundaries, right=True)\r\narray([[1, 3, 4],\r\n       [1, 3, 4]])\r\n```\r\n3. The two functions have consistent docstrings, but give opposite results.\r\n    To me the pytorch results **contradict the documentation**. (**Edited**)\r\n\r\n## Expected behavior\r\n\r\npytorch function behavior should agree with their documentations. (**Edited**)\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.6.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: CentOS Linux release 7.8.2003 (Core) (x86_64)\r\nGCC version: (GCC) 7.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.8 (64-bit runtime)\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.5\r\n[pip3] torch==1.6.0\r\n[pip3] torchvision==0.7.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cudatoolkit               10.2.89              hfd86e86_1  \r\n[conda] magma-cuda102             2.5.2                         1    pytorch\r\n[conda] mkl                       2020.1                      217  \r\n[conda] mkl-include               2020.1                      217  \r\n[conda] mkl-service               2.3.0            py38he904b0f_0  \r\n[conda] mkl_fft                   1.1.0            py38h23d657b_0  \r\n[conda] mkl_random                1.1.1            py38h0573a6f_0  \r\n[conda] numpy                     1.18.5           py38ha1c710e_0  \r\n[conda] numpy-base                1.18.5           py38hde5b4d6_0  \r\n[conda] pytorch                   1.6.0           py3.8_cuda10.2.89_cudnn7.6.5_0    pytorch\r\n[conda] torchvision               0.7.0                py38_cu102    pytorch\r\n\r\ncc @yf225 @glaringlee @mruberry @rgommers"},{"labels":[null,"api",null],"text":"## üöÄ Feature\r\nCurrently the only way that seems to be working to check whether a tensor is empty or not, is to check its `has_storage` method. However, it seems, this method always returns false for SparseTensors . Therefore its not considered a proper/canonical way for checking whether a at::Tensor is empty or not.  \r\nProviding a dedicated property or a method for this purpose, thus goes a long way and will be very much appreciated. \r\n\r\n## Motivation\r\n\r\nDealing with situations, where an empy tensor is returned becasue of meeting or not! meeting some creteria and it needs to be determined whether we are dealing with an empty tensor or not!\r\n\r\n## Pitch\r\n\r\nAdd a property for checking the emptiness of a tensor! in libtorch\r\n\r\n## Alternatives\r\n\r\nCurrently `has_storage` is being used for quering whether a tensor is acually empty or not, but its not the case for SparseTensors! \r\nand wont work for them! \r\n\r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null,null,null],"text":"## üêõ Bug\r\n\r\nF.mse_loss(a, b, reduction='elementwise_mean') has very different behaviors depending on if `b` require a gradient or not.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport torch\r\nfrom torch.nn import functional as F\r\n\r\nA = torch.ones(2)\r\nB = torch.zeros(2, requires_grad=True)\r\nprint(F.mse_loss(A, B, reduction='elementwise_mean'))\r\n\r\nC = torch.zeros(2)\r\nprint(F.mse_loss(A, C, reduction='elementwise_mean'))\r\n```\r\n\r\nreturns\r\n```\r\n# first call takes the sum\r\ntensor(2., grad_fn=<SumBackward0>)\r\n\r\n# second call takes the mean and prints a warning\r\n/home/vitchyr/anaconda2/envs/tmp/lib/python3.6/site-packages/torch/nn/_reduction.py:14: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\r\n  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\r\ntensor(1.)\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nI think the second behavior (a deprecation warning and taking the mean) is the correct/expected behavior, but the two behaviors do not match.\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.6.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Ubuntu 16.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nClang version: Could not collect\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6 (64-bit runtime)\r\nIs CUDA available: False\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.6\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.3\r\n[pip] torch==1.6.0\r\n[conda] numpy                     1.16.3                   pypi_0    pypi\r\n[conda] torch                     1.6.0                    pypi_0    pypi\r\n```\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\r\n\r\ncc @ezyang @gchanan @zou3519 @yf225 @glaringlee @albanD @mruberry"},{"labels":["api",null,null],"text":"Currently `src->deleter` is always called, regardless if it's `nullptr` or not. It would be mildly more hassle-free if it checked `if(src->deleter)` prior to calling, just like fromDLPack accommodates not-specified strides.\r\n\r\n```cpp\r\nauto deleter = [src](void* self) {\r\n    src->deleter(const_cast<DLManagedTensor*>(src));\r\n  };\r\n``` \r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/DLConvertor.cpp#L215\n\ncc @yf225 @glaringlee"},{"labels":["api",null],"text":"I followed [https://pytorch.org/tutorials/advanced/cpp_export.html](url) to trace a model from existing weights in python as shown below\r\n\r\n`        \r\n\r\n        if self.conf.use_cuda:\r\n            torch.set_default_tensor_type('torch.cuda.FloatTensor')\r\n        else:\r\n            torch.set_default_tensor_type('torch.FloatTensor')\r\n\r\n        self.learner = face_learner(self.conf, self.device)\r\n        # load pretrained model for learner object\r\n        self.learner.model.load_state_dict(torch.load(self.conf.learner_model_path))\r\n        self.learner.model.eval()\r\n\r\n        # read the example image used for tracing\r\n        image=cv2.imread(\"videos/example.jpg\",1)\r\n\r\n        test_transform = trans.Compose([\r\n                    trans.ToTensor(),\r\n                    trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\r\n                ])\r\n\r\n        resized_image = cv2.resize(image, (112, 112))\r\n        # Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\r\n        traced_script_module = torch.jit.trace(self.learner.model, test_transform(resized_image).unsqueeze(0).to(self.device))\r\n        traced_script_module.save(\"traced_facelearner_model_new.pt\")\r\n`\r\n\r\nThe face learner model gives 1x512 embedding as output.\r\n\r\n`\r\n\r\n                torch::jit::script::Module model = torch::jit::load(\"traced_facelearner_model_new.pt\");\r\n\t\tmodel.to(torch::kCUDA);\r\n\t\tmodel.eval();\r\n\r\n\t\tcv::Mat visibleFrame = cv::imread(\"example.jpg\");\r\n\r\n\t\tcv::resize(visibleFrame, visibleFrame, cv::Size(112, 112));\r\n\t\tat::Tensor tensor_image = torch::from_blob(visibleFrame.data, { 1, visibleFrame.rows, visibleFrame.cols, 3 }, at::kByte);\r\n\t\ttensor_image = tensor_image.permute({ 0, 3, 1, 2 });\r\n\t\ttensor_image = tensor_image.to(at::kFloat);\r\n\r\n\t\ttensor_image[0][0] = tensor_image[0][0].sub(0.5).div(0.5);\r\n\t\ttensor_image[0][1] = tensor_image[0][1].sub(0.5).div(0.5);\r\n\t\ttensor_image[0][2] = tensor_image[0][2].sub(0.5).div(0.5);\r\n\r\n\t\ttensor_image = tensor_image.to(torch::kCUDA);\r\n\t\tstd::vector<torch::jit::IValue> input;\r\n\t\tinput.emplace_back(tensor_image);\r\n\t\t// Execute the model and turn its output into a tensor.\r\n\t\tauto output = model.forward(input).toTensor();\r\n`\r\n\r\nI'm pretty sure my preprocessing steps on the image in libtorch and pytorch are the same. But the output embedding is still different. \r\nfor eg: python output of first 3 values in the embedding\r\ntensor([[ 2.3617e-02, -1.3115e-02,  7.1695e-02,\r\n\r\nc++ output of first 3 values\r\n Columns 1 to 10 -0.0248 -0.0245 -0.0204\r\n\r\n\r\nPC Specifications:\r\nIntel i7\r\nGtX 970M \r\nWindows 10\n\ncc @suo @gmagogsfm @yf225 @glaringlee"},{"labels":[null,"api",null,null],"text":"## üöÄ Feature\r\nIt would be great for the serialization to be much broader than what it is now in libtorch. at least by providing a way for the users to be able to use existing functionality to serialize their objects. \r\n\r\n## Motivation\r\n\r\nI was trying to serialize a `std::vector<std::tuple<std::string, torch::Tensor>>` in libtorch where I found out its simply impossible. there is no way to extend the `InputArchive ` so I can add the needed logic to load from it. \r\n\r\n## Pitch\r\n\r\nAdd support for `STL` containers at least when it comes to serialization. or allow the users to be able to extend existing functionality to support their own custom types.  \r\n\r\n## Alternatives\r\n\r\nI ultimately ended up using Protobuf for serialization \r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null,null],"text":"## üêõ Bug\r\n\r\nWe are building libtorch using ./scripts/build_anroid.sh.\r\n\r\nWe need the support for Aten Ops and TorchScript, so building without the BUILD_CAFFE2_MOBILE option.\r\n\r\nThe build is successful, but the library does not link ${TORCH_SRC_DIR}/csrc/api/src/optim/adam.cpp and dependencies because of NO_API being set for Mobile Builds.\r\n\r\nBecause of this I am unable to train the model and instantiate Adam Optimizer instance from the code.\r\n\r\ntorch::optim::Adam optimizer(parameters, lr); //Linker Error\r\noptimizer.zero_grad(); //Linker Error\r\noptimizer.step(); //Linker Error\r\n\r\nFollowing is the Linker error:\r\n/home/atibrewal/work/apprecommender/src/RNNRecommender.cpp:374: undefined reference to `torch::optim::AdamOptions::AdamOptions(double)'\r\n/home/atibrewal/work/apprecommender/src/RNNRecommender.cpp:375: undefined reference to `torch::optim::Optimizer::zero_grad()'\r\n/home/atibrewal/work/apprecommender/src/RNNRecommender.cpp:395: undefined reference to `torch::optim::Adam::step(std::__ndk1::function<at::Tensor ()>)'\r\nCMakeFiles/hxRecommenderEngine.dir/src/RNNRecommender.cpp.o: In function `OptimizerParamGroup':\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/optimizer.h:68: undefined reference to `torch::optim::OptimizerParamGroup::params() const'\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/optimizer.h:68: undefined reference to `torch::optim::OptimizerParamGroup::has_options() const'\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/optimizer.h:68: undefined reference to `torch::optim::OptimizerParamGroup::options() const'\r\nCMakeFiles/hxRecommenderEngine.dir/src/RNNRecommender.cpp.o: In function `AdamOptions':\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/adam.h:21: undefined reference to `vtable for torch::optim::AdamOptions'\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/adam.h:21: undefined reference to `vtable for torch::optim::AdamOptions'\r\nCMakeFiles/hxRecommenderEngine.dir/src/RNNRecommender.cpp.o: In function `Adam':\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/adam.h:52: undefined reference to `vtable for torch::optim::Adam'\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/adam.h:52: undefined reference to `vtable for torch::optim::Adam'\r\nCMakeFiles/hxRecommenderEngine.dir/src/RNNRecommender.cpp.o: In function `OptimizerOptions':\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/optimizer.h:49: undefined reference to `vtable for torch::optim::OptimizerOptions'\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/optimizer.h:49: undefined reference to `vtable for torch::optim::OptimizerOptions'\r\nCMakeFiles/hxRecommenderEngine.dir/src/RNNRecommender.cpp.o: In function `Optimizer':\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/optimizer.h:91: undefined reference to `vtable for torch::optim::Optimizer'\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/optimizer.h:91: undefined reference to `vtable for torch::optim::Optimizer'\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/optimizer.h:93: undefined reference to `torch::optim::Optimizer::add_param_group(torch::optim::OptimizerParamGroup const&)'\r\nCMakeFiles/hxRecommenderEngine.dir/src/RNNRecommender.cpp.o: In function `~Optimizer':\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/optimizer.h:103: undefined reference to `vtable for torch::optim::Optimizer'\r\n/home/atibrewal/work/hielibs_android/include/torch/csrc/api/include/torch/optim/optimizer.h:103: undefined reference to `vtable for torch::optim::Optimizer'\r\nCMakeFiles/hxRecommenderEngine.dir/src/RNNRecommender.cpp.o:(.data.rel.ro._ZTVN5torch5optim25OptimizerCloneableOptionsINS0_11AdamOptionsEEE[_ZTVN5torch5optim25OptimizerCloneableOptionsINS0_11AdamOptionsEEE]+0x18): undefined reference to `torch::optim::OptimizerOptions::serialize(torch::serialize::InputArchive&)'\r\nCMakeFiles/hxRecommenderEngine.dir/src/RNNRecommender.cpp.o:(.data.rel.ro._ZTVN5torch5optim25OptimizerCloneableOptionsINS0_11AdamOptionsEEE[_ZTVN5torch5optim25OptimizerCloneableOptionsINS0_11AdamOptionsEEE]+0x20): undefined reference to `torch::optim::OptimizerOptions::serialize(torch::serialize::OutputArchive&) const'\r\nCMakeFiles/hxRecommenderEngine.dir/src/RNNRecommender.cpp.o:(.data.rel.ro._ZTIN5torch5optim25OptimizerCloneableOptionsINS0_11AdamOptionsEEE[_ZTIN5torch5optim25OptimizerCloneableOptionsINS0_11AdamOptionsEEE]+0x10): undefined reference to `typeinfo for torch::optim::OptimizerOptions'\r\n\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): Pytorch 1.5.1 and 1.6.0\r\n - OS (e.g., Linux): Android arm64-v8a\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): /scripts/build_anroid.sh\r\n - Python version: NA\r\n - CUDA/cuDNN version: NA (using a CPU version)\r\n - GPU models and configuration: NA\r\n - Any other relevant information: Android NDK version 19c\r\n\r\n\n\ncc @yf225 @glaringlee @vincentqb"},{"labels":["api",null],"text":"## MultiheadAttention model parameters are different in python and C++.\r\nIn C++, MultiheadAttention module's parameter is no set, its Linear sub-module's parameters can be retrieved C++. \r\nIn python parameters of the module and sub module can be retrieved.\r\n\r\nThe corresponding C++ code with output is as follows\r\n```c++\r\n#include <torch/torch.h>\r\n#include <iostream>\r\nnamespace nn = torch::nn;\r\nint main() {\r\n  int64_t d_model = 4;\r\n  int64_t nhead = 2;\r\n  double dropout = 0.0;\r\n  nn::MultiheadAttention mmodel = nn::MultiheadAttention(nn::MultiheadAttentionOptions(d_model, nhead).dropout(dropout));\r\n  for (const auto& module : mmodel->modules()) {\r\n    std::cout<< \" Module name :\" << module->name() << std::endl;\r\n    for (auto& param : module->named_parameters(false)) {\r\n      std::cout<< param.key() << \":\" << param.value() << std::endl;\r\n    }\r\n  }\r\n}\r\n```\r\nC++ Output:\r\n\r\n Module name :torch::nn::MultiheadAttention\r\n Module name :torch::nn::LinearImpl\r\nweight:-0.3390  0.1080 -0.0194 -0.1426\r\n 0.0612  0.0071  0.1883  0.1399\r\n-0.2122  0.4950 -0.2251  0.0851\r\n 0.3598  0.0617 -0.3281 -0.0391\r\n[ CPUFloatType{4,4} ]\r\nbias: 0\r\n 0\r\n 0\r\n 0\r\n[ CPUFloatType{4} ]\r\n\r\nPython code with output is as follows\r\n\r\n```python\r\nimport torch.nn as nn\r\nd_model = 4\r\nnhead = 2\r\ndropout = 0.0\r\nnet = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\r\nfor m in net.modules():\r\n    print('Module name :', m)\r\n    for nm, p in m.named_parameters(recurse=False):\r\n        print(nm, \":\", p)\r\n```\r\nPython Output:\r\n\r\nModule name : MultiheadAttention(\r\n  (out_proj): _LinearWithBias(in_features=4, out_features=4, bias=True)\r\n)\r\nin_proj_weight : Parameter containing:\r\ntensor([[ 0.3215,  0.1712, -0.4258,  0.0608],\r\n        [ 0.1573, -0.3351, -0.5111,  0.1464],\r\n        [-0.2922,  0.0303,  0.5510, -0.5638],\r\n        [ 0.3144,  0.2177,  0.1631,  0.0863],\r\n        [ 0.5004, -0.2727,  0.3545, -0.1129],\r\n        [ 0.4676, -0.2856, -0.0074, -0.5064],\r\n        [-0.5378,  0.2378,  0.1775, -0.1606],\r\n        [-0.0225, -0.5279,  0.4279, -0.3752],\r\n        [ 0.4622, -0.2481, -0.3727,  0.5852],\r\n        [-0.6048, -0.4132, -0.6092, -0.5233],\r\n        [-0.3640,  0.1864, -0.3457, -0.5625],\r\n        [ 0.2493, -0.3487, -0.3142, -0.1200]], requires_grad=True)\r\nin_proj_bias : Parameter containing:\r\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\r\nModule name : _LinearWithBias(in_features=4, out_features=4, bias=True)\r\nweight : Parameter containing:\r\ntensor([[-0.3266,  0.3457,  0.2251, -0.3043],\r\n        [-0.0438,  0.4598, -0.0973,  0.3782],\r\n        [ 0.0520,  0.3891, -0.1004, -0.2929],\r\n        [ 0.0734,  0.2197, -0.2849,  0.2117]], requires_grad=True)\r\nbias : Parameter containing:\r\ntensor([0., 0., 0., 0.], requires_grad=True)\r\n\r\n\r\nAm I missing something in the C++ usage, or is there a bug. Need help to confirm C++ API usage. \r\n \r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null,null],"text":"## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\ntorch.nn.Unflatten has been added into pytorch in #41564, it should be added into C++ Frontend as well.\r\n## Motivation\r\nThis is to add torch.nn.Unflatten support in C++ Frontend, so people who is using pure c++ (libtorch for eg) can use this module as well.\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"## üöÄ Feature\r\nIn the PyTorch Python API, it is possible to move a tensor to shared memory via calling the `Tensor.share_memory_()` function. I could not find similar functionality on the  C++ side using `at::Tensor`.\r\n\r\n## Motivation\r\nI have code that reads tensors from network on the C++ side (using multiple threads). These tensors are then pulled into Python land and go through Python multi-processing transforms. I want to avoid copying tensors across process boundaries, so that the multi-processing transforms can be executed efficiently. To do this, I need to move the tensor storage to shared memory. However, for efficiency purposes, I want to move the tensor memory to shared memory on the C++ side, where I have a thread pool for reading tensors from the network. \r\n\r\nAnother potential area where I am planning to use this feature is out-of process execution of PyTorch scripts in C++ land, where again, we can use shared memory to minimize the cost of transferring tensors across processes.\r\n\r\n## Pitch\r\n\r\nWhat I want is a `share_memory()` method on `at::Tensor`, mimicking the one on PyTorch tensors.\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"## üêõ Bug\r\n\r\nOn Top of Master Linux libtorch torch::cuda::is_available() returns zero even when linked against torch_cuda. I have linked against the latest wget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip\r\n\r\nIf you put the code inside pytorch's build system with cpp /tests it works ok. \r\n\r\nSnippet:\r\n\r\n    torch::Device device = torch::kCPU;\r\n    std::cout << \"CUDA DEVICE COUNT: \" << torch::cuda::device_count() << std::endl;\r\n    if (torch::cuda::is_available()) {\r\n      std::cout << \"CUDA is available! Training on GPU.\" << std::endl;\r\n      device = torch::kCUDA;\r\n    }\r\n\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Download library from https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip\r\n\r\n1. link above program and run. Cuda is not detected. \r\n1.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nCuda should be detected. \r\n\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @malfet @yf225 @glaringlee"},{"labels":["api",null,null],"text":"I found that it is slow to overwrite existing tensor using tensor::save.\r\n\r\n## Code\r\n```\r\n#include <chrono>\r\n#include <filesystem>\r\n#include <torch/torch.h>\r\n\r\n uint64_t now_ms() {\r\n    return static_cast<uint64_t>(\r\n        std::chrono::time_point_cast<std::chrono::milliseconds>(\r\n            std::chrono::steady_clock::now())\r\n            .time_since_epoch()\r\n            .count());\r\n  }\r\n\r\nint main(int argc, char **argv) {\r\n\r\n  auto tensor = torch::randn({1, 200 * 1024});\r\n  auto begin_ms =now_ms();\r\n  for (int i = 0; i < 100; i++) {\r\n    torch::save(tensor, \"tmp_file\");\r\n  }\r\n  auto end_ms = now_ms();\r\n  std::cout << \"insertion used \" << end_ms - begin_ms << \" ms\" << std::endl;\r\n\r\n  begin_ms = now_ms();\r\n  for (int i = 0; i < 100; i++) {\r\n    std::filesystem::remove(\"tmp_file\");\r\n    torch::save(tensor, \"tmp_file\");\r\n  }\r\n  end_ms = now_ms();\r\n  std::cout << \"insertion used \" << end_ms - begin_ms << \" ms\" << std::endl;\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\n## result\r\ninsertion used 557 ms\r\ninsertion used 82 ms\r\n\r\n## Strace results\r\n\r\n% time     seconds  usecs/call     calls    errors syscall\r\n------ ----------- ----------- --------- --------- ---------------- \r\n54.03    0.051799         517       100           writev\r\n 25.79    0.024721         103       238        92 openat \r\n16.86    0.016168         110       146           close \r\n\r\n\r\n% time     seconds  usecs/call     calls    errors syscall\r\n------ ----------- ----------- --------- --------- ----------------\r\n 61.46    0.020940         209       100           writev \r\n19.27    0.006565          65       100           unlink    \r\n  6.30    0.002147           9       238        92 openat  \r\n3.76    0.001281           9       137           mmap             \r\n  2.01    0.000684           4       146           close \r\n\r\nThe only significant difference between these two blocks is that writev syscall is twice slower in the first case.\r\n\r\n## Expectation\r\nOverwrite should be as fast as removing the old tensor manually\r\n\r\n## Environment\r\nUbuntu 20.04\r\npytorch version is github master\r\n\r\n\n\ncc @yf225 @glaringlee @VitalyFedyunin @ngimel"},{"labels":["api",null,null],"text":"nn::Sequential(nn::Linear(dim_states, h_neurons),\r\n\t\t\t\t\t\t\tnn::Functional(torch::tanh),\r\n\t\t\t\t\t\t\tnn::Linear(h_neurons, h_neurons),\r\n\t                        nn::Functional(torch::tanh),\r\n\t\t\t\t\t\t\tnn::Linear(h_neurons, dim_acts),\r\n\t\t\t\t\t\t\tnn::Functional(torch::softmax(-1)));\r\n\r\ncompile error\r\n../src/actorcritic.cpp:13:40: error: no matching function for call to ‚Äòsoftmax(int)‚Äô\r\n\r\nnn::Functional(torch::softmax((int64_t)-1)  \r\nerror: no matching function for call to ‚Äòsoftmax(int64_t)‚Äô\n\ncc @yf225 @glaringlee @albanD @mruberry"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\nMultiheadAttention c++ API did not register parameters under some conditions. The most important ones are 'in_proj_weight' and 'in_proj_bias'.\r\n\r\nHere is the parameter registration phase in c++ impl:\r\nhttps://github.com/pytorch/pytorch/blob/47766e648ff16d4b1175d04710caed566de14ab4/torch/csrc/api/src/nn/modules/activation.cpp#L450\r\n\r\nHere is the parameter registration phase in python impl:\r\nhttps://github.com/pytorch/pytorch/blob/75a4862f639de666c66a0db240c993918b80707f/torch/nn/modules/activation.py#L842\r\n\r\nThe problem here is that in python, the following code will register parameter automatically, but c++ version doesn't.\r\n```\r\nself.in_proj_bias = Parameter(torch.empty(3 * embed_dim))\r\n```\r\nThe reason is that `Module` has a customized __setattr__ which handles the parameter/module registration automatically:\r\nhttps://github.com/pytorch/pytorch/blob/eace0533985641d9c2f36e43e3de694aca886bd9/torch/nn/modules/module.py#L657\r\n\r\nThis will cause problem when people want to update parameters' value by iterating parameter that returned by calling parameters(), since some of the parameters won't be returned by c++ version. Here is an example:\r\nhttps://github.com/pytorch/pytorch/blob/f71cccc457421ad220cf58914c8bc3801b072300/test/test_nn.py#L4678\r\n\r\nThe possible equivalent c++ impl will be something like this:\r\n```\r\ntorch::MultiheadAttention model(torch::MultiheadAttentionOptions(4, 2).dropout(0.0));\r\n{\r\n    torch::NoGradGuard guard;\r\n    for (auto& p : model->parameters()) {\r\n      auto sz = p.view(-1).size(0);\r\n      p.copy_(torch::cos(torch::arange(0, sz, tensor_options).view(p.sizes())));\r\n    }\r\n}\r\n```\r\nCurrently, c++ code will return two less parameters from module->parameters() than python version due to not registering the parameter correctly.\r\n\r\n## To Reproduce\r\n\r\nAbove c++ code piece is an example.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nPython and c++ api should return same number of parameters when calling parameters().\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n ```\r\nPyTorch version: 1.7.0a0+fced54a\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 7.3.1 20180303 (Red Hat 7.3.1-5)\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.8\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration: \r\nGPU 0: Tesla M40\r\nGPU 1: Tesla M40\r\n\r\nNvidia driver version: 418.126.02\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.7.6.5\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.1.4\r\n```\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\r\n\r\ncc @yf225 @glaringlee"},{"labels":["api",null,null],"text":"## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nWhen I try to load a plugin library built with 2 cpp file,\r\n```python\r\ntorch.ops.load_library(\"libtorch_plugins.so\")\r\n```\r\n\r\nI got the following error:\r\n```\r\nOnly a single TORCH_LIBRARY can be used to register the namespace decode; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  Previous registration of TORCH_LIBRARY was registered at /home/cloudhan/workspaces/torch_plugins/src/readjpeg_cpu.cpp:146; latest registration was registered at /home/cloudhan/workspaces/torch_plugins/src/readpng_cpu.cpp:87\r\n```\r\n\r\n## Motivation\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nCurrently, the only valid way to build a plugin library is to collect all op registration into one translation unit. \r\nThat is\r\n```cpp\r\n// file foo.cpp\r\n\r\n// define op foo\r\n```\r\n\r\n```cpp\r\n// file bar.cpp\r\n\r\n// define op bar\r\n```\r\n\r\n```cpp\r\n// registration.cpp\r\n\r\n#include \"foo.h\"\r\n#include \"bar.h\"\r\n\r\n// register ops foo and bar \r\n```\r\nIn this way, it is hard to maintain flexible extension op set. Since to add a new op,  you need to directly modify `registration.cpp`\r\nTo remove unneeded ops, you also need to modify `registration.cpp`\r\n\r\nIf distributed registration is supported, all I need is\r\n```cpp\r\n// file foo.cpp\r\n\r\n// define op foo\r\n// register op foo\r\n```\r\n\r\n```cpp\r\n// file bar.cpp\r\n\r\n// define op bar\r\n// register op bar \r\n```\r\n\r\nSimply control which cpp file to be compiled and linked, I can now control the registration easier.\r\n\r\n\n\ncc @yf225 @glaringlee @ezyang @bhosmer @smessmer @ljk53"},{"labels":[null,"api",null],"text":"## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nFor a torch::PackedTensorAccessor object, when we access the elements of its, check, or cast the indexing number type.\r\n## Motivation\r\n```\r\ntorch::PackedTensorAccessor<scalar_t,1,torch::RestrictPtrTraits,size_t> some_values;\r\n\r\nconst float idx_float = some_index;\r\nconst int idx_int = some_index;\r\n\r\nsome_values[idx_float ] // may get weird results while no warning or error at all\r\nsome_values[idx_int]    // correct way\r\n```\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\nplease consider adding type checking or type casting to the indexing number.\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\n\ncc @yf225 @glaringlee"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\ni am working on translating pytorch code to c++ environment.\r\nat the pytorch code there is support to take and provide tensors of indexes along different dimensions, which change the output tensor shape,\r\nthis feature is not supported at the c++ framework  \r\n\r\nSteps to reproduce the behavior:\r\nat pytorch:\r\n\r\n1. tensor = torch.arange(25*4*96*170).reshape(25,4,96,170)\r\n2. a = torch.arange(25)\r\n3. x=a\r\n4. y=a\r\n5. output = tensor[a, :, y, x]\r\n\r\noutput shape is: [25,4]\r\n\r\nthought maybe to use the torch::index function, but it fails when number of dimensions is larger than 3.\r\n\r\nhow do i produce the same behavior at the c++ framework?\r\n\r\n - PyTorch Version (e.g., 1.0): torch==1.3.0\r\n - OS (e.g., Linux): Linux\r\n - Python version: Python 3.6.8\r\n\r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,"api"],"text":"I have a code that successfully compiles on some machines but not on the others. \r\n\r\nproject structure:\r\nforeach_cuda\r\n--[libtorch]\r\n--CMakeLists.txt\r\n--main.cu\r\n--[build] \r\n\r\n**How am i running it**\r\n-> module list \r\n 1) cuda/10.2   2) cudnn/v7.6.5.32-cuda.10.2\r\n\r\n-> cd build \r\n-> cmake -DCMAKE_PREFIX_PATH=/private/home/iuriiz/fun/cuda-tutorial/libtorch ..\r\n-> cmake --build . --config Release\r\nFAIL\r\n\r\nBuild error: \r\n_make[2]: *** No rule to make target '/usr/local/cuda/lib64/libnvToolsExt.so', needed by 'foreach_cuda'.  Stop.\r\nCMakeFiles/Makefile2:72: recipe for target 'CMakeFiles/foreach_cuda.dir/all' failed\r\nmake[1]: *** [CMakeFiles/foreach_cuda.dir/all] Error 2\r\nMakefile:83: recipe for target 'all' failed\r\nmake: *** [all] Error 2_\r\n\r\n**Setup**\r\nMachine: internal devfair\r\nCUDA version: 10.2\r\nLibtorch: https://download.pytorch.org/libtorch/cu102/libtorch-shared-with-deps-1.5.1.zip\r\n\r\n**Code**\r\nCMakeLists.txt\r\n```\r\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\r\nproject(foreach_cuda)\r\n\r\nfind_package(Torch REQUIRED)\r\nfind_package(CUDA REQUIRED)\r\n\r\ncuda_add_executable(foreach_cuda main.cu)\r\n\r\ntarget_link_libraries(foreach_cuda \"${TORCH_LIBRARIES}\")\r\nset_property(TARGET foreach_cuda PROPERTY CXX_STANDARD 14)\r\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\")\r\n```\r\n\r\nmain.cu\r\n\r\n```\r\n#include <iostream>\r\n#include <math.h>\r\n#include <torch/torch.h>\r\n#include <torch/cuda.h>\r\n#include <cuda.h>\r\n#include <cuda_runtime.h>\r\n#include <ATen/ATen.h>\r\n#include <ATen/AccumulateType.h>\r\n#include <ATen/cuda/CUDAContext.h>\r\n#include <ATen/cuda/Exceptions.h>\r\n#include <chrono>\r\n\r\n\r\nint main(void)\r\n{\r\n  std::cout << \"CUDA available: \" << torch::cuda::is_available() << std::endl;\r\n  \r\n  return 0;\r\n}\r\n```\r\n\r\n\n\ncc @malfet @yf225 @glaringlee"},{"labels":[null,null,"api",null,null],"text":"We now fully support undefined Tensors in the backward pass to represent a Tensor full of zeroes.\r\nWe should leverage this and stop creating full size Tensors for custom Function (both on python and cpp side)\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @yf225 @glaringlee @VitalyFedyunin @ngimel"},{"labels":["api",null,null],"text":"I want build some preprocess tensor in my c++ codeÔºå example as\r\n\r\n    # include \"common/libtorch/include/torch/script.h\"\r\n    # include \"common/libtorch/include/torch/csrc/api/include/torch/torch.h\"\r\n    # include <iostream>\r\n    # include <vector>\r\n    using namespace std;\r\n    int main(int argc, char *argv[]) {\r\n        // ÊûÑÂª∫Á§∫‰æãËæìÂÖ•\r\n        // std::vector<torch::jit::IValue> inputs;\r\n        std::vector<int64_t> res_data;\r\n        res_data.resize(1 * 3* 16 * 16);\r\n        cout << res_data.size() << endl;\r\n        torch::Tensor res_tensor = torch::from_blob(res_data.data(),{1, 3, 16, 16}, torch::kInt64);\r\n        cout << \"OK\" << endl;\r\n    }\r\n\r\nbut i got the error:\r\n\r\n    terminate called after throwing an instance of 'std::system_error'\r\n    what():  Unknown error -1\r\n    Aborted (core dumped)\r\n\r\nmy g++ version=5.5Ôºåusing std=c++14 optionÔºåmy libtorch version=1.5. Is someone know about this error ? thank you very much !!\r\n\r\n    \n\ncc @yf225 @glaringlee"},{"labels":["api",null,null],"text":"## üìö Documentation\r\n\r\nhttps://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor4itemEv\r\nand\r\nhttps://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4I0ENK2at6Tensor4itemE1Tv\r\n\r\nare both missing descriptions.\r\n\r\n--------\r\n\r\nI am trying to get a `float` out of a `Tensor` (that is a single element, kFloat32) on the GPU.\r\n\r\ne.g.\r\n`float x = x_tensor.item()`\r\nHowever, this gives a compile-time error:\r\n`error: cannot convert ‚Äòc10::Scalar‚Äô to ‚Äòdouble‚Äô in assignment`\r\n\r\nUsing `float x = x_tensor<float>.item()` compiles just fine, doesn't work either (program spins forever, probably waiting for CUDA kernel to end after a bad memory access or something).\r\n\r\nWhat is the proper way to get the float out of the tensor (and back into main memory)? It prints just fine, so it must be doable.\r\nI'd prefer to do it without printing and parsing or moving the entire tensor back to the CPU.\n\ncc @yf225 @glaringlee @jlin27"},{"labels":[null,"api",null,null],"text":"My previous program based on  libtorch  1.0.0 Slow down 100ms after the upgrade. And then I find that the same code ran libtorch1.0.0 would be faster than 1.3.0-1.5.1. That's why?  1.1.0-1.2.0 I have not test.\r\n\n\ncc @suo @gmagogsfm @yf225 @glaringlee @VitalyFedyunin @ngimel"},{"labels":["api",null,null],"text":"I'm trying to deploy a yolov5s model in my C++ program.\r\nI followed the instructions in [https://gist.github.com/jakepoz/eb36163814a8f1b6ceb31e8addbba270](url) to get a torchscript converted model.\r\nThese are my C++ code, I put these code in a thread in ORBSLAM (a SLAM system), meanwhile there are other SLAM threads running.\r\n```cpp\r\n  modelpath = \"yolov5s.torchscript\";\r\n  cout << \"before loading\" << endl;\r\n  long start = time_in_ms();\r\n  model = torch::jit::load(modelpath);\r\n  long end = time_in_ms();\r\n  cout << \"it took \" << end - start << \" ms to load the model\" << endl;\r\n  torch::jit::getProfilingMode() = false;\r\n  torch::jit::getExecutorMode() = false;\r\n  torch::jit::setGraphExecutorOptimize(false);\r\n\r\n  tensor_image = torch::zeros((1, 3, 640,640));\r\n  long start = time_in_ms();\r\n  std::vector<torch::jit::IValue> inputs;\r\n  inputs.push_back(torch::ones({1, 3, 640, 640}));\r\n  //inputs.emplace_back(tensor_image);\r\n  torch::jit::IValue output = model.forward(inputs);\r\n  long end = time_in_ms();\r\n  cout << \"it took \" << end - start << \" ms to run the model once\" << endl;\r\n```\r\n\r\nIt took 720ms to load the model and 1300ms to run the model once.\r\nBut when I run this model in python environment, it only takes 200ms.\r\nI would like to know if it's reasonable or what should I do to accelerate this.\r\n\r\n\r\n\r\n\r\n\r\n\n\ncc @yf225 @glaringlee @VitalyFedyunin @ngimel"},{"labels":[null,"api",null,null],"text":"When I work on torchvision I would like to see color from my compiler, but I don't have it.\n\ncc @yf225 @glaringlee"},{"labels":["api",null,null],"text":"## üêõ Bug\r\n\r\nI followed the example for [bind_module](https://pytorch.org/cppdocs/api/function_namespacetorch_1_1python_1a977cbbe6d9378ef36203873c87858095.html?highlight=pybind11_module) to test a simple python wrapper for a `torch::nn::Module subclass`. It compiles just fine, but when I try to import the compiled module, I get the error `ImportError: generic_type: type \"Net\" referenced unknown base type \"torch::nn::Module\"`.\r\n\r\n## To Reproduce\r\nSteps to reproduce the behavior:\r\n1. Create the following file structure\r\n```\r\n‚îú‚îÄ‚îÄ setup.py\r\n‚îú‚îÄ‚îÄ src\r\n     ‚îú‚îÄ‚îÄ pybind\r\n             ‚îî‚îÄ‚îÄ test_pybindings.cpp\r\n```\r\n2. `test_pybindings.cpp`:\r\n```\r\n#include <torch/torch.h>\r\n#include <torch/python.h>\r\n\r\nstruct Net : torch::nn::Module {\r\n    Net(int in, int out) { }\r\n    torch::Tensor forward(torch::Tensor x) { return x; }\r\n};\r\n\r\nPYBIND11_MODULE(test_module, m) {\r\n    torch::python::bind_module<Net>(m, \"Net\")\r\n        .def(py::init<int, int>())\r\n        .def(\"forward\", &Net::forward);\r\n}\r\n```\r\n\r\n3. `setup.py` (based on the example from [here](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.CppExtension):\r\n```\r\nimport sys\r\nimport torch.cuda\r\nfrom setuptools import setup\r\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension, CUDAExtension\r\nfrom torch.utils.cpp_extension import CUDA_HOME\r\n\r\next_modules = [\r\n    CppExtension(\r\n        'test_module',\r\n        ['src/pybind/test_pybindings.cpp'],\r\n        extra_compile_args=['-O3', '-g', '-Werror', '-fopenmp'])\r\n]\r\nsetup(name='test_module', ext_modules=ext_modules,\r\n        cmdclass={'build_ext': BuildExtension})\r\n```\r\n\r\n4. Alternatively, `CMakeLists.txt` (gives the same error):\r\n```    \r\ncmake_minimum_required(VERSION 2.8.8)\r\nproject(test_module)\r\n\r\n# Set this to something else on the command line if the path is different\r\nif (NOT CUDA_TOOLKIT_ROOT_DIR)\r\n    set(CUDA_TOOLKIT_ROOT_DIR /usr/local/cuda)\r\nendif()\r\n\r\nfind_package(Torch REQUIRED)\r\n\r\n# Try to compile with c++14\r\n# http://stackoverflow.com/a/25836953\r\ninclude(CheckCXXCompilerFlag)\r\nCHECK_CXX_COMPILER_FLAG(\"-std=c++14\" COMPILER_SUPPORTS_CXX14)\r\nif(COMPILER_SUPPORTS_CXX14)\r\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14\")\r\n    message(STATUS \"The compiler ${CMAKE_CXX_COMPILER} supports C++14.\")\r\nelse()\r\n    message(STATUS \"The compiler ${CMAKE_CXX_COMPILER} has no C++14 support. Please use a different C++ compiler.\")\r\nendif()\r\n\r\n# Enable compile optimizations and necessary flags\r\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -O3 -fopenmp -fPIC\")\r\n\r\n# Enable debug flags (use if you want to debug in gdb)\r\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -g3 -Wall\")\r\n\r\n# Include our header files\r\ninclude_directories(${TORCH_INCLUDE_DIRS})\r\n\r\n\r\n# TORCH_LIBRARIES doesn't include torch_python, so just get the path ourselves\r\nlink_directories(${TORCH_INSTALL_PREFIX}/lib/)\r\nadd_subdirectory(pybind11)\r\n\r\npybind11_add_module(test_module src/pybind/test_pybindings.cpp)\r\ntarget_link_libraries(test_module\r\n        PRIVATE c10 torch torch_cpu torch_python)\r\n```\r\n\r\n## Expected behavior\r\n\r\nWithin the proper build directory (or by adding it to the `PYTHONPATH`) I would expect `python3 -c \"import test_module\"` to run without error, but it gives:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: generic_type: type \"Net\" referenced unknown base type \"torch::nn::Module\"\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.6.0a0+dfbf016\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: GPU 0: GeForce RTX 2070 SUPER\r\nNvidia driver version: 450.36.06\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.19.0\r\n[pip3] torch==1.6.0a0+dfbf016\r\n[conda] Could not collect\r\n\r\n\r\n## Additional context\r\n\r\nNote that I installed pytorch from source with `python3 setup.py install --user`, and set `Torch_DIR` in my `.bashrc` to `~/.local/lib/python3.6/site-packages/torch/share/cmake/Torch` to allow the cmake build (which is preferred but not required for me), and to have the C++ torch and python version be the same build. \r\nI am on the master branch commit dfbf0164c9d47e89ec019668f3cc92ac345cfc8f.\r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"## üöÄ Feature\r\n\r\nThe ability to turn off scientific notation (standard form) using the C++ API.\r\n\r\n## Motivation\r\n\r\nIt would be useful to turn off the scientific notation used to display tensors. Sometimes I will often get numbers like: 9.9999e-01 which - in the common tongue - is more frequently known as 0.9999 or 1.0. This notation can be really helpful for very small numbers close to zero but in certain situations it is just confusing.\r\n\r\n## Pitch\r\n\r\nIt would be nice to have an option when creating tensors to turn off scientific notation.\r\n\n\ncc @yf225 @glaringlee"},{"labels":["api",null,null,null],"text":"## üêõ Bug\r\n\r\nUsing the data_parallel C++ interface results in code that is much slower on multiple GPUs than on a single GPU.  In addition, the GPU utilization is less than 10% with muliple GPUs compared to over 96% with a single GPU.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n Implement a non-trivial model (e.g. ResNet50 or SlowFast) using the libtorch c++ interface (I used libtorch-win-shared-with-deps-1.5.0.zip + cuda 10.1)\r\n\r\nAdd mulitiple GPU support via torch::nn::parallel::data_parallel\r\n\r\nTime training runs with a single GPU and with multiple GPUs \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nRuns with multiple GPUs should be faster than runs on a single GPU.  Definitely not *much* slower.\r\n\r\n## Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): LibTorch 1.5.0 pre-built library\r\n - OS (e.g., Linux): Windows 7\r\n - How you installed PyTorch (`conda`, `pip`, source): N/A\r\n - Build command you used (if compiling from source): \r\n - Python version: N/A\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: 2x GTX 1080\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\nThis is in line with what @dmagee reported in #18837. Looking though the code, it appears as if replicas of the modules are cloned and deleted on every iteration of training. Is there a way to use data_parallel and avoid this overhead?\n\ncc @VitalyFedyunin @ngimel @yf225 @glaringlee @albanD @mruberry"},{"labels":[null,"api",null,null],"text":"I want to perform on the GPU non-maximum-suppression on the output of a darknet/yolo CNN. \r\nThe following testing code works fine : \r\n\r\n//Credits: adapted from https://github.com/pprp to test nms parallel algo with at::Tensor as input\r\n#include <stdio.h>\r\n#include <stdlib.h>\r\n#include \"string\"\r\n//#include <windows.h>//needed for LoadLibrary\r\n#include <cuda_runtime.h>\r\n#include <iostream>\r\n#include <opencv2/core/core.hpp>\r\n#include <opencv2/highgui/highgui.hpp>\r\n#include \"opencv2/imgproc/imgproc.hpp\"\r\n#include \"device_launch_parameters.h\"\r\n\r\n#include \"device_functions.h\"\r\n\r\n#include <ATen/ATen.h>\r\n#include <ATen/cuda/CUDAContext.h>\r\n#include <THC/THC.h>\r\n#include <THC/THCDeviceUtils.cuh>\r\n\r\n\r\n#include \"torch/torch.h\"//->including torch gives problems!!!???\r\n\r\n\r\nusing namespace std;\r\n\r\n#define HANDLE_ERROR(ans) { gpuAssert((ans), __FILE__, __LINE__); }\r\n\r\ninline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\r\n{\r\n\tif (code != cudaSuccess)\r\n\t{\r\n\t\tfprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\r\n\t\tif (abort) exit(code);\r\n\t}\r\n}\r\n\r\n\r\ntypedef struct\r\n{\r\n\tdouble x,y,w,h;\r\n\tchar s[100];\r\n\tchar cls[100];\r\n\tdouble cmps;\r\n}box;\r\n\r\n__device__ inline float devIoU(float const* const   b1, float const* const  b2) {\r\n\tfloat ai = (float)(b1[2] + 1) * (b1[3] + 1);\r\n\tfloat aj = (float)(b2[2] + 1) * (b2[3] + 1);\r\n\tfloat x_inter, x2_inter, y_inter, y2_inter;\r\n\r\n\tx_inter = max(b1[0], b2[0]);\r\n\ty_inter = max(b1[1], b2[1]);\r\n\r\n\tx2_inter = min((b1[0] + b1[2]), (b2[0] + b2[2]));\r\n\ty2_inter = min((b1[1] + b1[3]), (b2[1] + b2[3]));\r\n\r\n\tfloat w = (float)max((float)0, x2_inter - x_inter);\r\n\tfloat h = (float)max((float)0, y2_inter - y_inter);\r\n\r\n\tfloat inter = ((w * h) / (ai + aj - w * h));\r\n\treturn inter;\r\n}\r\n\r\n\r\n__global__ void NMS_GPU(const int n_boxes, const float nms_overlap_thresh,\r\n\tconst float* dev_boxes, bool* d_res) {\r\n\tunsigned int xIndex = blockIdx.x * blockDim.x + threadIdx.x;\r\n\t//unsigned int xIndex = threadIdx.x;//only 1 block with index 0!\r\n\tfloat cur_box[5];\r\n\tfloat a_box[5];\r\n\tcur_box[0] = dev_boxes[xIndex * 5 + 0];\r\n\tcur_box[1] = dev_boxes[xIndex * 5 + 1];\r\n\tcur_box[2] = dev_boxes[xIndex * 5 + 2];\r\n\tcur_box[3] = dev_boxes[xIndex * 5 + 3];\r\n\tcur_box[4] = dev_boxes[xIndex * 5 + 4];\r\n\t//__syncthreads();//not necessary as cur_box is not a shared resource\r\n\tfor (int i = 0; i < 19; i++)\r\n\t{\r\n\t\tif (i != xIndex)\r\n\t\t{\r\n\t\t\ta_box[0] = dev_boxes[i * 5 + 0];\r\n\t\t\ta_box[1] = dev_boxes[i * 5 + 1];\r\n\t\t\ta_box[2] = dev_boxes[i * 5 + 2];\r\n\t\t\ta_box[3] = dev_boxes[i * 5 + 3];\r\n\t\t\ta_box[4] = dev_boxes[i * 5 + 4];\r\n\t\t\tif (a_box[4] < cur_box[4] )\r\n\t\t\t{\r\n\t\t\t\tif (devIoU(a_box, cur_box) > nms_overlap_thresh)\r\n\t\t\t\t{\r\n\t\t\t\t\td_res[i] = false;\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n\r\n\r\n\r\nint main()\r\n{\r\n\tint const threadsPerBlock = sizeof(unsigned long long) * 8;//Bufo : =64(float size)\r\n\t//LoadLibrary(TEXT(\"D:\\\\dev\\\\Cpp\\\\dependencies\\\\torchnew\\\\lib\\\\torch_cuda.dll\"));\r\n\tat::DeviceType device_type;\r\n\r\n\tif (at::cuda::is_available()) {\r\n\t\tdevice_type = at::kCUDA;\r\n\t}\r\n\telse {\r\n\t\tdevice_type = at::kCPU;\r\n\t\tstd::cout << \"No GPU avalaible, sorry ...\" << std::endl;\r\n\t\treturn 0;\r\n\t}\r\n\tat::Device device(device_type);\r\n\tstd::cout << \"Device : \" << device_type << std::endl;\r\n\r\n\tconst int count = 19;\r\n\tcv::Mat temp = cv::imread(\"Cow_45.jpg\",1);\r\n\r\n\tbool *h_res =(bool*)malloc(sizeof(bool)*count);//contains the result of the nms algo (cpu context)\r\n\tfor(int i=0; i<count; i++)\r\n\t{\r\n\t\th_res[i] = true;\r\n\t}\r\n\r\n\tbox b[count];\r\n\tb[0].x = 996.000000;b[0].y = 2566.420000;b[0].w = 170.793000;b[0].h=172.580000;\r\n\tstrcpy(b[0].cls,\"nose\");strcpy(b[0].s,\"0.983194\");b[0].cmps=0.983194;\r\n\tb[1].x = 4238.937000;b[1].y = 1594.513000;b[1].w = 160.063000;b[1].h=148.487000;\r\n\tstrcpy(b[1].cls,\"eye\");strcpy(b[1].s,\"0.992166\");b[1].cmps=0.992166;\r\n\tb[2].x = 4656.389000;b[2].y = 2175.186000;b[2].w = 316.180000;b[2].h=221.552000;\r\n\tstrcpy(b[2].cls,\"nose\");strcpy(b[2].s,\"0.994816\");b[2].cmps=0.994816;\r\n\tb[3].x = 4316.000000;b[3].y = 1660.000000;b[3].w = 127.474000;b[3].h=113.452000;\r\n\tstrcpy(b[3].cls,\"eye\");strcpy(b[3].s,\"0.990833\");b[3].cmps=0.990833;\r\n\tb[4].x = 997.013000;b[4].y = 2664.408000;b[4].w = 222.214000;b[4].h=229.068000;\r\n\tstrcpy(b[4].cls,\"nose\");strcpy(b[4].s,\"0.985067\");b[4].cmps=0.985067;\r\n\tb[5].x = 666.069000;b[5].y = 2029.219000;b[5].w = 135.689000;b[5].h=160.833000;\r\n\tstrcpy(b[5].cls,\"eye\");strcpy(b[5].s,\"0.993240\");b[5].cmps=0.993240;\r\n\tb[6].x = 4653.547000;b[6].y = 2324.000000;b[6].w = 338.125000;b[6].h=133.902000;\r\n\tstrcpy(b[6].cls,\"nose\");strcpy(b[6].s,\"0.982858\");b[6].cmps=0.982858;\r\n\tb[7].x = 4476.556000;b[7].y = 2131.557000;b[7].w = 253.402000;b[7].h=273.601000;\r\n\tstrcpy(b[7].cls,\"nose\");strcpy(b[7].s,\"0.959098\");b[7].cmps=0.959098;\r\n\tb[8].x = 754.326000;b[8].y = 2571.066000;b[8].w = 324.674000;b[8].h=161.605000;\r\n\tstrcpy(b[8].cls,\"nose\");strcpy(b[8].s,\"0.993699\");b[8].cmps=0.993699;\r\n\tb[9].x = 729.962000;b[9].y = 2658.741000;b[9].w = 349.038000;b[9].h=192.046000;\r\n\tstrcpy(b[9].cls,\"nose\");strcpy(b[9].s,\"0.986209\");b[9].cmps=0.986209;\r\n\tb[10].x = 1271.863000;b[10].y = 2058.679000;b[10].w = 138.781000;b[10].h=137.553000;\r\n\tstrcpy(b[10].cls,\"eye\");strcpy(b[10].s,\"0.989965\");b[10].cmps=0.989965;\r\n\tb[11].x = 4316.000000;b[11].y = 1601.751000;b[11].w = 134.204000;b[11].h=141.249000;\r\n\tstrcpy(b[11].cls,\"eye\");strcpy(b[11].s,\"0.988307\");b[11].cmps=0.988307;\r\n\tb[12].x = 650.901000;b[12].y = 2032.621000;b[12].w = 91.484000;b[12].h=42.112000;\r\n\tstrcpy(b[12].cls,\"eye\");strcpy(b[12].s,\"0.969982\");b[12].cmps=0.969982;\r\n\tb[13].x = 1328.000000;b[13].y = 2058.692000;b[13].w = 103.849000;b[13].h=136.518000;\r\n\tstrcpy(b[13].cls,\"eye\");strcpy(b[13].s,\"0.987316\");b[13].cmps=0.987316;\r\n\tb[14].x = 214.809000;b[14].y = 1599.809000;b[14].w = 1553.705000;b[14].h=1319.679000;\r\n\tstrcpy(b[14].cls,\"head\");strcpy(b[14].s,\"0.997623\");b[14].cmps=0.997623;\r\n\tb[15].x = 3826.177000;b[15].y = 1072.206000;b[15].w = 1254.063000;b[15].h=1412.903000;\r\n\tstrcpy(b[15].cls,\"head\");strcpy(b[15].s,\"0.997487\");b[15].cmps=0.997487;\r\n\tb[16].x = 729.632000;b[16].y = 2578.523000;b[16].w = 442.495000;b[16].h=302.378000;\r\n\tstrcpy(b[16].cls,\"nose\");strcpy(b[16].s,\"0.960093\");b[16].cmps=0.960093;\r\n\tb[17].x = 655.430000;b[17].y = 2031.151000;b[17].w = 91.570000;b[17].h=148.691000;\r\n\tstrcpy(b[17].cls,\"eye\");strcpy(b[17].s,\"0.993275\");b[17].cmps=0.993275;\r\n\tb[18].x = 4251.712000;b[18].y = 1660.000000;b[18].w = 147.288000;b[18].h=105.309000;\r\n\tstrcpy(b[18].cls,\"eye\");strcpy(b[18].s,\"0.992576\");b[18].cmps=0.992576;\r\n\r\n\t//***************************************************************************************************************************************\r\n\t//copy boxes to a  at::tensor\r\n\tat::Tensor boxes = at::zeros({ count,5 });\r\n\r\n\tfor (int i = 0; i < count; i++) {\r\n\t\tboxes[i][0] = b[i].x;\r\n\t\tboxes[i][1] = b[i].y;\r\n\t\tboxes[i][2] = b[i].w;\r\n\t\tboxes[i][3] = b[i].h;\r\n\t\tboxes[i][4] = b[i].cmps;\r\n\t}\r\n\r\n\tboxes  = boxes.to(device);\r\n\tfloat* boxes_as_array = boxes.data<float>();//convert at::tensor to a flat array\r\n\r\n\tfloat nms_overlap_thresh = 0.1;\r\n\r\n\t//Comment: this piece of code is apparently necessary to assign the Torch cuda context to the global cuda context\r\n\tTHCState* state = at::globalContext().lazyInitCUDA(); // TODO replace with getTHCState\r\n\tconst int col_blocks = THCCeilDiv(count, threadsPerBlock);\r\n\tunsigned long long* mask_dev = NULL;\r\n\tmask_dev = (unsigned long long*) THCudaMalloc(state,count * col_blocks * sizeof(unsigned long long));\r\n\t//-------------------------------------------------------------------------------------------------------------\r\n\r\n\tbool *d_res;\r\n    //port h_res to GPU\r\n\tHANDLE_ERROR(cudaMalloc((void**)&d_res, count*sizeof(bool)));\r\n\tHANDLE_ERROR(cudaMemcpy(d_res, h_res,sizeof(bool)*count, cudaMemcpyHostToDevice));\r\n\r\n\tNMS_GPU<<<dim3(1,count,1),count>>>(count,nms_overlap_thresh, boxes_as_array,d_res);\r\n\t\r\n\t//port d_res to CPU\r\n\tHANDLE_ERROR(cudaMemcpy(h_res, d_res, sizeof(bool)*count, cudaMemcpyDeviceToHost));\r\n\r\n\t//display result\r\n\tfor(int i =0; i<count ; i++)\r\n\t{\r\n\t\tif(*(h_res+i) == true)\r\n\t\t{\r\n\t\t\t//printf(\"GPU Draw: %d--%d\\n\",i,*(h_res+i));\r\n\t\t\tcv::putText(temp,b[i].cls,cv::Point((int)b[i].x,(int)b[i].y-5),cv::FONT_HERSHEY_SIMPLEX,1.7,cv::Scalar(255,255,255),5,8,0);\r\n\t\t\tcv::putText(temp,b[i].s,cv::Point((int)b[i].x+120,(int)b[i].y-5),cv::FONT_HERSHEY_SIMPLEX,1.7,cv::Scalar(255,255,255),5,8,0);\r\n\t\t\tcv::rectangle(temp,cv::Point((int)b[i].x,(int)b[i].y),cv::Point((int)b[i].x + (int)b[i].w,(int)b[i].y + (int)b[i].h),cv::Scalar(92.185,194),8,8,0);\r\n\t\t}\r\n\t}\r\n\tcv::namedWindow(\"Window\",0);\r\n\tcv::resizeWindow(\"Window\",1064,800);\r\n\tcv::imshow(\"Window\",temp);\r\n\tcv::waitKey(0);\r\n\treturn 0;\r\n}\r\n\r\nProblem : When I include \"torch/torch.h\"  (uncomment line 23) - something I need for rest of the project -  I get the following error (Visual Studio 2019):\r\n\r\n\"Error\t\tmember \"torch::jit::detail::ParameterPolicy::all_slots\" may not be initialized\tacudaNMSTEST\tD:\\dev\\Cpp\\dependencies\\torchnew\\include\\torch\\csrc\\jit\\api\\module.h\t490\t\"\r\n\r\nI struggled a couple of days to find a solution, but in vain. Any idea what is going on/what I did wrong?\r\n\r\n\n\ncc @malfet @yf225 @glaringlee @peterjc123 @nbcsm @guyang3532"},{"labels":[null,"api",null],"text":"## ‚ùì Questions and Help\r\nif in pytorch\r\n    `torch.manual_seed(0)`\r\nthen in libtorch\r\n    `torch::manual_seed(0);`\r\n\r\nso, in pytorch\r\n```\r\n    torch.cuda.manual_seed(0)\r\n    torch.cuda.manual_seed_all(0)\r\n```\r\nin libtorch\r\n`??`\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,null,"api",null,null,null,null],"text":"## üêõ Bug\r\n\r\nTensor.std(0) returns a scalar instead of a vector.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nfloat data[] = { 1, 2, 3, 4, 5, 6 };\r\ntorch::Tensor f = torch::from_blob(data, {2, 3});\r\nstd::cout << f.mean(0) << std::endl;\r\n// yields a 1-d tensor as expected.\r\n// 2.5000\r\n// 3.5000\r\n// 4.5000\r\n// [ CPUFloatType{3} ]\r\nstd::cout << f.std(0) << std::endl;\r\n// yields a scalar, unexpected.\r\n// 1.70783\r\n// [ CPUFloatType{} ]\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nTensor.std(0) should return a vector to be consistent with other methods. As @ptrblck pointed out in this thread https://discuss.pytorch.org/t/tensor-std-0-returns-a-scalar-instead-of-a-vector/85974, the reason this is happening is because 0 is being interpreted as the `unbiased` argument here, and suggests `f.std(true, 0)` as a workaround. However, even `f.std(true, 0)` actually collapses dimension 1, which is also unintuitive.\r\n\r\nIdeally, these behaviors should be fixed or listed in the documentation.\r\n\r\n## Environment\r\n\r\n`Libtorch 1.4.0` on XCode 11.4 \r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @ezyang @gchanan @zou3519 @yf225 @glaringlee @bhosmer @smessmer @ljk53 @SsnL"},{"labels":[null,"api",null],"text":"## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nAdd C++ API for [`torch.autograd.functional.jacobian`](https://pytorch.org/docs/stable/_modules/torch/autograd/functional.html#jacobian)\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nI would like to calculate the jacobian of my model output wrt to the input and the feature is available in the python api but was dismayed to find out that it was not in the C++ API.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nInclude `torch::autograd::functional::jacobian` functionality in C++ API\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\nN/A\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\r\nJust some links for easy browsing:\r\nhttps://github.com/pytorch/pytorch/tree/master/torch/csrc/autograd\r\nhttps://pytorch.org/cppdocs/api/namespace_torch__autograd.html\r\nhttps://pytorch.org/docs/stable/_modules/torch/autograd/functional.html#jacobian\r\n\r\n\r\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @yf225 @glaringlee"},{"labels":["api",null,null],"text":"\r\nThis is my first question ever on Github, so please accept my apologies if I have not posted this in the right place. With the release of the 1.5 stable version of the C++ API for PyTorch, there are some changes in some of the object interfaces. For instance, now\r\n\r\n`optimizer.options.learning_rate();`\r\n\r\nwon't work (here the optimiser being used is Adam) since `learning_rate` has changed to `lr` (see https://github.com/pytorch/pytorch/releases) but moreover the optimiser no longer has options (`no member named 'options' in 'torch::optim::Adam'`). So my question is: how would one run\r\n\r\n`optimizer.options.learning_rate();`\r\n\r\nor update the learning rate\r\n\r\n`optimizer.options.learning_rate(updatedlearningrate);`\r\n\r\nwith the new release? Any help will be appreciated! Thank you\n\ncc @yf225 @glaringlee @vincentqb"},{"labels":[null,null,"api",null],"text":"## üêõ Bug\r\n\r\n`libtorch/include/c10/util/logging_is_not_google_glog.h` exposes `ERROR`, `FATAL`, etc in the global namespace.\r\nThis is included by `torch/script.h`.\r\n\r\nThis causes the conflict with a user code.\r\n\r\n## Expected behavior\r\n\r\nPut them in some namespace.\r\n\r\n## Environment\r\n\r\nbuild-hash: 4ff3872a2099993bf7e...\r\nbuild-version: 1.5.0+cpu\r\n\n\ncc @malfet @yf225 @glaringlee"},{"labels":[null,"api",null,null],"text":"## üêõ Bug\r\n\r\nOn some macs, loading libtorch 1.5 crash the program right at initialization.\r\nThis does not happen on every mac.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Link your app to the official build of libtorch 1.5 macos https://download.pytorch.org/libtorch/cpu/libtorch-macos-1.5.0.zip\r\n2. Run your app\r\n\r\n**Here's the call stack from the crash report, look in particular from step 12 to 9:**\r\n\r\n7 libc++abi.dylib 0x00007fff6e660f86 __cxxabiv1::failed_throw(__cxxabiv1::__cxa_exception) + 27\r\n8 libc++abi.dylib 0x00007fff6e653f99 __cxa_throw + 113\r\n9 libtorch_cpu.dylib 0x0000000125ae3a5e Xbyak::CodeArray::CodeArray(unsigned long, void*, Xbyak::Allocator*) + 462\r\n10 libtorch_cpu.dylib 0x0000000125ae2f01 Xbyak::CodeGenerator::CodeGenerator(unsigned long, void*, Xbyak::Allocator*) + 33\r\n11 libtorch_cpu.dylib 0x0000000125a8bdbc mkldnn::impl::cpu::jit_avx512_core_cvt_ps_to_bf16_t::jit_avx512_core_cvt_ps_to_bf16_t(unsigned long) + 44\r\n12 libtorch_cpu.dylib 0x0000000125a90bd5 _GLOBAL__sub_I_bfloat16_utils.cpp + 2389\r\n13  dyld                          \t0x0000000110735592 ImageLoaderMachO::doModInitFunctions(ImageLoader::LinkContext const&) + 506\r\n\r\n**Step 9. of this stack points to lines 990-1003 from here:**\r\nhttps://github.com/oneapi-src/oneDNN/blob/rls-v1.2/src/cpu/xbyak/xbyak.h\r\n\r\nHere's the full Apple crash report I get:\r\n\r\nProcess:               SpectraLayers [30150]\r\nPath:                  /Applications/SpectraLayers 7.app/Contents/MacOS/SpectraLayers\r\nIdentifier:            com.Steinberg.SpectraLayers7\r\nVersion:               7.0.0.209 (7.0.0.209)\r\nCode Type:             X86-64 (Native)\r\nParent Process:        ??? [1]\r\nResponsible:           SpectraLayers [30150]\r\nUser ID:               501\r\n\r\nDate/Time:             2020-06-13 10:23:25.225 +0200\r\nOS Version:            Mac OS X 10.14.6 (18G5033)\r\nReport Version:        12\r\nBridge OS Version:     4.5 (17P5300)\r\nAnonymous UUID:        2B222A5C-DC20-4A64-D70C-25B43525FBE8\r\n\r\nSleep/Wake UUID:       68B4EFCE-FE40-4D40-A3A3-55D804AD0000\r\n\r\nTime Awake Since Boot: 220000 seconds\r\nTime Since Wake:       550 seconds\r\n\r\nSystem Integrity Protection: enabled\r\n\r\nCrashed Thread:        0  Dispatch queue: com.apple.main-thread\r\n\r\nException Type:        EXC_CRASH (SIGABRT)\r\nException Codes:       0x0000000000000000, 0x0000000000000000\r\nException Note:        EXC_CORPSE_NOTIFY\r\n\r\nApplication Specific Information:\r\n/Applications/SpectraLayers 7.app/Contents/MacOS/../Frameworks/libtorch_cpu.dylib\r\nterminating with uncaught exception of type Xbyak::Error: can't protect\r\nabort() called\r\n\r\nThread 0 Crashed:: Dispatch queue: com.apple.main-thread\r\n0   libsystem_kernel.dylib        \t0x00007fff715102c2 __pthread_kill + 10\r\n1   libsystem_pthread.dylib       \t0x00007fff715cbbf1 pthread_kill + 284\r\n2   libsystem_c.dylib             \t0x00007fff7147a6a6 abort + 127\r\n3   libc++abi.dylib               \t0x00007fff6e655641 abort_message + 231\r\n4   libc++abi.dylib               \t0x00007fff6e6557c7 default_terminate_handler() + 243\r\n5   libobjc.A.dylib               \t0x00007fff6fc08eeb _objc_terminate() + 105\r\n6   libc++abi.dylib               \t0x00007fff6e66119e std::__terminate(void (*)()) + 8\r\n7   libc++abi.dylib               \t0x00007fff6e660f86 __cxxabiv1::failed_throw(__cxxabiv1::__cxa_exception*) + 27\r\n8   libc++abi.dylib               \t0x00007fff6e653f99 __cxa_throw + 113\r\n9   libtorch_cpu.dylib            \t0x0000000125ae3a5e Xbyak::CodeArray::CodeArray(unsigned long, void*, Xbyak::Allocator*) + 462\r\n10  libtorch_cpu.dylib            \t0x0000000125ae2f01 Xbyak::CodeGenerator::CodeGenerator(unsigned long, void*, Xbyak::Allocator*) + 33\r\n11  libtorch_cpu.dylib            \t0x0000000125a8bdbc mkldnn::impl::cpu::jit_avx512_core_cvt_ps_to_bf16_t::jit_avx512_core_cvt_ps_to_bf16_t(unsigned long) + 44\r\n12  libtorch_cpu.dylib            \t0x0000000125a90bd5 _GLOBAL__sub_I_bfloat16_utils.cpp + 2389\r\n13  dyld                          \t0x0000000110735592 ImageLoaderMachO::doModInitFunctions(ImageLoader::LinkContext const&) + 506\r\n14  dyld                          \t0x0000000110735798 ImageLoaderMachO::doInitialization(ImageLoader::LinkContext const&) + 40\r\n15  dyld                          \t0x0000000110730bea ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 362\r\n16  dyld                          \t0x0000000110730b80 ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 256\r\n17  dyld                          \t0x0000000110730b80 ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 256\r\n18  dyld                          \t0x000000011072fd73 ImageLoader::processInitializers(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 133\r\n19  dyld                          \t0x000000011072fe05 ImageLoader::runInitializers(ImageLoader::LinkContext const&, ImageLoader::InitializerTimingList&) + 73\r\n20  dyld                          \t0x000000011071f765 dyld::initializeMainExecutable() + 199\r\n21  dyld                          \t0x0000000110724709 dyld::_main(macho_header const*, unsigned long, int, char const**, char const**, char const**, unsigned long*) + 6213\r\n22  dyld                          \t0x000000011071e503 dyldbootstrap::start(macho_header const*, int, char const**, long, macho_header const*, unsigned long*) + 1167\r\n23  dyld                          \t0x000000011071e036 _dyld_start + 54\r\n\r\nThread 0 crashed with X86 Thread State (64-bit):\r\n  rax: 0x0000000000000000  rbx: 0x00000001107bf5c0  rcx: 0x00007ffeec4851c8  rdx: 0x0000000000000000\r\n  rdi: 0x0000000000000307  rsi: 0x0000000000000006  rbp: 0x00007ffeec485200  rsp: 0x00007ffeec4851c8\r\n   r8: 0x00007ffeec485090   r9: 0x00007ffeec485260  r10: 0x0000000000000000  r11: 0x0000000000000206\r\n  r12: 0x0000000000000307  r13: 0x0000003000000008  r14: 0x0000000000000006  r15: 0x000000000000002d\r\n  rip: 0x00007fff715102c2  rfl: 0x0000000000000206  cr2: 0x00007fffa7b65188\r\n  \r\nLogical CPU:     0\r\nError Code:      0x02000148\r\nTrap Number:     133\r\n\r\n\r\nBinary Images:\r\n       0x103777000 -        0x10545dfff +com.Steinberg.SpectraLayers7 (7.0.0.209 - 7.0.0.209) <B039FAC5-7A29-38D2-900A-ED51613ED7DA> /Applications/SpectraLayers 7.app/Contents/MacOS/SpectraLayers\r\n       0x105504000 -        0x105505fff +libtorch.dylib (0) <45C1863F-6DFD-3F6D-8980-792B90E37513> /Applications/SpectraLayers 7.app/Contents/Frameworks/libtorch.dylib\r\n       0x10550b000 -        0x105534ff3 +libc10.dylib (0) <835FB93F-21EC-3E19-A899-E44F1917693C> /Applications/SpectraLayers 7.app/Contents/Frameworks/libc10.dylib\r\n       0x105557000 -        0x10599efff +org.qt-project.QtWidgets (5.15 - 5.15.0) <FB5EEB60-BD0B-3818-A2E9-CF69E0B1D74A> /Applications/SpectraLayers 7.app/Contents/Frameworks/QtWidgets.framework/Versions/5/QtWidgets\r\n       0x105b54000 -        0x106049fe3 +org.qt-project.QtGui (5.15 - 5.15.0) <FF68F42D-ECAB-3C23-AF28-A25D3930B7B2> /Applications/SpectraLayers 7.app/Contents/Frameworks/QtGui.framework/Versions/5/QtGui\r\n       0x1061a6000 -        0x1062b6ff3 +org.qt-project.QtNetwork (5.15 - 5.15.0) <FE520B4B-F1E2-3AC0-9D0E-FB64D77897CB> /Applications/SpectraLayers 7.app/Contents/Frameworks/QtNetwork.framework/Versions/5/QtNetwork\r\n       0x106316000 -        0x10631affb +org.qt-project.QtConcurrent (5.15 - 5.15.0) <82AD3E1D-C55A-356C-9AEF-D1B8A463985C> /Applications/SpectraLayers 7.app/Contents/Frameworks/QtConcurrent.framework/Versions/5/QtConcurrent\r\n       0x106323000 -        0x10686d657 +org.qt-project.QtCore (5.15 - 5.15.0) <EACC2818-F08D-3861-A8EF-EA2F17800B66> /Applications/SpectraLayers 7.app/Contents/Frameworks/QtCore.framework/Versions/5/QtCore\r\n       0x106965000 -        0x107a19c17 +libtensorflow_framework.1.dylib (0) <93EA5990-E075-3827-B1F8-3FE1797F3F7F> /Applications/SpectraLayers 7.app/Contents/Frameworks/libtensorflow_framework.1.dylib\r\n       0x108465000 -        0x108598fc7 +libiomp5.dylib (0) <52F67CC7-A4B0-3F4D-A80D-7DC28D4A776A> /Applications/SpectraLayers 7.app/Contents/Frameworks/libiomp5.dylib\r\n       0x11071d000 -        0x11078770f  dyld (655.1.1) <DF71FC3D-E58F-3D48-9165-3B96FF8BFA22> /usr/lib/dyld\r\n       0x1107e8000 -        0x116a28feb +libtensorflow.1.dylib (0) <F2A0BBE5-BBE7-3D75-8755-9A865602E0CD> /Applications/SpectraLayers 7.app/Contents/Frameworks/libtensorflow.1.dylib\r\n       0x11cb55000 -        0x1284fcf4f +libtorch_cpu.dylib (0) <17C36C10-9231-3506-8008-B4D7185D1ECE> /Applications/SpectraLayers 7.app/Contents/Frameworks/libtorch_cpu.dylib\r\n    0x7fff41465000 -     0x7fff41469fff  com.apple.agl (3.3.2 - AGL-3.3.2) <A5954DED-265B-395D-B907-3CEC000B10B6> /System/Library/Frameworks/AGL.framework/Versions/A/AGL\r\n    0x7fff41814000 -     0x7fff41814fff  com.apple.Accelerate (1.11 - Accelerate 1.11) <762942CB-CFC9-3A0C-9645-A56523A06426> /System/Library/Frameworks/Accelerate.framework/Versions/A/Accelerate\r\n    0x7fff4182c000 -     0x7fff41ec5fef  com.apple.vImage (8.1 - ???) <53FA3611-894E-3158-A654-FBD2F70998FE> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vImage.framework/Versions/A/vImage\r\n    0x7fff41ec6000 -     0x7fff4213fff3  libBLAS.dylib (1243.200.4) <417CA0FC-B6CB-3FB3-ACBC-8914E3F62D20> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib\r\n    0x7fff42140000 -     0x7fff421b2ffb  libBNNS.dylib (38.250.1) <538D12A2-9B9D-3E22-9896-F90F6E69C06E> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBNNS.dylib\r\n    0x7fff421b3000 -     0x7fff4255cff3  libLAPACK.dylib (1243.200.4) <92175DF4-863A-3780-909A-A3E5C410F2E9> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib\r\n    0x7fff4255d000 -     0x7fff42572feb  libLinearAlgebra.dylib (1243.200.4) <CB671EE6-DEA1-391C-9B2B-AA09A46B4D7A> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLinearAlgebra.dylib\r\n    0x7fff42573000 -     0x7fff42578ff3  libQuadrature.dylib (3.200.2) <1BAE7E22-2862-379F-B334-A3756067730F> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libQuadrature.dylib\r\n    0x7fff42579000 -     0x7fff425f5ff3  libSparse.dylib (79.200.5) <E78B33D3-672A-3C53-B512-D3DDB2E9AC8D> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libSparse.dylib\r\n    0x7fff425f6000 -     0x7fff42609fe3  libSparseBLAS.dylib (1243.200.4) <E9243341-DB77-37C1-97C5-3DFA00DD70FA> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libSparseBLAS.dylib\r\n    0x7fff4260a000 -     0x7fff427f1ff7  libvDSP.dylib (671.250.4) <7B110627-A9C1-3FB7-A077-0C7741BA25D8> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvDSP.dylib\r\n    0x7fff427f2000 -     0x7fff428a5ff7  libvMisc.dylib (671.250.4) <D5BA4812-BFFC-3CD0-B382-905CD8555DA6> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib\r\n    0x7fff428a6000 -     0x7fff428a6fff  com.apple.Accelerate.vecLib (3.11 - vecLib 3.11) <74288115-EF61-30B6-843F-0593B31D4929> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/vecLib\r\n    0x7fff42a48000 -     0x7fff437fdffb  com.apple.AppKit (6.9 - 1671.60.109) <78DB9AAE-C127-3BAA-8BAE-145AAFBFFBA2> /System/Library/Frameworks/AppKit.framework/Versions/C/AppKit\r\n    0x7fff4384f000 -     0x7fff4384ffff  com.apple.ApplicationServices (50.1 - 50.1) <DD5FDF45-E7C1-335C-8757-3D714CBA9367> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/ApplicationServices\r\n    0x7fff43850000 -     0x7fff438bbfff  com.apple.ApplicationServices.ATS (377 - 453.11.2.2) <A258DA73-114B-3102-A056-4AAAD3CEB9DD> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/ATS\r\n    0x7fff43954000 -     0x7fff43a6bff7  libFontParser.dylib (228.6.2.5) <CEDF1D5A-8897-3621-A1DC-558301F8BE05> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libFontParser.dylib\r\n    0x7fff43a6c000 -     0x7fff43aaefff  libFontRegistry.dylib (228.12.2.4) <6DDE44EC-FF6B-3893-9209-45E0955ABDD5> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libFontRegistry.dylib\r\n    0x7fff43b9f000 -     0x7fff43ba3ff3  com.apple.ColorSyncLegacy (4.13.0 - 1) <E8E9342C-47EB-359D-A373-554AC19B174A> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ColorSyncLegacy.framework/Versions/A/ColorSyncLegacy\r\n    0x7fff43c3e000 -     0x7fff43c90ff7  com.apple.HIServices (1.22 - 628) <2BE461FF-80B9-30D3-A574-AED5724B1C1B> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/HIServices.framework/Versions/A/HIServices\r\n    0x7fff43c91000 -     0x7fff43ca0fff  com.apple.LangAnalysis (1.7.0 - 1.7.0) <F5617A2A-FEA6-3832-B5BA-C2111B98786F> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/LangAnalysis.framework/Versions/A/LangAnalysis\r\n    0x7fff43ca1000 -     0x7fff43ceaff7  com.apple.print.framework.PrintCore (14.7 - 503.8) <E1D0FCBC-155E-372E-A90F-4A20B94FC114> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/PrintCore.framework/Versions/A/PrintCore\r\n    0x7fff43ceb000 -     0x7fff43d24ff7  com.apple.QD (3.12 - 407.2) <28C7D39F-59C9-3314-BECC-67045487229C> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/QD.framework/Versions/A/QD\r\n    0x7fff43d25000 -     0x7fff43d31fff  com.apple.speech.synthesis.framework (8.1.3 - 8.1.3) <5E7B9BD4-122B-3012-A044-3259C97E7509> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/SpeechSynthesis.framework/Versions/A/SpeechSynthesis\r\n    0x7fff43d32000 -     0x7fff43fa9fff  com.apple.audio.toolbox.AudioToolbox (1.14 - 1.14) <32487CB2-246B-3B80-8F60-D65DFC367DDC> /System/Library/Frameworks/AudioToolbox.framework/Versions/A/AudioToolbox\r\n    0x7fff43fab000 -     0x7fff43fabfff  com.apple.audio.units.AudioUnit (1.14 - 1.14) <B489CFDA-DEF3-38F5-A815-23EC30B8DA03> /System/Library/Frameworks/AudioUnit.framework/Versions/A/AudioUnit\r\n    0x7fff44304000 -     0x7fff446a6fff  com.apple.CFNetwork (978.3 - 978.3) <6A5459BD-77A4-386C-872D-9BB297D83588> /System/Library/Frameworks/CFNetwork.framework/Versions/A/CFNetwork\r\n    0x7fff446bb000 -     0x7fff446bbfff  com.apple.Carbon (158 - 158) <38182BEA-597C-39AC-B4BA-4849E24EE84E> /System/Library/Frameworks/Carbon.framework/Versions/A/Carbon\r\n    0x7fff446bc000 -     0x7fff446bfffb  com.apple.CommonPanels (1.2.6 - 98) <1CD6D56D-8EC7-3528-8CBC-FC69533519B5> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/CommonPanels.framework/Versions/A/CommonPanels\r\n    0x7fff446c0000 -     0x7fff449b7fff  com.apple.HIToolbox (2.1.1 - 918.7) <88D7F19C-8C9D-384B-BAB5-8205CA282F2C> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/HIToolbox.framework/Versions/A/HIToolbox\r\n    0x7fff449b8000 -     0x7fff449bbff3  com.apple.help (1.3.8 - 66) <A08517EB-8958-36C9-AEE0-1A8FEEACBE3F> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Help.framework/Versions/A/Help\r\n    0x7fff449bc000 -     0x7fff449c1ff7  com.apple.ImageCapture (9.0 - 1534.2) <DB063E87-ED8F-3E4E-A7E2-A6B45FA73EF7> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/ImageCapture.framework/Versions/A/ImageCapture\r\n    0x7fff449c2000 -     0x7fff44a57ff3  com.apple.ink.framework (10.9 - 225) <7C7E9483-2E91-3DD3-B1E0-C238F42CA0DD> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Ink.framework/Versions/A/Ink\r\n    0x7fff44a58000 -     0x7fff44a70ff7  com.apple.openscripting (1.7 - 179.1) <9B8C1ECC-5864-3E21-9149-863E884EA25C> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/OpenScripting.framework/Versions/A/OpenScripting\r\n    0x7fff44a90000 -     0x7fff44a91ff7  com.apple.print.framework.Print (14.2 - 267.4) <A7A9D2A0-D4E0-35EF-A0F7-50521F707C33> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Print.framework/Versions/A/Print\r\n    0x7fff44a92000 -     0x7fff44a94ff7  com.apple.securityhi (9.0 - 55006) <05717F77-7A7B-37E6-AB3E-03F063E9095B> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SecurityHI.framework/Versions/A/SecurityHI\r\n    0x7fff44a95000 -     0x7fff44a9bff7  com.apple.speech.recognition.framework (6.0.3 - 6.0.3) <3CC050FB-EBCB-3087-8EA5-F378C8F99217> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SpeechRecognition.framework/Versions/A/SpeechRecognition\r\n    0x7fff44bbd000 -     0x7fff44bbdfff  com.apple.Cocoa (6.11 - 23) <BB61D501-2D32-3DA2-9573-0C884189B211> /System/Library/Frameworks/Cocoa.framework/Versions/A/Cocoa\r\n    0x7fff44bcb000 -     0x7fff44d1aff7  com.apple.ColorSync (4.13.0 - 3345.6) <356BA478-76DE-3087-86BE-5E884276AB83> /System/Library/Frameworks/ColorSync.framework/Versions/A/ColorSync\r\n    0x7fff44ea6000 -     0x7fff44f2cfff  com.apple.audio.CoreAudio (4.3.0 - 4.3.0) <1E8E64E6-0E58-375A-97F7-07CB4EE181AC> /System/Library/Frameworks/CoreAudio.framework/Versions/A/CoreAudio\r\n    0x7fff44f90000 -     0x7fff44fbaffb  com.apple.CoreBluetooth (1.0 - 1) <4F2DDEF0-1F92-384B-8CDA-4958725D0A8E> /System/Library/Frameworks/CoreBluetooth.framework/Versions/A/CoreBluetooth\r\n    0x7fff44fbb000 -     0x7fff45340fef  com.apple.CoreData (120 - 866.6) <132CB39B-8D58-30FA-B8AD-49BFFF34B293> /System/Library/Frameworks/CoreData.framework/Versions/A/CoreData\r\n    0x7fff45341000 -     0x7fff45431ff7  com.apple.CoreDisplay (101.3 - 110.18) <6DD41271-E145-3E99-9D49-7CC8AC1C65B6> /System/Library/Frameworks/CoreDisplay.framework/Versions/A/CoreDisplay\r\n    0x7fff45432000 -     0x7fff45877ff7  com.apple.CoreFoundation (6.9 - 1575.235) <BECD568A-70AE-32BC-A6F1-A69BD61C1D37> /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\r\n    0x7fff45879000 -     0x7fff45f09fe7  com.apple.CoreGraphics (2.0 - 1265.10) <92E5B053-A926-3788-B3BB-E563B2B96836> /System/Library/Frameworks/CoreGraphics.framework/Versions/A/CoreGraphics\r\n    0x7fff45f0b000 -     0x7fff4622bfff  com.apple.CoreImage (14.4.0 - 750.0.140) <11026E39-D2FF-3CF6-8ACE-7BA293F9853E> /System/Library/Frameworks/CoreImage.framework/Versions/A/CoreImage\r\n    0x7fff46689000 -     0x7fff46689fff  com.apple.CoreServices (946 - 946) <BC6F47BD-5947-32C3-BDC9-4C63AF659F4E> /System/Library/Frameworks/CoreServices.framework/Versions/A/CoreServices\r\n    0x7fff4668a000 -     0x7fff46706ff7  com.apple.AE (773 - 773) <55AE7C9E-27C3-30E9-A047-3B92A6FD53B4> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/AE.framework/Versions/A/AE\r\n    0x7fff46707000 -     0x7fff469defff  com.apple.CoreServices.CarbonCore (1178.33 - 1178.33) <CB87F0C7-2CD6-3983-8E32-B6A2EC925352> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/CarbonCore.framework/Versions/A/CarbonCore\r\n    0x7fff469df000 -     0x7fff46a27ff7  com.apple.DictionaryServices (1.2 - 284.16.4) <746EB200-DC51-30AE-9CBC-608A7B4CC8DA> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/DictionaryServices.framework/Versions/A/DictionaryServices\r\n    0x7fff46a28000 -     0x7fff46a30ffb  com.apple.CoreServices.FSEvents (1239.200.13 - 1239.200.13) <5913F08D-4AA2-3200-B998-012E6A19A66D> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/FSEvents.framework/Versions/A/FSEvents\r\n    0x7fff46a31000 -     0x7fff46be2ff7  com.apple.LaunchServices (946 - 946) <A0C91634-9410-38E8-BC11-7A5A369E6BA5> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/LaunchServices.framework/Versions/A/LaunchServices\r\n    0x7fff46be3000 -     0x7fff46c81ff7  com.apple.Metadata (10.7.0 - 1191.58) <89DA10B4-5695-3FD9-A920-C34C33957868> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/Metadata.framework/Versions/A/Metadata\r\n    0x7fff46c82000 -     0x7fff46cccff7  com.apple.CoreServices.OSServices (946 - 946) <20C4EEF8-D5AC-39A0-9B4A-78F88E3EFBCC> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/OSServices.framework/Versions/A/OSServices\r\n    0x7fff46ccd000 -     0x7fff46d34ff7  com.apple.SearchKit (1.4.0 - 1.4.0) <DA08AA6F-A6F1-36C0-87F4-E26294E51A3A> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SearchKit.framework/Versions/A/SearchKit\r\n    0x7fff46d35000 -     0x7fff46d56ff3  com.apple.coreservices.SharedFileList (71.28 - 71.28) <487A8464-729E-305A-B5D1-E3FE8EB9CFC5> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SharedFileList.framework/Versions/A/SharedFileList\r\n    0x7fff47061000 -     0x7fff471c3ff3  com.apple.CoreText (352.0 - 584.26.3.4) <7247EA49-9F90-3258-AABF-932C6343BD6A> /System/Library/Frameworks/CoreText.framework/Versions/A/CoreText\r\n    0x7fff471c4000 -     0x7fff47204ff3  com.apple.CoreVideo (1.8 - 281.4) <10CF8E52-07E3-382B-8091-2CEEEFFA69B4> /System/Library/Frameworks/CoreVideo.framework/Versions/A/CoreVideo\r\n    0x7fff47205000 -     0x7fff47294fff  com.apple.framework.CoreWLAN (13.0 - 1375.2) <EC43DDAD-D838-3469-88F1-667EF5963AD1> /System/Library/Frameworks/CoreWLAN.framework/Versions/A/CoreWLAN\r\n    0x7fff474eb000 -     0x7fff474f0ffb  com.apple.DiskArbitration (2.7 - 2.7) <0D7444BE-7F82-3480-8873-08E8EBC6DE47> /System/Library/Frameworks/DiskArbitration.framework/Versions/A/DiskArbitration\r\n    0x7fff476b7000 -     0x7fff47a64ffb  com.apple.Foundation (6.9 - 1575.235) <60DBA0A0-F514-3755-9E19-A62216D7C856> /System/Library/Frameworks/Foundation.framework/Versions/C/Foundation\r\n    0x7fff47ad3000 -     0x7fff47b02ffb  com.apple.GSS (4.0 - 2.0) <E2B90D08-3857-3155-9FCC-07D778988EC9> /System/Library/Frameworks/GSS.framework/Versions/A/GSS\r\n    0x7fff47c02000 -     0x7fff47d0cfff  com.apple.Bluetooth (6.0.14 - 6.0.14d8) <526047AA-211F-3E6E-9418-B0379AE193F5> /System/Library/Frameworks/IOBluetooth.framework/Versions/A/IOBluetooth\r\n    0x7fff47d6f000 -     0x7fff47dfefff  com.apple.framework.IOKit (2.0.2 - 1483.260.4) <8A90F547-86EF-3DFB-92FE-0E2C0376DD84> /System/Library/Frameworks/IOKit.framework/Versions/A/IOKit\r\n    0x7fff47e00000 -     0x7fff47e0fffb  com.apple.IOSurface (255.6.1 - 255.6.1) <85F85EBB-EA59-3A8B-B3EB-7C20F3CC77AE> /System/Library/Frameworks/IOSurface.framework/Versions/A/IOSurface\r\n    0x7fff47e63000 -     0x7fff47feffef  com.apple.ImageIO.framework (3.3.0 - 1850.2.6) <CA362EE8-5EFD-33C3-AAB8-241846C486F1> /System/Library/Frameworks/ImageIO.framework/Versions/A/ImageIO\r\n    0x7fff47ff0000 -     0x7fff47ff4ffb  libGIF.dylib (1850.2.6) <22A37594-65B6-38D3-BE4E-FB4ECFA1D93C> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libGIF.dylib\r\n    0x7fff47ff5000 -     0x7fff480d1fe7  libJP2.dylib (1850.2.6) <5CC65160-9FF8-3E92-858D-17FB9E80B43D> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libJP2.dylib\r\n    0x7fff480d2000 -     0x7fff480f7feb  libJPEG.dylib (1850.2.6) <E484FD1B-0033-3177-A1C5-2E2795E9CBD8> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libJPEG.dylib\r\n    0x7fff483ba000 -     0x7fff483e0feb  libPng.dylib (1850.2.6) <05430FA2-EF4E-3765-AF93-463F01E6EBA7> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libPng.dylib\r\n    0x7fff483e1000 -     0x7fff483e3ffb  libRadiance.dylib (1850.2.6) <AD9283A9-9CA9-3ECE-9236-1D142B94645B> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libRadiance.dylib\r\n    0x7fff483e4000 -     0x7fff48431feb  libTIFF.dylib (1850.2.6) <BB39423E-2C3A-3C65-A497-2F951354E0A1> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libTIFF.dylib\r\n    0x7fff495a4000 -     0x7fff495bdfff  com.apple.Kerberos (3.0 - 1) <DB1E0679-37E1-3B93-9789-32F63D660C3B> /System/Library/Frameworks/Kerberos.framework/Versions/A/Kerberos\r\n    0x7fff49fd9000 -     0x7fff4a081ff7  com.apple.Metal (162.2 - 162.2) <B65C71BF-D40E-3BB3-940C-117DDD203551> /System/Library/Frameworks/Metal.framework/Versions/A/Metal\r\n    0x7fff4a09d000 -     0x7fff4a0bcff7  com.apple.MetalPerformanceShaders.MPSCore (1.0 - 1) <44CE8362-E972-3697-AD6F-15BC863BAEB8> /System/Library/Frameworks/MetalPerformanceShaders.framework/Frameworks/MPSCore.framework/Versions/A/MPSCore\r\n    0x7fff4a0bd000 -     0x7fff4a139fe7  com.apple.MetalPerformanceShaders.MPSImage (1.0 - 1) <EE8440DA-66DF-3923-ABBC-E0543211C069> /System/Library/Frameworks/MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Versions/A/MPSImage\r\n    0x7fff4a13a000 -     0x7fff4a161fff  com.apple.MetalPerformanceShaders.MPSMatrix (1.0 - 1) <E64450DF-2B96-331E-B7F4-666E00571C70> /System/Library/Frameworks/MetalPerformanceShaders.framework/Frameworks/MPSMatrix.framework/Versions/A/MPSMatrix\r\n    0x7fff4a162000 -     0x7fff4a28dff7  com.apple.MetalPerformanceShaders.MPSNeuralNetwork (1.0 - 1) <F2CF26B6-73F1-3644-8FE9-CDB9B2C4501F> /System/Library/Frameworks/MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Versions/A/MPSNeuralNetwork\r\n    0x7fff4a28e000 -     0x7fff4a2a8fff  com.apple.MetalPerformanceShaders.MPSRayIntersector (1.0 - 1) <B33A35C3-0393-366B-ACFB-F4BB6A5F7B4A> /System/Library/Frameworks/MetalPerformanceShaders.framework/Frameworks/MPSRayIntersector.framework/Versions/A/MPSRayIntersector\r\n    0x7fff4a2a9000 -     0x7fff4a2aaff7  com.apple.MetalPerformanceShaders.MetalPerformanceShaders (1.0 - 1) <69F14BCF-C5C5-3BF8-9C31-8F87D2D6130A> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/MetalPerformanceShaders\r\n    0x7fff4b0a1000 -     0x7fff4b0adff7  com.apple.NetFS (6.0 - 4.0) <E917806F-0607-3292-B2D6-A15404D61B99> /System/Library/Frameworks/NetFS.framework/Versions/A/NetFS\r\n    0x7fff4db4b000 -     0x7fff4dba2ff7  com.apple.opencl (2.15.3 - 2.15.3) <3F72F3B0-F607-39E5-BDF6-5C37C9B67430> /System/Library/Frameworks/OpenCL.framework/Versions/A/OpenCL\r\n    0x7fff4dba3000 -     0x7fff4dbbeff7  com.apple.CFOpenDirectory (10.14 - 207.200.4) <F03D84EB-49B2-3A00-9127-B9A269824026> /System/Library/Frameworks/OpenDirectory.framework/Versions/A/Frameworks/CFOpenDirectory.framework/Versions/A/CFOpenDirectory\r\n    0x7fff4dbbf000 -     0x7fff4dbcaffb  com.apple.OpenDirectory (10.14 - 207.200.4) <A8020CEE-5B78-3581-A735-EA2833683F31> /System/Library/Frameworks/OpenDirectory.framework/Versions/A/OpenDirectory\r\n    0x7fff4e51a000 -     0x7fff4e51cfff  libCVMSPluginSupport.dylib (17.7.3) <83C36A70-5F35-37D1-A124-A2CD497F1915> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCVMSPluginSupport.dylib\r\n    0x7fff4e51d000 -     0x7fff4e522ff3  libCoreFSCache.dylib (166.5) <5BC99EE7-7FFD-3F30-9AEE-EEDC25067AC4> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCoreFSCache.dylib\r\n    0x7fff4e523000 -     0x7fff4e527fff  libCoreVMClient.dylib (166.5) <B8FA5858-8185-3992-AD3B-A81AF08C3CDD> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCoreVMClient.dylib\r\n    0x7fff4e528000 -     0x7fff4e530ff7  libGFXShared.dylib (17.7.3) <09F50639-F0CB-3312-8BAC-5AA8083350F6> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGFXShared.dylib\r\n    0x7fff4e531000 -     0x7fff4e53cfff  libGL.dylib (17.7.3) <4EEC82D4-A3C6-336D-9F90-67F1D24ED35B> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGL.dylib\r\n    0x7fff4e53d000 -     0x7fff4e577fef  libGLImage.dylib (17.7.3) <B175F261-69D1-3366-A009-DF75CB32E2F9> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLImage.dylib\r\n    0x7fff4e6eb000 -     0x7fff4e729fff  libGLU.dylib (17.7.3) <5BB2F84A-4D96-35A1-8A5B-99AE39CD9E59> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLU.dylib\r\n    0x7fff4f0c6000 -     0x7fff4f0d5ffb  com.apple.opengl (17.7.3 - 17.7.3) <9C1FCE6E-FED6-3A09-9FDC-4C81F8CDB2A6> /System/Library/Frameworks/OpenGL.framework/Versions/A/OpenGL\r\n    0x7fff4fedf000 -     0x7fff50136ff7  com.apple.QuartzCore (1.11 - 701.14) <1E82D0E6-EB06-3CC6-AEB5-06E3365887D0> /System/Library/Frameworks/QuartzCore.framework/Versions/A/QuartzCore\r\n    0x7fff5096c000 -     0x7fff50c6cff7  com.apple.security (7.0 - 58286.270.6) <0FA49A7B-26C3-3DEF-B369-046E6D0CDF12> /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n    0x7fff50c6d000 -     0x7fff50cf9fff  com.apple.securityfoundation (6.0 - 55185.260.1) <C5C23F73-34A8-3676-9FFB-B18ABB42DA1A> /System/Library/Frameworks/SecurityFoundation.framework/Versions/A/SecurityFoundation\r\n    0x7fff50d2b000 -     0x7fff50d2ffff  com.apple.xpc.ServiceManagement (1.0 - 1) <7F9EC269-38C8-334B-976A-3951021B28B7> /System/Library/Frameworks/ServiceManagement.framework/Versions/A/ServiceManagement\r\n    0x7fff510c8000 -     0x7fff51135fff  com.apple.SystemConfiguration (1.17 - 1.17) <30C8327F-3EFF-3520-9C50-016F8B6B954F> /System/Library/Frameworks/SystemConfiguration.framework/Versions/A/SystemConfiguration\r\n    0x7fff54371000 -     0x7fff54416fff  com.apple.APFS (1.0 - 1) <06284DE8-5883-39F8-B04D-0D5EA74D12ED> /System/Library/PrivateFrameworks/APFS.framework/Versions/A/APFS\r\n    0x7fff54e2b000 -     0x7fff54e2cff7  com.apple.AggregateDictionary (1.0 - 1) <A6AF8AC4-1F25-37C4-9157-A02E9C200926> /System/Library/PrivateFrameworks/AggregateDictionary.framework/Versions/A/AggregateDictionary\r\n    0x7fff5542d000 -     0x7fff55459ff7  com.apple.framework.Apple80211 (13.0 - 1380.2) <E02D473E-2CE6-34EA-833C-758E479E066A> /System/Library/PrivateFrameworks/Apple80211.framework/Versions/A/Apple80211\r\n    0x7fff55581000 -     0x7fff55590fc7  com.apple.AppleFSCompression (96.200.3 - 1.0) <3CF60CE8-976E-3CB8-959D-DD0948C1C2DE> /System/Library/PrivateFrameworks/AppleFSCompression.framework/Versions/A/AppleFSCompression\r\n    0x7fff5568c000 -     0x7fff55697fff  com.apple.AppleIDAuthSupport (1.0 - 1) <2E9D1398-DBE6-328B-ADDA-20FA5FAD7405> /System/Library/PrivateFrameworks/AppleIDAuthSupport.framework/Versions/A/AppleIDAuthSupport\r\n    0x7fff556d8000 -     0x7fff55721ff3  com.apple.AppleJPEG (1.0 - 1) <4C1F426B-7D77-3980-9633-7DBD8C666B9A> /System/Library/PrivateFrameworks/AppleJPEG.framework/Versions/A/AppleJPEG\r\n    0x7fff55975000 -     0x7fff55997fff  com.apple.applesauce (1.0 - ???) <F49107C7-3C51-3024-8EF1-C57643BE4F3B> /System/Library/PrivateFrameworks/AppleSauce.framework/Versions/A/AppleSauce\r\n    0x7fff55af6000 -     0x7fff55b0affb  com.apple.AssertionServices (1.0 - 1) <456E507A-4561-3628-9FBE-173ACE7429D8> /System/Library/PrivateFrameworks/AssertionServices.framework/Versions/A/AssertionServices\r\n    0x7fff55ed9000 -     0x7fff55fc5ff7  com.apple.AuthKit (1.0 - 1) <2765ABE9-54F2-3E45-8A93-1261E251B90D> /System/Library/PrivateFrameworks/AuthKit.framework/Versions/A/AuthKit\r\n    0x7fff56187000 -     0x7fff5618ffff  com.apple.coreservices.BackgroundTaskManagement (1.0 - 57.1) <2A396FC0-7B79-3088-9A82-FB93C1181A57> /System/Library/PrivateFrameworks/BackgroundTaskManagement.framework/Versions/A/BackgroundTaskManagement\r\n    0x7fff56190000 -     0x7fff56225fff  com.apple.backup.framework (1.10.5 - ???) <4EEC51E2-AE4C-340A-B686-901810152C12> /System/Library/PrivateFrameworks/Backup.framework/Versions/A/Backup\r\n    0x7fff56226000 -     0x7fff56293ff3  com.apple.BaseBoard (360.28 - 360.28) <68FA8044-F3CD-3BC6-9DAB-27DACF52BFC0> /System/Library/PrivateFrameworks/BaseBoard.framework/Versions/A/BaseBoard\r\n    0x7fff57f00000 -     0x7fff57f09ffb  com.apple.CommonAuth (4.0 - 2.0) <93335CB6-ABEB-3EC7-A040-8A667F40D5F3> /System/Library/PrivateFrameworks/CommonAuth.framework/Versions/A/CommonAuth\r\n    0x7fff58be2000 -     0x7fff58bf3ff7  com.apple.CoreEmoji (1.0 - 69.19.9) <228457B3-E191-356E-9A5B-3C0438D05FBA> /System/Library/PrivateFrameworks/CoreEmoji.framework/Versions/A/CoreEmoji\r\n    0x7fff5919d000 -     0x7fff59203ff7  com.apple.CoreNLP (1.0 - 130.15.22) <27877820-17D0-3B02-8557-4014E876CCC7> /System/Library/PrivateFrameworks/CoreNLP.framework/Versions/A/CoreNLP\r\n    0x7fff594b0000 -     0x7fff594b8ff7  com.apple.CorePhoneNumbers (1.0 - 1) <11F97C7E-C183-305F-8E6C-9B374F50E26B> /System/Library/PrivateFrameworks/CorePhoneNumbers.framework/Versions/A/CorePhoneNumbers\r\n    0x7fff59a2c000 -     0x7fff59ab0fff  com.apple.CoreSymbolication (10.2 - 64490.25.1) <28B2FF2D-3FDE-3A20-B343-341E5BD4E22F> /System/Library/PrivateFrameworks/CoreSymbolication.framework/Versions/A/CoreSymbolication\r\n    0x7fff59b40000 -     0x7fff59c6bff7  com.apple.coreui (2.1 - 499.10) <A80F4B09-F940-346F-A9DF-4EFADD9220A8> /System/Library/PrivateFrameworks/CoreUI.framework/Versions/A/CoreUI\r\n    0x7fff59c6c000 -     0x7fff59e0cfff  com.apple.CoreUtils (5.9 - 590.16) <9D7E165D-EB34-3E01-A02B-FF6BFBD81DD5> /System/Library/PrivateFrameworks/CoreUtils.framework/Versions/A/CoreUtils\r\n    0x7fff59e60000 -     0x7fff59ec3ff7  com.apple.framework.CoreWiFi (13.0 - 1375.2) <343139CE-6BCC-3B6B-91B4-24A503AEE607> /System/Library/PrivateFrameworks/CoreWiFi.framework/Versions/A/CoreWiFi\r\n    0x7fff59ec4000 -     0x7fff59ed5ff3  com.apple.CrashReporterSupport (10.13 - 938.28) <74CC266D-FEF3-32DB-A16F-0ECB8D79C993> /System/Library/PrivateFrameworks/CrashReporterSupport.framework/Versions/A/CrashReporterSupport\r\n    0x7fff59f65000 -     0x7fff59f74fff  com.apple.framework.DFRFoundation (1.0 - 211.1) <E3F02F2A-2059-39CC-85DA-969676EB88EB> /System/Library/PrivateFrameworks/DFRFoundation.framework/Versions/A/DFRFoundation\r\n    0x7fff59f75000 -     0x7fff59f79ff7  com.apple.DSExternalDisplay (3.1 - 380) <787B9748-B120-3453-B8FE-61D9E363A9E0> /System/Library/PrivateFrameworks/DSExternalDisplay.framework/Versions/A/DSExternalDisplay\r\n    0x7fff59ffa000 -     0x7fff5a06fffb  com.apple.datadetectorscore (7.0 - 590.27) <06FB1A07-7AE6-3ADD-8E7E-41955FAB38E8> /System/Library/PrivateFrameworks/DataDetectorsCore.framework/Versions/A/DataDetectorsCore\r\n    0x7fff5a0bb000 -     0x7fff5a0f8ff7  com.apple.DebugSymbols (190 - 190) <6F4FAACA-E06B-38AD-A0C2-14EA5408A231> /System/Library/PrivateFrameworks/DebugSymbols.framework/Versions/A/DebugSymbols\r\n    0x7fff5a0f9000 -     0x7fff5a234ff7  com.apple.desktopservices (1.13.5 - ???) <ED60E493-4E56-3622-A55C-2CABF5D02316> /System/Library/PrivateFrameworks/DesktopServicesPriv.framework/Versions/A/DesktopServicesPriv\r\n    0x7fff5b17b000 -     0x7fff5b596fff  com.apple.vision.FaceCore (3.3.4 - 3.3.4) <A576E2DA-BF6F-3B18-8FEB-324E5C5FA9BD> /System/Library/PrivateFrameworks/FaceCore.framework/Versions/A/FaceCore\r\n    0x7fff604eb000 -     0x7fff604f0fff  com.apple.GPUWrangler (3.50.15 - 3.50.15) <B42426AC-C5E9-3ECD-9798-22811555C9F6> /System/Library/PrivateFrameworks/GPUWrangler.framework/Versions/A/GPUWrangler\r\n    0x7fff612fc000 -     0x7fff6130bfff  com.apple.GraphVisualizer (1.0 - 5) <48D020B7-5938-3FAE-B468-E291AEE2C06F> /System/Library/PrivateFrameworks/GraphVisualizer.framework/Versions/A/GraphVisualizer\r\n    0x7fff61471000 -     0x7fff614e5ffb  com.apple.Heimdal (4.0 - 2.0) <D97FCF19-EAD6-3E2F-BE88-F817E45CAE96> /System/Library/PrivateFrameworks/Heimdal.framework/Versions/A/Heimdal\r\n    0x7fff627ea000 -     0x7fff627f1ffb  com.apple.IOAccelerator (404.14 - 404.14) <8E1BB4BA-15A7-3711-8502-AD2770EE368F> /System/Library/PrivateFrameworks/IOAccelerator.framework/Versions/A/IOAccelerator\r\n    0x7fff627f5000 -     0x7fff6280dfff  com.apple.IOPresentment (1.0 - 42.6) <890C4723-37AB-344B-9BF8-BFD436C06EE8> /System/Library/PrivateFrameworks/IOPresentment.framework/Versions/A/IOPresentment\r\n    0x7fff62bb5000 -     0x7fff62be2ff7  com.apple.IconServices (379 - 379) <7BAD562D-4FA3-3E11-863C-1EEBE2406D2C> /System/Library/PrivateFrameworks/IconServices.framework/Versions/A/IconServices\r\n    0x7fff62e75000 -     0x7fff62e87ff3  com.apple.security.KeychainCircle.KeychainCircle (1.0 - 1) <70CE5230-195D-3443-94EE-EFA57039724F> /System/Library/PrivateFrameworks/KeychainCircle.framework/Versions/A/KeychainCircle\r\n    0x7fff62ea2000 -     0x7fff62f7dff7  com.apple.LanguageModeling (1.0 - 159.15.15) <3DE3CE61-542B-37B7-883E-4B9717CAC65F> /System/Library/PrivateFrameworks/LanguageModeling.framework/Versions/A/LanguageModeling\r\n    0x7fff62f7e000 -     0x7fff62fbaff7  com.apple.Lexicon-framework (1.0 - 33.15.10) <4B5E843E-2809-3E70-9560-9254E2656419> /System/Library/PrivateFrameworks/Lexicon.framework/Versions/A/Lexicon\r\n    0x7fff62fc1000 -     0x7fff62fc6fff  com.apple.LinguisticData (1.0 - 238.25) <F529B961-098C-3E4C-A3E9-9DA9BFA1B3F0> /System/Library/PrivateFrameworks/LinguisticData.framework/Versions/A/LinguisticData\r\n    0x7fff63cbc000 -     0x7fff63ce4ff7  com.apple.spotlight.metadata.utilities (1.0 - 1191.58) <23E8580B-19C0-3E4F-A9FE-368DA80EAA6F> /System/Library/PrivateFrameworks/MetadataUtilities.framework/Versions/A/MetadataUtilities\r\n    0x7fff63ce5000 -     0x7fff63d72ff7  com.apple.gpusw.MetalTools (1.0 - 1) <9B542958-6363-3041-A265-EC7AC7BD7A43> /System/Library/PrivateFrameworks/MetalTools.framework/Versions/A/MetalTools\r\n    0x7fff63f1d000 -     0x7fff63f38ffb  com.apple.MobileKeyBag (2.0 - 1.0) <39337CBB-1D39-3DDC-A998-591194C76523> /System/Library/PrivateFrameworks/MobileKeyBag.framework/Versions/A/MobileKeyBag\r\n    0x7fff63fc1000 -     0x7fff63febffb  com.apple.MultitouchSupport.framework (2450.1 - 2450.1) <42A23EC9-64A7-31C7-BF33-DF4412ED8A3F> /System/Library/PrivateFrameworks/MultitouchSupport.framework/Versions/A/MultitouchSupport\r\n    0x7fff64227000 -     0x7fff64231fff  com.apple.NetAuth (6.2 - 6.2) <0D01BBE5-0269-310D-B148-D19DAE143DEB> /System/Library/PrivateFrameworks/NetAuth.framework/Versions/A/NetAuth\r\n    0x7fff64a92000 -     0x7fff64ae3ff3  com.apple.OTSVG (1.0 - ???) <B7620B5E-CDE2-3BDD-B6EC-8AB143FC5C4E> /System/Library/PrivateFrameworks/OTSVG.framework/Versions/A/OTSVG\r\n    0x7fff65c7c000 -     0x7fff65c8bff7  com.apple.PerformanceAnalysis (1.218.2 - 218.2) <65F3DB3E-6D4E-33A0-B510-EF768D323DAB> /System/Library/PrivateFrameworks/PerformanceAnalysis.framework/Versions/A/PerformanceAnalysis\r\n    0x7fff67b1d000 -     0x7fff67b3bff7  com.apple.ProtocolBuffer (1 - 263.2) <907D6C95-D050-31DE-99CA-16A5135BC6F9> /System/Library/PrivateFrameworks/ProtocolBuffer.framework/Versions/A/ProtocolBuffer\r\n    0x7fff67cd3000 -     0x7fff67d23fff  com.apple.ROCKit (27.6 - 27.6) <756C2253-E8B1-3C48-9945-DE8D6AD24DE2> /System/Library/PrivateFrameworks/ROCKit.framework/Versions/A/ROCKit\r\n    0x7fff67e7d000 -     0x7fff67e9ffff  com.apple.RemoteViewServices (2.0 - 128) <8FB0E4EB-DCBB-32E6-94C6-AA9BA9EE4CAC> /System/Library/PrivateFrameworks/RemoteViewServices.framework/Versions/A/RemoteViewServices\r\n    0x7fff696a9000 -     0x7fff697c7fff  com.apple.Sharing (1288.62.5 - 1288.62.5) <BB647030-839A-3271-B205-D4325346DD3B> /System/Library/PrivateFrameworks/Sharing.framework/Versions/A/Sharing\r\n    0x7fff6a5db000 -     0x7fff6a88afff  com.apple.SkyLight (1.600.0 - 340.55) <1F737945-E242-3F03-A7D6-1A5955AB1298> /System/Library/PrivateFrameworks/SkyLight.framework/Versions/A/SkyLight\r\n    0x7fff6b02e000 -     0x7fff6b03afff  com.apple.SpeechRecognitionCore (5.0.21 - 5.0.21) <7A6A67DB-C813-328E-AAFB-D267A5B50B3D> /System/Library/PrivateFrameworks/SpeechRecognitionCore.framework/Versions/A/SpeechRecognitionCore\r\n    0x7fff6b78b000 -     0x7fff6b816fc7  com.apple.Symbolication (10.2 - 64490.38.1) <9FDCC98D-5B32-35AD-A9BF-94DF2B78507F> /System/Library/PrivateFrameworks/Symbolication.framework/Versions/A/Symbolication\r\n    0x7fff6bcfe000 -     0x7fff6bd0affb  com.apple.TCC (1.0 - 1) <73CF6FA9-44CE-30C9-887F-235940976585> /System/Library/PrivateFrameworks/TCC.framework/Versions/A/TCC\r\n    0x7fff6bf70000 -     0x7fff6c038ff3  com.apple.TextureIO (3.8.4 - 3.8.1) <7CEAC05A-D283-3D5A-B1E3-C849285FA0BF> /System/Library/PrivateFrameworks/TextureIO.framework/Versions/A/TextureIO\r\n    0x7fff6c0f5000 -     0x7fff6c2adffb  com.apple.UIFoundation (1.0 - 551.5) <A0FDC3A4-45C6-3C87-B77F-7DC394374C08> /System/Library/PrivateFrameworks/UIFoundation.framework/Versions/A/UIFoundation\r\n    0x7fff6d7da000 -     0x7fff6d7ddfff  com.apple.dt.XCTTargetBootstrap (1.0 - 14490.66) <7AE3457F-AF40-3508-93FB-1D9E31EB1C9D> /System/Library/PrivateFrameworks/XCTTargetBootstrap.framework/Versions/A/XCTTargetBootstrap\r\n    0x7fff6dbde000 -     0x7fff6dbe0ffb  com.apple.loginsupport (1.0 - 1) <3F8D6334-BCD6-36C1-BA20-CC8503A84375> /System/Library/PrivateFrameworks/login.framework/Versions/A/Frameworks/loginsupport.framework/Versions/A/loginsupport\r\n    0x7fff6deaa000 -     0x7fff6dedefff  libCRFSuite.dylib (41.15.4) <406DAC06-0C77-3F90-878B-4D38F11F0256> /usr/lib/libCRFSuite.dylib\r\n    0x7fff6dee1000 -     0x7fff6deebff7  libChineseTokenizer.dylib (28.15.3) <9B7F6109-3A5D-3641-9A7E-31D2239D73EE> /usr/lib/libChineseTokenizer.dylib\r\n    0x7fff6df79000 -     0x7fff6df7affb  libDiagnosticMessagesClient.dylib (107) <A14D0819-0970-34CD-8680-80E4D7FE8C2C> /usr/lib/libDiagnosticMessagesClient.dylib\r\n    0x7fff6dfb1000 -     0x7fff6e208ff3  libFosl_dynamic.dylib (18.3.4) <1B5DD4E2-8AE0-315E-829E-D5BFCD264EA8> /usr/lib/libFosl_dynamic.dylib\r\n    0x7fff6e259000 -     0x7fff6e278fff  libMobileGestalt.dylib (645.270.1) <99A06C8A-97D6-383D-862C-F453BABB48A4> /usr/lib/libMobileGestalt.dylib\r\n    0x7fff6e279000 -     0x7fff6e279fff  libOpenScriptingUtil.dylib (179.1) <4D603146-EDA5-3A74-9FF8-4F75D8BB9BC6> /usr/lib/libOpenScriptingUtil.dylib\r\n    0x7fff6e3b9000 -     0x7fff6e3baffb  libSystem.B.dylib (1252.250.1) <82F463D9-8E43-30B7-8CB2-A65E6EBDDB34> /usr/lib/libSystem.B.dylib\r\n    0x7fff6e436000 -     0x7fff6e437fff  libThaiTokenizer.dylib (2.15.1) <ADB37DC3-7D9B-3E73-A72A-BCC3433C937A> /usr/lib/libThaiTokenizer.dylib\r\n    0x7fff6e449000 -     0x7fff6e45fffb  libapple_nghttp2.dylib (1.24.1) <6F04250A-6686-3FDC-9A8D-290C64B06502> /usr/lib/libapple_nghttp2.dylib\r\n    0x7fff6e460000 -     0x7fff6e489ffb  libarchive.2.dylib (54.250.1) <47289946-8504-3966-9127-6CE39993DC2C> /usr/lib/libarchive.2.dylib\r\n    0x7fff6e48a000 -     0x7fff6e509fff  libate.dylib (1.13.8) <92B44EDB-369D-3EE8-AEC5-61F8B9313DBF> /usr/lib/libate.dylib\r\n    0x7fff6e50d000 -     0x7fff6e50dff3  libauto.dylib (187) <3E3780E1-96F3-3A22-91C5-92F9A5805518> /usr/lib/libauto.dylib\r\n    0x7fff6e5df000 -     0x7fff6e5efffb  libbsm.0.dylib (39.200.18) <CF381E0B-025B-364F-A83D-2527E03F1AA3> /usr/lib/libbsm.0.dylib\r\n    0x7fff6e5f0000 -     0x7fff6e5fdfff  libbz2.1.0.dylib (38.200.3) <272953A1-8D36-329B-BDDB-E887B347710F> /usr/lib/libbz2.1.0.dylib\r\n    0x7fff6e5fe000 -     0x7fff6e651ff7  libc++.1.dylib (400.9.4) <9A60A190-6C34-339F-BB3D-AACE942009A4> /usr/lib/libc++.1.dylib\r\n    0x7fff6e652000 -     0x7fff6e667ff7  libc++abi.dylib (400.17) <38C09CED-9090-3719-90F3-04A2749F5428> /usr/lib/libc++abi.dylib\r\n    0x7fff6e668000 -     0x7fff6e668ff3  libcharset.1.dylib (51.200.6) <2A27E064-314C-359C-93FC-8A9B06206174> /usr/lib/libcharset.1.dylib\r\n    0x7fff6e669000 -     0x7fff6e679ffb  libcmph.dylib (6.15.1) <9C52B2FE-179F-32AC-B87E-2AFC49ABF817> /usr/lib/libcmph.dylib\r\n    0x7fff6e67a000 -     0x7fff6e692ffb  libcompression.dylib (52.250.2) <7F4BB18C-1FB4-3825-8D8B-6E6B168774C6> /usr/lib/libcompression.dylib\r\n    0x7fff6e907000 -     0x7fff6e91dfff  libcoretls.dylib (155.220.1) <4C64BE3E-41E3-3020-8BB7-07E90C0C861C> /usr/lib/libcoretls.dylib\r\n    0x7fff6e91e000 -     0x7fff6e91fff3  libcoretls_cfhelpers.dylib (155.220.1) <0959B3E9-6643-3589-8BB3-21D52CDF0EF1> /usr/lib/libcoretls_cfhelpers.dylib\r\n    0x7fff6edcb000 -     0x7fff6ee21ff3  libcups.2.dylib (462.15) <9A487009-8412-3D77-8F55-DE4BBCFBE58C> /usr/lib/libcups.2.dylib\r\n    0x7fff6ef55000 -     0x7fff6ef55fff  libenergytrace.dylib (17.200.1) <80BB567A-FD18-3497-BF97-353F57D98CDD> /usr/lib/libenergytrace.dylib\r\n    0x7fff6ef87000 -     0x7fff6ef8cff7  libgermantok.dylib (17.15.2) <E5F0F794-FF27-3D64-AE52-C78C6A84DD67> /usr/lib/libgermantok.dylib\r\n    0x7fff6ef8d000 -     0x7fff6ef92ff7  libheimdal-asn1.dylib (520.270.1) <73F60D6F-76F8-35EF-9C86-9A81225EE4BE> /usr/lib/libheimdal-asn1.dylib\r\n    0x7fff6efbd000 -     0x7fff6f0adfff  libiconv.2.dylib (51.200.6) <2047C9B7-3F74-3A95-810D-2ED8F0475A99> /usr/lib/libiconv.2.dylib\r\n    0x7fff6f0ae000 -     0x7fff6f30fffb  libicucore.A.dylib (62141.0.1) <A0D63918-76E9-3C1B-B255-46F4C1DA7FE8> /usr/lib/libicucore.A.dylib\r\n    0x7fff6f35c000 -     0x7fff6f35dfff  liblangid.dylib (128.15.1) <22D05C4F-769B-3075-ABCF-44A0EBACE028> /usr/lib/liblangid.dylib\r\n    0x7fff6f35e000 -     0x7fff6f376ff3  liblzma.5.dylib (10.200.3) <E1F4FD60-1CE4-37B9-AD95-29D348AF1AC0> /usr/lib/liblzma.5.dylib\r\n    0x7fff6f38e000 -     0x7fff6f432ff7  libmecab.1.0.0.dylib (779.24.1) <A8D0379B-85FA-3B3D-89ED-5CF2C3826AB2> /usr/lib/libmecab.1.0.0.dylib\r\n    0x7fff6f433000 -     0x7fff6f637fff  libmecabra.dylib (779.24.1) <D71F71E0-30E2-3DB3-B636-7DE13D51FB4B> /usr/lib/libmecabra.dylib\r\n    0x7fff6f80f000 -     0x7fff6fb60ff7  libnetwork.dylib (1229.250.15) <72C7E9E3-B2BE-3300-BE1B-64606222022C> /usr/lib/libnetwork.dylib\r\n    0x7fff6fbf2000 -     0x7fff70377fdf  libobjc.A.dylib (756.2) <7C312627-43CB-3234-9324-4DEA92D59F50> /usr/lib/libobjc.A.dylib\r\n    0x7fff70389000 -     0x7fff7038dffb  libpam.2.dylib (22.200.1) <586CF87F-349C-393D-AEEB-FB75F94A5EB7> /usr/lib/libpam.2.dylib\r\n    0x7fff70390000 -     0x7fff703c5fff  libpcap.A.dylib (79.250.3) <97B8CE1B-3EF6-3443-95EF-5659733139C9> /usr/lib/libpcap.A.dylib\r\n    0x7fff704de000 -     0x7fff704f6ffb  libresolv.9.dylib (65.200.3) <1FB0982D-84D9-36E0-B3D8-C808891EFF50> /usr/lib/libresolv.9.dylib\r\n    0x7fff70549000 -     0x7fff70726fff  libsqlite3.dylib (274.26) <6404BA3B-BCA4-301F-B2FE-8776105A2AA3> /usr/lib/libsqlite3.dylib\r\n    0x7fff7093f000 -     0x7fff70942ff7  libutil.dylib (51.200.4) <CE9B18C9-66ED-32D4-9D29-01F8FCB467B0> /usr/lib/libutil.dylib\r\n    0x7fff70943000 -     0x7fff70950fff  libxar.1.dylib (417.1) <39CCF46B-C81A-34B1-92A1-58C4E5DA846E> /usr/lib/libxar.1.dylib\r\n    0x7fff70955000 -     0x7fff70a38ff3  libxml2.2.dylib (32.15) <2748446B-C53C-3B6C-BB5D-B9153D7243E1> /usr/lib/libxml2.2.dylib\r\n    0x7fff70a39000 -     0x7fff70a61ff3  libxslt.1.dylib (16.7) <EC50E503-AEEE-3F50-956F-55E4AF4584D9> /usr/lib/libxslt.1.dylib\r\n    0x7fff70a62000 -     0x7fff70a74ff7  libz.1.dylib (70.200.4) <B048FC1F-058F-3A08-A1FE-81D5308CB3E6> /usr/lib/libz.1.dylib\r\n    0x7fff71258000 -     0x7fff7125cff3  libcache.dylib (81) <1987D1E1-DB11-3291-B12A-EBD55848E02D> /usr/lib/system/libcache.dylib\r\n    0x7fff7125d000 -     0x7fff71267ff3  libcommonCrypto.dylib (60118.250.2) <1765BB6E-6784-3653-B16B-CB839721DC9A> /usr/lib/system/libcommonCrypto.dylib\r\n    0x7fff71268000 -     0x7fff7126fff7  libcompiler_rt.dylib (63.4) <5212BA7B-B7EA-37B4-AF6E-AC4F507EDFB8> /usr/lib/system/libcompiler_rt.dylib\r\n    0x7fff71270000 -     0x7fff71279ff7  libcopyfile.dylib (146.250.1) <98CD00CD-9B91-3B5C-A9DB-842638050FA8> /usr/lib/system/libcopyfile.dylib\r\n    0x7fff7127a000 -     0x7fff712fefc3  libcorecrypto.dylib (602.260.2) <01464D24-570C-3B83-9D18-467769E0FCDD> /usr/lib/system/libcorecrypto.dylib\r\n    0x7fff71385000 -     0x7fff713beff7  libdispatch.dylib (1008.270.1) <97273678-E94C-3C8C-89F6-2E2020F4B43B> /usr/lib/system/libdispatch.dylib\r\n    0x7fff713bf000 -     0x7fff713ebff7  libdyld.dylib (655.1.1) <002418CC-AD11-3D10-865B-015591D24E6C> /usr/lib/system/libdyld.dylib\r\n    0x7fff713ec000 -     0x7fff713ecffb  libkeymgr.dylib (30) <0D0F9CA2-8D5A-3273-8723-59987B5827F2> /usr/lib/system/libkeymgr.dylib\r\n    0x7fff713ed000 -     0x7fff713f9ff3  libkxld.dylib (4903.278.35) <AF9234BD-D3A5-323A-B170-1525C841DD7E> /usr/lib/system/libkxld.dylib\r\n    0x7fff713fa000 -     0x7fff713faff7  liblaunch.dylib (1336.261.4) <AEBAE502-D691-3D26-BFD9-CB41090C0360> /usr/lib/system/liblaunch.dylib\r\n    0x7fff713fb000 -     0x7fff71400fff  libmacho.dylib (927.0.3) <A377D608-77AB-3F6E-90F0-B4F251A5C12F> /usr/lib/system/libmacho.dylib\r\n    0x7fff71401000 -     0x7fff71403ff7  libquarantine.dylib (86.270.1) <3F36A3D6-9606-3D90-B520-809BAEF981C3> /usr/lib/system/libquarantine.dylib\r\n    0x7fff71404000 -     0x7fff71405ff7  libremovefile.dylib (45.200.2) <9FBEB2FF-EEBE-31BC-BCFC-C71F8D0E99B6> /usr/lib/system/libremovefile.dylib\r\n    0x7fff71406000 -     0x7fff7141dff3  libsystem_asl.dylib (356.200.4) <A62A7249-38B8-33FA-9875-F1852590796C> /usr/lib/system/libsystem_asl.dylib\r\n    0x7fff7141e000 -     0x7fff7141eff7  libsystem_blocks.dylib (73) <A453E8EE-860D-3CED-B5DC-BE54E9DB4348> /usr/lib/system/libsystem_blocks.dylib\r\n    0x7fff7141f000 -     0x7fff714a6fff  libsystem_c.dylib (1272.250.1) <7EDACF78-2FA3-35B8-B051-D70475A35117> /usr/lib/system/libsystem_c.dylib\r\n    0x7fff714a7000 -     0x7fff714aaffb  libsystem_configuration.dylib (963.270.3) <2B4A836D-68A4-33E6-8D48-CD4486B03387> /usr/lib/system/libsystem_configuration.dylib\r\n    0x7fff714ab000 -     0x7fff714aeff7  libsystem_coreservices.dylib (66) <719F75A4-74C5-3BA6-A09E-0C5A3E5889D7> /usr/lib/system/libsystem_coreservices.dylib\r\n    0x7fff714af000 -     0x7fff714b5fff  libsystem_darwin.dylib (1272.250.1) <EC9B39A5-9592-3577-8997-7DC721D20D8C> /usr/lib/system/libsystem_darwin.dylib\r\n    0x7fff714b6000 -     0x7fff714bcffb  libsystem_dnssd.dylib (878.270.3) <D5352ABD-0311-3327-8E64-93F29EB19BF1> /usr/lib/system/libsystem_dnssd.dylib\r\n    0x7fff714bd000 -     0x7fff71508ffb  libsystem_info.dylib (517.200.9) <D09D5AE0-2FDC-3A6D-93EC-729F931B1457> /usr/lib/system/libsystem_info.dylib\r\n    0x7fff71509000 -     0x7fff71531ff7  libsystem_kernel.dylib (4903.278.35) <253AF05A-457D-37A6-890B-C3CD9CE8A3FD> /usr/lib/system/libsystem_kernel.dylib\r\n    0x7fff71532000 -     0x7fff7157dff7  libsystem_m.dylib (3158.200.7) <F19B6DB7-014F-3820-831F-389CCDA06EF6> /usr/lib/system/libsystem_m.dylib\r\n    0x7fff7157e000 -     0x7fff715a8fff  libsystem_malloc.dylib (166.270.1) <011F3AD0-8E6A-3A89-AE64-6E5F6840F30A> /usr/lib/system/libsystem_malloc.dylib\r\n    0x7fff715a9000 -     0x7fff715b3ff7  libsystem_networkextension.dylib (767.250.2) <FF06F13A-AEFE-3A27-A073-910EF78AEA36> /usr/lib/system/libsystem_networkextension.dylib\r\n    0x7fff715b4000 -     0x7fff715bbfff  libsystem_notify.dylib (172.200.21) <145B5CFC-CF73-33CE-BD3D-E8DDE268FFDE> /usr/lib/system/libsystem_notify.dylib\r\n    0x7fff715bc000 -     0x7fff715c5fef  libsystem_platform.dylib (177.270.1) <9D1FE5E4-EB7D-3B3F-A8D1-A96D9CF1348C> /usr/lib/system/libsystem_platform.dylib\r\n    0x7fff715c6000 -     0x7fff715d0ff7  libsystem_pthread.dylib (330.250.2) <2D5C08FF-484F-3D59-9132-CE1DCB3F76D7> /usr/lib/system/libsystem_pthread.dylib\r\n    0x7fff715d1000 -     0x7fff715d4ff7  libsystem_sandbox.dylib (851.270.3) <0F89B133-8D87-3B2E-BA5A-C7138738C581> /usr/lib/system/libsystem_sandbox.dylib\r\n    0x7fff715d5000 -     0x7fff715d7ff3  libsystem_secinit.dylib (30.260.2) <EF1EA47B-7B22-35E8-BD9B-F7003DCB96AE> /usr/lib/system/libsystem_secinit.dylib\r\n    0x7fff715d8000 -     0x7fff715dfff3  libsystem_symptoms.dylib (820.267.1) <03F1C2DD-0F5A-3D9D-88F6-B26C0F94EB52> /usr/lib/system/libsystem_symptoms.dylib\r\n    0x7fff715e0000 -     0x7fff715f5ff7  libsystem_trace.dylib (906.260.2) <12C1B9A2-39D6-3428-AE60-2303BD201A57> /usr/lib/system/libsystem_trace.dylib\r\n    0x7fff715f7000 -     0x7fff715fcffb  libunwind.dylib (35.4) <24A97A67-F017-3CFC-B0D0-6BD0224B1336> /usr/lib/system/libunwind.dylib\r\n    0x7fff715fd000 -     0x7fff7162cfff  libxpc.dylib (1336.261.4) <7A9D1BF7-F17F-3B87-9373-B0079544E8C5> /usr/lib/system/libxpc.dylib\r\n\r\nExternal Modification Summary:\r\n  Calls made by other processes targeting this process:\r\n    task_for_pid: 0\r\n    thread_create: 0\r\n    thread_set_state: 0\r\n  Calls made by this process:\r\n    task_for_pid: 0\r\n    thread_create: 0\r\n    thread_set_state: 0\r\n  Calls made by all processes on this machine:\r\n    task_for_pid: 177097\r\n    thread_create: 0\r\n    thread_set_state: 0\r\n\r\nVM Region Summary:\r\nReadOnly portion of Libraries: Total=848.5M resident=0K(0%) swapped_out_or_unallocated=848.5M(100%)\r\nWritable regions: Total=596.0M written=0K(0%) resident=0K(0%) swapped_out=0K(0%) unallocated=596.0M(100%)\r\n \r\n                                VIRTUAL   REGION \r\nREGION TYPE                        SIZE    COUNT (non-coalesced) \r\n===========                     =======  ======= \r\nKernel Alloc Once                    8K        1 \r\nMALLOC                           203.0M       15 \r\nMALLOC guard page                   16K        4 \r\nMALLOC_NANO (reserved)           384.0M        1         reserved VM address space (unallocated)\r\nSTACK GUARD                       56.0M        1 \r\nStack                             8192K        1 \r\n__DATA                            29.5M      248 \r\n__FONT_DATA                          4K        1 \r\n__LINKEDIT                       357.0M       14 \r\n__TEXT                           491.5M      242 \r\n__UNICODE                          564K        1 \r\nshared memory                        8K        2 \r\n===========                     =======  ======= \r\nTOTAL                              1.5G      531 \r\nTOTAL, minus reserved VM space     1.1G      531 \n\ncc @malfet @yf225 @glaringlee @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @VitalyFedyunin"},{"labels":["api",null,null],"text":"I find that nonzero function in C++ API much slower than python.\r\nHere comes my test code:\r\n\r\n```\r\nauto test = torch::ones({1,368,1224});\r\n    for(int i = 0;i < 368;++i){\r\n        for(int j = 0;j<1224;++j){\r\n            float f;\r\n            fin>>f;\r\n            test.index_put_({0,i,j},f);\r\n        }\r\n    }    //just read the a tensor from file \r\n    test = test.cuda();\r\n    clock_t total = 0;\r\n    for(int i = 0;i<100;++i){\r\n        clock_t startTime = clock();\r\n        auto t = at::nonzero(test);\r\n        clock_t endTime = clock();\r\n        total += endTime-startTime;\r\n    }\r\n\r\n    cout<<(double)(total)/CLOCKS_PER_SEC * 1000 / 100<<endl;\r\n```\r\n\r\nand python code:\r\n```\r\ntest = torch.Tensor(test).cuda()\r\ntotal = 0.0\r\nfor i in range(100):\r\n    torch.cuda.synchronize()\r\n    start = time.time()\r\n    t = torch.nonzero(test)\r\n    torch.cuda.synchronize()\r\n    end = time.time()\r\n    total += end-start\r\n\r\nprint((end-start)*10,\"ms\")\r\n```\r\n\r\nC++ result is: 0.32987 ms\r\npython result is : 0.0018024444580078125 ms\r\n\r\nThe env of my computer:\r\n```\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce GTX 1660 Ti\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\n```\r\nthe libtorch bulid-version is 1.6.0.dev20200603+cu101\r\n\r\ncc @yf225 @glaringlee @VitalyFedyunin @ngimel"},{"labels":["api",null,null],"text":"## üêõ Bug\r\nIs was following the example for saving a `torch::tensor` in `c++` and loading it in python kindly provided by @xiangpan-osu at the bottom of #20356. In particular, I wrote \r\n```c++\r\n#include <torch/script.h>\r\n\r\n#include <iostream>\r\n#include <memory>\r\n\r\nint main() {\r\n  auto x = torch::ones({3, 3});\r\n  auto bytes = torch::jit::pickle_save(x);\r\n  std::ofstream fout(\"x.zip\", std::ios::out | std::ios::binary);\r\n  fout.write(bytes.data(), bytes.size());\r\n  fout.close();\r\n  return 0;\r\n}\r\n```\r\nand compiled the program with \r\n```c++\r\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\r\nfind_package(Torch REQUIRED)\r\n\r\nadd_executable(example-app test.cpp)\r\ntarget_link_libraries(example-app  PUBLIC \"${TORCH_LIBRARIES}\")\r\nset_property(TARGET example-app PROPERTY CXX_STANDARD 17)\r\n```\r\nWhen loading the saved file `x.zip` in python, I receive the following error: \r\n```python\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-2-f3cc1cba2382> in <module>\r\n----> 1 torch.load('x.zip')\r\n\r\n~/.local/lib/python3.7/site-packages/torch/serialization.py in load(f, map_location, pickle_module, **pickle_load_args)\r\n    526         if _is_zipfile(opened_file):\r\n    527             with _open_zipfile_reader(f) as opened_zipfile:\r\n--> 528                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\r\n    529         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n    530 \r\n\r\n~/.local/lib/python3.7/site-packages/torch/serialization.py in _load(zip_file, map_location, pickle_module, **pickle_load_args)\r\n    780     unpickler = pickle_module.Unpickler(data_file, **pickle_load_args)\r\n    781     unpickler.persistent_load = persistent_load\r\n--> 782     result = unpickler.load()\r\n    783 \r\n    784     return result\r\n\r\n~/.local/lib/python3.7/site-packages/torch/serialization.py in persistent_load(saved_id)\r\n    772         data_type, key, location, size = data\r\n    773         if key not in loaded_storages:\r\n--> 774             load_tensor(data_type(size), size, key, _maybe_decode_ascii(location))\r\n    775         storage = loaded_storages[key]\r\n    776         return storage\r\n\r\n~/.local/lib/python3.7/site-packages/torch/serialization.py in load_tensor(obj, size, key, location)\r\n    757         name = 'tensors/{}'.format(key)\r\n    758         size_long = struct.pack(\"<Q\", size)\r\n--> 759         tensor_file = io.BytesIO(size_long + zip_file.get_record(name))\r\n    760         offset = None\r\n    761         is_real_file = False\r\n\r\nRuntimeError: [enforce fail at inline_container.cc:197] . file not found: archive/tensors/0\r\nframe #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x47 (0x7f7db9c0fd37 in /home/fabian/.local/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: caffe2::serialize::PyTorchStreamReader::getRecordID(std::string const&) + 0xe0 (0x7f7dbcd92e30 in /home/fabian/.local/lib/python3.7/site-packages/torch/lib/libtorch.so)\r\nframe #2: caffe2::serialize::PyTorchStreamReader::getRecord(std::string const&) + 0x25 (0x7f7dbcd959f5 in /home/fabian/.local/lib/python3.7/site-packages/torch/lib/libtorch.so)\r\nframe #3: <unknown function> + 0x6a6488 (0x7f7e04ec6488 in /home/fabian/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #4: <unknown function> + 0x295a74 (0x7f7e04ab5a74 in /home/fabian/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: _PyMethodDef_RawFastCallKeywords + 0x13b (0x5c85bb in /usr/bin/python)\r\nframe #6: _PyObject_FastCallKeywords + 0x6c9 (0x5ca149 in /usr/bin/python)\r\nframe #7: /usr/bin/python() [0x535a11]\r\nframe #8: _PyEval_EvalFrameDefault + 0x4511 (0x53c5a1 in /usr/bin/python)\r\nframe #9: _PyEval_EvalCodeWithName + 0xb87 (0x536f27 in /usr/bin/python)\r\nframe #10: _PyFunction_FastCallKeywords + 0x488 (0x5c9468 in /usr/bin/python)\r\nframe #11: /usr/bin/python() [0x535880]\r\nframe #12: _PyEval_EvalFrameDefault + 0x552 (0x5385e2 in /usr/bin/python)\r\nframe #13: _PyEval_EvalCodeWithName + 0xb87 (0x536f27 in /usr/bin/python)\r\nframe #14: _PyFunction_FastCallDict + 0x34e (0x5ca63e in /usr/bin/python)\r\nframe #15: /usr/bin/python() [0x5cbcf4]\r\nframe #16: PyObject_CallFunctionObjArgs + 0x89 (0x5cc109 in /usr/bin/python)\r\nframe #17: /usr/bin/python() [0x46f5c3]\r\nframe #18: _PyMethodDescr_FastCallKeywords + 0x1c3 (0x4daca3 in /usr/bin/python)\r\nframe #19: /usr/bin/python() [0x535956]\r\nframe #20: _PyEval_EvalFrameDefault + 0x683 (0x538713 in /usr/bin/python)\r\nframe #21: _PyEval_EvalCodeWithName + 0xb87 (0x536f27 in /usr/bin/python)\r\nframe #22: _PyFunction_FastCallDict + 0x34e (0x5ca63e in /usr/bin/python)\r\nframe #23: _PyEval_EvalFrameDefault + 0x19a2 (0x539a32 in /usr/bin/python)\r\nframe #24: _PyEval_EvalCodeWithName + 0x247 (0x5365e7 in /usr/bin/python)\r\nframe #25: _PyFunction_FastCallKeywords + 0x488 (0x5c9468 in /usr/bin/python)\r\nframe #26: /usr/bin/python() [0x535880]\r\nframe #27: _PyEval_EvalFrameDefault + 0x4511 (0x53c5a1 in /usr/bin/python)\r\nframe #28: _PyEval_EvalCodeWithName + 0x247 (0x5365e7 in /usr/bin/python)\r\nframe #29: PyEval_EvalCode + 0x23 (0x64cbb3 in /usr/bin/python)\r\nframe #30: /usr/bin/python() [0x64e321]\r\nframe #31: _PyMethodDef_RawFastCallKeywords + 0x70 (0x5c84f0 in /usr/bin/python)\r\nframe #32: /usr/bin/python() [0x535990]\r\nframe #33: _PyEval_EvalFrameDefault + 0x552 (0x5385e2 in /usr/bin/python)\r\nframe #34: /usr/bin/python() [0x4d90d4]\r\nframe #35: _PyEval_EvalFrameDefault + 0x1a7f (0x539b0f in /usr/bin/python)\r\nframe #36: /usr/bin/python() [0x4d90d4]\r\nframe #37: _PyEval_EvalFrameDefault + 0x1a7f (0x539b0f in /usr/bin/python)\r\nframe #38: /usr/bin/python() [0x4d90d4]\r\nframe #39: _PyMethodDescr_FastCallKeywords + 0x34d (0x4dae2d in /usr/bin/python)\r\nframe #40: /usr/bin/python() [0x535956]\r\nframe #41: _PyEval_EvalFrameDefault + 0x683 (0x538713 in /usr/bin/python)\r\nframe #42: _PyFunction_FastCallKeywords + 0x18b (0x5c916b in /usr/bin/python)\r\nframe #43: /usr/bin/python() [0x535880]\r\nframe #44: _PyEval_EvalFrameDefault + 0x552 (0x5385e2 in /usr/bin/python)\r\nframe #45: _PyFunction_FastCallKeywords + 0x18b (0x5c916b in /usr/bin/python)\r\nframe #46: /usr/bin/python() [0x535880]\r\nframe #47: _PyEval_EvalFrameDefault + 0x683 (0x538713 in /usr/bin/python)\r\nframe #48: _PyEval_EvalCodeWithName + 0x247 (0x5365e7 in /usr/bin/python)\r\nframe #49: _PyFunction_FastCallKeywords + 0x488 (0x5c9468 in /usr/bin/python)\r\nframe #50: /usr/bin/python() [0x535880]\r\nframe #51: _PyEval_EvalFrameDefault + 0x1451 (0x5394e1 in /usr/bin/python)\r\nframe #52: _PyEval_EvalCodeWithName + 0x247 (0x5365e7 in /usr/bin/python)\r\nframe #53: _PyFunction_FastCallKeywords + 0x488 (0x5c9468 in /usr/bin/python)\r\nframe #54: /usr/bin/python() [0x535880]\r\nframe #55: _PyEval_EvalFrameDefault + 0x683 (0x538713 in /usr/bin/python)\r\nframe #56: _PyEval_EvalCodeWithName + 0x247 (0x5365e7 in /usr/bin/python)\r\nframe #57: _PyFunction_FastCallKeywords + 0x488 (0x5c9468 in /usr/bin/python)\r\nframe #58: /usr/bin/python() [0x535880]\r\nframe #59: _PyEval_EvalFrameDefault + 0x683 (0x538713 in /usr/bin/python)\r\nframe #60: _PyFunction_FastCallKeywords + 0x18b (0x5c916b in /usr/bin/python)\r\nframe #61: /usr/bin/python() [0x535880]\r\nframe #62: _PyEval_EvalFrameDefault + 0x683 (0x538713 in /usr/bin/python)\r\nframe #63: _PyEval_EvalCodeWithName + 0x247 (0x5365e7 in /usr/bin/python)\r\n``` \r\nDoes somebody have any ideas I cannot load the tensor in python? I find it difficult to make sense of the error message above and would appreciate any hints and suggestions! \r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 19.10\r\nGCC version: (Ubuntu 9.2.1-9ubuntu2) 9.2.1 20191008\r\nCMake version: version 3.13.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration: GPU 0: Quadro T1000\r\nNvidia driver version: 435.21\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.4\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.0\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.5.0\r\n[conda] Could not collect\n\ncc @yf225 @glaringlee"},{"labels":["api",null],"text":"Valgrind is my go-to for wrangling possible memory leaks. It is a beautiful piece of software, but is unfortunately (and necessarily) imperfect. I just ran a libtorch-based application through a relatively brief optimization of a CNN model, and it generated a fair number of loss records. Fortunately, all of them appear to be of the ‚Äúpossibly lost‚Äù variety (as opposed to ‚Äúdefinitely lost‚Äù); I was using all of the available leak-check-heuristics available to valgrind. Many of these records reflect pytorch-based allocations embedded in pthread-related activities, which may just suggest that threads are not being thoroughly cleaned up on exit.\r\nHowever, there are a number of records which, at least going by the traceback, don‚Äôt reflect an allocation embedded in thread creation. For brevity, I won't quote any of them here, because I just want to ask a larger question: Is libtorch (v. 1.5, specifically) being subjected to any kind of careful memory-leak vetting, whether by valgrind or some other checker? I know valgrind is not necessarily the ultimate authority; something as simple as a -fsanitize=address compilation could do as well or better.\n\ncc @yf225 @glaringlee"},{"labels":["api",null],"text":"If C++ extension uses `ATen/Parallel.h` it requires [`AT_PARALLEL_OPENMP`/`AT_PARALLEL_NATIVE`/`AT_PARALLEL_NATIVE_TBB`](https://github.com/pytorch/pytorch/blob/46447045ea450069ab9a9cbbf71e86110013fb0b/aten/src/ATen/Parallel.h#L147-L153) which were used to build installed PyTorch. Currently they are unavailable after build, so these values should be included into `ATen/Config.h.in`\n\ncc @yf225 @glaringlee"},{"labels":["api",null,null],"text":"## üìö Documentation\r\n\r\nThe document enclosing the details of how one installs the C++ distribution of Pytorch, found [here](https://pytorch.org/cppdocs/installing.html), is missing one paragraph of code between the sentences _In that case CMake configuration step would look something like follows:_ and _If all goes well, it will look something like this:_\r\n\r\nApologies if this is not the place to report this, this was the best fit according to the template text.\r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"## üöÄ Feature\r\nAdd cpack support for creating prepackaged builds of libtorch\r\n\r\n## Motivation\r\nDue to https://github.com/pytorch/pytorch/issues/14573 I need to compile libtorch from source and link against system protobuf.  libtorch is installed as part of a CI pipeline and rather than build it from source each time, I would like to be able to point to a prebuilt `.deb` package to install.\r\n\r\n## Pitch\r\nAdd [the few lines](https://gitlab.kitware.com/cmake/community/-/wikis/doc/cpack/Packaging-With-CPack) it would it take to support cpack\r\n\r\n\n\ncc @malfet @yf225 @glaringlee"},{"labels":[null,"api",null,null],"text":"Please complete torchvision for libtorch c++.\r\nlike torchvision.ops ,torchvision.models.detection\r\nIt is best to provide a compiled torchvision.lib for libtorch.\r\n\n\ncc @yf225 @glaringlee @fmassa"},{"labels":[null,null,"api",null,null],"text":"## üêõ Bug\r\n`IntegrationTest.CartPole` from `test/cpp/api/integration.cpp` sometimes times out on `pytorch_bazel_build` CI jobs, see for example:\r\nSee failure: https://circleci.com/api/v1.1/project/github/pytorch/pytorch/5577465/output/104/0?file=true&allocation-id=5ecd716dc9b52d5635d36866-0-build%2F5B5F7F46\r\n\n\ncc @ezyang @gchanan @zou3519 @malfet @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"## üêõ Bug\r\n\r\nLibTorch 1.5.0 seemed built on top of GLIBC 2.23 where PyTorch 1.5.0 pip wheel built on lower version. Do we have a plan to support LibTorch on version below 2.23? Or any instruction for user on GLIBC 2.23 and below to use it?\r\n\r\n## To Reproduce\r\n\r\nIf you try to use it on Cent OS 7, you can easily fall into the following issue:\r\n\r\n```\r\ndownload\r\nhttps://download.pytorch.org/libtorch/cu102/libtorch-cxx11-abi-shared-with-deps-1.5.0.zip\r\n```\r\n\r\nError message\r\n```\r\n/lib64/libm.so.6: version `GLIBC_2.23' not found (required by /home/centos/libtorch_cpu.so)\r\n```\r\n\r\n## Expected behavior\r\n\r\nIt should be fine to use the libtorch for lower GLIBC version as claimed here: https://pytorch.org/get-started/locally/#supported-linux-distributions\r\n\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.5.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: version 3.6.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.4\r\n[pip3] torch==1.5.0\r\n[conda] Could not collect\r\n\r\n\n\ncc @malfet @yf225 @glaringlee"},{"labels":[null,"api",null,null],"text":"## üêõ Bug\r\n\r\nI tried to build with my custom C++ application with Cent OS 7 and it failed. Then I tried to build from source with pytorch 1.5.0 and it crashed in a different place.\r\n\r\nSince the website claim Cent OS 7 is supported with GLIbc version > 2.17, how I can use it on Cent OS 7?\r\n\r\n## To Reproduce\r\n\r\n```\r\n# download https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.5.0%2Bcpu.zip\r\n# My custom cmake follow the guideline\r\n```\r\n\r\nerror messaage\r\n```\r\n-- The C compiler identification is GNU 4.8.5\r\n-- The CXX compiler identification is GNU 4.8.5\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\nBuilding torch with the host...\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Looking for pthread_create\r\n-- Looking for pthread_create - not found\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE\r\n-- Found torch: /home/centos/pytorch/pytorch-native/libtorch/lib/libtorch.so\r\n-- Found JNI: /usr/lib/jvm/jre/lib/amd64/libjawt.so\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/centos/pytorch/build\r\n\r\nIn file included from /home/centos/pytorch/pytorch-native/libtorch/include/c10/util/typeid.h:22:0,\r\n                 from /home/centos/pytorch/pytorch-native/src/pytorch_jni_utils.h:16,\r\n                 from /home/centos/pytorch/pytorch-native/src/ai_pytorch_jni_PyTorchLibrary_nn_functional.cc:15:\r\n/home/centos/pytorch/pytorch-native/libtorch/include/c10/util/C++17.h:16:2: error: #error \"You're trying to build PyTorch with a too old version of GCC. We need GCC 5 or later.\"\r\n #error \"You're trying to build PyTorch with a too old version of GCC. We need GCC 5 or later.\"\r\n  ^\r\n/home/centos/pytorch/pytorch-native/libtorch/include/c10/util/C++17.h:24:2: error: #error You need C++14 to compile PyTorch\r\n #error You need C++14 to compile PyTorch\r\n  ^\r\nIn file included from /home/centos/djl/pytorch/pytorch-native/libtorch/include/c10/util/ArrayRef.h:19:0,\r\n                 from /home/centos/djl/pytorch/pytorch-native/libtorch/include/c10/core/MemoryFormat.h:5,\r\n                 from /home/centos/djl/pytorch/pytorch-native/libtorch/include/ATen/core/TensorBody.h:5,\r\n                 from /home/centos/djl/pytorch/pytorch-native/libtorch/include/ATen/Tensor.h:11,\r\n                 from /home/centos/djl/pytorch/pytorch-native/libtorch/include/ATen/Context.h:4,\r\n                 from /home/centos/djl/pytorch/pytorch-native/libtorch/include/ATen/ATen.h:5,\r\n                 from /home/centos/djl/pytorch/pytorch-native/libtorch/include/torch/csrc/api/include/torch/types.h:3,\r\n                 from /home/centos/djl/pytorch/pytorch-native/libtorch/include/torch/script.h:3,\r\n                 from /home/centos/djl/pytorch/pytorch-native/src/ai_djl_pytorch_jni_PyTorchLibrary_inference.cc:13:\r\n/home/centos/djl/pytorch/pytorch-native/libtorch/include/c10/util/C++17.h:16:2: error: #error \"You're trying to build PyTorch with a too old version of GCC. We need GCC 5 or later.\"\r\n #error \"You're trying to build PyTorch with a too old version of GCC. We need GCC 5 or later.\"\r\n  ^\r\n/home/centos/djl/pytorch/pytorch-native/libtorch/include/c10/util/C++17.h:24:2: error: #error You need C++14 to compile PyTorch\r\n #error You need C++14 to compile PyTorch\r\n  ^\r\nIn file included from /home/centos/djl/pytorch/pytorch-native/libtorch/include/c10/util/typeid.h:22:0,\r\n                 from /home/centos/djl/pytorch/pytorch-native/src/djl_pytorch_jni_utils.h:16,\r\n                 from /home/centos/djl/pytorch/pytorch-native/src/ai_djl_pytorch_jni_PyTorchLibrary_torch_isjm.cc:15:\r\n/home/centos/djl/pytorch/pytorch-native/libtorch/include/c10/util/C++17.h:16:2: error: #error \"You're trying to build PyTorch with a too old version of GCC. We need GCC 5 or later.\"\r\n```\r\n\r\n\r\nAfter this, build from source\r\n\r\n```\r\n# checkout tags/v1.5.0\r\nmkdir build && cd build\r\npython3 ../tools/build_libtorch.py\r\n```\r\n\r\nError message\r\n```\r\ncmake -DBUILD_PYTHON=False -DBUILD_TEST=True -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_COMPILER=/usr/bin/cmake -DCMAKE_INSTALL_PREFIX=/home/centos/pytorch/torch -DCMAKE_PREFIX_PATH=/usr/lib/python3.6/site-packages -DNUMPY_INCLUDE_DIR=/home/centos/.local/lib/python3.6/site-packages/numpy/core/include -DPYTHON_EXECUTABLE=/usr/bin/python3 -DPYTHON_INCLUDE_DIR=/usr/include/python3.6m -DUSE_NUMPY=True /home/centos/pytorch\r\n-- The CXX compiler identification is unknown\r\n-- The C compiler identification is GNU 4.8.5\r\n-- Check for working CXX compiler: /usr/bin/cmake\r\n-- Check for working CXX compiler: /usr/bin/cmake -- broken\r\nCMake Error at /usr/local/share/cmake-3.6/Modules/CMakeTestCXXCompiler.cmake:54 (message):\r\n  The C++ compiler \"/usr/bin/cmake\" is not able to compile a simple test\r\n  program.\r\n\r\n  It fails with the following output:\r\n\r\n   Change Dir: /home/centos/pytorch/build_libtorch/build/CMakeFiles/CMakeTmp\r\n\r\n\r\n\r\n  Run Build Command:\"/usr/bin/gmake\" \"cmTC_53171/fast\"\r\n\r\n  /usr/bin/gmake -f CMakeFiles/cmTC_53171.dir/build.make\r\n  CMakeFiles/cmTC_53171.dir/build\r\n\r\n  gmake[1]: Entering directory\r\n  `/home/centos/pytorch/build_libtorch/build/CMakeFiles/CMakeTmp'\r\n\r\n  Building CXX object CMakeFiles/cmTC_53171.dir/testCXXCompiler.cxx.o\r\n\r\n  /usr/bin/cmake -o CMakeFiles/cmTC_53171.dir/testCXXCompiler.cxx.o -c\r\n  /home/centos/pytorch/build_libtorch/build/CMakeFiles/CMakeTmp/testCXXCompiler.cxx\r\n\r\n\r\n  CMake Error: The source directory\r\n  \"/home/centos/pytorch/build_libtorch/build/CMakeFiles/CMakeTmp/testCXXCompiler.cxx\"\r\n  is a file, not a directory.\r\n\r\n  Specify --help for usage, or press the help button on the CMake GUI.\r\n\r\n  gmake[1]: *** [CMakeFiles/cmTC_53171.dir/testCXXCompiler.cxx.o] Error 1\r\n\r\n  gmake[1]: Leaving directory\r\n  `/home/centos/pytorch/build_libtorch/build/CMakeFiles/CMakeTmp'\r\n\r\n  gmake: *** [cmTC_53171/fast] Error 2\r\n\r\n\r\n\r\n\r\n\r\n  CMake will not be able to correctly generate this project.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:23 (project)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/home/centos/pytorch/build_libtorch/build/CMakeFiles/CMakeOutput.log\".\r\nSee also \"/home/centos/pytorch/build_libtorch/build/CMakeFiles/CMakeError.log\".\r\nTraceback (most recent call last):\r\n  File \"../tools/build_libtorch.py\", line 23, in <module>\r\n    rerun_cmake=True, cmake_only=False, cmake=CMake())\r\n  File \"/home/centos/pytorch/tools/build_pytorch_libs.py\", line 59, in build_caffe2\r\n    rerun_cmake)\r\n  File \"/home/centos/pytorch/tools/setup_helpers/cmake.py\", line 323, in generate\r\n    self.run(args, env=my_env)\r\n  File \"/home/centos/pytorch/tools/setup_helpers/cmake.py\", line 141, in run\r\n    check_call(command, cwd=self.build_dir, env=env)\r\n  File \"/usr/lib64/python3.6/subprocess.py\", line 311, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['cmake', '-DBUILD_PYTHON=False', '-DBUILD_TEST=True', '-DCMAKE_BUILD_TYPE=Release', '-DCMAKE_CXX_COMPILER=/usr/bin/cmake', '-DCMAKE_INSTALL_PREFIX=/home/centos/pytorch/torch', '-DCMAKE_PREFIX_PATH=/usr/lib/python3.6/site-packages', '-DNUMPY_INCLUDE_DIR=/home/centos/.local/lib/python3.6/site-packages/numpy/core/include', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DPYTHON_INCLUDE_DIR=/usr/include/python3.6m', '-DUSE_NUMPY=True', '/home/centos/pytorch']' returned non-zero exit status 1.\r\n```\r\n\r\n\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nIs debug build: No\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: version 3.6.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\n\ncc @malfet @yf225 @glaringlee"},{"labels":["api",null],"text":"Now the libtorch header file is too slow to debug and compile.Will the next version of libtorch provide modules instead of header files in the c++20 standard?\r\n\r\nlike this:\r\nimport torch\r\ntorch::Tensor a=torch::tensor({1,2});\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"Can you add an overloaded operator &,|,^ to a libtorch tensor\r\nexample:\r\npython: \r\na=torch.tensor((1))  \r\nb=torch.tensor((1))  \r\nc=a&b\r\nlibtorch:\r\nauto a=torch::tensor({1});\r\nauto b=torch::tensor({1});\r\nauto c=a&b;////no support\r\n\n\ncc @yf225 @glaringlee @ezyang"},{"labels":[null,"api",null,null,null],"text":"## üêõ Bug\r\n\r\nSegment Fault After model inference all images usnig C++ API\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. c++ code load my model\r\n2. do inference \r\n3. input all images\r\n4. before return, error occur\r\n\r\nMy dbg error message:\r\n\r\n```\r\n== Switch to GPU mode\r\n[New Thread 0x7fff215ea700 (LWP 30485)]\r\n[New Thread 0x7fff20de9700 (LWP 30486)]\r\n== ResNet50 loaded!\r\n== Label loaded! Let's try it\r\n== Input image path: [enter Q to exit]\r\n/home/yyh/test/PyTorch-CPP/pic/dog.jpg\r\n[New Thread 0x7fff2d664700 (LWP 30487)]\r\n[New Thread 0x7fff2ce63700 (LWP 30488)]\r\n[New Thread 0x7fff29fff700 (LWP 30489)]\r\n[New Thread 0x7fff297fe700 (LWP 30490)]\r\n[New Thread 0x7fff28ffd700 (LWP 30491)]\r\n[New Thread 0x7fff287fc700 (LWP 30492)]\r\n[New Thread 0x7fff27ffb700 (LWP 30493)]\r\n[New Thread 0x7fff03fff700 (LWP 30494)]\r\n[New Thread 0x7fff0233d700 (LWP 30495)]\r\n[New Thread 0x7fff01b3c700 (LWP 30496)]\r\n[New Thread 0x7ffeeffff700 (LWP 30497)]\r\n[New Thread 0x7ffeef7fe700 (LWP 30498)]\r\n[New Thread 0x7ffeeeffd700 (LWP 30499)]\r\n[New Thread 0x7ffeddfff700 (LWP 30500)]\r\n[New Thread 0x7ffedd7fe700 (LWP 30501)]\r\n[New Thread 0x7ffedcffd700 (LWP 30502)]\r\n[New Thread 0x7ffedc7fc700 (LWP 30503)]\r\n[New Thread 0x7ffedbffb700 (LWP 30504)]\r\n[New Thread 0x7ffedb7fa700 (LWP 30505)]\r\n[New Thread 0x7ffed54e9700 (LWP 30506)]\r\n[New Thread 0x7ffed4ce8700 (LWP 30507)]\r\n[New Thread 0x7ffebbfff700 (LWP 30508)]\r\n[New Thread 0x7ffebb7fe700 (LWP 30509)]\r\n[New Thread 0x7ffebaffd700 (LWP 30510)]\r\n[New Thread 0x7ffeba7fc700 (LWP 30511)]\r\n[New Thread 0x7ffeb9ffb700 (LWP 30512)]\r\n[New Thread 0x7ffeb97fa700 (LWP 30513)]\r\n[New Thread 0x7ffeb8ff9700 (LWP 30514)]\r\n[New Thread 0x7ffeb87f8700 (LWP 30515)]\r\n[New Thread 0x7ffeb7ff7700 (LWP 30516)]\r\n[New Thread 0x7ffeb77f6700 (LWP 30517)]\r\n[New Thread 0x7ffeb6ff5700 (LWP 30518)]\r\n[New Thread 0x7ffeb67f4700 (LWP 30519)]\r\n[New Thread 0x7ffeb5ff3700 (LWP 30520)]\r\n[New Thread 0x7ffeb57f2700 (LWP 30521)]\r\n[New Thread 0x7ffeb4ff1700 (LWP 30522)]\r\n[New Thread 0x7ffeb47f0700 (LWP 30523)]\r\n[New Thread 0x7ffeb3fef700 (LWP 30524)]\r\n[New Thread 0x7ffeb37ee700 (LWP 30525)]\r\n[New Thread 0x7ffeb2fed700 (LWP 30526)]\r\n[New Thread 0x7ffeb27ec700 (LWP 30527)]\r\n[New Thread 0x7ffeb1feb700 (LWP 30528)]\r\n[New Thread 0x7ffeb17ea700 (LWP 30529)]\r\n[New Thread 0x7ffeb0fe9700 (LWP 30530)]\r\n[New Thread 0x7ffeb07e8700 (LWP 30531)]\r\n[New Thread 0x7ffeaffe7700 (LWP 30532)]\r\n== image size: [976 x 549] ==\r\n== simply resize: [224 x 224] ==\r\n============= Top-1 =============\r\nLabel: beagle\r\nWith Probability: 99.1227%\r\n============= Top-2 =============\r\nLabel: Walker hound, Walker foxhound\r\nWith Probability: 0.469355%\r\n============= Top-3 =============\r\nLabel: English foxhound\r\nWith Probability: 0.110916%\r\n== Input image path: [enter Q to exit]\r\nQ\r\n\r\nThread 1 \"classifier\" received signal SIGSEGV, Segmentation fault.\r\n0x00007fffec81723e in ?? () from /usr/local/cuda-10.2/lib64/libcudart.so.10.2\r\nMissing separate debuginfos, use: yum debuginfo-install libgcc-8.3.1-4.5.el8.x86_64 libgomp-8.3.1-4.5.el8.x86_64 libstdc++-8.3.1-4.5.el8.x86_64 zlib-1.2.11-10.el8.x86_64\r\n(gdb) bt\r\n#0 0x00007fffec81723e in ?? () from /usr/local/cuda-10.2/lib64/libcudart.so.10.2\r\n#1 0x00007fffec81c70b in ?? () from /usr/local/cuda-10.2/lib64/libcudart.so.10.2\r\n#2 0x00007fffec8492d0 in cudaStreamDestroy () from /usr/local/cuda-10.2/lib64/libcudart.so.10.2\r\n#3 0x00007fff68f0551d in cudnnDestroy () from /usr/local/lib/libtorch_cuda.so\r\n#4 0x00007fff68207a05 in at::cuda::(anonymous namespace)::DeviceThreadHandlePool<cudnnContext*, &at::native::(anonymous namespace)::createCuDNNHandle, &at::native::(anonymous namespace)::destroyCuDNNHandle>::~DeviceThreadHandlePool() () from /usr/local/lib/libtorch_cuda.so\r\n#5 0x00007fff63e66677 in __cxa_finalize () from /lib64/libc.so.6\r\n#6 0x00007fff65a68a83 in __do_global_dtors_aux () from /usr/local/lib/libtorch_cuda.so\r\n#7 0x00007fffffffe0b0 in ?? ()\r\n#8 0x00007ffff7de4106 in _dl_fini () from /lib64/ld-linux-x86-64.so.2\r\nBacktrace stopped: frame did not save the PC\r\n(gdb)\r\n```\r\n\r\nMy code:\r\n\r\n```\r\n// One-stop header.\r\n#include <torch/script.h>\r\n\r\n// headers for opencv\r\n#include <opencv2/highgui/highgui.hpp>\r\n#include <opencv2/imgproc/imgproc.hpp>\r\n#include <opencv2/opencv.hpp>\r\n\r\n#include <cmath>\r\n#include <iostream>\r\n#include <memory>\r\n#include <string>\r\n#include <vector>\r\n\r\n#define kIMAGE_SIZE 224\r\n#define kCHANNELS 3\r\n#define kTOP_K 3\r\n\r\nbool LoadImage(std::string file_name, cv::Mat &image) {\r\n    image = cv::imread(file_name);  // CV_8UC3\r\n    if (image.empty() || !image.data) {\r\n        return false;\r\n    }\r\n    cv::cvtColor(image, image, cv::COLOR_BGR2RGB);\r\n    std::cout << \"== image size: \" << image.size() << \" ==\" << std::endl;\r\n\r\n    // scale image to fit\r\n    cv::Size scale(kIMAGE_SIZE, kIMAGE_SIZE);\r\n    cv::resize(image, image, scale);\r\n    std::cout << \"== simply resize: \" << image.size() << \" ==\" << std::endl;\r\n\r\n    // convert [unsigned int] to [float]\r\n    image.convertTo(image, CV_32FC3, 1.0f / 255.0f);\r\n\r\n    return true;\r\n}\r\n\r\nbool LoadImageNetLabel(std::string file_name,\r\n                       std::vector<std::string> &labels) {\r\n    std::ifstream ifs(file_name);\r\n    if (!ifs) {\r\n        return false;\r\n    }\r\n    std::string line;\r\n    while (std::getline(ifs, line)) {\r\n        labels.push_back(line);\r\n    }\r\n    return true;\r\n}\r\n\r\nint main(int argc, const char *argv[]) {\r\n    if (argc != 3) {\r\n        std::cerr << \"Usage: classifier <path-to-exported-script-module> \"\r\n                     \"<path-to-lable-file>\"\r\n                  << std::endl;\r\n        return -1;\r\n    }\r\n\r\n    torch::jit::script::Module module = torch::jit::load(argv[1]);\r\n    std::cout << \"== Switch to GPU mode\" << std::endl;\r\n    // to GPU\r\n    module.to(at::kCUDA);\r\n\r\n    std::cout << \"== ResNet50 loaded!\\n\";\r\n    std::vector<std::string> labels;\r\n    if (LoadImageNetLabel(argv[2], labels)) {\r\n        std::cout << \"== Label loaded! Let's try it\\n\";\r\n    } else {\r\n        std::cerr << \"Please check your label file path.\" << std::endl;\r\n        return -1;\r\n    }\r\n\r\n    std::string file_name = \"\";\r\n    cv::Mat image;\r\n    while (true) {\r\n        std::cout << \"== Input image path: [enter Q to exit]\" << std::endl;\r\n        std::cin >> file_name;\r\n        if (file_name == \"Q\") {\r\n            break;\r\n        }\r\n        if (LoadImage(file_name, image)) {\r\n            auto input_tensor = torch::from_blob(\r\n                    image.data, {1, kIMAGE_SIZE, kIMAGE_SIZE, kCHANNELS});\r\n            input_tensor = input_tensor.permute({0, 3, 1, 2});\r\n            input_tensor[0][0] = input_tensor[0][0].sub_(0.485).div_(0.229);\r\n            input_tensor[0][1] = input_tensor[0][1].sub_(0.456).div_(0.224);\r\n            input_tensor[0][2] = input_tensor[0][2].sub_(0.406).div_(0.225);\r\n\r\n            // to GPU\r\n            input_tensor = input_tensor.to(at::kCUDA);\r\n\r\n            torch::Tensor out_tensor = module.forward({input_tensor}).toTensor();\r\n\r\n            auto results = out_tensor.sort(-1, true);\r\n            auto softmaxs = std::get<0>(results)[0].softmax(0);\r\n            auto indexs = std::get<1>(results)[0];\r\n\r\n            for (int i = 0; i < kTOP_K; ++i) {\r\n                auto idx = indexs[i].item<int>();\r\n                std::cout << \"    ============= Top-\" << i + 1\r\n                          << \" =============\" << std::endl;\r\n                std::cout << \"    Label:  \" << labels[idx] << std::endl;\r\n                std::cout << \"    With Probability:  \"\r\n                          << softmaxs[i].item<float>() * 100.0f << \"%\" << std::endl;\r\n            }\r\n\r\n        } else {\r\n            std::cout << \"Can't load the image, please check your path.\" << std::endl;\r\n        }\r\n    }\r\n    std::cout << \"Before return, I'm OK!\" << std::endl; **//This can print out**\r\n    return 0;\r\n}\r\n```\r\n\r\n## Expected behavior\r\n\r\nNo error occur.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): Libtorch-1.5.0\r\n - OS (e.g., Linux): CentOS-8.1-1911\r\n - How you installed PyTorch (`conda`, `pip`, source): \r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.2/7.6.5\r\n - GPU models and configuration: Tesla T4\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @ezyang @gchanan @zou3519 @yf225 @glaringlee @ngimel"},{"labels":[null,"api",null],"text":"I am currently trying to implement the Pytorch C++ API into production. RTOS are indispensable in production environments so I'm trying to cross-compile with cmake for QNX Neutrino 7.0.0 (x86_64). \r\nHowever, when compiling a simple test program (empty main() function just including \"#include <torch/torch.h>\") \r\nI get the following compiler errors:\r\n\r\nhttps://github.com/shyney7/libtorch-cross-RTOS-test/blob/master/Compilerlogs/comperr-without-stdlib-flag.md\r\n\r\nI had to set the following command so that cmake finds the torch library in the host path.\r\n\r\n` SET(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY BOTH)`\r\n\r\nThis is how my **cmake Toolchain File** looks like:\r\n\r\nhttps://github.com/shyney7/libtorch-cross-RTOS-test/blob/master/cmake/qnx_7.0.0_linux_x86_64.cmake\r\n\r\nThis is my **CMakeLists.txt** file:\r\n\r\nhttps://github.com/shyney7/libtorch-cross-RTOS-test/blob/master/CMakeLists.txt\r\n\r\nUnfortunately I am not a computer scientist but a mechanical engineer working at a research lab of a german university trying to implement libtorch into production. So I am not sure what the compiler errors mean. Thats why I searched the web for similar errors and found an issue of another repo here: \r\nhttps://github.com/eProsima/Fast-RTPS/issues/300\r\nThere it is mentioned that it is also needed to prevent the qcc compiler from using the LLVM Compiler Infrastructure from v1/memory by using the `-stdlib=libstdc++` flag.\r\nI also tried this by changing my CMakeLists.txt file to use the following flags:\r\n`set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=gnu++14 -stdlib=libstdc++ ${TORCH_CXX_FLAGS}\")` \r\n\r\nBut then I'm getting the following **compiler errors**:\r\n\r\nhttps://github.com/shyney7/libtorch-cross-RTOS-test/blob/master/Compilerlogs/comperr_with-stdlib%3Dlibstdc%2B%2B_flag.md\r\n\r\nIn the announcement videos where Libtorch and the C++ API were introduced, it was always said that the C++ API is specifically for production environments. The fact is that in production environments you mostly work with real-time operating systems like QNX or FreeRTOS, so I'm sure that there is somehow a way to get libtorch working on QNX.\r\n\r\nHere is the repo with all the files:\r\nhttps://github.com/shyney7/libtorch-cross-RTOS-test\r\n\r\nHere is also a link to my first post on the discussion forums where I faced some issues with setting up the cmake toolchain file correctly:\r\nhttps://discuss.pytorch.org/t/cross-compiling-with-cmake-for-embedded-systems-qnx/79681\r\n\r\n## Environment\r\n\r\n - Libtorch Version : 1.5 Release\r\n - OS : cross-compiling on Ubuntu 18.04 (VM) for **QNX 7.0.0 (x84_64)**\r\n - How you installed PyTorch : libtorch 1.5 zip from pytorch.org (for linux tried with both Pre & cxx11 ABI)\r\n - Build command you used : \r\n\r\n`cmake -DCMAKE_TOOLCHAIN_FILE=../cmake/qnx_7.0.0_linux_x86_64.cmake ..`\r\n`make`\r\n - GNU version: 5.4.0\r\n - CUDA/cuDNN version: none\r\n - GPU models and configuration: none\r\n - cmake version: 3.10.2 \r\n\r\n\r\ncc @malfet @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"## üêõ Bug\r\n\r\nWhen I compile a simple package (**a** for instance) using cmake, the exported ${TORCH_LIBRARIES}$ is as follows,\r\n```\r\ntorch;torch_library;/usr/lib/libc10.so;/opt/cuda/lib/stubs/libcuda.so;/opt/cuda/lib/libnvrtc.so;/opt/cuda/lib/libnvToolsExt.so;/opt/cuda/lib64/libcudart.so;/usr/lib/libc10_cuda.so\r\n```\r\nPackage **a** compiles successfully even if the linker fails to link **torch_library**. \r\n\r\nThe actual problem arises when I have another package (**b**), dependent on the already created package **a**. When I build, it fails with error\r\n\r\n```\r\nCMake Error at /home/vsury/dev/ros/pytorch-example/devel_isolated/a/share/a/cmake/aConfig.cmake:150 (message):\r\n  Project 'b' tried to find library '-Wl,--no-as-needed,$<TARGET_FILE:torch>\r\n  -Wl,--as-needed'.  The library is neither a target nor built/installed\r\n  properly.  Did you compile project 'a'? Did you find_package() it before\r\n  the subdirectory containing its code is included?\r\n```\r\n\r\nThe reason has to do with **torch_library** because there is no library such as **libtorch_library.so**.\r\n\r\nLooking further, it seems like that ${TORCH_LIBRARIES} is linked to ${Caffe2_MAIN_LIBS},\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/cmake/TorchConfig.cmake.in#L41\r\n\r\nand  ${Caffe2_MAIN_LIBS} is linked to **torch_library**,\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/cmake/Caffe2Config.cmake.in#L121\r\n\r\nI do not understand the need for **torch_library** and removing it from $TORCH_LIBRARIES$ fixes the entire problem.\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Create a package **a** with any script with torch functionalities (basically CMakeLists should contain find_package(Torch REQUIRED) and build target with torch libraries.\r\n2. Create another package **b** and make it depend on **a**.\r\n3. Compile package b to get the error.\r\n\r\n## Expected behavior\r\n\r\nSuccessful build\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n\r\n - PyTorch Version (e.g., 1.0): 1.5.0\r\n - OS (e.g., Linux): Archlinux\r\n - How you installed PyTorch (`conda`, `pip`, source): pacman -S\r\n - Build command you used (if compiling from source): cmake and make\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\n\ncc @malfet @yf225 @glaringlee"},{"labels":[null,null,"api",null,null,null],"text":"## üêõ Bug\r\n\r\nDear community,\r\n\r\nI am a dev from [KeOps project](https://github.com/getkeops/keops/) building optimized operations written in C++/Cuda that are compatible with pytorch (among others scientific languages). \r\n\r\nWe successfully build python modules compatible with pyTorch since the pyTorch v0.2... But the last v1.5 broke our modules with a runtime error complaining about [missing symbols](https://github.com/getkeops/keops/issues/59).\r\n\r\n\r\nTo simplify the analysis you may find below a minimal working example that builds a module through pybind11n the spirit of [pytorch doc](https://pytorch.org/tutorials/advanced/cpp_frontend.html#writing-a-basic-application). \r\n\r\nIt should output a file `test_module.cpython-38-x86_64-linux-gnu.so` that can be imported from python. This module works well when building with pytorch 1.4 but raised a runtime error when building with pytorch v1.5. \r\n\r\n\r\n## To Reproduce\r\n\r\nAssuming pyTorch and pybind11 installed (e.g. through conda).  There are 2 files\r\n\r\n`module_test.cpp` contains\r\n\r\n```cpp\r\n#include <torch/extension.h>\r\n#include <pybind11/pybind11.h>\r\n// Main function\r\nat::Tensor foo(int s) {\r\n     return torch::eye(s);\r\n}    \r\n\r\n// PyBind11 entry point \r\nPYBIND11_MODULE(test_module, m) {\r\nm.def(\"foo\", &foo, \"Entry point to test module\");\r\n}\r\n```   \r\nand `CMakeList.txt` contains\r\n```\r\nproject(test_module LANGUAGES CXX)\r\n\r\nfind_package(Torch REQUIRED)\r\nfind_package(pybind11  REQUIRED)\r\n\r\npybind11_add_module(test_module ${CMAKE_CURRENT_SOURCE_DIR}/test_module.cpp)\r\ntarget_link_libraries(test_module PUBLIC \"${TORCH_LIBRARIES}\")\r\n```\r\nit can be compile with\r\n\r\n ```bash\r\n$ mkdir build\r\n$ cd build\r\n$ cmake -DCMAKE_PREFIX_PATH=\"/home/bcharlier/.conda/envs/keops/lib/python3.8/site-packages/torch/\" .. && make\r\n```\r\nand run with (note that `torch` is imported first)\r\n```\r\n$ python -c \"import torch; print(torch.__version__); import test_module; print(test_module.foo(3))\"\r\n\r\n1.5.0\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: /home/bcharlier/src/test_module/build/test_module.cpython-38-x86_64-linux-gnu.so: undefined symbol: _Z16THPVariable_WrapN2at6TensorE\r\n```\r\nThe missing symbol is not exactly the same when [building a module with the keops library](https://github.com/getkeops/keops/issues/59)... But I guess this is irrelevant.\r\n\r\n## Expected behavior\r\n\r\nIt works fine when compiling with pytorch v1.4\r\n\r\n```bash\r\n$ cmake -DCMAKE_PREFIX_PATH=\"/home/bcharlier/.conda/envs/keops_torch14/lib/python3.8/site-packages/torch/\" .. && make\r\n$ python -c \"import torch; print(torch.__version__); import test_module; print(test_module.foo(3))\"\r\n1.4.0\r\ntensor([[1., 0., 0.],\r\n        [0., 1., 0.],\r\n        [0., 0., 1.]])\r\n```\r\n\r\n## Environment\r\n\r\nThe bug was reported by various users running on linux. Here is my config:\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.5.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Arch Linux\r\nGCC version: (Arch Linux 9.3.0-1) 9.3.0\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.8\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: GPU 0: Quadro T2000\r\nNvidia driver version: 440.82\r\ncuDNN version: /usr/lib/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.4\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cudatoolkit               10.2.89              hfd86e86_1  \r\n[conda] mkl                       2020.0                      166  \r\n[conda] mkl-service               2.3.0            py38he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py38ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py38h962f231_0  \r\n[conda] numpy                     1.18.1           py38h4f9e942_0  \r\n[conda] numpy-base                1.18.1           py38hde5b4d6_1  \r\n[conda] pytorch                   1.5.0           py3.8_cuda10.2.89_cudnn7.6.5_0    pytorch\r\n\r\n```\n\ncc @ezyang @gchanan @zou3519 @malfet @yf225 @glaringlee"},{"labels":["api",null,null],"text":"In the latest 1.5 release, I am unable to find Transformer model api in C++ (equivalent to the torch.nn.Transformer in its python counterpart). Any timeline for its implementation in C++?\n\ncc @yf225 @glaringlee"},{"labels":["api",null],"text":"i use libtorch to do inference, and i got a two-dimensional tensor, but how can i obatian the value, or how can i transform the tensor in C++ to  a array?\r\n\n\ncc @yf225 @glaringlee"},{"labels":["api",null],"text":"how to create tensor from tensorRT fp16 half type pointer in libtorch?\r\nI am working on a detection model. I change the backbone of it to tensorRT to do FP16 inference, and the detection code such as decode boxes and nms is done in libtorch and torchvisoin, so how to create fp16 tensor from tensorRT half type pointers?\r\nThe important code is to illustrate the issue:\r\n```\r\n// tensorRT code to get half type outpus\r\nhalf_float::half* outputs[18];\r\ndoInference(*engine, data, outputs, 1);\r\n// to get the final outputs with libtorch\r\nvector<torch::Tensor> output;\r\n//???? how to feed the date in outpus to output????\r\n// get the result with libtorch method detect_trt->forward\r\n auto res = detect_trt->forward(output); \r\n```\r\nThanks in advance.\n\ncc @yf225 @glaringlee"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\nHi\r\n\r\nI've encountered a problem with the code in \"c10/utils/variant.h\" while trying to build a C++ application.\r\nI've created a simple test program to explain the problem. \r\nThe code I‚Äôm trying to build is the following one\r\n\r\n```cpp\r\n//Include standard libraries\r\n#include <iostream>\r\n\r\n//Define namespace data\r\n//This can be included for example by another header file\r\nnamespace data\r\n{\r\n}\r\n\r\n//Include torch library\r\n#include <torch/torch.h>\r\n\r\nint main(int argc, char* argv[])\r\n{\r\n  std::cout << torch::randn({3, 3}) << std::endl;\r\n  return 0;\r\n}\r\n```\r\n\r\nThe CMakeLists.txt is\r\n\r\n```\r\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\r\nproject(test_variant)\r\n\r\nfind_package(Torch REQUIRED)\r\n\r\nadd_executable(test_variant main.cpp)\r\ntarget_link_libraries(test_variant \"${TORCH_LIBRARIES}\")\r\nset_property(TARGET test_variant PROPERTY CXX_STANDARD 14)\r\n```\r\n\r\nI‚Äôve added an empty ‚Äúdata‚Äù namespace to better explain the problem.\r\nWhen I build the example I get a lot of errors like\r\n\r\n```\r\n~/test_variant/libtorch/include/c10/util/variant.h:2519:7: error: invalid use of ‚Äòvoid‚Äô\r\n       AUTO_RETURN(v && holds_alternative<I>(*v)\r\n```\r\n\r\nI‚Äôve looked inside the file and it seems that the problem is the statement (line 1165 c10/utils/variant.h)\r\n\r\n```\r\nAUTO_REFREF_RETURN(recursive_union::get_alt(\r\n              data(lib::forward<V>(v)), in_place_index_t<I>{}))\r\n```\r\n\r\nWhere ‚Äúdata‚Äù is incorrectly detected as the namespace I‚Äôve added before the <torch/torch.h> inclusion.\r\nI think it should refer to the ‚Äúdata‚Äù function which can be found in the same file at line 1743 (I tried to change its name and indeed the error is solved).\r\nCould this be an issue or it‚Äôs something I‚Äôm doing wrong in my code?\r\n\r\nThank you very much for your help\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): LibTorch 1.4.0\r\n - OS (e.g., Linux): Ubuntu 16.04 (default compiler)\r\n - How you installed PyTorch (`conda`, `pip`, source): pre-built libraries\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: N/A\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information: cpu mode only \n\ncc @yf225"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nThe output from a traced model using (1) python and (2) c++ are different.  \r\n\r\nxpost: https://discuss.pytorch.org/t/c-and-pytorch-inference-discrepancy/77388?u=jonrbates\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n**1. Create model in python**\r\n```\r\nimport torch\r\nfrom transformers import GPT2LMHeadModel\r\nmodel = GPT2LMHeadModel.from_pretrained(\"distilgpt2\", \r\n           pad_token_id=50256,\r\n           torchscript=True)\r\ntraced_model = torch.jit.trace(model, [torch.tensor([[464, 6842, 1816,  625,  262, 8598,  284,  766]])])\r\ntorch.jit.save(traced_model, \"traced_distilgpt2.pt\")\r\n```\r\n\r\n**2. Inference in pytorch**\r\n```\r\nloaded_model = torch.jit.load(\"traced_distilgpt2.pt\")\r\noutputs = loaded_model(torch.tensor([[464, 6842, 1816,  625,  262, 8598,  284,  766]]))\r\n\r\ntorch.max(outputs[0],2)\r\n>> torch.return_types.max(values=tensor([[-26.7050, -53.3268, -66.3666, -50.1084, -54.8050, -72.4056, -55.2586,-63.5215]], grad_fn=<MaxBackward0>),\r\nindices=tensor([[ 383,  373,  319,  262, 1353,  290,  262,  262]]))\r\n\r\ntorch.min(outputs[0][:, :, :],2)\r\n>>torch.return_types.min(\r\nvalues=tensor([[ -47.3794,  -77.6341,  -95.8365,  -75.3026,  -81.6899, -103.6701, -83.0335,  -93.0259]], grad_fn=<MinBackward0>),\r\nindices=tensor([[  154,  7134, 31204, 22997, 10298, 31204, 22997, 31573]]))\r\n```\r\n\r\n**3. Inference in c**\r\n```\r\nint main(int argc, const char* argv[]) {\r\n\r\n  torch::jit::script::Module module;\r\n  module = torch::jit::load(argv[1]);\r\n  std::cout << \"Model loaded.\\n\";\r\n  module.eval(); // just in case?\r\n\r\n  torch::Tensor x = torch::tensor({464, 6842, 1816,  625,  262, 8598,  284,  766},\r\n    torch::dtype(torch::kInt64)).reshape({8, 1});\r\n  std::vector<torch::jit::IValue> inputs;\r\n  inputs.push_back(x);\r\n\r\n  // Execute the model and turn its output into a tensor (all_encoder_layers).\r\n  torch::Tensor out = module.forward(inputs).toTuple()->elements()[0].toTensor();\r\n\r\n  auto z = torch::max(out, /*dim=*/2);\r\n  std::cout << std::get<0>(z) << '\\n';\r\n  std::cout << std::get<1>(z) << '\\n';\r\n\r\n  z = torch::min(out, /*dim=*/2);\r\n  std::cout << std::get<0>(z) << '\\n';\r\n  std::cout << std::get<1>(z) << '\\n';\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\nOutput from running c exe (with argv[1] = \"traced_distilgpt2.pt\") :\r\n```\r\n-26.7050\r\n-28.0251\r\n-29.0539\r\n-28.2608\r\n-26.3226\r\n-28.0652\r\n-26.8328\r\n-28.5087\r\n[ Variable[CPUFloatType]{8,1} ]\r\n 383\r\n 383\r\n 383\r\n 383\r\n 383\r\n 383\r\n 383\r\n 383\r\n[ Variable[CPULongType]{8,1} ]\r\n-47.3794\r\n-48.2470\r\n-49.5565\r\n-49.0826\r\n-46.3104\r\n-48.5934\r\n-47.3013\r\n-49.1308\r\n[ Variable[CPUFloatType]{8,1} ]\r\n 154\r\n 154\r\n 154\r\n 154\r\n 154\r\n 154\r\n 154\r\n 154\r\n[ Variable[CPULongType]{8,1} ]\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nThe models agree for the first token (position=0) but for positions 1-7 the encoder outputs disagree.  All outputs should be equal. \r\n<!-- -->\r\n\r\n## Environment (Pytorch)\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Microsoft Windows 10 Pro\r\nGCC version: (x86_64-posix-seh-rev0, Built by MinGW-W64 project) 5.4.0\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Quadro M500M\r\nNvidia driver version: 442.23\r\ncuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] _tflow_select             2.3.0                       mkl\r\n[conda] blas                      1.0                         mkl\r\n[conda] cudatoolkit               10.1.243             h74a9793_0\r\n[conda] libmklml                  2019.0.5                      0\r\n[conda] mkl                       2019.4                      245\r\n[conda] mkl-include               2020.0                      166\r\n[conda] mkl-service               2.3.0            py37hb782905_0\r\n[conda] mkl_fft                   1.0.14           py37h14836fe_0\r\n[conda] mkl_random                1.1.0            py37h675688f_0\r\n[conda] numpy                     1.16.5           py37h19fb1c0_0\r\n[conda] numpy-base                1.16.5           py37hc3f5095_0\r\n[conda] numpydoc                  0.9.1                      py_0\r\n[conda] pytorch                   1.4.0           py3.7_cuda101_cudnn7_0    pytorch\r\n[conda] tensorflow                2.1.0           mkl_py37ha977152_0\r\n[conda] tensorflow-base           2.1.0           mkl_py37h230818c_0\r\n[conda] torchvision               0.5.0                py37_cu101    pytorch\r\n\r\n## Environment (C++)\r\n\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 2.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n\n\ncc @yf225"},{"labels":[null,"api",null,null],"text":"similar to issue #33192\r\n\r\nif we start with AnyModule a, there is no good way to add a name, \r\nto create a NamedAnyModule\r\ni.e. in  _nn/modules/container/named_any.h_\r\n\r\n```\r\nprivate:\r\n  /// Creates a `NamedAnyModule` from a type-erased `AnyModule`.\r\n  NamedAnyModule(std::string name, AnyModule any_module)\r\n    : name_(std::move(name)), module_(std::move(any_module)) {}\r\n```\r\nI can create the pr to move this method to public if it makes sense.\r\n(similar to pr #34208)\n\ncc @yf225"},{"labels":["api",null],"text":"## üöÄ Feature\r\nI would be able to clone a model into another model. Such as being done in the [Reinforcement Learning (DQN) Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) at **Training**.\r\n\r\nThe requested functions that do exist in python but not C++ are:\r\n```\r\nload_state_dict()\r\nstate_dict()\r\ntarget_net.load_state_dict(policy_net.state_dict())\r\n```\r\n\r\n## Motivation\r\nIt would be neat to be able to follow the pytorch example listed above. However the C++ library are missing the necessary functions for doing this.\r\n\n\ncc @yf225"},{"labels":[null,"api",null],"text":"## üêõ Bug\r\n\r\nLoad pytorch tensor created by torch.save(tensor_name, tensor_path) in c++ libtorch failed.\r\nHow can I save some tensor in python, but load it in libtorch?\r\n## To Reproduce\r\n\r\nusing the following code:\r\n\r\nI save tensor named piror using python, using the code:\r\n```\r\nprior = torch.ones(32145, 4)\r\ntorch.save(prior,  'prior.pth')\r\n```\r\nAnd I load the tensor in libtorch using C++, by the following code:\r\n```\r\ntorch::Tensor priors = torch::ones({32145, 4});\r\ntorch::load(priors , \"/app/model/prior.pth\");\r\n```\r\n\r\n## Expected behavior\r\nload the tensor successfully.\r\nAnd get exact same value as in pytorch python-api.\r\n\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: version 3.13.2\r\n\r\nPython version: 3.5\r\nIs CUDA available: N/A\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: TITAN X (Pascal)\r\n\r\nNvidia driver version: 430.26\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n\r\n## Additional context\r\nBut I got the error:\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  `torch::jit::load()` received a file from `torch.save()`, but `torch::jit::load()` can only load files produced by `torch.jit.save()` (load at ../torch/csrc/jit/serialization/import.cpp:285)\r\n\r\nWhy is that? I do not use torch::jit::load but torch::load, so how to load tensor saved in pytorch?\r\n\r\nThanks in advance.\n\ncc @suo @yf225"},{"labels":[null,"api",null],"text":"How can I save some tensor in python, but load it in libtorch:\r\n\r\nI save tensor named piror using python, using the code:\r\n```\r\ntorch.save(prior,  'prior.pth')\r\n```\r\nAnd I load the tensor in libtorch using C++, by the following code:\r\n```\r\nstd::vector<torch::Tensor> tensorVec;\r\ntorch::load(tensorVec, \"/app/model/prior.pth\");\r\ntorch::Tensor priors = tensorVec[0];\r\n```\r\nBut I got the error:\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  `torch::jit::load()` received a file from `torch.save()`, but `torch::jit::load()` can only load files produced by `torch.jit.save()` (load at ../torch/csrc/jit/serialization/import.cpp:285)\r\n\r\nWhy is that? And what should I do to solve the issue? Thanks in advance.\n\ncc @suo @yf225"},{"labels":[null,null,"api",null,null,null,null,null],"text":"Hi, I want to use my pretrained model(which is trained on pytorch) in my C++ project. I have saved my network in Python and loaded in C++ using jit.\r\n\r\n```\r\n//Python\r\ntraced_script_module = torch.jit.trace(model, input)\r\ntraced_script_module.save(\"model.pt\")\r\n//C++\r\ntorch::jit::script::Module model =torch::jit::load(file_name);\r\n```\r\n\r\nI am passing CUDAFloatTensor as an input. I am supposed to have three outputs from my model.\r\n\r\n```\r\nauto outputs = model.forward({ input });\r\nauto heatmap= outputs.toTuple()->elements()[0].toTensor(); \r\nauto bbox = outputs.toTuple()->elements()[1].toTensor();\r\nauto scan_rec= outputs.toTuple()->elements()[2].toTensor();\r\n```\r\nI realized forward pass doesn‚Äôt always give correct results. Sometimes it gives answer below.(which is the right answer I checked on pytorch and outputs on there are always consistent)\r\n`first three elements of bbox : 0.4311, 0.4620, 0.3915`\r\n\r\nBut most of the times I get values like this in each three output and I am having exception when I am trying to use my outputs.\r\n`first three elements of bbox : = 9.3593e-36, 3.9978e+07,-5.3179e+37`\r\n\r\nWhich is really weird. I am assuming there is some kind of overflow going on. I don‚Äôt want to train any network in C++. I just want to use my pretrained model as a deterministic function. I also set these in any case before calling forward pass but it didn‚Äôt help. I have no idea what is going on.\r\n\r\n```\r\ntorch::manual_seed(0);\r\ntorch::NoGradGuard no_grad;\r\n\r\n```\r\nEnvironment:\r\nWindows 10\r\nCUDA 10.1\r\nVisual Studio 2019\r\nLibtorch Nightly Release latest\r\n\n\ncc @ezyang @gchanan @zou3519 @suo @yf225 @peterjc123"},{"labels":[null,"api",null],"text":"## üêõ Bug\r\n\r\n(not sure how much of a bug this is, maybe it's expected)\r\nI'm currently working on adapting some [rust bindings](https://github.com/LaurentMazare/tch-rs) for PyTorch and in the process came across the following issue for which I don't have a good solution. The issue can be show on C++ code.\r\n- When linking the final binary with `-ltorch -ltorch_cpu -lc10`, `torch::cuda::is_available()` returns false.\r\n- When linking the final binary with `-Wl,--no-as-needed -ltorch -ltorch_cpu -lc10`, `torch::cuda::is_available()` returns true.\r\n- When linking without `-ltorch_cpu`, I get a missing symbol error for: `c10::Dispatcher::singleton`.\r\n\r\nI tried compiling some C++ pytorch code with cmake and it seems to use --no-as-needed to get this to work.\r\n\r\nIs there a way to get some external code to compile without libtorch_cpu?\r\nOne difficulty is that the rust build system does not let you specify arbitrary linker flags so I cannot easily set `-Wl,--no-as-needed`.\r\n\r\n## To Reproduce\r\n\r\nThe issue can be reproduced using the C++ code below.\r\n\r\n```c++\r\n#include <torch/torch.h>\r\n#include <iostream>\r\n\r\nint main() {\r\n  std::cout << torch::cuda::is_available() << std::endl;\r\n}\r\n```\r\nThen:\r\n- `g++ test.cpp -std=gnu++14 -ltorch -ltorch_cpu -lc10 && ./a.out` prints 0.\r\n- `g++ test.cpp -std=gnu++14 -Wl,--no-as-needed -ltorch -ltorch_cpu -lc10 && ./a.out` prints 1.\r\n\r\n## Expected behavior\r\n\r\nI would have hoped for cuda to be reported as available without the `-Wl,--no-as-needed` flag.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): release/1.5 branch as of 2020-04-11.\r\n - OS (e.g., Linux): Linux (ubuntu 18.04)\r\n - How you installed PyTorch (`conda`, `pip`, source): source, compiled with cuda support\r\n - Build command you used (if compiling from source): `python setup.py build`\r\n - Python version: 3.7.1.\r\n - CUDA/cuDNN version: 10.0/none.\r\n - GPU models and configuration: 1x GeForce RTX 2080.\r\n - Any other relevant information: gcc/g++ 7.5.0, ld 2.3.0\r\n\r\n## Additional context\n\ncc @yf225"},{"labels":["api",null],"text":"I previous using this get cuda stream:\r\n\r\n```\r\nmodulated_deformable_col2im_coord_cuda(THCState_getCurrentStream(state),\r\n```\r\n\r\nI found this API gone `THCState_getCurrentStream` without even a deprecation warning, what's the altinate of this API?\r\nin torch 1.5?\n\ncc @yf225"},{"labels":["api",null],"text":"This is to better match the Python API `ctx.saved_tensors`.\r\n\r\nI believe we originally named it `get_saved_variables()` because there was still a Tensor vs. Variable distinction at that time. Now that Tensor and Variable are the same, we can deprecate `get_saved_variables()` and replace it with `get_saved_tensors()`.\r\n\r\ncc @yf225"},{"labels":["api",null,null,null],"text":"## üêõ Bug\r\n\r\nThere are some minor labelling errors in the default options check in  _optim/adah.h_: \r\n```\r\n     auto betas = defaults.betas();\r\n     TORCH_CHECK(std::get<0>(betas) >= 0, \"Invalid learning rate: \", std::get<0>(betas));\r\n     TORCH_CHECK(std::get<1>(betas) >= 0, \"Invalid learning rate: \", std::get<1>(betas));\r\n     TORCH_CHECK(defaults.weight_decay() >= 0, \"Invalid learning rate: \", defaults.weight_decay());\r\n```\r\nI am also seeing worse convergance on some simple GAN's with MNIST,\r\ne.g. the tutorial example, using the newer adam vs the 1.4 version,\r\nbut I don't have a good way to reproduce this yet.\r\nthe GAN's produce noticeably less convincing digits than using the 1.4 version for the same settings and number of iterations.\r\n\n\ncc @yf225 @vincentqb"},{"labels":["api",null],"text":"## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nCan we remove \"using namespace ...\" from the public headers of libtorch?\r\n\r\n## Motivation\r\nE.g., `torch/csrc/api/include/torch/nn/modules/_functions.h` has this line `using namespace torch::autograd;`. This pollutes the namespace of some applications.\r\n\r\n\n\ncc @yf225"},{"labels":["api",null,null],"text":"In Python, we have the following tensor print behavior:\r\n```python\r\n>>> x=torch.ones(2, 2, requires_grad=True)\r\n>>> x\r\ntensor([[1., 1.],\r\n        [1., 1.]], requires_grad=True)\r\n>>> y=x*2\r\n>>> y\r\ntensor([[2., 2.],\r\n        [2., 2.]], grad_fn=<MulBackward0>)\r\n```\r\nHowever in C++, we have the following behavior:\r\n```cpp\r\nauto x = torch::ones({2, 2}, torch::requires_grad());\r\nstd::cout << x << std::endl;\r\nauto y = x * 2;\r\nstd::cout << y << std::endl;\r\n```\r\n```cpp\r\n 1  1\r\n 1  1\r\n[ CPUFloatType{2,2} ]\r\n 2  2\r\n 2  2\r\n[ CPUFloatType{2,2} ]\r\n```\r\nIt would be nice to show `requires_grad` and `grad_fn` in C++ tensor print as well.\n\ncc @yf225"},{"labels":["api",null,null],"text":"I build 32bit libtorch myself.And I use torch.jit.trace to convert my python module into c++ module. When I try to load module in vs2017 project , an error occur.There is an unhandled exception at 0x00007ffb0ee5db8e (ucrtbase. DLL) (in libtorch1.3.1. Exe): a serious program exit was requested. \r\n\r\n# Python code\r\n```python\r\ndef package():\r\n    model = BiSINet(p=2, q=8)\r\n    model.load_state_dict(\r\n        weight_convert(torch.load(\"weights/temp/SINetDIS_decoder_360_3c.pth\", \"cpu\")))\r\n    model.cpu()\r\n    model.eval()\r\n    with torch.no_grad():\r\n        model_input = torch.rand(1, 3, 224, 224)\r\n        trace_model_script = torch.jit.trace(model, model_input)\r\n        trace_model_script.save('weights/convert/SINet_decoder_3c_x86.pt')\r\n```\r\n\r\n# C++ code\r\n```c++\r\n#include <torch/script.h>\r\n#include <iostream>\r\n\r\nint main() {\r\n\ttorch::jit::script::Module module;\r\n\ttry {\r\n\t\tmodule = torch::jit::load(\"A:\\\\projects\\\\libtorchs\\\\sinet_50.pt\");\r\n\t}\r\n\tcatch (const c10::Error &e) {\r\n\t\tstd::cerr << \"error loading the model\\n\";\r\n\t\treturn -1;\r\n\t}\r\n\tstd::cout << \"ok\\n\";\r\n\treturn 0;\r\n}\r\n```\r\n\r\n# Error Message\r\n0x00007FFB0EE5DB8E (ucrtbase.dll) (libtorch1.3.1.exe ‰∏≠)Â§ÑÊúâÊú™ÁªèÂ§ÑÁêÜÁöÑÂºÇÂ∏∏: ËØ∑Ê±Ç‰∫Ü‰∏•ÈáçÁöÑÁ®ãÂ∫èÈÄÄÂá∫„ÄÇ Âá∫Áé∞‰∫Ü \r\n\r\ncc @yf225 @peterjc123"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\nI got the following error when calling `optimizer.backward()`.\r\n`one of the variables needed for gradient computation has been modified by an inplace operation: [CPUFloatType [50, 40]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True). after train`\r\n\r\n## To Reproduce\r\nNot sure, since I can't see which operation that makes my code crash.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nI expected the function that was listed in the error to exist.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): Libtorch 1.4\r\n - OS (e.g., Linux): Windows 7 64 bit\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): none\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: none\r\n - GPU models and configuration: none\r\n - Any other relevant information: none\r\n\r\n\n\ncc @yf225"},{"labels":[null,"api",null],"text":"# üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Compile & Run code below.\r\n\r\n```C++\r\n#include <torch/torch.h>\r\n\r\n// Define a new Module.\r\nstruct Net : torch::nn::Module {\r\n  Net() {\r\n    // Construct and register two Linear submodules.\r\n    fc1 = register_module(\"fc1\", torch::nn::Linear(784, 64));\r\n    fc2 = register_module(\"fc2\", torch::nn::Linear(64, 32));\r\n    fc3 = register_module(\"fc3\", torch::nn::Linear(32, 10));\r\n  }\r\n\r\n  // Implement the Net's algorithm.\r\n  torch::Tensor forward(torch::Tensor x) {\r\n    // Use one of many tensor manipulation functions.\r\n    x = torch::relu(fc1->forward(x.reshape({x.size(0), 784})));\r\n    x = torch::dropout(x, /*p=*/0.5, /*train=*/is_training());\r\n    x = torch::relu(fc2->forward(x));\r\n    x = torch::log_softmax(fc3->forward(x), /*dim=*/1);\r\n    return x;\r\n  }\r\n\r\n  // Use one of many \"standard library\" modules.\r\n  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};\r\n};\r\n\r\nint main() {\r\n  // Create a new Net.\r\n  auto net = std::make_shared<Net>();\r\n\r\n  // Create a multi-threaded data loader for the MNIST dataset.\r\n  auto data_loader = torch::data::make_data_loader(\r\n      torch::data::datasets::MNIST(\"./data\").map(\r\n          torch::data::transforms::Stack<>()),\r\n      /*batch_size=*/64);\r\n\r\n  // Instantiate an SGD optimization algorithm to update our Net's parameters.\r\n  torch::optim::SGD optimizer(net->parameters(), /*lr=*/0.01);\r\n\r\n  for (size_t epoch = 1; epoch <= 10; ++epoch) {\r\n    size_t batch_index = 0;\r\n    // Iterate the data loader to yield batches from the dataset.\r\n    torch::Device device = epoch % 2 == 1 ? torch::kCUDA : torch::kCPU;\r\n    net->to(device);\r\n    std::cout << \"Device of network is \" << device << std::endl;\r\n    for (auto& batch : *data_loader) {\r\n      // Reset gradients.\r\n      optimizer.zero_grad();\r\n      // Execute the model on the input data.\r\n      torch::Tensor prediction = net->forward(batch.data.to(device));\r\n      // Compute a loss value to judge the prediction of our model.\r\n      torch::Tensor loss = torch::nll_loss(prediction, batch.target.to(device));\r\n      // Compute gradients of the loss w.r.t. the parameters of our model.\r\n      if(batch_index == 0) std::cout << \"Device of loss is \" << loss.device() << \"\\n\";\r\n      loss.backward();\r\n      if(batch_index == 0) std::cout << \"loss.backward() success\" << \"\\n\";\r\n\t \t\r\n      // Update the parameters based on the calculated gradients.\r\n      optimizer.step();\r\n      // Output the loss and checkpoint every 100 batches.\r\n      if (++batch_index % 100 == 0) {\r\n        std::cout << \"Epoch: \" << epoch << \" | Batch: \" << batch_index\r\n                  << \" | Loss: \" << loss.item<float>() << std::endl;\r\n        // Serialize your model periodically as a checkpoint.\r\n        torch::save(net, \"net.pt\");\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nand output is\r\n\r\n```\r\nDevice of network is cuda\r\nDevice of loss is cuda:0\r\nloss.backward() success\r\nEpoch: 1 | Batch: 100 | Loss: 2.2953\r\nEpoch: 1 | Batch: 200 | Loss: 2.252\r\nEpoch: 1 | Batch: 300 | Loss: 2.22181\r\nEpoch: 1 | Batch: 400 | Loss: 2.13327\r\nEpoch: 1 | Batch: 500 | Loss: 2.04376\r\nEpoch: 1 | Batch: 600 | Loss: 1.80017\r\nEpoch: 1 | Batch: 700 | Loss: 1.65806\r\nEpoch: 1 | Batch: 800 | Loss: 1.47708\r\nEpoch: 1 | Batch: 900 | Loss: 1.40919\r\nDevice of network is cpu\r\nDevice of loss is cpu\r\nterminate called after throwing an instance of 'torch::utils::FutureError'\r\n  what():  expected device cuda:0 but got device cpu (compute_types at /pytorch/aten/src/ATen/native/TensorIterator.cpp:246)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f0850510576 in /home/sonic/Downloaded_Library/libtorch/lib/libc10.so)\r\nframe #1: at::TensorIterator::compute_types() + 0x17d4 (0x7f08426fe214 in /home/sonic/Downloaded_Library/libtorch/lib/libtorch_cpu.so)\r\nframe #2: at::TensorIterator::build() + 0x44 (0x7f0842700104 in /home/sonic/Downloaded_Library/libtorch/lib/libtorch_cpu.so)\r\nframe #3: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x146 (0x7f08427007b6 in /home/sonic/Downloaded_Library/libtorch/lib/libtorch_cpu.so)\r\nframe #4: at::native::add_out(at::Tensor&, at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x3c (0x7f084242155c in /home/sonic/Downloaded_Library/libtorch/lib/libtorch_cpu.so)\r\nframe #5: <unknown function> + 0x101717d (0x7f0805af717d in /home/sonic/Downloaded_Library/libtorch/lib/libtorch_cuda.so)\r\nframe #6: <unknown function> + 0x2b517ea (0x7f084443e7ea in /home/sonic/Downloaded_Library/libtorch/lib/libtorch_cpu.so)\r\nframe #7: <unknown function> + 0x2cf9c5c (0x7f08445e6c5c in /home/sonic/Downloaded_Library/libtorch/lib/libtorch_cpu.so)\r\nframe #8: torch::autograd::AccumulateGrad::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0xdc (0x7f08445e81bc in /home/sonic/Downloaded_Library/libtorch/lib/libtorch_cpu.so)\r\nframe #9: <unknown function> + 0x2cf5745 (0x7f08445e2745 in /home/sonic/Downloaded_Library/libtorch/lib/libtorch_cpu.so)\r\nframe #10: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f08445dfa43 in /home/sonic/Downloaded_Library/libtorch/lib/libtorch_cpu.so)\r\nframe #11: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f08445e0822 in /home/sonic/Downloaded_Library/libtorch/lib/libtorch_cpu.so)\r\nframe #12: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f08445d8e99 in /home/sonic/Downloaded_Library/libtorch/lib/libtorch_cpu.so)\r\nframe #13: <unknown function> + 0xc70f (0x7f085074970f in /home/sonic/Downloaded_Library/libtorch/lib/libtorch.so)\r\nframe #14: <unknown function> + 0x76db (0x7f08416d56db in /lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #15: clone + 0x3f (0x7f0803dec88f in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\nAborted (core dumped)\r\n```\r\n\r\n## Expected behavior\r\n\r\nRun w/o any segmentation faults.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce RTX 2070 SUPER\r\nNvidia driver version: 430.50\r\ncuDNN version: /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.1\r\n[pip3] torch==1.2.0\r\n[pip3] torchvision==0.4.0\r\n[conda] Could not collect\r\n\r\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @yf225"},{"labels":["api",null,null],"text":"Repro:\r\n```\r\nauto options = F::BatchNormFuncOptions().weight(w).bias(b).momentum(m).eps(e).training(t);\r\nstd::cout << options << std::endl;\r\n```\r\n\r\nExpected result: \r\nPrint value for each property.\r\n\r\nActual Result:\r\n```\r\n‚Äòtorch::nn::functional::BatchNormFuncOptions‚Äô is not derived from ‚Äòconst torch::ExpandingArray<D, T>‚Äô\r\n         std::cout << options << std::endl;\r\n                      ^~~~~~~\r\n```\n\ncc @yf225"},{"labels":[null,"api",null],"text":"## üêõ Bug\r\n\r\nRunning the following code in cuda-enabled libtorch throws error \"CUDA error: driver shutting down\", even though the code doesn't use CUDA. Running the same code in cpu-only libtorch doesn't throw any error.\r\n\r\n```cpp\r\n#include <iostream>\r\n#include <torch/torch.h>\r\n\r\nusing namespace torch::autograd;\r\n\r\nclass MulConstant : public Function<MulConstant> {\r\n public:\r\n  static Variable forward(AutogradContext *ctx, Variable variable, double constant) {\r\n    ctx->saved_data[\"constant\"] = constant;\r\n    return variable * constant;\r\n  }\r\n\r\n  static variable_list backward(AutogradContext *ctx, variable_list grad_outputs) {\r\n    return {grad_outputs[0] * ctx->saved_data[\"constant\"].toDouble(), Variable()};\r\n  }\r\n};\r\n\r\nint main(int argc, char* argv[])\r\n{\r\n  auto x = torch::randn({2}).requires_grad_();\r\n  auto y = MulConstant::apply(x, 5.5);\r\n  y.sum().backward();\r\n  std::cout << x.grad() << std::endl;\r\n}\r\n```\r\nError:\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\nterminate called recursively\r\n  what():  CUDA error: driver shutting down (setDevice at /pytorch/c10/cuda/impl/CUDAGuardImpl.h:42)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fedc6342656 in /data/libtorch/libtorch_nightly_cu92/libtorch/lib/libc10.so)\r\nframe #1: <unknown function> + 0xc6c2 (0x7fed7bad26c2 in /data/libtorch/libtorch_nightly_cu92/libtorch/lib/libc10_cuda.so)\r\nframe #2: torch::autograd::Engine::set_device(int) + 0x159 (0x7fedb9c36b39 in /data/libtorch/libtorch_nightly_cu92/libtorch/lib/libtorch_cpu.so)\r\nframe #3: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x34 (0x7fedb9c39064 in /data/libtorch/libtorch_nightly_cu92/libtorch/lib/libtorch_cpu.so)\r\nframe #4: <unknown function> + 0xc70f (0x7fedc657b70f in /data/libtorch/libtorch_nightly_cu92/libtorch/lib/libtorch.so)\r\nframe #5: <unknown function> + 0x76ba (0x7fed7c3756ba in /lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #6: clone + 0x6d (0x7fed7c8bc41d in /lib/x86_64-linux-gnu/libc.so.6)\r\nAborted (core dumped)\r\n```\r\nBetter backtrace:\r\n```\r\nThread 4 \"example-app\" hit Catchpoint 1 (exception thrown), 0x00007fffccab38bd in __cxa_throw () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n(gdb) bt\r\n#0  0x00007fffccab38bd in __cxa_throw () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#1  0x00007fffc4d14ab9 in c10::cuda::impl::CUDAGuardImpl::getDevice (this=0x997920) at ../c10/cuda/impl/CUDAGuardImpl.h:37\r\n#2  0x00007fffc4d14ed6 in c10::cuda::impl::CUDAGuardImpl::setDevice (this=0x997920, d=...) at ../c10/cuda/impl/CUDAGuardImpl.h:51\r\n#3  0x00007ffff0f101db in torch::autograd::Engine::set_device (this=0x7ffff7b16bc0 <torch::autograd::Engine::get_base_engine()::engine>, device=1) at ../torch/csrc/autograd/engine.cpp:264\r\n#4  0x00007ffff0f1034d in torch::autograd::Engine::thread_init (this=0x7ffff7b16bc0 <torch::autograd::Engine::get_base_engine()::engine>, device=1, ready_queue=std::shared_ptr (count 2, weak 0) 0x1b33aa0)\r\n    at ../torch/csrc/autograd/engine.cpp:293\r\n#5  0x00007ffff0f3613e in std::_Mem_fn_base<void (torch::autograd::Engine::*)(int, std::shared_ptr<torch::autograd::ReadyQueue> const&), true>::operator()<int, std::shared_ptr<torch::autograd::ReadyQueue>, void>(torch::autograd::Engine*, int&&, std::shared_ptr<torch::autograd::ReadyQueue>&&) const (this=0x1b340d8, __object=0x7ffff7b16bc0 <torch::autograd::Engine::get_base_engine()::engine>) at /usr/include/c++/5/functional:600\r\n#6  0x00007ffff0f360a1 in std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int, std::shared_ptr<torch::autograd::ReadyQueue> const&)> (torch::autograd::Engine*, int, std::shared_ptr<torch::autograd::ReadyQueue>)>::_M_invoke<0ul, 1ul, 2ul>(std::_Index_tuple<0ul, 1ul, 2ul>) (this=0x1b340b8) at /usr/include/c++/5/functional:1531\r\n#7  0x00007ffff0f35cb8 in std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int, std::shared_ptr<torch::autograd::ReadyQueue> const&)> (torch::autograd::Engine*, int, std::shared_ptr<torch::autograd::ReadyQueue>)>::operator()() (this=0x1b340b8) at /usr/include/c++/5/functional:1520\r\n#8  0x00007ffff0f35ac8 in std::thread::_Impl<std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int, std::shared_ptr<torch::autograd::ReadyQueue> const&)> (torch::autograd::Engine*, int, std::shared_ptr<torch::autograd::ReadyQueue>)> >::_M_run() (this=0x1b340a0) at /usr/include/c++/5/thread:115\r\n#9  0x00007fffccadec80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#10 0x00007fffcbafa6ba in start_thread (arg=0x7fff9cd8d700) at pthread_create.c:333\r\n#11 0x00007fffcc24441d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n```\r\nUpdate: I noticed that if we initialize a cuda tensor (e.g. `auto cuda_tensor = torch::randn({3, 4}, torch::kCUDA); std::cout << cuda_tensor << std::endl;`) before running the C++ custom autograd function, the whole thing would pass and there is no error.\r\n\r\n## Expected behavior\r\n\r\nIt should just work without throwing any error.\r\n\r\n## Environment\r\n\r\nLatest libtorch nightly\r\n\r\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @yf225"},{"labels":[null,"api",null],"text":"## üöÄ Feature\r\nHello, this may already be available and undocumented, but is there any way to access the full C++ frontend API on the mobile build? \r\n\r\nCurrently the mobile build only exposes the TorchScript portion of libtorch, so, e.g. I can't use `torch::nn` operations on mobile. \r\n\n\ncc @yf225"},{"labels":["api",null],"text":"I've tried to run mnist train example (https://github.com/pytorch/examples/tree/master/cpp/mnist)\r\nCode crushes on train_dataset creation whith message \"Unhandled exception at 0x00007FFE7B879179 in MNIST.exe: Microsoft C++ exception: c10::Error at memory location 0x00000068D2EFF4B0. occurred\".\r\n[auto train_dataset = torch::data::datasets::MNIST(kDataRoot)\r\n        .map(torch::data::transforms::Normalize<>(0.1307, 0.3081))\r\n        .map(torch::data::transforms::Stack<>());]\r\nEnvironment:\r\nWin10, VS 2019, Release, CPU.\r\n\r\nI'm not sure if mnist-data prpared correctly. Just put them to ./data catalog\r\n26.03.2020  12:10         1¬†648¬†877 t10k-images-idx3-ubyte.gz\r\n26.01.1998  18:07         7¬†840¬†016 t10k-images.idx3-ubyte\r\n26.03.2020  12:10             4¬†542 t10k-labels-idx1-ubyte.gz\r\n26.01.1998  18:07            10¬†008 t10k-labels.idx1-ubyte\r\n26.03.2020  12:09         9¬†912¬†422 train-images-idx3-ubyte.gz\r\n18.11.1996  18:36        47¬†040¬†016 train-images.idx3-ubyte\r\n26.03.2020  12:09            28¬†881 train-labels-idx1-ubyte.gz\r\n18.11.1996  18:36            60¬†008 train-labels.idx1-ubyte\r\n\r\nWhat is wrong?\n\ncc @yf225"},{"labels":["api",null,null],"text":"## üìö Documentation\r\n\r\ntorch::cat is missing documentation? Following this link, where I searched for the documentation only returned other functions where torch::cat are begin used in the body?\r\n\r\nhttps://pytorch.org/cppdocs/search.html?q=torch%3A%3Acat&check_keywords=yes&area=default\r\n"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\nSince this commit : https://github.com/pytorch/pytorch/commit/76035f050b215d0606fe786901dcd07b5c9544fe#diff-b8c53e7a2010d3dae3200c9911950551\r\nIt's no longer possible to directly access the options variable of Adam in libtorch, making it impossible to change the learning rate on the fly with libtorch.\r\nHow can I change the learning rate ?\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Install the latest nightly of libtorch\r\n2. Create an Adam optimizer\r\n3. Try to access its .options variable to change the learning rate : options does not exist anymore\r\n\n\ncc @yf225"},{"labels":["api",null],"text":"I use ConvTranspose2d in a Sequential, and forward called with one parameter, it invoke above issue. I think it is a bug of libtorch,  ConvTranspose2d need another forward overload function with one parameter.\r\n\n\ncc @yf225"},{"labels":["api",null],"text":"`torch::normal` only supports (double, double) as input, but `at::normal` supports (double, double) / (double, Tensor) / (Tensor, double) / (Tensor, Tensor) as input.\r\n\r\nPython `torch.normal` also supports all four types of inputs (https://pytorch.org/docs/stable/torch.html#torch.normal), and we should do the same for `torch::normal`.\r\n\r\nSome notes:\r\n- `torch::normal` is currently defined in`torch/csrc/autograd/generated/variable_factories.h`\r\n- Per conversation with Ed, we should put the tracing logic in VariableType and just dispatch `torch::normal` to VariableType\r\n\r\n(This issue was originally reported by @ailzhang)\r\n\r\ncc @yf225 \r\n\r\ncc @ailzhang @ezyang "},{"labels":[null,"api",null],"text":"## To Reproduce\r\n\r\nIn python, accessing the .grad attribute of a non-leaf tensor is a common pitfall users run into. For example:\r\n```\r\nx = torch.randn(3, requires_grad=True).cuda()\r\ny = x ** 2\r\ny.sum().backward()\r\nx.grad\r\n```\r\nLuckily, this raises a nice warning in Python:\r\n```\r\n/scratch/rzou/pt/cudnn_double_Bwd/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\r\n  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\r\n```\r\n\r\nIn the C++ API, no warning happens: https://discuss.pytorch.org/t/very-confused-with-changes-in-autograd/74355 . It would be great for usability to add that warning.\r\n\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @yf225"},{"labels":["api",null,null,null],"text":"## üìö Documentation\r\nWhen I use function `torch::randint()` it takes arguments `int64_t high, at::IntArrayRef size, at::TensorOptions &options = {}` however, at the documentation it says there should also be a `at::Generator generator`\r\n\r\nhttps://pytorch.org/cppdocs/api/function_namespacetorch_1ad86f0f5e4b92098660b54e584342a188.html?highlight=randint\r\n\r\nI am using libtorch 1.4\n\ncc @yf225"},{"labels":["api",null,null],"text":"When compiling custom-dataset example receive errors:\r\n1) no instance of constructor \"torch::nn::Functional::Functional\" matches the argument list\tCustomDataSet\t...\\CustomDataSet\\CTorch.cpp\t125\t\r\n2) cannot determine which instance of overloaded function \"torch::log_softmax\" is intended\tCustomDataSet\t...\\CustomDataSet\\CTorch.cpp\t125\t\r\n line 125 is: push_back(Functional(torch::log_softmax, 1, torch::nullopt));\r\n\r\nCould anybody help with this?\n\ncc @yf225 @peterjc123"},{"labels":["api",null,null],"text":"I tried to do it in #34968 but that PR added c-tor Generator(Device device) to C++ API to match Python API, to avoid code duplication the logic was moved from THPGenerator_pynew to corresponding at::Generator c-tor. Unfortunately it made Generator.h dependent on CPUGenerator.h and CUDAGenerator.h, but this is how our Python API works :(\n\ncc @yf225"},{"labels":["api",null,null],"text":"## üêõ Bug\r\nresult in memory leak when use libtorch. but i think that it is very strange. because i only define `torch::DeiviceType`  , then got **bug1**\r\n### bug1\r\n```\r\nstill reachable: 884,654 bytes in 13,927 blocks of which reachable via heuristic: stdstring :436062 bytes in 5832 blocks\r\n```\r\n\r\n### bug2\r\n```\r\npossibly lost: 3104 bytes in 22 blocks\r\nstill reachable: 1231592 bytes in 14005 blocks of which reachable via heuristic: stdstring :436062 bytes in 5832 blocks\r\n```\r\n## To Reproduce\r\n### bug1\r\n```c++\r\nint main()\r\n{\r\n torch::DeviceType device_type;\r\n}\r\n```\r\n### bug2\r\n```c++\r\nint main()\r\n{\r\n torch::DeviceType device_type;\r\nif(torch::cuda::is_available())\r\n{\r\ndevice_type = torch::kCUDA;\r\n}\r\nelse\r\n{\r\ndevice_type = torch::kCPU;\r\n}\r\ntorch::Device device(device_type)\r\n}\r\n```\r\n\r\n\r\n## Environment\r\n\r\n - PyTorch Version : 1.4\r\n- libtorch Version: 1.4\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.5\r\n - CUDA/cuDNN version: 10\r\n\r\n\r\n## update\r\n- the memory leak caused by the `valgrind` tool, not bug for libtorch.\r\n- `cuda-memcheck` is need for detect memory leak instead of `valgrind`\r\n\r\nBut `cuda-memcheck` report this error below:\r\n```shell\r\nProgram hit cudaErrorCudartUnloading (error 4) due to \"driver shutting down\" on CUDA API call to cudaEventDestroy.\r\n=========     Saved host backtrace up to driver entry point at error\r\n=========     Host Frame:/usr/local/nvidia/lib64/libcuda.so.1 [0x377f93]\r\n=========     Host Frame:/xxx/xxx/software/libtorch/lib/libcudart-1b201d85.so.10.1 (cudaEventDestroy + 0x18e) [0x49c8e]\r\n=========     Host Frame:/xxx/xxx/software/libtorch/lib/libtorch.so (_ZN19cublasFixedSizePool8tearDownEv + 0xe4) [0x10a4ac34]\r\n=========     Host Frame:/xxx/xxx/software/libtorch/lib/libtorch.so (_ZN36cublasFixedSizePoolWithGraphSuppport8tearDownEv + 0x16) [0x1083f666]\r\n=========     Host Frame:/xxx/xxx/software/libtorch/lib/libtorch.so (cublasDestroy_v2 + 0xd0) [0xebccfd0]\r\n=========     Host Frame:/xxx/xxx/software/libtorch/lib/libtorch.so (cudnnDestroy + 0x219) [0xf473e99]\r\n=========     Host Frame:/xxx/xxx/software/libtorch/lib/libtorch.so [0x46cc655]\r\n=========     Host Frame:/lib/x86_64-linux-gnu/libc.so.6 (__cxa_finalize + 0x9a) [0x3a36a]\r\n=========     Host Frame:/xxx/xxx/software/libtorch/lib/libtorch.so [0x18ab893]\r\n```\r\n\r\n\r\ncc @yf225"},{"labels":[null,"api",null,null],"text":"## üöÄ Feature\r\nC++ API should support Famous Open Datasets CIFAR10 and CIFAR100.\r\n**Dataset Reference**: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)\r\n\r\n## Motivation\r\nWhile creating PyTorch C++ tutorials [https://github.com/prabhuomkar/pytorch-cpp](https://github.com/prabhuomkar/pytorch-cpp), I came to know that PyTorch has support for these datasets via [vision](https://github.com/pytorch/vision/tree/master/torchvision/datasets). As of now, [C++ API for Datasets](https://github.com/pytorch/pytorch/tree/master/torch/csrc/api/src/data/datasets) only has MNIST.\r\nSupport for standard datasets like CIFAR, COCO, ImageNet, etc out of the box will help developers play with C++ API easily.\r\n\r\n## Pitch\r\n- Add [cifar.h](https://github.com/pytorch/pytorch/tree/master/torch/csrc/api/include/torch/data/datasets) header with _`TORCH_API`_ `CIFAR` class.\r\n- Add [cifar.cpp](https://github.com/pytorch/pytorch/tree/master/torch/csrc/api/src/data/datasets) with both `CIFAR10` and `CIFAR100` support similar to vision for reading images, targets and getting train/test data. \r\n\r\n## Alternatives\r\nN/A\r\n\r\n## Additional context\r\nReference C++ Implementation: [cifar10.h](https://github.com/prabhuomkar/pytorch-cpp/blob/master/tutorials/intermediate/deep_residual_network/include/cifar10.h) and [cifar10.cpp](https://github.com/prabhuomkar/pytorch-cpp/blob/master/tutorials/intermediate/deep_residual_network/src/cifar10.cpp)\r\nReference Python Implementation: [cifar.py](https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py)\r\n\n\ncc @yf225 @fmassa"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\nI intend to use libtorch in a plugin project for the VFX compositing software Nuke (https://www.foundry.com/products/nuke). When I include torch using the standard line:\r\n#include <torch/torch.h>\r\nI'm getting a class name ambiguity with the class \"Node\" in Nuke's API. This shouldn't really happen since everything is properly namespaces. I've traced the problem down to these sources in torch: \r\ntorch/csrc/autograd/VariableTypeUtils.h\r\ntorch/csrc/api/include/torch/nn/modules/_functions.h\r\n\r\nThis makes torch::autograd::Node to appear in the root namespace and clashes with a Node from Nuke that is also declared in the root namespace.\r\n\r\nSince Nuke is a commercial product it's hard to make any changes there, so I hope that somebody can correct these sources so they internally just specify the whole namespace to the functions internally so that the \"using namespace\" lines can be removed.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behaviour:\r\n\r\n1. Start a clean C++ project and write this code snippet:\r\n```\r\n#include <torch/torch.h>\r\nstruct Node{};\r\nvoid bogus(Node *node){}\r\n```\r\n2. struct Node{}; in this case is coming from a third party library that you haven't got control over.\r\n3. Compile, and you will get an ambiguity error\r\n\r\nAlternatively, if you got Nuke available on your system:\r\n\r\n1. Copy one of the example plugins from Nuke\r\n2. Include torch using the standard line \"#include <torch/torch.h>\"\r\n3. Compile, and you will get the following error output:\r\n\r\nScanning dependencies of target ilp_MLTest\r\n[ 72%] Built target py\r\n[ 81%] Built target extension_files\r\n[ 90%] Building CXX object plugins/ilp_MLTest/CMakeFiles/ilp_MLTest.dir/ilp_MLTest.cpp.o\r\n/users/dawa/dev/nuke_ml/plugins/ilp_MLTest/ilp_MLTest.cpp:35:17: error: expected ‚Äò)‚Äô before ‚Äò*‚Äô token\r\n  ilp_MLTest(Node* node) : DD::Image::Iop(node)\r\n                 ^\r\n/users/dawa/dev/nuke_ml/plugins/ilp_MLTest/ilp_MLTest.cpp:63:26: error: reference to ‚ÄòNode‚Äô is ambiguous\r\n static Iop* ilp_MLTest_c(Node* node) { return new ilp_MLTest(node); }\r\n                          ^~~~\r\nIn file included from /ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Op.h:13:0,\r\n                 from /users/dawa/dev/nuke_ml/plugins/ilp_MLTest/ilp_MLTest.cpp:8:\r\n/ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Description.h:15:7: note: candidates are: class Node\r\n class Node;\r\n       ^~~~\r\nIn file included from /ice/rez/packages/native/libtorch/1.4.0/platform-linux/include/torch/csrc/autograd/custom_function.h:3:0,\r\n                 from /ice/rez/packages/native/libtorch/1.4.0/platform-linux/include/torch/csrc/api/include/torch/nn/modules/_functions.h:3,\r\n                 from /ice/rez/packages/native/libtorch/1.4.0/platform-linux/include/torch/csrc/api/include/torch/nn/modules/normalization.h:4,\r\n                 from /ice/rez/packages/native/libtorch/1.4.0/platform-linux/include/torch/csrc/api/include/torch/nn/modules.h:26,\r\n                 from /ice/rez/packages/native/libtorch/1.4.0/platform-linux/include/torch/csrc/api/include/torch/nn.h:7,\r\n                 from /ice/rez/packages/native/libtorch/1.4.0/platform-linux/include/torch/csrc/api/include/torch/all.h:7,\r\n                 from /ice/rez/packages/native/libtorch/1.4.0/platform-linux/include/torch/csrc/api/include/torch/torch.h:3,\r\n                 from /users/dawa/dev/nuke_ml/plugins/ilp_MLTest/ilp_MLTest.cpp:19:\r\n/ice/rez/packages/native/libtorch/1.4.0/platform-linux/include/torch/csrc/autograd/function.h:88:18: note:                 struct torch::autograd::Node\r\n struct TORCH_API Node : std::enable_shared_from_this<Node> {\r\n                  ^~~~\r\n/users/dawa/dev/nuke_ml/plugins/ilp_MLTest/ilp_MLTest.cpp:63:32: error: ‚Äònode‚Äô was not declared in this scope\r\n static Iop* ilp_MLTest_c(Node* node) { return new ilp_MLTest(node); }\r\n                                ^~~~\r\n/users/dawa/dev/nuke_ml/plugins/ilp_MLTest/ilp_MLTest.cpp:63:32: note: suggested alternative: ‚ÄòNode‚Äô\r\n static Iop* ilp_MLTest_c(Node* node) { return new ilp_MLTest(node); }\r\n                                ^~~~\r\n                                Node\r\n/users/dawa/dev/nuke_ml/plugins/ilp_MLTest/ilp_MLTest.cpp:68:78: error: no matching function for call to ‚ÄòDD::Image::Op::Description::Description(const char* const&, const char [18], DD::Image::Iop*&)‚Äô\r\n const Iop::Description ilp_MLTest::d(CLASS, \"Filter/ilp_MLTest\", ilp_MLTest_c);\r\n                                                                              ^\r\nIn file included from /users/dawa/dev/nuke_ml/plugins/ilp_MLTest/ilp_MLTest.cpp:8:0:\r\n/ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Op.h:1952:9: note: candidate: DD::Image::Op::Description::Description(const char*, const char*, DD::Image::Op::Description::IopConstructor)\r\n         Description(const char* n, const char* /*menu*/, IopConstructor c) :\r\n         ^~~~~~~~~~~\r\n/ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Op.h:1952:9: note:   no known conversion for argument 3 from ‚ÄòDD::Image::Iop*‚Äô to ‚ÄòDD::Image::Op::Description::IopConstructor {aka DD::Image::Iop* (*)(Node*)}‚Äô\r\n/ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Op.h:1942:9: note: candidate: DD::Image::Op::Description::Description(const char*, const char*, DD::Image::Op::Description::OpConstructor)\r\n         Description(const char* n, const char* /*menu*/, OpConstructor c) :\r\n         ^~~~~~~~~~~\r\n/ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Op.h:1942:9: note:   no known conversion for argument 3 from ‚ÄòDD::Image::Iop*‚Äô to ‚ÄòDD::Image::Op::Description::OpConstructor {aka DD::Image::Op* (*)(Node*)}‚Äô\r\n/ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Op.h:1932:9: note: candidate: DD::Image::Op::Description::Description(const char*, DD::Image::Op::Description::OpConstructor, DD::Image::Description::NodeBuilder)\r\n         Description(const char* n, OpConstructor c, NodeBuilder nodeBuilder) :\r\n         ^~~~~~~~~~~\r\n/ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Op.h:1932:9: note:   no known conversion for argument 2 from ‚Äòconst char [18]‚Äô to ‚ÄòDD::Image::Op::Description::OpConstructor {aka DD::Image::Op* (*)(Node*)}‚Äô\r\n/ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Op.h:1925:9: note: candidate: DD::Image::Op::Description::Description(const char*, DD::Image::Op::Description::OpConstructor, DD::Image::License*)\r\n         Description(const char* n, OpConstructor c, License * l = 0) :\r\n         ^~~~~~~~~~~\r\n/ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Op.h:1925:9: note:   no known conversion for argument 2 from ‚Äòconst char [18]‚Äô to ‚ÄòDD::Image::Op::Description::OpConstructor {aka DD::Image::Op* (*)(Node*)}‚Äô\r\n/ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Op.h:1897:25: note: candidate: constexpr DD::Image::Op::Description::Description(const DD::Image::Op::Description&)\r\n       class DDImage_API Description : public DD::Image::Description\r\n                         ^~~~~~~~~~~\r\n/ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Op.h:1897:25: note:   candidate expects 1 argument, 3 provided\r\n/ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Op.h:1897:25: note: candidate: constexpr DD::Image::Op::Description::Description(DD::Image::Op::Description&&)\r\n/ice/rez/packages/native/nuke/11.3.6/platform-linux/include/DDImage/Op.h:1897:25: note:   candidate expects 1 argument, 3 provided\r\nmake[2]: *** [plugins/ilp_MLTest/CMakeFiles/ilp_MLTest.dir/build.make:63: plugins/ilp_MLTest/CMakeFiles/ilp_MLTest.dir/ilp_MLTest.cpp.o] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:170: plugins/ilp_MLTest/CMakeFiles/ilp_MLTest.dir/all] Error 2\r\nmake: *** [Makefile:130: all] Error 2\r\n11:59:52 ERROR    BuildError: The cmake build system failed.\r\n\r\n\r\n## Expected behavior\r\n\r\nThat the plugin compiles just as normal, with no build errors from including the torch headers.\r\n\r\n## Environment\r\n\r\nPyTorch 1.4.0\r\nOS: CentOS Linux release 7.6.1810 (Core) \r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCMake version: version 2.8.12.2\r\n\n\ncc @yf225"},{"labels":["api"],"text":"## üêõ Bug\r\n\r\nI want to make my `current_state`, which is made of `std::vector<double>` to a `torch::Tensor`.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1: Have a dobule vector, i.e `0.0642382 0.0395936 0 0.219108 0.372894 0.422909 0.554452 0.302765 1`\r\n2: `torch::Tensor current_state = torch::from_blob(current_state.data(), { 9}).to(*device);`\r\n3: std::cout << \"next state is: \\n\" << next_state << std::endl;\r\n\r\nobserve the output: \r\n```\r\nnext state is: \r\n-1.7024e+35\r\n1.3785e+00\r\n-6.2066e+14\r\n1.2834e+00\r\n0.0000e+00\r\n0.0000e+00\r\n-1.0163e+17\r\n1.5941e+00\r\nnan\r\n```\r\n\r\n## Expected behavior\r\n\r\nTo get a tensor with the same values as my vector...\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): Libtorch 1.4\r\n - OS (e.g., Linux): Windows 7 64 bit\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source): `cmake --build . --config Release --target INSTALL`\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: NA\r\n - GPU models and configuration: NA\r\n - Any other relevant information: NA\r\n\r\n## Additional context\r\nThe following line gives me a tensor that looks like I want, however it crashes when I try to feed it through the network...\r\n\r\n`torch::Tensor test_next_state = torch::tensor(current_state);`\r\n\n\ncc @yf225"},{"labels":["api",null,null],"text":"## üêõ Bug\r\n`torch::nn::Linear` without bias throws an error when `to` is called. \r\n\r\n## To Reproduce\r\n```c++\r\n#include<torch/torch.h>\r\nint main(int argc, char* argv[])\r\n{\r\n  torch::Tensor x = torch::ones({100, 4});\r\n  auto lopt       = torch::nn::LinearOptions(4, 4).bias(false);\r\n  auto lin        = torch::nn::Linear(lopt);\r\n  // works\r\n  auto y          = lin->forward(x);\r\n  // error\r\n  lin->to(torch::kFloat64);\r\n  x = torch::ones({100, 4}, torch::kFloat64);\r\n  y = lin->forward(x);\r\n}\r\n```\r\nThis is the output, but this MWE is only executed as a test in a larger project\r\nso the output might differ.\r\n```\r\n terminate called after throwing an instance of 'c10::Error'\r\n  what():  tensor does not have a device (device at /home/nls/gasnew/pytorch/torch/include/c10/core/TensorImpl.h:461)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6a (0x7ffff220035a in /home/nls/gasnew/pytorch/torch/lib/libc10.so)\r\nframe #1: c10::TensorImpl::device() const + 0xf8 (0x7ffff7f0e288 in /home/nls/gasnew/gasPyTorch/build/gt/src/libgt.so)\r\nframe #2: at::Tensor::device() const + 0x20 (0x7ffff7f117d4 in /home/nls/gasnew/gasPyTorch/build/gt/src/libgt.so)\r\nframe #3: at::Tensor::options() const + 0x71 (0x7ffff7f5fb41 in /home/nls/gasnew/gasPyTorch/build/gt/src/libgt.so)\r\nframe #4: at::native::to(at::Tensor const&, c10::ScalarType, bool, bool, c10::optional<c10::MemoryFormat>) + 0x6c (0x7ffff368fabc in /home/nls/gasnew/pytorch/torch/lib/libtorch.so)\r\nframe #5: <unknown function> + 0x1714a6b (0x7ffff392aa6b in /home/nls/gasnew/pytorch/torch/lib/libtorch.so)\r\nframe #6: <unknown function> + 0x32572d0 (0x7ffff546d2d0 in /home/nls/gasnew/pytorch/torch/lib/libtorch.so)\r\nframe #7: <unknown function> + 0x173c003 (0x7ffff3952003 in /home/nls/gasnew/pytorch/torch/lib/libtorch.so)\r\nframe #8: at::Tensor c10::KernelFunction::callUnboxedOnly<at::Tensor, at::Tensor const&, c10::ScalarType, bool, bool, c10::optional<c10::MemoryFormat> >(at::Tensor const&, c10::ScalarType, bool, bool, c10::optional<c10::MemoryFormat>) const + 0x141 (0x55555559bfd3 in /home/nls/gasnew/gasPyTorch/build/test/testgt)\r\nframe #9: at::Tensor c10::Dispatcher::callUnboxedOnly<at::Tensor, at::Tensor const&, c10::ScalarType, bool, bool, c10::optional<c10::MemoryFormat> >(c10::OperatorHandle const&, at::Tensor const&, c10::ScalarType, bool, bool, c10::optional<c10::MemoryFormat>) const + 0x16f (0x5555555968c9 in /home/nls/gasnew/gasPyTorch/build/test/testgt)\r\nframe #10: at::Tensor::to(c10::ScalarType, bool, bool, c10::optional<c10::MemoryFormat>) const + 0x1de (0x55555558f6dc in /home/nls/gasnew/gasPyTorch/build/test/testgt)\r\nframe #11: void torch::nn::Module::to_impl<c10::ScalarType&, bool&>(c10::ScalarType&, bool&) + 0x149 (0x7ffff59d5f19 in /home/nls/gasnew/pytorch/torch/lib/libtorch.so)\r\nframe #12: torch::nn::Module::to(c10::ScalarType, bool) + 0x1c (0x7ffff59d2bac in /home/nls/gasnew/pytorch/torch/lib/libtorch.so)\r\n```\r\n## Environment\r\n\r\n - PyTorch Version: 1.4 from source\r\n - OS: Linux\n\ncc @yf225"},{"labels":["api",null,null,null],"text":"https://pytorch.org/cppdocs/api/function_namespacetorch_1a99dc9f736064b2179cc58e6436f7a021.html#exhale-function-namespacetorch-1a99dc9f736064b2179cc58e6436f7a021\r\n\r\nhttps://pytorch.org/cppdocs/api/function_namespacetorch_1a4b369494adfb10b9a005aeb0bb6207cb.html#exhale-function-namespacetorch-1a4b369494adfb10b9a005aeb0bb6207cb\r\n\r\nsee issue: https://github.com/pytorch/pytorch/issues/33862#issuecomment-593518890\n\ncc @yf225 @vincentqb"},{"labels":[null,null,null,"api",null],"text":"## üêõ Bug\r\n\r\nThe library size of torch is very large (~267MB without CUDA, 1.2 GB with CUDA).\r\nThis can be caused by using a library like Intel IPP similar to this opencv bug.\r\nhttps://github.com/opencv/opencv/issues/15177\r\nThis limits the use of pytorch models in small environments (without CUDA).\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n1. Download the libtorch C++ library\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nLibrary size without CUDA should be a lot less than that. or at least providing libtorch_tiny or ways to create it.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n - PyTorch Version: 1.4\r\n - OS: Linux\r\n\n\ncc @ezyang @gchanan @zou3519 @seemethere @yf225"},{"labels":["api",null,null],"text":"WARNING: ThreadSanitizer: data race (pid=17) Read of size 8 at 0x7fd6718c2ea0 by thread T2:\r\n\r\n```#0 std::__1::shared_ptr<at::CPUGenerator>::get() const external/clang/include/c++/v1/memory:3800:49 (libexternal_Spytorch_Slibaten.so+0x37dba2)\r\n#1 at::detail::getDefaultCPUGenerator() external/pytorch/aten/src/ATen/CPUGenerator.cpp:27:26 (libexternal_Spytorch_Slibaten.so+0x37dba2)\r\n#2 THFloatTensor_uniform external/pytorch/aten/src/TH/generic/THTensorRandom.cpp:81:73 (libexternal_Spytorch_Slibaten.so+0x1801ace)\r\n#3 at::native::legacy::cpu::_th_uniform_(at::Tensor&, double, double, at::Generator*) bazel-out/k8-dbg-tsan/bin/external/pytorch/ATen/LegacyTHFunctionsCPU.cpp:9339:13 (libexternal_Spytorch_Slibaten.so+0x1a165cf)\r\n#4 at::CPUType::(anonymous namespace)::uniform_(at::Tensor&, double, double, at::Generator*) bazel-out/k8-dbg-tsan/bin/external/pytorch/ATen/CPUType.cpp:2079:12 (libexternal_Spytorch_Slibaten.so+0x192487f)\r\n#5 c10::detail::WrapRuntimeKernelFunctor_<at::Tensor& (*)(at::Tensor&, double, double, at::Generator*), at::Tensor&, c10::guts::typelist::typelist<at::Tensor&, double, double, at::Generator*> >::operator()(at::Tensor&, double, double, at::Generator*) external/pytorch/aten/src/ATen/core/boxing/kernel_lambda.h:23:14 (libexternal_Spytorch_Slibpytorch.so+0x1321734)\r\n#6 c10::detail::wrap_kernel_functor_unboxed_<c10::detail::WrapRuntimeKernelFunctor_<at::Tensor& (*)(at::Tensor&, double, double, at::Generator*), at::Tensor&, c10::guts::typelist::typelist<at::Tensor&, double, double, at::Generator*> >, at::Tensor& (at::Tensor&, double, double, at::Generator*)>::call(c10::OperatorKernel*, at::Tensor&, double, double, at::Generator*) external/pytorch/aten/src/ATen/core/boxing/kernel_functor.h:262:14 (libexternal_Spytorch_Slibpytorch.so+0x1321734)\r\n#7 at::Tensor& c10::KernelFunction::callUnboxed<at::Tensor&, at::Tensor&, double, double, at::Generator*>(c10::OperatorHandle const&, at::Tensor&, double, double, at::Generator*) const external/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:62:16 (libexternal_Spytorch_Slibpytorch.so+0x8d4c21)\r\n#8 at::Tensor& c10::Dispatcher::callUnboxed<at::Tensor&, at::Tensor&, double, double, at::Generator*>(c10::OperatorHandle const&, at::Tensor&, double, double, at::Generator*) const external/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:179:26 (libexternal_Spytorch_Slibpytorch.so+0x954957)\r\n#9 at::Tensor& c10::OperatorHandle::callUnboxed<at::Tensor&, at::Tensor&, double, double, at::Generator*>(at::Tensor&, double, double, at::Generator*) const external/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:154:41 (libexternal_Spytorch_Slibpytorch.so+0x954957)\r\n#10 at::Tensor::uniform_(double, double, at::Generator*) const bazel-out/k8-dbg-tsan/bin/external/pytorch/ATen/core/TensorMethods.h:4653:15 (libexternal_Spytorch_Slibpytorch.so+0x954957)\r\n#11 at::native::rand(c10::ArrayRef<long>, at::Generator*, c10::TensorOptions const&) external/pytorch/aten/src/ATen/native/TensorFactories.cpp:441:17 (libexternal_Spytorch_Slibaten.so+0x9155fb)\r\n#12 at::native::rand(c10::ArrayRef<long>, c10::TensorOptions const&) external/pytorch/aten/src/ATen/native/TensorFactories.cpp:436:10 (libexternal_Spytorch_Slibaten.so+0x915572)\r\n#13 at::TypeDefault::rand(c10::ArrayRef<long>, c10::TensorOptions const&) bazel-out/k8-dbg-tsan/bin/external/pytorch/ATen/TypeDefault.cpp:3284:12 (libexternal_Spytorch_Slibaten.so+0x1a80570)\r\n#14 c10::detail::WrapRuntimeKernelFunctor_<at::Tensor (*)(c10::ArrayRef<long>, c10::TensorOptions const&), at::Tensor, c10::guts::typelist::typelist<c10::ArrayRef<long>, c10::TensorOptions const&> >::operator()(c10::ArrayRef<long>, c10::TensorOptions const&) external/pytorch/aten/src/ATen/core/boxing/kernel_lambda.h:23:14 (libexternal_Spytorch_Slibpytorch.so+0x132b8bd)\r\n#15 c10::detail::wrap_kernel_functor_unboxed_<c10::detail::WrapRuntimeKernelFunctor_<at::Tensor (*)(c10::ArrayRef<long>, c10::TensorOptions const&), at::Tensor, c10::guts::typelist::typelist<c10::ArrayRef<long>, c10::TensorOptions const&> >, at::Tensor (c10::ArrayRef<long>, c10::TensorOptions const&)>::call(c10::OperatorKernel*, c10::ArrayRef<long>, c10::TensorOptions const&) external/pytorch/aten/src/ATen/core/boxing/kernel_functor.h:262:14 (libexternal_Spytorch_Slibpytorch.so+0x132b8bd)\r\n#16 at::Tensor c10::KernelFunction::callUnboxed<at::Tensor, c10::ArrayRef<long>, c10::TensorOptions const&>(c10::OperatorHandle const&, c10::ArrayRef<long>, c10::TensorOptions const&) const external/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:62:16 (test_pytorch+0xd1fcb)\r\n#17 at::Tensor c10::Dispatcher::callUnboxed<at::Tensor, c10::ArrayRef<long>, c10::TensorOptions const&>(c10::OperatorHandle const&, c10::ArrayRef<long>, c10::TensorOptions const&) const external/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:179:26 (test_pytorch+0xcf638)\r\n#18 at::Tensor c10::OperatorHandle::callUnboxed<at::Tensor, c10::ArrayRef<long>, c10::TensorOptions const&>(c10::ArrayRef<long>, c10::TensorOptions const&) const external/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:154:41 (test_pytorch+0xcf638)\r\n#19 at::rand(c10::ArrayRef<long>, c10::TensorOptions const&) bazel-out/k8-dbg-tsan/bin/external/pytorch/ATen/Functions.h:6638:15 (libros_Ssrc_Sa_Ulibtorch_Uuser_Sliba_Ulibtorch_Uuser.so+0x16794)\r\n#20 torch::rand(c10::ArrayRef<long>, c10::TensorOptions const&)::'lambda'()::operator()() const bazel-out/k8-dbg-tsan/bin/external/pytorch/torch/csrc/autograd/generated/variable_factories.h:534:12 (libros_Ssrc_Sa_Ulibtorch_Uuser_Sliba_Ulibtorch_Uuser.so+0x16794)\r\n#21 torch::rand(c10::ArrayRef<long>, c10::TensorOptions const&) bazel-out/k8-dbg-tsan/bin/external/pytorch/torch/csrc/autograd/generated/variable_factories.h:532:23 (libros_Ssrc_Sa_Ulibtorch_Uuser_Sliba_Ulibtorch_Uuser.so+0x12194)\r\n#22 a_libtorch_user::ALibtorchUser::call_inference(int) ros/src/a_libtorch_user/src/ALibtorchUser.cpp:32:26 (libros_Ssrc_Sa_Ulibtorch_Uuser_Sliba_Ulibtorch_Uuser.so+0x10bf7)\r\n#23 decltype(*(std::__1::forward<a_libtorch_user::ALibtorchUser*>(fp0)).*fp(std::__1::forward<int>(fp1))) std::__1::__invoke<void (a_libtorch_user::ALibtorchUser::*)(int), a_libtorch_user::ALibtorchUser*, int, void>(void (a_libtorch_user::ALibtorchUser::*&&)(int), a_libtorch_user::ALibtorchUser*&&, int&&) external/clang/include/c++/v1/type_traits:3471:1 (libros_Ssrc_Sa_Ulibtorch_Uuser_Sliba_Ulibtorch_Uuser.so+0x267ed)\r\n#24 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (a_libtorch_user::ALibtorchUser::*)(int), a_libtorch_user::ALibtorchUser*, int, 2ul, 3ul>(std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (a_libtorch_user::ALibtorchUser::*)(int), a_libtorch_user::ALibtorchUser*, int>&, std::__1::__tuple_indices<2ul, 3ul>) external/clang/include/c++/v1/thread:277:5 (libros_Ssrc_Sa_Ulibtorch_Uuser_Sliba_Ulibtorch_Uuser.so+0x267ed)\r\n#25 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (a_libtorch_user::ALibtorchUser::*)(int), a_libtorch_user::ALibtorchUser*, int> >(void*) external/clang/include/c++/v1/thread:287:5 (libros_Ssrc_Sa_Ulibtorch_Uuser_Sliba_Ulibtorch_Uuser.so+0x267ed)```\r\n\r\ntest.cpp\r\n--------\r\nvoid call_inference() {\r\n    int batch_size = 100;\r\n    torch::Tensor data = torch::rand({batch_size, 100, 30}).to(device);\r\n    torch::Tensor h0 = torch::rand({1, batch_size, 256}).to(device);\r\n    torch::Tensor c0 = torch::rand({1, batch_size, 256}).to(device);\r\n    module.run_method(\"forward\", data, h0, c0);\r\n}\r\n...\r\nfor (int32_t i = 0; i < n; i++) {\r\n    std::thread t(&all_inference, i);\r\n}\r\n```\n\ncc @yf225"},{"labels":["api",null,null],"text":"## üêõ Can't save to ostream\r\n\r\nI want to save my model as a `istream` for later processing.\r\nI found the following on the api:\r\n\r\nhttps://pytorch.org/cppdocs/api/function_namespacetorch_1a99dc9f736064b2179cc58e6436f7a021.html#exhale-function-namespacetorch-1a99dc9f736064b2179cc58e6436f7a021\r\n\r\nhttps://pytorch.org/cppdocs/api/function_namespacetorch_1a4b369494adfb10b9a005aeb0bb6207cb.html#exhale-function-namespacetorch-1a4b369494adfb10b9a005aeb0bb6207cb\r\n\r\n## To Reproduce\r\n`torch::optim::SGD sgd(0.9);` give me error: \r\n\r\n>Error (active)\tE0289\tno instance of constructor \"torch::optim::SGD::SGD\" matches the argument list\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): LibTorch 1.4\r\n - OS (e.g., Linux): Windows 7 64 bit\r\n - How you installed PyTorch (`conda`, `pip`, source): source: https://download.pytorch.org/libtorch/cpu/libtorch-win-shared-with-deps-1.4.0.zip\r\n - Build command you used (if compiling from source): cmake --build . --config Release --target INSTALL\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: NA\r\n - GPU models and configuration: NA\r\n - Any other relevant information: NA\r\n\r\n\n\ncc @yf225"},{"labels":["api",null,null],"text":"## üêõ Bug\r\n\r\nModels saved in C++ LibTorch with torch::save, cannot be loaded in python using torch.load. When I save a custom model (a class which inherits from torch::nn::Module) using torch::save(model, filepath), the result is a zip archive (.pt). The archive has the same structure as [it should](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/serialization.md) but python comes up with the error \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Save a class which inherits from the torch::nn::Module class in C++ using torch::save\r\n2. Try to load that saved archive in python using torch.load\r\n3. Find this error below: \r\n\r\n`Traceback (most recent call last):\r\n  File \"torch_plotting.py\", line 59, in <module>\r\n    policy_model, value_model = load_models(containing_path, model_number)\r\n  File \"torch_plotting.py\", line 37, in load_models\r\n    policy_model = torch.load(policy_file)\r\n  File \"/home/jamie/anaconda3/envs/tensorflow/lib/python3.7/site-packages/torch/serialization.py\", line 528, in load\r\n    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\r\n  File \"/home/jamie/anaconda3/envs/tensorflow/lib/python3.7/site-packages/torch/serialization.py\", line 782, in _load\r\n    result = unpickler.load()\r\nModuleNotFoundError: No module named '__torch__'`\r\n\r\n## Expected behavior\r\n\r\nExpect to have a fully loaded model that behaves the same way as the c++ one which I can use to perform inference.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 19.10\r\nGCC version: (Ubuntu 9.2.1-9ubuntu2) 9.2.1 20191008\r\nCMake version: version 3.16.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1050 Ti\r\nNvidia driver version: 440.33.01\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] numpydoc==0.9.1\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.5.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] torch                     1.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.5.0                    pypi_0    pypi\r\n\r\n## Additional context\r\n\r\nI am using the libtorch provided by [this link](https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.4.0%2Bcpu.zip) and using cmake to build my project. The file to open is \r\n[policy_9999.zip](https://github.com/pytorch/pytorch/files/4255490/policy_9999.zip) (renamed to policy_9999.zip instead of the original policy_9999.pt so it could be uploaded.)\r\n\r\n\r\n\n\ncc @yf225 @glaringlee @SsnL"},{"labels":["api",null,null],"text":"## üêõ Bug\r\n\r\nWith PR #33027, the fix to allow Sequential to call forward with default args skipped,\r\nthe options element in RNNImpl is protected (was public before)\r\n\r\n```\r\n protected:\r\n  FORWARD_HAS_DEFAULT_ARGS({1, AnyValue(Tensor())})\r\n\r\n  RNNOptions options;\r\n```\n\ncc @yf225"},{"labels":["api",null],"text":"Instead of\r\n\r\n```\r\nclass TensorOptions {\r\n  TensorOptions device(optional<Device>) { ... } // setter\r\n}\r\n```\r\n\r\nwe should have\r\n\r\n```\r\nclass TensorOptions {\r\n  TensorOptions device(Device) { ... } // setter\r\n  TensorOptions device_opt(optional<Device>) { ... } // setter\r\n}\r\n```\r\n\r\nThis makes the class more user friendly as now `x.device({kCUDA, 1})` works; it also makes it symmetric with `at::device({kCUDA, 1})`.\n\ncc @yf225"},{"labels":["api",null,null],"text":"struct Net : torch::nn::Module {\r\n\tNet()\r\n\t\t: conv1(torch::nn::Conv2dOptions(1, 20, /*kernel_size=*/5).stride(1)),\r\n\t\tconv2(torch::nn::Conv2dOptions(20, 40, /*kernel_size=*/5)),\r\n\t\tfc1(640, 120),\r\n\t\tfc2(120, 10) {\r\n\t\tregister_module(\"conv1\", conv1);\r\n\t\tregister_module(\"conv2\", conv2);\r\n\t\tregister_module(\"conv2_drop\", conv2_drop);\r\n\t\tregister_module(\"fc1\", fc1);\r\n\t\tregister_module(\"fc2\", fc2);\r\n\t}\r\n\ttorch::Tensor forward(torch::Tensor x) {\r\n\t\tx = torch::relu(torch::max_pool2d(conv1->forward(x), 2));//(28-5)+1=24,12 x 12 x 10\r\n\t\tx = torch::relu(torch::max_pool2d(conv2_drop->forward(conv2->forward(x)), 2));//(12-5)+1=8,4 x 4 x 20\r\n\t\t//x = torch::relu(torch::avg_pool2d(conv2_drop->forward(conv2->forward(x)), 2));//(12-5)+1=8,4 x 4 x 20\r\n\r\n\t\tx = x.view({ -1, 640 });\r\n\t\tx = torch::relu(fc1->forward(x));\r\n\t\tx = torch::dropout(x, /*p=*/0.5, /*training=*/is_training());\r\n\t\tx = fc2->forward(x);\r\n\t\treturn torch::log_softmax(x, /*dim=*/1);\r\n\t}\r\n\ttorch::nn::Conv2d conv1;\r\n\ttorch::nn::Conv2d conv2;\r\n\ttorch::nn::Dropout2d conv2_drop;\r\n\ttorch::nn::Linear fc1;\r\n\ttorch::nn::Linear fc2;\r\n};\n\ncc @yf225 @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof"},{"labels":["api",null,null],"text":"## üêõ Bug\r\n\r\nUsing `torch::var_out` in the C++ API with dimnames instead of the index of the dimension seems to return the standard deviation instead of the variance.\r\n\r\n## To Reproduce\r\n\r\nRunning the following program:\r\n\r\n```\r\n#include <torch/torch.h>\r\n#include <iostream>\r\n\r\nint main() {\r\n    auto d = torch::Dimname::fromSymbol(torch::Symbol::dimname(\"a\"));\r\n    std::vector<torch::Dimname> ds;\r\n    ds.push_back(d);\r\n    \r\n    auto x = torch::rand(100, ds);\r\n    \r\n    auto y = torch::zeros(1);\r\n    torch::var_out(y, x, {0});\r\n    std::cout << y << std::endl;\r\n    \r\n    auto z = torch::zeros(1);\r\n    torch::var_out(z, x, ds);\r\n    std::cout << z << std::endl;\r\n}\r\n```\r\n\r\nReturns: \r\n\r\n```\r\n(base) dfalbel@Daniels-MacBook-Pro build % ./example-app\r\nWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (operator() at ../c10/core/TensorImpl.h:845)\r\n0.073097\r\n[ CPUFloatType{} ]\r\n0.270365\r\n[ CPUFloatType{} ]\r\n```\r\n\r\nSince:\r\n\r\n```\r\nsqrt(0.073097)\r\n[1] 0.2703646\r\n```\r\n\r\nIt seems that when using dimnames `torch_var_out` is returning the standard deviation.\r\n\r\n## Expected behavior\r\n\r\nExpected both values to be identical.\r\n\r\n## Environment\r\n\r\nI am using Pytorch C++ 1.4 - downloaded from here: https://download.pytorch.org/libtorch/cpu/libtorch-macos-1.4.0.zip on MacOS\r\n\n\ncc @yf225 @zou3519"},{"labels":["api",null],"text":"The class was explicitly designed to be two words large, we should pass it by value.\n\ncc @yf225"},{"labels":[null,"api",null],"text":"## üöÄ Feature\r\n`torch.round` to take an optimal argument which specifies the decimal place to which rounding should occur\r\n\r\n## Pitch\r\n`torch.round` should have the same functionality as `numpy.around`. Specifying a decimal place will round _up to_ that decimal place, using the same rounding strategy as `torch.round` currently uses. The default value of the decimal parameter should be 0 to keep current behaviour.\r\nE.g.\r\n```\r\n>>> tensor = torch.tensor([0.1234, 0.1237])\r\n>>>torch.round(tensor, decimals=3)\r\ntensor([0.123, 0.124])\r\n```\r\n\r\n## Alternatives\r\nYou can use `numpy.around` on the tensor, but I assume this has some performance hit?\r\n\n\ncc @yf225"},{"labels":[null,"api",null],"text":"## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nAdd the same strong wolfe line search option to the LibTorch interface as the Python counterpart.\r\n\r\nhttps://pytorch.org/docs/master/optim.html\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nUnify the interface between Python and C++\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nImplement the strong wolfe line search algorithm and add a flag to the `LBFGSOptions` struct\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @yf225"},{"labels":["api",null,null],"text":"in https://github.com/pytorch/pytorch/pull/33189 we switch RREf to be managed by intrusive_ptr, and we made the UserRRef/OwnerRRef public to make `c10::make_intrusive<OwnerRRef>(getWorkerId(), rrefId, type)` work since it does not support private constructor, and `intrusive_ptr<OwnerRRef>(new OwnerRRef())` does not work because of intrusive_ptr limitation. \r\n\r\nWe should figure out a way to make it private again before we announce C++ API for rpc.\n\ncc @yf225 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\nThe fractional max pooling options for output ratio need to be an expanding array of doubles.\r\ncurrently:\r\n\r\n```\r\ntemplate <size_t D>\r\nstruct FractionalMaxPoolOptions {\r\n  FractionalMaxPoolOptions(ExpandingArray<D> kernel_size)\r\n      : kernel_size_(kernel_size) {}\r\n\r\n  /// the size of the window to take a max over\r\n  TORCH_ARG(ExpandingArray<D>, kernel_size);\r\n\r\n  /// the target output size of the image\r\n  TORCH_ARG(c10::optional<ExpandingArray<D>>, output_size) = c10::nullopt;\r\n\r\n  /// If one wants to have an output size as a ratio of the input size, this option can be given.\r\n  /// This has to be a number or tuple in the range (0, 1)\r\n  TORCH_ARG(c10::optional<ExpandingArray<D>>, output_ratio) = c10::nullopt;\r\n```\r\n\r\nlater, in nn/functional/pooling.h, if no output sizes, output ratios are used:\r\n```\r\n  if (output_size_ == c10::nullopt) {\r\n    TORCH_INTERNAL_ASSERT(output_ratio != c10::nullopt);\r\n    output_size_ = {(int64_t)(input.sizes()[2] * (*output_ratio.value())[0]),\r\n                    (int64_t)(input.sizes()[3] * (*output_ratio.value())[1]),\r\n                    (int64_t)(input.sizes()[4] * (*output_ratio.value())[2])};\r\n  }\r\n```\r\n\r\nthe code compiles with integers, but won't allow for output ratios of 0.5, for example\r\n\r\n\r\n\n\ncc @yf225"},{"labels":["api",null],"text":"Achieving API parity between our Python and C++ frontend is crucial for broadening the number of language communities PyTorch can address. By the PyTorch 1.5 release, we will provide the following improvements to the PyTorch C++ frontend:\r\n\r\n**Python/C++ API Parity:**\r\n- [x] torch.nn modules and functional (tracking issue: https://github.com/pytorch/pytorch/issues/25883)\r\n  - [x] RNN (https://github.com/pytorch/pytorch/pull/34322)\r\n  - [x] LSTM (https://github.com/pytorch/pytorch/pull/34322)\r\n  - [x] GRU (https://github.com/pytorch/pytorch/pull/34322)\r\n  - [x] RNNCell (https://github.com/pytorch/pytorch/pull/34400)\r\n  - [x] LSTMCell (https://github.com/pytorch/pytorch/pull/34400)\r\n  - [x] GRUCell (https://github.com/pytorch/pytorch/pull/34400)\r\n  - [x] AdaptiveLogSoftmaxWithLoss (https://github.com/pytorch/pytorch/pull/29076)\r\n  - [x] PackedSequence (https://github.com/pytorch/pytorch/pull/33652)\r\n  - [x] pack_padded_sequence (https://github.com/pytorch/pytorch/pull/33652)\r\n  - [x] pad_packed_sequence (https://github.com/pytorch/pytorch/pull/33652)\r\n  - [x] pad_sequence (https://github.com/pytorch/pytorch/pull/33652)\r\n  - [x] pack_sequence (https://github.com/pytorch/pytorch/pull/33652)\r\n- [ ] Python/C++ API parity test for torch.nn modules and functional\r\n  - [x] Fix AdaptiveAvgPool{2,3}d and AdaptiveMaxPool{2,3}d implementation (https://github.com/pytorch/pytorch/pull/35022)\r\n  - [x] Fix Conv and ConvTranspose implementation (https://github.com/pytorch/pytorch/pull/35023)\r\n  - [x] Fix fractional_max_pool3d_with_indices implementation (https://github.com/pytorch/pytorch/pull/35024)\r\n  - [x] Fix F::interpolate and torch::nn::Upsample implementation (https://github.com/pytorch/pytorch/pull/35025)\r\n  - [x] Add inplace tests for several torch::nn modules / functionals (#35147)\r\n  - [x] Renaming: MultiLabelMarginLossFuncOptions -> MultilabelMarginLossFuncOptions, MultiLabelSoftMarginLossFuncOptions -> MultilabelSoftMarginLossFuncOptions (#35163)\r\n  - [ ] Turn on parity test (https://github.com/pytorch/pytorch/pull/35189, https://github.com/pytorch/pytorch/pull/35190)\r\n- [ ] torch.optim optimizers (tracking issue: https://github.com/pytorch/pytorch/issues/28440, make sure to test that the new design doesn't break serialization BC)\r\n- [x] tensor multi-dim indexing API\r\n  - [x] https://github.com/pytorch/pytorch/pull/32841\r\n  - [x] https://github.com/pytorch/pytorch/pull/30426\r\n  - [x] https://github.com/pytorch/pytorch/pull/30427\r\n- [x] C++ tensor autograd API (tracking issue: https://github.com/pytorch/pytorch/issues/25874). Remaining items:\r\n  - [x] grad_fn (https://github.com/pytorch/pytorch/pull/28287)\r\n  - [x] register_hook (https://github.com/pytorch/pytorch/pull/28287)\r\n  - [x] retain_grad (https://github.com/pytorch/pytorch/pull/33349)\r\n  - [x] _base (https://github.com/pytorch/pytorch/pull/33316)\r\n\r\n**Deprecation**\r\n- [x] Remove deprecated torch::nn::BatchNorm / FeatureDropout / modules_ordered_dict and torch::nn::init::Nonlinearity / FanMode (https://github.com/pytorch/pytorch/pull/34508)\r\n\r\n**Bug fixes:**\r\n- [x] Allow skipping default arguments in module's forward method when module is used in Sequential (PR: https://github.com/pytorch/pytorch/pull/33027)\r\n- [x] ModuleList compile error: error: 'begin' was not declared in this scope (Issue: https://github.com/pytorch/pytorch/issues/32414, PR: https://github.com/pytorch/pytorch/pull/34463)\r\n- [x] C++ nn::FractionalMaxPool2d/3d output_ratio option is integer, should be double (Issue: https://github.com/pytorch/pytorch/issues/33240, PR: https://github.com/pytorch/pytorch/pull/33304)\r\n- [x] Remove `using namespace torch::autograd` from header files (Issue: https://github.com/pytorch/pytorch/issues/34371. PR: https://github.com/pytorch/pytorch/pull/34423)\r\n\r\n**Docs:**\r\n- [x] For each `torch::nn` layer, document how to use `*Options` to specify options (ideally right above `torch::nn` layer doc) (https://github.com/pytorch/pytorch/pull/34522)\r\n- [x] For each `torch::nn` functional, document how to use `*Options` to specify options (ideally right above `torch::nn` functional doc) (https://github.com/pytorch/pytorch/pull/34688) (https://github.com/pytorch/pytorch/pull/34752) (ETA: 3/13)\r\n- [x] Fix torch::Tensor doc generation (Issue: https://github.com/pytorch/pytorch/issues/25845. PR: https://github.com/pytorch/pytorch/pull/34467)\r\n- [ ] How to use C++ tensor multi-dim indexing (ETA: 4/3)\r\n  - [ ] Maybe a table to show the translation between two languages\r\n\r\n**Tutorials:**\r\n- [ ] Autograd in C++ (ETA: 4/3)\r\n  - [ ] Basic autograd operations (https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html, https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_autograd.html?highlight=autograd)\r\n  - [ ] How to use C++ custom autograd function (https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd)\r\n  - [ ] How to use `at::Tensor::register_hook`\r\n  - [ ] How to compute higher-order gradients in C++ (e.g. issue: https://github.com/pytorch/pytorch/issues/18173)\r\n\r\n**Blog post:**\r\n- [ ] C++ frontend revamp (ETA: 4/3)\r\n\r\n\r\n\r\ncc @yf225 @gchanan "},{"labels":["api",null],"text":"## üöÄ Feature\r\ncan these private _push_back_ functions be made public?\r\n\r\n```\r\n/// Adds a type-erased `AnyModule` to the `Sequential`.\r\n  void push_back(AnyModule any_module) {\r\n    push_back(c10::to_string(modules_.size()), std::move(any_module));\r\n  }\r\n\r\n  void push_back(std::string name, AnyModule any_module) {\r\n    modules_.push_back(std::move(any_module));\r\n    const auto index = modules_.size() - 1;\r\n    register_module(std::move(name), modules_[index].ptr());\r\n  }\r\n```\r\n\r\n## Motivation\r\n\r\nIf i already have an `AnyModule a`,\r\nI can't figure out how to add it to a _Sequential_ without something like\r\n`if(auto* m=a.as<torch::nn::Conv2d>()) seq->push_back<torch::nn::Conv2d>(*m)`\r\nwhich will then turn it back into an AnyModule and add it.\r\n\r\nOr making the single AnyModule into a vector:\r\n`seq->extend(std::vector<AnyModule>{a})`\r\n\r\nThanks\n\ncc @yf225"},{"labels":[null,"api",null,null],"text":"## Issue description\r\n\r\nI have downloaded the LibTorch zip from [here](https://pytorch.org/get-started/locally/). I used the 1.4 stable build for windows with CUDA 10.1 support. However the torch folder within the include directory does not contain the \"torch.h\" header file used in many [examples](https://pytorch.org/cppdocs/installing.html#minimal-example). This error comes up when I use the line:\r\n`\r\n#include <Torch/torch.h>\r\n`\r\nThe error simply says \"cannot open torch.h\".\r\n\r\nI am using Microsoft Visual Studio 2019 and I have already added the additional include and library directories. The only \"torch.h\" file I can find is at \"....\\include\\torch\\csrc\\api\\include\\torch\\torch.h\" but this does not include all of the other header files I need such as for the torch::nn::Module class etc.\r\n\r\n## System Info\r\n\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Microsoft Windows 10 Pro\r\nGCC version: (MinGW.org GCC-6.3.0-1) 6.3.0\r\nCMake version: version 3.15.0-rc2\r\n\r\nPython version: 3.6\r\nIs CUDA available: N/A\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce GTX 1050 Ti\r\nNvidia driver version: 431.36\r\ncuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.14.3\r\n[pip3] numpydoc==0.8.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2018.0.2                      1\r\n[conda] mkl-service               1.1.2            py36h57e144c_4\r\n[conda] mkl_fft                   1.0.1            py36h452e1ab_0\r\n[conda] mkl_random                1.0.1            py36h9258bd6_0\r\n\n\ncc @ezyang @gchanan @zou3519 @yf225 @peterjc123"},{"labels":[null,"api",null,null],"text":"Currently torch::allclose() fails with\r\n`C++ exception with description \"Subtraction, the `-` operator, with a bool tensor is not supported. If you are trying to invert a mask, use the `~` or `logical_not()` operator instead. (sub_check at ../aten/src/ATen/native/BinaryOps.h:24)`\r\non bool tensors\n\ncc @yf225 @izdeby"},{"labels":["api",null,null],"text":"```\r\n  const auto a = torch::tensor({std::numeric_limits<int64_t>::lowest() + 1}, torch::kInt64);\r\n  ASSERT_TRUE(torch::allclose(a, a)); // works\r\n\r\n  const auto b = torch::tensor({std::numeric_limits<int64_t>::lowest()}, torch::kInt64);\r\n  ASSERT_TRUE(torch::allclose(b, b)); // does not\r\n```\n\ncc @yf225"},{"labels":[null,"api",null,null],"text":"## üöÄ Feature\r\n\r\nThe `at::Tensor` datatype does not have `&` overloaded the way it is in the python API, thus not allowing for boolean element-wise operations between tensors.\r\n\r\n## Motivation\r\n\r\nI want to call the `at::where()` function with a boolean tensor that is the outcome of the AND of two conditions.\r\n\r\n## Pitch\r\n\r\nSomething like this should be possible:\r\n``` c++\r\n  auto output = at::where((target > -EPSILON) & (target < EPSILON), all_zeros, whatever_values);\r\n```\r\n## Alternatives\r\n\r\nThis is ugly and tedious IMO:\r\n\r\n``` c++\r\n  auto output = at::where(target > -EPSILON, output_pos, zeros);\r\n  output = at::where(output < EPSILON, zeros, output_pos);\r\n```\r\n\n\ncc @yf225"},{"labels":["api",null,null,null,null],"text":"## üêõ Bug\r\n\r\nThe `MagmaInitializesCorrectly_CUDA` test case in `test/cpp/api/tensor_cuda.cpp` fails with the following error:\r\n\r\n```\r\n[0;32m[ RUN      ] #[mTensorTest.MagmaInitializesCorrectly_CUDA\r\nunknown file: Failure\r\nC++ exception with description \"inverse_cuda: U(4,4) is zero, singular U. (singleCheckErrors at /home/jenkins/pytorch/aten/src/ATen/native/LinearAlgebraUtils.h:138)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xc0 (0x1000135086a0 in /home/jenkins/pytorch/build/lib/libc10.so)\r\nframe #1: <unknown function> + 0x1550eb0 (0x1000058e0eb0 in /home/jenkins/pytorch/build/lib/libtorch_cuda.so)\r\nframe #2: at::native::_inverse_helper_cuda(at::Tensor const&) + 0x9bc (0x1000058e8c1c in /home/jenkins/pytorch/build/lib/libtorch_cuda.so)\r\nframe #3: <unknown function> + 0x3072224 (0x100007402224 in /home/jenkins/pytorch/build/lib/libtorch_cuda.so)\r\nframe #4: <unknown function> + 0xec6f20 (0x100000f46f20 in /home/jenkins/pytorch/build/lib/libtorch_cpu.so)\r\nframe #5: at::native::inverse(at::Tensor const&) + 0x14c (0x1000009f53fc in /home/jenkins/pytorch/build/lib/libtorch_cpu.so)\r\nframe #6: <unknown function> + 0xf9b674 (0x10000101b674 in /home/jenkins/pytorch/build/lib/libtorch_cpu.so)\r\nframe #7: <unknown function> + 0xec6f20 (0x100000f46f20 in /home/jenkins/pytorch/build/lib/libtorch_cpu.so)\r\nframe #8: at::Tensor c10::Dispatcher::callUnboxed<at::Tensor, at::Tensor const&>(c10::OperatorHandle const&, at::Tensor const&) const + 0xd4 (0x10f009ef4 in build/bin/test_api)\r\nframe #9: <unknown function> + 0x2aabffc (0x100002b2bffc in /home/jenkins/pytorch/build/lib/libtorch_cpu.so)\r\nframe #10: <unknown function> + 0xec6f20 (0x100000f46f20 in /home/jenkins/pytorch/build/lib/libtorch_cpu.so)\r\nframe #11: TensorTest_MagmaInitializesCorrectly_CUDA_Test::TestBody() + 0x804 (0x10f398de4 in build/bin/test_api)\r\nframe #12: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) + 0x78 (0x10f45de48 in build/bin/test_api)\r\nframe #13: <unknown function> + 0x880774 (0x10f450774 in build/bin/test_api)\r\nframe #14: <unknown function> + 0x880cb4 (0x10f450cb4 in build/bin/test_api)\r\nframe #15: <unknown function> + 0x881174 (0x10f451174 in build/bin/test_api)\r\nframe #16: testing::internal::UnitTestImpl::RunAllTests() + 0xeac (0x10f45252c in build/bin/test_api)\r\nframe #17: testing::UnitTest::Run() + 0xb8 (0x10f452938 in build/bin/test_api)\r\nframe #18: main + 0x108 (0x10edc25e8 in build/bin/test_api)\r\nframe #19: <unknown function> + 0x2441c (0x1000139f441c in /lib/powerpc64le-linux-gnu/libc.so.6)\r\nframe #20: __libc_start_main + 0xb8 (0x1000139f4618 in /lib/powerpc64le-linux-gnu/libc.so.6)\r\n\" thrown in the test body.\r\n#[0;31m[  FAILED  ] #[mTensorTest.MagmaInitializesCorrectly_CUDA (288 ms)\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Build PyTorch from source, and make sure to build the cpp API tests along with it\r\n2. Run the `tensor_cuda.cpp` tests (probably with `test_api` in the `build/bin` directory)\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nThe test should pass\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): master\r\n - OS (e.g., Linux): RHEL\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): `python setup.py install`\r\n - Python version: `3.6`\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: 4x Tesla V100\r\n - Any other relevant information: Ran on Power architecture\r\n\r\n## Aditional Information\r\n\r\nI've submitted a PR to fix this here: https://github.com/pytorch/pytorch/pull/32547\r\n\n\ncc @yf225 @ngimel"},{"labels":[null,null,null,"api",null],"text":"## üêõ Bug\r\nUse PyTorch model in C++, compilation breaks when using the default route.\r\nCMake `find_package(Torch REQUIRED)` \r\n\r\nreturns corrupted `TORCH_LIBRARIES` list\r\n\r\n```\r\ntorch\r\ntorch_library\r\n/usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so\r\n/usr/local/cuda/lib64/stubs/libcuda.so\r\n/usr/local/cuda/lib64/libnvrtc.so\r\n/usr/local/cuda/lib64/libnvToolsExt.so\r\n/usr/local/cuda/lib64/libcudart.so\r\n/usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so\r\n```\r\n\r\nIt should not contain absolute paths to the other libraries but rather the lib names.\r\nThe library paths should be added to the environment if necessary\r\n\r\n## Solution\r\n\r\nInstead of \r\n\r\n`target_link_libraries(MyTARGET PUBLIC ${TORCH_LIBRARIES})`\r\n\r\ndo\r\n\r\n```\r\nlink_directories(/usr/local/lib/python3.6/dist-packages/torch/lib/)\r\nlink_directories(/usr/local/cuda/lib64/)\r\nlink_directories(/usr/local/cuda/lib64/stubs)\r\n\r\ntarget_link_libraries(MyTARGET PUBLIC torch torch_library libc10.so libcuda.so libnvrtc.so libnvToolsExt.so libcudart.so libc10_cuda.so)\r\n```\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Create `CMakeLists.txt`\r\n2.`find_package(Torch REQUIRED)`\r\n3. `message(\"TORCH LIBS ${TORCH_LIBRARIES}\")`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nReturns linker errors as `c10` can not be found\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Backend.h:107: undefined reference to `c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nSunlightNoiseClassifier/libSunlightNoiseClassifier.a(SunlightNoiseClassifier.cpp.o): In function `c10::backendToDeviceType(c10::Backend)':\r\n```\r\n\r\n## Environment\r\n\r\n```\r\ncmake version 3.5.1\r\n\r\npython collect_env.py \r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy==0.4.4.2\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchvision==0.4.0\r\n[conda] Could not collect\r\n```\r\n\n\ncc @ezyang @gchanan @zou3519 @yf225"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\nCode crashes in C++ API when torch tensor is read from specific buffer\r\n\r\n## To Reproduce\r\n\r\nbuild and run this code in release build\r\n\r\n```c++\r\n#include <iostream>\r\n#include <torch/extension.h>\r\n\r\nint main() {\r\n    std::string buffer;\r\n    buffer.resize((1 << 19) + 128);\r\n    std::memset(buffer.data(), 0, buffer.size());\r\n    size_t start_offset;\r\n    // This would work fine\r\n    start_offset = 1 << 10;\r\n    // This crashes\r\n    start_offset = (1 << 10) - 1;\r\n    size_t element_count = 319;\r\n    std::cerr << buffer.size() << \" vs \" << torch::elementSize(torch::kInt32) * element_count + start_offset;\r\n    auto tensor = torch::from_blob(buffer.data() + start_offset, {element_count}, torch::CPU(torch::kInt32));\r\n    tensor.clone();\r\n}\r\n```\r\n\r\n```cmake\r\ncmake_minimum_required(VERSION 3.12)\r\nproject(from_blob_sigsegv)\r\n\r\n\r\nset(CMAKE_CXX_STANDARD 17)\r\nset(CMAKE_PREFIX_PATH /home/alxmopo3ov/libtorch)\r\n\r\nfind_package(Torch REQUIRED)\r\ninclude_directories(${TORCH_INCLUDE_DIRS})\r\nfind_package(Python3 COMPONENTS Interpreter Development)\r\ninclude_directories(${Python3_INCLUDE_DIRS})\r\n\r\nadd_executable(from_blob_sigsegv main.cpp)\r\ntarget_link_libraries(from_blob_sigsegv ${Python3_LIBRARIES} ${TORCH_LIBRARIES})\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nDon't crash\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 14.04.6 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~14.04~ppa1) 7.4.0\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.1\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.0                      118  \r\n[conda] mkl-service               1.1.2            py37h90e4bf4_5  \r\n[conda] mkl_fft                   1.0.4            py37h4414c95_1  \r\n[conda] mkl_random                1.0.1            py37h4414c95_1  \r\n[conda] torch                     1.4.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\n\ncc @yf225"},{"labels":["api",null,null],"text":"## üöÄ Feature\r\n\r\nDistributedStreamSampler: support stream sampler in distributed setting\r\n\r\n## Motivation\r\n\r\nA new class `torch::data::samplers::DistributedStreamSampler` both works in distributed setting like `torch::data::samplers::DistributedSequentialSampler` and works with `torch::data::datasets::StreamDataset` like `torch::data::samplers::StreamSampler`.\r\n\r\n## Pitch\r\n\r\nworks with StreamDataset in distributed setting.\r\n\r\n## Alternatives\r\n\r\nimplement this by user\r\n\r\n## Additional context\r\n\r\nnone\r\n\n\ncc @yf225 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\nThe example for ModuleList found in the documentation [here](https://pytorch.org/cppdocs/api/classtorch_1_1nn_1_1_module_list_impl.html#_CPPv4N5torch2nn14ModuleListImplE) does not seem to compile.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nAttempt to compile the following source on GCC 7.4, taken from the documentation:\r\n\r\n```cpp\r\n#include <torch/torch.h>\r\n\r\nint main() {\r\n\r\n  torch::nn::ModuleList mlist(\r\n    torch::nn::Linear(3, 4),\r\n    torch::nn::BatchNorm(4),\r\n    torch::nn::Dropout(0.5)\r\n  );\r\n\r\n  for (const auto &module : mlist) {\r\n    module.pretty_print();\r\n}\r\n```\r\n\r\nCompile error:\r\n```\r\nC++ compilation of rule '//src/learning:pytorch' failed (Exit 1) gcc failed: error executing command /usr\r\n/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 38 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nsrc/learning/pytorch_test.cpp: In function 'int main()':\r\nsrc/learning/pytorch_test.cpp:12:29: error: 'begin' was not declared in this scope\r\n   for (const auto &module : mlist) {\r\n                             ^~~~~\r\nsrc/learning/pytorch_test.cpp:12:29: note: suggested alternative:\r\nIn file included from /usr/include/c++/7/string:51:0,\r\n                 from /usr/include/c++/7/stdexcept:39,\r\n                 from /usr/include/c++/7/array:39,\r\n                 from /usr/include/c++/7/tuple:39,\r\n                 from /usr/include/c++/7/bits/unique_ptr.h:37,\r\n                 from /usr/include/c++/7/memory:80,\r\n                 from external/pytorch/libtorch/include/c10/core/Allocator.h:4,\r\n                 from external/pytorch/libtorch/include/ATen/ATen.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/types.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/all.h:4,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/torch.h:3,\r\n                 from src/learning/pytorch_test.cpp:1:\r\n/usr/include/c++/7/bits/range_access.h:105:37: note:   'std::begin'\r\n   template<typename _Tp> const _Tp* begin(const valarray<_Tp>&);\r\n                                     ^~~~~\r\nsrc/learning/pytorch_test.cpp:12:29: error: 'end' was not declared in this scope\r\n   for (const auto &module : mlist) {\r\n                             ^~~~~\r\nsrc/learning/pytorch_test.cpp:12:29: note: suggested alternatives:\r\nIn file included from /usr/include/c++/7/string:51:0,\r\n                 from /usr/include/c++/7/stdexcept:39,\r\n                 from /usr/include/c++/7/array:39,\r\n                 from /usr/include/c++/7/tuple:39,\r\n                 from /usr/include/c++/7/bits/unique_ptr.h:37,\r\n                 from /usr/include/c++/7/memory:80,\r\n                 from external/pytorch/libtorch/include/c10/core/Allocator.h:4,\r\n                 from external/pytorch/libtorch/include/ATen/ATen.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/types.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/all.h:4,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/torch.h:3,\r\n                 from src/learning/pytorch_test.cpp:1:\r\n/usr/include/c++/7/bits/range_access.h:107:37: note:   'std::end'\r\n   template<typename _Tp> const _Tp* end(const valarray<_Tp>&);\r\n                                     ^~~\r\nIn file included from external/pytorch/libtorch/include/ATen/core/Dimname.h:5:0,\r\n                 from external/pytorch/libtorch/include/ATen/core/NamedTensor.h:4,\r\n                 from external/pytorch/libtorch/include/ATen/core/TensorBody.h:20,\r\n                 from external/pytorch/libtorch/include/ATen/Tensor.h:11,\r\n                 from external/pytorch/libtorch/include/ATen/Context.h:4,\r\n                 from external/pytorch/libtorch/include/ATen/ATen.h:5,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/types.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/data.h:3,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/all.h:4,\r\n                 from external/pytorch/libtorch/include/torch/csrc/api/include/torch/torch.h:3,\r\n                 from src/learning/pytorch_test.cpp:1:\r\nexternal/pytorch/libtorch/include/ATen/core/interned_strings.h:374:1: note:   'c10::attr::end'\r\n FORALL_NS_SYMBOLS(DEFINE_SYMBOL)\r\n ^\r\nexternal/pytorch/libtorch/include/ATen/core/interned_strings.h:374:1: note:   'c10::attr::end'\r\nsrc/learning/pytorch_test.cpp:14:1: error: expected '}' at end of input\r\n }\r\n```\r\n\r\n \r\n## Expected behavior\r\n\r\nI believe this should pretty print the submodules...\r\n\r\n## Environment\r\n\r\n - GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n - PyTorch Version (e.g., 1.0): 1.4\r\n - OS (e.g., Linux): Ubuntu 18.04.1 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): Linux CPU-only binary\r\n - Build command you used (if compiling from source): n/a\r\n - Python version: n/a\r\n - CUDA/cuDNN version: n/a\r\n - GPU models and configuration: n/a\r\n - Any other relevant information: Using GCC 7 via Bazel (see trace above for full compile flags)\r\n\r\n## Additional context\r\n\r\nAlso, is there way to initialize an empty ModuleList an do operations on it? If I try to push_back on an empty ModuleList I get ```  what():  Accessing empty ModuleHolder (get at external/pytorch/libtorch/include/torch/csrc/api/include/torch/nn/pimpl.h:107)``` which makes it not so useful for my use-case of creating variable sized networks at runtime. \n\ncc @yf225"},{"labels":["api",null,null],"text":"## üìö Documentation\r\n\r\nCurrently, PyTorch C++ API docs (https://pytorch.org/cppdocs/) only tracks the master branch. Ideally we should add PyTorch version selector into it, similar to the Python API docs (https://pytorch.org/docs/stable/index.html).\r\n\n\ncc @yf225 @ezyang @zou3519"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\nIn Python, we have the following behavior:\r\n```python\r\n>>> torch.set_default_dtype(torch.double)\r\n>>> a = 1.1\r\n>>> t = torch.tensor([a, a])\r\n>>> t.dtype\r\ntorch.float64\r\n```\r\n\r\nHowever, currently in C++, we have the following behavior:\r\n```cpp\r\ntorch::set_default_dtype(torch::scalarTypeToTypeMeta(torch::kDouble));\r\nfloat a = 1.1;\r\ntorch::tensor({a, a}).dtype()  // prints: float\r\n```\r\n\r\nWe should fix the C++ API behavior (by returning a double tensor instead of a float tensor in the above case), to match the Python API.\r\n\r\ncc @yf225"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\nThis has come up before, issue #14079 and pr #11040\r\n\r\nIt looks like the python side was fixed to return longs (but documentation still has: \"_dtype (torch.dtype, optional) ‚Äì the desired data type of returned tensor. Default: if None, uses a global default_\")\r\n\r\n## To Reproduce\r\n\r\n```\r\n>>> import torch\r\n>>> torch.randint(10,(3,)).dtype\r\ntorch.int64\r\n```\r\n```\r\nstd::cerr << torch::randint(10,{3}) << \"\\n\";\r\n 5\r\n 0\r\n 9\r\n[ CPUFloatType{3} ]\r\n```\r\n\r\n## Expected behavior\r\n\r\nBoth behaviours are reasonable, but I guess I would expect the c++ implementation to match python.\n\ncc @yf225"},{"labels":["api",null],"text":"Please improve the error prompt for libtorch.\r\n0x00007FFF9026A839 Â§Ñ(‰Ωç‰∫é ConsoleApplication1.exe ‰∏≠)ÂºïÂèëÁöÑÂºÇÂ∏∏: Microsoft C++ ÂºÇÂ∏∏: c10::ErrorÔºå‰Ωç‰∫éÂÜÖÂ≠ò‰ΩçÁΩÆ 0x0000002C2A9AF7F0 Â§Ñ„ÄÇ\r\n0x00007FFF9026A839 Â§Ñ(‰Ωç‰∫é ConsoleApplication1.exe ‰∏≠)ÊúâÊú™ÁªèÂ§ÑÁêÜÁöÑÂºÇÂ∏∏: Microsoft C++ ÂºÇÂ∏∏: c10::ErrorÔºå‰Ωç‰∫éÂÜÖÂ≠ò‰ΩçÁΩÆ 0x0000002C2A9AF7F0 Â§Ñ„ÄÇ\r\n-----------------------------------------------------------\r\nwin10 x64,libtorch 1.3.1 version release no gpu, 2020.01.13\r\n----------------------------------------------------------\r\n#include \"torch/torch.h\"\r\n#include <iostream>\r\n\r\nstruct Net : torch::nn::Module {\r\n\tNet() {\r\n\t\t// Construct and register two Linear submodules.\r\n\t\tfc1 = register_module(\"fc1\", torch::nn::Linear(784, 64));\r\n\t\tfc2 = register_module(\"fc2\", torch::nn::Linear(64, 32));\r\n\t\tfc3 = register_module(\"fc3\", torch::nn::Linear(32, 10));\r\n\t}\r\n\r\n\t// Implement the Net's algorithm.\r\n\ttorch::Tensor forward(torch::Tensor x) {\r\n\t\t// Use one of many tensor manipulation functions.\r\n\t\tx = torch::relu(fc1->forward(x.reshape({ x.size(0), 784 })));\r\n\t\tx = torch::dropout(x, /*p=*/0.5, /*train=*/is_training());\r\n\t\tx = torch::relu(fc2->forward(x));\r\n\t\tx = torch::log_softmax(fc3->forward(x), /*dim=*/1);\r\n\t\treturn x;\r\n\t}\r\n\r\n\t// Use one of many \"standard library\" modules.\r\n\ttorch::nn::Linear fc1{ nullptr }, fc2{ nullptr }, fc3{ nullptr };\r\n\r\n};\r\n\r\nint main() {\r\n\t// Create a new Net.\r\n\tauto net = std::make_shared<Net>();\r\n\r\n\t// Create a multi-threaded data loader for the MNIST dataset.\r\n\tauto data_loader = torch::data::make_data_loader(\r\n\t\ttorch::data::datasets::MNIST(\"./data\").map(\r\n\t\t\ttorch::data::transforms::Stack<>()),\r\n\t\t/*batch_size=*/64);\r\n\r\n\t// Instantiate an SGD optimization algorithm to update our Net's parameters.\r\n\ttorch::optim::SGD optimizer(net->parameters(), /*lr=*/0.01);\r\n\r\n\tfor (size_t epoch = 1; epoch <= 10; ++epoch) {\r\n\t\tsize_t batch_index = 0;\r\n\t\t// Iterate the data loader to yield batches from the dataset.\r\n\t\tfor (auto& batch : *data_loader) {\r\n\t\t\t// Reset gradients.\r\n\t\t\toptimizer.zero_grad();\r\n\t\t\t// Execute the model on the input data.\r\n\t\t\ttorch::Tensor prediction = net->forward(batch.data);\r\n\t\t\t// Compute a loss value to judge the prediction of our model.\r\n\t\t\ttorch::Tensor loss = torch::nll_loss(prediction, batch.target);\r\n\t\t\t// Compute gradients of the loss w.r.t. the parameters of our model.\r\n\t\t\tloss.backward();\r\n\t\t\t// Update the parameters based on the calculated gradients.\r\n\t\t\toptimizer.step();\r\n\t\t\t// Output the loss and checkpoint every 100 batches.\r\n\t\t\tif (++batch_index % 100 == 0) {\r\n\t\t\t\tstd::cout << \"Epoch: \" << epoch << \" | Batch: \" << batch_index\r\n\t\t\t\t\t<< \" | Loss: \" << loss.item<float>() << std::endl;\r\n\t\t\t\t// Serialize your model periodically as a checkpoint.\r\n\t\t\t\ttorch::save(net, \"net.pt\");\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n\n\ncc @yf225"},{"labels":["api",null,null],"text":"## üêõ Bug\r\n\r\nnightly Pytorch cannot compile cpp extension\r\n\r\n## To Reproduce\r\n\r\nExtension [here](https://pytorch.org/tutorials/advanced/cpp_extension.html#writing-a-mixed-c-cuda-extension) defines:\r\n\r\n```c++\r\n#define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\r\n```\r\n\r\nresults in deprecation:\r\n\r\n```text\r\n\r\nwarning: ‚Äòat::DeprecatedTypeProperties& at::Tensor::type() const‚Äô is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement\r\n\r\n```\r\n\r\nbut there is no `is_cuda` in `options` struct.\r\nThe fix was to call `x.is_cuda()` directly, which is nicer.\r\n\r\n```c++\r\n#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\r\n```\r\n\r\n## Expected behavior\r\n\r\nExamples to be tested\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.5.0.dev20200113\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 418.87.01\r\ncuDNN version: /usr/local/lib/libcudnn.so.5.1.10\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.4\r\n[pip] torch==1.5.0.dev20200113\r\n[pip] torchvision==0.5.0a0+e50d746\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.5.0.dev20200113 py3.6_cuda10.0.130_cudnn7.6.3_0    pytorch-nightly\r\n[conda] torchvision               0.5.0.dev20200113      py36_cu100    pytorch-nightly\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @yf225"},{"labels":[null,"api",null,null],"text":"A C++ extension calling backward() on a tensor hangs when called from python.\r\n\r\nI've posted this same issue with some back tracing [in the forum](https://discuss.pytorch.org/t/tensor-backward-called-within-c-extension-hangs/65473).\r\n\r\n## To Reproduce\r\nHere is the basic extension, following the tutorial on C++/CUDA extensions. I get the same issue if I install the extension via `load_inline`.\r\n\r\n1. Source `diff.cpp`\r\n```cpp\r\n#include <torch/extension.h>\r\n\r\nvoid backw(torch::Tensor tens) {\r\n    tens.backward({}, true, true);\r\n}\r\n\r\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\r\n    m.def(\"backw\", &backw, \"DIFF backw\");\r\n}\r\n```\r\n\r\n2. `setup.py` file\r\n```python\r\nfrom setuptools import setup, Extension\r\nfrom torch.utils import cpp_extension\r\n\r\nsetup(name='diff_cpp',\r\n      ext_modules=[cpp_extension.CppExtension('diff_cpp', ['diff.cpp'])],\r\n      cmdclass={'build_ext': cpp_extension.BuildExtension})\r\n```\r\n\r\n3. Install via `python setup.py install`\r\nThe installation proceeds without error, and the extension can subsequently be installed as `diff_cpp`. The following test script hangs at the last line:\r\n```python\r\nimport torch\r\nimport diff_cpp    # fine\r\nx = torch.tensor([1.0, 2.0], requires_grad=True)\r\ny = torch.sum(x)\r\ndiff_cpp.backw(y)  # hangs\r\n```\r\n\r\n## Environment\r\nI have so far tried this on two machines in multiple configurations, each time inside a fresh `python -m venv env` virtual environment and installing pytorch using pip.\r\n- python 3.7, torch 1.3.1, CPU\r\n- python 3.6, torch 1.3.1, CUDA 10.1\r\n- python 3.6, nightly, CUDA 10.1\r\n\r\nHere are the details of one of the environments I've tried this on:\r\n```\r\nPyTorch version: 1.3.1+cpu\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 9.2.0\r\nCMake version: version 3.16.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.0\r\n[pip3] torch==1.3.1+cpu\r\n[pip3] torchvision==0.4.2+cpu\r\n[conda] Could not collect\r\n```\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @yf225"},{"labels":[null,"api",null],"text":"Hi,guys, i am a newer for pytorch and libtorch, and i want to compile the libtorch by source official code,but there are some problem happened.I compile the source official code by  this(https://github.com/pytorch/pytorch/blob/master/docs/libtorch.rst) guide directly(i don't compile the pytorch),but the problem happened like this:\r\n[2066/2066] Install the project...\r\n-- Install configuration: \"Release\"\r\nCMake Error at third_party/protobuf/cmake/cmake_install.cmake:48 (file):\r\n  file INSTALL cannot find\r\n  \"/home/cyj/Documents/pytorch-test/build_libtorch/build/third_party/protobuf/cmake/CMakeFiles/CMakeRelink.dir/protoc\".\r\nCall Stack (most recent call first):\r\n  cmake_install.cmake:80 (include)\r\n  \r\nFAILED: cd /home/cyj/Documents/pytorch-test/build_libtorch/build && /usr/bin/cmake -P cmake_install.cmake\r\nninja: build stopped: subcommand failed.\r\nTraceback (most recent call last):\r\n  File \"../tools/build_libtorch.py\", line 23, in <module>\r\n    rerun_cmake=True, cmake_only=False, cmake=CMake())\r\n  File \"/home/cyj/Documents/pytorch-test/tools/build_pytorch_libs.py\", line 62, in build_caffe2\r\n    cmake.build(my_env)\r\n  File \"/home/cyj/Documents/pytorch-test/tools/setup_helpers/cmake.py\", line 339, in build\r\n    self.run(build_args, my_env)\r\n  File \"/home/cyj/Documents/pytorch-test/tools/setup_helpers/cmake.py\", line 141, in run\r\n    check_call(command, cwd=self.build_dir, env=env)\r\n  File \"/usr/lib/python3.5/subprocess.py\", line 581, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '8']' returned non-zero exit status 1\r\nI want to know the reason why i met such problem,Does any deploy i have not set? I just follow the official guide!My environment is ubuntu16.04 cuda10.0 gcc 5.4.0 cmake 3.5.1,the source code is newest.Hope for help,tkx.\n\ncc @ezyang @yf225"},{"labels":[null,"api",null],"text":"## üöÄ Feature\r\nThe `setup.py` should emit the CPP version being used for the compilation along with things like `USE_CUDA`, etc. \r\n\r\n## Motivation\r\n\r\nI recently ran into an issue where I got an error stating that pytorch requires atleast C++14 for compilation. Although I managed to compile pytorch after some changes, it was hard to figure out how exactly it is being compiled since I had to browse the compilation output to check the `-std=c++14` flag being passed to gcc.\r\n\r\n## Pitch\r\n\r\nThere should be a `CPP_VERSION: <version number>` in the compilation summary.\r\n\r\n## Alternatives\r\n\r\nMaybe output the library versions used in some text file for easy reference.\r\n\n\ncc @yf225"},{"labels":[null,"api",null,null],"text":"## üöÄ Feature\r\nParallelization: more balanced work distribution among workers\r\n\r\n## Motivation\r\n\r\nI recently checked the code for `at::parallel_for` method and this is what I observed.\r\nSuppose there are `N` indices and `T` workers, then worker `i (0 <= i < T)` receives\r\nindices `[i * ceil(N/T), min(N, (i+1) * ceil(N/T)) )`.\r\n\r\nNow, suppose `N=10, T=4`, then this scheme will have the following distribution : `3|3|3|1`.\r\nDefinitely, something like `3|3|2|2` looks better. The issues is that the current approach is biased against the last worker(s) - it (them) always receive(s) the least work, which **could be even zero**. To elaborate, the whole situation becomes even worse if, for example, `N=100, T=40`. Then work of size 3 is scheduled for each worker, meaning that only 34 workers are going to do something useful, while 6 workers do nothing. And, in general, the situation gets worse if the number of workers scales up with the size of an input.\r\n\r\n## Pitch\r\nWhat about a slightly different distribution? This one **uses all the workers**!\r\nFor a worker `i: 0 <= i < T` let\r\n```\r\nbegin(i) = ceil(N*i/T),\r\nend(i) = begin(i+1),\r\n```\r\nand the worker receives indices `[begin(i), end(i) )`\r\n\r\nIt can be shown that for any `N >= T >= 1`, any `i,j: 0 <= i,j < T`:\r\n```\r\n|(end(i) - begin(i)) - (end(j) - begin(j))| <= 1, \r\nend(i) - begin(i) >= 1, begin(i) < N,\r\n```\r\nso this new scheme is optimally balanced, and each worker will do at least something!\r\n\r\nThe only issue I see is a more likely chance of overflow in computing `begin`\r\n\r\ncc @yf225 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"},{"labels":[null,"api",null,null],"text":"## üêõ Bug\r\n\r\nIt looks like the modules attempt to register undefined tensors without turning off _requires_grad_ and cause a warning when _affine=false_  (default condition for InstanceNorm)\r\n\r\n## To Reproduce\r\n```\r\ntorch::nn::InstanceNorm2d m(64);\r\n\r\nWarning: An undefined tensor cannot require grad. Ignoring the `requires_grad=true` function parameter. (register_parameter at /pytorch/torch/csrc/api/src/nn/module.cpp:316)\r\n```\r\n## Expected behavior\r\n\r\nThere's logic in _nn/modules/batchnorm.h_ that checks the value of affine before attempting to register weight & bias:\r\n```\r\nvoid reset() override {\r\n    if (options.affine()) {\r\n      weight = this->register_parameter(\"weight\", torch::empty({options.num_features()}));\r\n      bias = this->register_parameter(\"bias\", torch::empty({options.num_features()}));\r\n    } else {\r\n      weight = this->register_parameter(\"weight\", Tensor());\r\n      bias = this->register_parameter(\"bias\", Tensor());\r\n    }\r\n```\r\nWhen _affine_ is false, I think the 3rd arg to _register_parameter_ should be false to turn off _requires_grad_ property, or perhaps these parameters need not be registered at all..?\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @yf225"},{"labels":["api",null],"text":"I have notice that `batchnorm.h` in macOS is not the same as the Linux's version.\r\n\r\nIn macOS version, the `batchnorm.h` is \r\n\r\n```\r\n#pragma once\r\n\r\n#include <torch/nn/cloneable.h>\r\n#include <torch/nn/options/batchnorm.h>\r\n#include <torch/nn/pimpl.h>\r\n#include <torch/types.h>\r\n\r\n#include <cstdint>\r\n\r\nnamespace torch {\r\nnamespace nn {\r\n\r\n/// Applies [Batch Normalization](https://arxiv.org/abs/1502.03167) to an input.\r\n///\r\n/// Refer to the documentation for\r\n/// [`BatchNorm1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d)\r\n/// in PyTorch to learn more about the exact semantics of this module, __but see\r\n/// the note below regarding differences between the Python and C++ API__.\r\n///\r\n/// \\rst\r\n/// .. attention::\r\n///   In the Python API, there are separate implementations for 1-D, 2-D and 3-D\r\n///   BatchNorm. In C++, there is only one `BatchNorm` module, which works for\r\n///   any of these dimensions.\r\n/// \\endrst\r\nclass TORCH_API BatchNormImpl : public torch::nn::Cloneable<BatchNormImpl> {\r\n public:\r\n  explicit BatchNormImpl(int64_t features)\r\n      : BatchNormImpl(BatchNormOptions(features)) {}\r\n  explicit BatchNormImpl(const BatchNormOptions& options_);\r\n\r\n  void reset() override;\r\n\r\n  /// Pretty prints the `BatchNorm` module into the given `stream`.\r\n  void pretty_print(std::ostream& stream) const override;\r\n\r\n  /// Applies batch normalization on the `input` using the stored mean and\r\n  /// variance.\r\n  ///\r\n  /// The module must be constructed with `stateful = true` when calling this\r\n  /// method, as the module will otherwise not store running statistics. If you\r\n  /// want to supply the mean and variance yourself, use `pure_forward`.\r\n  Tensor forward(const Tensor& input);\r\n\r\n  /// Applies batch normalization on the `input` using the given `mean` and\r\n  /// `variance` statistics.\r\n  Tensor pure_forward(\r\n      const Tensor& input,\r\n      const Tensor& mean,\r\n      const Tensor& variance);\r\n\r\n  /// The options with which this module was constructed.\r\n  BatchNormOptions options;\r\n\r\n  /// The learned weight.\r\n  /// Only defined if the `affine` option was `true` upon construction.\r\n  Tensor weight;\r\n\r\n  /// The learned bias.\r\n  /// Only defined if the `affine` option was `true` upon construction.\r\n  Tensor bias;\r\n\r\n  /// The running mean.\r\n  /// Only defined if the `stateful` option was `true` upon construction.\r\n  Tensor running_mean;\r\n\r\n  /// The running variance.\r\n  /// Only defined if the `stateful` option was `true` upon construction.\r\n  Tensor running_var;\r\n};\r\n\r\n/// A `ModuleHolder` subclass for `BatchNormImpl`.\r\n/// See the documentation for `BatchNormImpl` class to learn what methods it\r\n/// provides, or the documentation for `ModuleHolder` to learn about PyTorch's\r\n/// module storage semantics.\r\nTORCH_MODULE(BatchNorm);\r\n\r\n} // namespace nn\r\n} // namespace torch\r\n\r\n```\r\n\r\nBut in the Linux's version seems having more contents than macOS's version.\r\n```\r\n#pragma once\r\n\r\n#include <torch/arg.h>\r\n#include <torch/csrc/WindowsTorchApiMacro.h>\r\n#include <torch/nn/options/common.h>\r\n#include <torch/types.h>\r\n\r\nnamespace torch {\r\nnamespace nn {\r\n\r\n/// Options for the `BatchNorm` module.\r\nstruct TORCH_API BatchNormOptions {\r\n  /* implicit */ BatchNormOptions(int64_t num_features);\r\n\r\n  /// The number of features of the input tensor.\r\n  /// Changing this parameter after construction __has no effect__.\r\n  TORCH_ARG(int64_t, num_features);\r\n\r\n  /// The epsilon value added for numerical stability.\r\n  /// Changing this parameter after construction __is effective__.\r\n  TORCH_ARG(double, eps) = 1e-5;\r\n\r\n  /// A momentum multiplier for the mean and variance.\r\n  /// Changing this parameter after construction __is effective__.\r\n  TORCH_ARG(c10::optional<double>, momentum) = 0.1;\r\n\r\n  /// Whether to learn a scale and bias that are applied in an affine\r\n  /// transformation on the input.\r\n  /// Changing this parameter after construction __has no effect__.\r\n  TORCH_ARG(bool, affine) = true;\r\n\r\n  /// Whether to store and update batch statistics (mean and variance) in the\r\n  /// module.\r\n  /// Changing this parameter after construction __has no effect__.\r\n  TORCH_ARG(bool, track_running_stats) = true;\r\n};\r\n\r\nusing BatchNorm1dOptions = BatchNormOptions;\r\nusing BatchNorm2dOptions = BatchNormOptions;\r\nusing BatchNorm3dOptions = BatchNormOptions;\r\n\r\n// ============================================================================\r\n\r\nnamespace functional {\r\n\r\n/// Options for the `BatchNorm` functional.\r\nstruct TORCH_API BatchNormFuncOptions {\r\n  TORCH_ARG(Tensor, weight) = Tensor();\r\n\r\n  TORCH_ARG(Tensor, bias) = Tensor();\r\n\r\n  TORCH_ARG(bool, training) = false;\r\n\r\n  /// A momentum multiplier for the mean and variance.\r\n  /// Changing this parameter after construction __is effective__.\r\n  TORCH_ARG(c10::optional<double>, momentum) = 0.1;\r\n\r\n  /// The epsilon value added for numerical stability.\r\n  /// Changing this parameter after construction __is effective__.\r\n  TORCH_ARG(double, eps) = 1e-5;\r\n};\r\n\r\n} // namespace functional\r\n\r\n} // namespace nn\r\n} // namespace torch\r\n\r\n```\r\n\n\ncc @yf225"},{"labels":["api",null,null,null,null],"text":"## üêõ Bug\r\n\r\nMy program is suffering from a weird bug. It works well with -O0 optimization but crashes with any higher optimization level (-O, -O2 and -O3).\r\n\r\n## To Reproduce\r\n\r\nMy program load the program with the following code\r\n```\r\n    torch::jit::script::Module module;\r\n    try\r\n    {\r\n        // Deserialize the ScriptModule from a file using torch::jit::load().\r\n        module = torch::jit::load(\"../c_model.th\");\r\n    }\r\n    catch (const c10::Error& e)\r\n    {\r\n        std::cerr << \"error loading the model\\n\";\r\n        return -1;\r\n    }\r\n```\r\n\r\nThen pass it by reference to another function, and work about it with the following code\r\n\r\n```\r\n    torch::IntArrayRef size{1,1,64,256};\r\n    at::TensorOptions options(torch::kFloat32);\r\n    auto tensor = torch::empty(size, options);\r\n    //Sadly we have to write the loop to set the value by ourselves.\r\n    auto accessor = tensor.accessor<float, 4>();\r\n    for(size_t i = 0; const auto &frame : mag)\r\n    {\r\n        for(size_t j = 0; const auto mag_entry : frame)\r\n        {\r\n            accessor[0][0][i][j] = mag_entry;\r\n            j++;\r\n        }\r\n        i++;\r\n    }\r\n\r\n    auto result = module.forward({tensor}).toTensor();\r\n    auto result_accessor = result.accessor<float, 2>();\r\n    float x = result_accessor[0][0];\r\n    float y = result_accessor[0][1];\r\n```\r\n\r\nHowever program crashes with the following info printed to console in -O1 or higher optimization level.\r\n\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  [enforce fail at CPUAllocator.cpp:47] ((ptrdiff_t)nbytes) >= 0. alloc_cpu() seems to have been called with negative number: 10177264015417475072\r\nframe #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*) + 0x6a (0x7ff247458fea in /home/liu/software/pytorch/torch/lib/libc10.so)\r\nframe #1: c10::alloc_cpu(unsigned long) + 0x4f0 (0x7ff2474403f0 in /home/liu/software/pytorch/torch/lib/libc10.so)\r\nframe #2: <unknown function> + 0x179ea (0x7ff2474419ea in /home/liu/software/pytorch/torch/lib/libc10.so)\r\nframe #3: at::native::empty_cpu(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x10b (0x7ff2483c174b in /home/liu/software/pytorch/torch/lib/libtorch.so)\r\nframe #4: <unknown function> + 0x11c0ba0 (0x7ff248632ba0 in /home/liu/software/pytorch/torch/lib/libtorch.so)\r\nframe #5: <unknown function> + 0x11c9dd7 (0x7ff24863bdd7 in /home/liu/software/pytorch/torch/lib/libtorch.so)\r\nframe #6: <unknown function> + 0x16a31 (0x5648e8b7da31 in /home/liu/work/PyTorchCppTest/cmake-build-debug/PyTorchCppTest)\r\nframe #7: <unknown function> + 0x851e (0x5648e8b6f51e in /home/liu/work/PyTorchCppTest/cmake-build-debug/PyTorchCppTest)\r\nframe #8: <unknown function> + 0xa473 (0x5648e8b71473 in /home/liu/work/PyTorchCppTest/cmake-build-debug/PyTorchCppTest)\r\nframe #9: __libc_start_main + 0xeb (0x7ff245714b6b in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #10: <unknown function> + 0x79aa (0x5648e8b6e9aa in /home/liu/work/PyTorchCppTest/cmake-build-debug/PyTorchCppTest)\r\n```\r\n\r\nI used the debugger to trace down the control flow, it looks like program crashes in ```torch::empty``` function.\r\n\r\n## Expected behavior\r\n\r\nRun correctly in higher optimization level.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0a0+18ec463\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 19.04\r\nGCC version: (Ubuntu 9.2.1-9ubuntu2) 9.2.1 20191008\r\nCMake version: version 3.13.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.4\r\n[pip3] torchsummary==1.5.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] torch                     1.4.0a0+18ec463          pypi_0    pypi\r\n## Additional context\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @yf225"},{"labels":[null,"api",null,null],"text":"## üöÄ Feature\r\nBuild `libtorch` with AMD support. \r\n\r\n## Motivation\r\n\r\nI will be working on a new machine that has AMD GPUs, and we would like to avoid the overhead of Python.\r\n\r\n## Pitch\r\n\r\nIf possible, I would like to contribute to this effort. I would appreciate any guidance in that process.\r\n\r\n## Alternatives\r\n\r\nThere could be a binary dump and all associated CMAKE files like the current release of libtorch, but I'd prefer to be able to cook it up myself.\r\n\r\n## Additional context\r\n\r\nThis would be a great step in enabling Pytorch to run on the next generation of supercomputers. :wink:\r\n\n\ncc @yf225"},{"labels":[null,null,"api",null,null],"text":"## üêõ Bug\r\n\r\nAfter the cmake is done, when compiling .cu files in my project, the following error will occur:\r\n```bash\r\nnvcc fatal   : Unknown option 'Wall'\r\n```\r\nThe `flags.make` shows the following result:\r\n```bash\r\n$cat CMakeFiles/train.dir/flags.make\r\n# CMAKE generated file: DO NOT EDIT!\r\n# Generated by \"Unix Makefiles\" Generator, CMake Version 3.15\r\n\r\n# compile CUDA with /usr/local/cuda-9.2/bin/nvcc\r\n# compile CXX with /usr/bin/c++\r\nCUDA_FLAGS =  -O3   -D_GLIBCXX_USE_CXX11_ABI=1 -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-missing-braces -fopenmp -std=c++14\r\n\r\nCUDA_DEFINES = -DAT_PARALLEL_OPENMP=1\r\n......\r\n```\r\nThe `-O3` flag was added using `CMAKE_CUDA_FLAGS` in the `CMakeLists.txt`\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Write a .cu file in the project\r\n2. cmake then make\r\n3. The error occurs\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nThose -Wxxx flags should not be added as cuda flags.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): Compile from source\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source):\r\n```bash\r\nBUILD_TORCH=ON \\\r\nCMAKE_PREFIX_PATH=\"/usr/bin/;/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64/;/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/;/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/include/\" \\\r\nCUDA_BIN_PATH=/usr/local/cuda/bin \\\r\nCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda/ \\\r\nCUDNN_LIB_DIR=/usr/local/cuda/lib64 \\\r\nUSE_CUDA=1 \\\r\nUSE_NNPACK=1 \\\r\nMAX_JOBS=8 \\\r\npython3 setup.py build\r\n```\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 9.2/7.5\r\n - GPU models and configuration: NVIDIA GTX Titan\r\n - Any other relevant information:\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @malfet @yf225 @glaringlee @ngimel"},{"labels":[null,"api",null],"text":"## üìö Documentation\r\n\r\nDownload libtorch from website, old program using libtorch can not link now:\r\n\r\n```\r\nlibtorch/include/c10/util/C++17.h:20:2: error: #error You need C++14 to compile PyTorch\r\n #error You need C++14 to compile PyTorch\r\n\r\n```\r\n\r\nWhile I have already set C++14 in cmakelists:\r\n\r\n```\r\n\r\nset(CMAKE_CXX_STANDARD 14)\r\nadd_definitions(-std=c++14)\r\n```\r\n\r\nBut keep get such error.\n\ncc @yf225"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\nI'm sharing a minor error. \r\nSteps to reproduce the behavior:\r\n\r\n```c++\r\n/*\r\nimages[index] is C * H * W  : 3 *480*640  2D Image tensor\r\nWhen calling interpolation, unknowexception occurs if the data type is not Kfloat.\r\n*/\r\nimages[index] = F::interpolate(images[index], F::InterpolateFuncOptions().scale_factor({ scale_factor }).mode(torch::kLinear).align_corners(false));\r\nimages[index] = normalizeChannels(images[index]);\r\n\r\n/*\r\nIf change data type, it works normally\r\nimg_tensor = img_tensor.toType(at::kFloat);\t\t\r\n*/\r\n```\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): Libtorch Nightly build date : 19.11.22\r\n - OS (e.g., Linux): Windows 10\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version: 10.2 , 7.5\r\n - GPU models and configuration: 2080TI\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @yf225"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\nRunning the c++ example fails when running the forward calculation on the generator:\r\n\r\n`what():  Calculated padded input size per channel: (1 x 1). Kernel size: (4 x 4). Kernel size can't be greater than actual input size (check_shape_forward at /pytorch/aten/src/ATen/native/Convolution.cpp:436)`\r\n\r\nIs this due to recent changes to use NNPACK for strided convolutions..?\r\n\r\n(There's also a warning about batchnorm being deprecated,\r\nbut that's a simpler fix.)\r\n\r\nThanks\n\ncc @yf225"},{"labels":[null,"api",null,null,null],"text":"## üêõ Bug\r\n\r\nWhen trying to compile with VS2019 but targeting v141 toolchain and cuda 10.0 there seems to be some issues with rewrite of dispatching that was done in recent weeks.\r\n\r\nThis snippet was from building using facebook internal Buck, but I was able to reproduce same thing using cmake.\r\n\r\nCompiling with /permissive- fixed the \"no matching overload\" error, but raises some other cuda errors related to `__nv_hdl_create_wrapper_t`\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. checkout pytorch master\r\n1. pass v141 as toolchain to cmake when generating\r\n1. set cuda sdk root to 10.0\r\n1. try and build torch_cuda\r\n\r\nError without /permissive-\r\n```\r\ncaffe2\\aten\\src\\aten\\core\\boxing\\KernelFunction_impl.h(67): error C2672: 'c10::impl::boxAndCallBoxedFunc': no matching overloaded function found\r\ncaffe2/aten/src\\ATen/core/dispatch/Dispatcher.h(180): note: see reference to function template instantiation 'Return c10::KernelFunction::callUnboxed<Return,const at::Tensor&,const at::Tensor&,bool,bool>(const c10::OperatorHandle &,const at::Tensor &,const at::Tensor &,bool,bool) const' being compiled\r\n        with\r\n        [\r\n            Return=void\r\n        ]\r\ncaffe2/aten/src\\ATen/core/dispatch/Dispatcher.h(155): note: see reference to function template instantiation 'Return c10::Dispatcher::callUnboxed<Return,const at::Tensor&,const at::Tensor&,bool,bool>(const c10::OperatorHandle &,const at::Tensor &,const at::Tensor &,bool,bool) const' being compiled\r\n        with\r\n        [\r\n            Return=void\r\n        ]\r\ncaffe2\\ATen/core/TensorMethods.h(66): note: see reference to function template instantiation 'Return c10::OperatorHandle::callUnboxed<void,const at::Tensor&,const at::Tensor&,bool,bool>(const at::Tensor &,const at::Tensor &,bool,bool) const' being compiled\r\n        with\r\n        [\r\n            Return=void\r\n        ]\r\ncaffe2\\aten\\src\\aten\\core\\boxing\\KernelFunction_impl.h(67): error C2770: invalid explicit template argument(s) for 'Result c10::impl::boxAndCallBoxedFunc(c10::KernelFunction::InternalBoxedKernelFunction (__cdecl *),c10::OperatorKernel *,const c10::OperatorHandle &,Args...,enable_if<_Test,_Ty>::type)'\r\n        with\r\n        [\r\n            _Ty=int\r\n        ]\r\ncaffe2/aten/src\\ATen/core/boxing/boxing.h(53): note: see declaration of 'c10::impl::boxAndCallBoxedFunc'\r\ncaffe2\\aten\\src\\aten\\core\\boxing\\KernelFunction_impl.h(67): error C2893: Failed to specialize function template 'Result c10::impl::boxAndCallBoxedFunc(c10::KernelFunction::InternalBoxedKernelFunction (__cdecl *),c10::OperatorKernel *,const c10::OperatorHandle &,Args...,enable_if<_Test,_Ty>::type)'\r\n        with\r\n        [\r\n            _Ty=int\r\n        ]\r\ncaffe2\\aten\\src\\aten\\core\\boxing\\KernelFunction_impl.h(67): note: With the following template arguments:\r\ncaffe2\\aten\\src\\aten\\core\\boxing\\KernelFunction_impl.h(67): note: 'Result=Return'\r\ncaffe2\\aten\\src\\aten\\core\\boxing\\KernelFunction_impl.h(67): note: 'Args={}'\r\n```\r\nError with /permissive-\r\n```\r\nnvcc_internal_extended_lambda_implementation(542): error C3861: 'args': identifier not found\r\ncaffe2/aten/src/ATen/native/cuda/Copy.cu(62): note: see reference to class template instantiation '__nv_hdl_create_wrapper_t<false,true,__nv_dl_tag<void (__cdecl *)(at::TensorIterator &,bool),at::native::copy_device_to_device,1>>' being compiled\r\n```\r\n\r\n## Expected behavior\r\n\r\nfiles compile\r\n\n\ncc @ezyang @gchanan @zou3519 @yf225 @peterjc123"},{"labels":["api",null],"text":"As part of pull request #28790,\r\nF is redefined globally in nn/modules/batchnorm.h\r\n\r\n`namespace F = torch::nn::functional;`\r\n\r\nF is then used once in batchnorm.h and again in instancenorm.h\r\n\r\nnot a bug, but a problem for anyone using F in some other way.\r\nit's solvable, but I don't see this pattern anywhere else in libtorch,\r\nwas wondering if this use is an anomaly,\r\nand if we could use\r\n\r\n```\r\nreturn torch::nn::functional::detail::batch_norm(\r\nreturn torch::nn::functional::detail::instance_norm(\r\n```\r\nthanks\r\n\n\ncc @yf225"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\n## To Reproduce\r\n - Libtorch Version (1.3.1 stable debug version):\r\n - OS (windows 10):\r\n - How you installed Libtorch ( source):\r\n - Build command you used (visual stdio):\r\n - cpp version:c++11\r\n - cpu version:\r\n\r\n## Additional context\r\nacording to  libtorch documentation FUNCTION TORCH::NN::FUNCTIONAL::PAD should be \r\nin this directory (\"torch/csrc/api/include/torch/nn/functional/\") but   some header files such as  padding.h and ... dont exist\r\n\n\ncc @yf225"},{"labels":[null,"api",null],"text":"## üêõ Bug\r\n\r\nIf a Linear or Conv module is constructed without bias, a subsequent move to another device fails\r\n\r\n## To Reproduce\r\n\r\n```cpp\r\n#include <torch/torch.h>\r\n\r\nint main(int argc, char** argv) {\r\n  torch::nn::Linear test(torch::nn::LinearOptions(10,20).bias(false));\r\n  test->to(torch::kCUDA);\r\n  return 0;\r\n}\r\n```\r\n\r\n\r\n<details><summary>Stack trace</summary>\r\n<p>\r\n\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  tensor does not have a device (device at /opt/pytorch/c10/core/TensorImpl.h:463)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6a (0x7fffe51c9b1a in /usr/local/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0x1fd83a6 (0x7fffe73ba3a6 in /usr/local/torch/lib/libtorch.so)\r\nframe #2: at::native::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) + 0xb36 (0x7fffe8c6b1f6 in /usr/local/torch/lib/libtorch.so)\r\nframe #3: <unknown function> + 0x3c22636 (0x7fffe9004636 in /usr/local/torch/lib/libtorch.so)\r\nframe #4: <unknown function> + 0x5882857 (0x7fffeac64857 in /usr/local/torch/lib/libtorch.so)\r\nframe #5: <unknown function> + 0x3c89f2f (0x7fffe906bf2f in /usr/local/torch/lib/libtorch.so)\r\nframe #6: <unknown function> + 0x25e1277 (0x7fffe79c3277 in /usr/local/torch/lib/libtorch.so)\r\nframe #7: void torch::nn::Module::to_impl<c10::Device&, bool&>(c10::Device&, bool&) + 0x196 (0x7fffeb3a8266 in /usr/local/torch/lib/libtorch.so)\r\nframe #8: torch::nn::Module::to(c10::Device, bool) + 0x18 (0x7fffeb3a2fe8 in /usr/local/torch/lib/libtorch.so)\r\nframe #9: main + 0xba (0x4052e8 in /home/tobi/coar_ws/devel/lib/dgcnn/dev_stuff)\r\nframe #10: __libc_start_main + 0xf0 (0x7fffe4855830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #11: _start + 0x29 (0x405049 in /home/tobi/coar_ws/devel/lib/dgcnn/dev_stuff)\r\n\r\n\r\nThread 1 \"dev_stuff\" received signal SIGABRT, Aborted.\r\n0x00007fffe486a428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n54      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n(gdb) backtrace\r\n#0  0x00007fffe486a428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n#1  0x00007fffe486c02a in __GI_abort () at abort.c:89\r\n#2  0x00007fffe4ea484d in __gnu_cxx::__verbose_terminate_handler() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#3  0x00007fffe4ea26b6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#4  0x00007fffe4ea2701 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#5  0x00007fffe4ea2919 in __cxa_throw () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#6  0x00007fffe73ba3fc in at::Tensor::options() const () from /usr/local/torch/lib/libtorch.so\r\n#7  0x00007fffe8c6b1f6 in at::native::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /usr/local/torch/lib/libtorch.so\r\n#8  0x00007fffe9004636 in at::TypeDefault::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /usr/local/torch/lib/libtorch.so\r\n#9  0x00007fffeac64857 in torch::autograd::VariableType::(anonymous namespace)::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /usr/local/torch/lib/libtorch.so\r\n#10 0x00007fffe906bf2f in c10::detail::wrap_kernel_functor_unboxed_<c10::detail::WrapRuntimeKernelFunctor_<at::Tensor (*)(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat> > >, at::Tensor (at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>)>::call(c10::OperatorKernel*, at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /usr/local/torch/lib/libtorch.so\r\n#11 0x00007fffe79c3277 in at::Tensor::to(c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) const () from /usr/local/torch/lib/libtorch.so\r\n#12 0x00007fffeb3a8266 in void torch::nn::Module::to_impl<c10::Device&, bool&>(c10::Device&, bool&) () from /usr/local/torch/lib/libtorch.so\r\n#13 0x00007fffeb3a2fe8 in torch::nn::Module::to(c10::Device, bool) [clone .localalias.489] () from /usr/local/torch/lib/libtorch.so\r\n#14 0x00000000004052e8 in main (argc=1, argv=0x7fffffffccc8) at /home/tobi/coar_ws/src/dgcnn/src/dev_stuff.cpp:5\r\n```\r\n</p>\r\n</details>\r\n\r\n## Expected behavior\r\n\r\nGetting no error.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nCollecting environment information...\r\nPyTorch version: 1.4.0a0+829499e\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.15.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Quadro RTX 6000\r\nGPU 1: Quadro RTX 6000\r\n\r\nNvidia driver version: 418.87.00\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.4\r\n[pip3] torch==1.4.0a0+94016b1\r\n[conda] Could not collect\r\n\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4.0a0+829499e\r\n - OS (e.g., Linux): Ubuntu 18.04.3 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): compiled from source\r\n - Build command you used (if compiling from source): ```CFLAGS=' -D_GLICXX_USE_CXX11_ABI ' USE_OPENCV=1 USE_CUDA=1 MAX_JOBS=7 BUILD_TEST=0 python3 setup.py install ```\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.5\r\n - GPU models and configuration: 2x Quadro RTX 6000\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @yf225"},{"labels":["api",null,null],"text":"## üêõ Bug\r\n\r\nHi, maybe I found a bug in LibTorch while run codes in CentOS, the codes are very simplify. I have try it in 2 CentOS Machines and 1 Ubuntu Machine, Ubuntu Machine can run it normally, but all of the CentOS Machines run it like follow outputs.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. download libtorch and unzip\r\n2. compile the cpp file\r\n3. run\r\n\r\nHere are the codes:\r\n```c++\r\n#include<iostream>\r\n#include<torch/torch.h>\r\n#include <torch/script.h>\r\n\r\nint main() {\r\n\ttorch::Tensor a = torch::ones({2,4});\r\n\tstd::cout << a << std::endl;\r\n\treturn 0;\r\n}\r\n```\r\n\r\nHere is the CMakeList:\r\n```cmake\r\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\r\nproject(tr)\r\n\r\nfind_package(Torch REQUIRED)\r\naux_source_directory(. SRC_LIST)\r\nadd_executable(test ${SRC_LIST})\r\nset(CMAKE_CXX_STANDARD 11)\r\ntarget_link_libraries(test \"${TORCH_LIBRARIES}\")\r\nset_property(TARGET test PROPERTY CXX_STANDARD 11)\r\n```\r\n\r\nHere is the output:\r\n```\r\n 1  1  1  1\r\n 1  1  1  1\r\n[ Variable[CPUFloatType]{2,4} ]\r\n*** Error in `./test': free(): invalid pointer: 0x00007f040f871090 ***\r\n======= Backtrace: =========\r\n/lib64/libc.so.6(+0x81679)[0x7f04022b3679]\r\n./test(_ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEE10deallocateEPS3_m+0x20)[0x41fd0e]\r\n./test(_ZNSt10_HashtableISsSt4pairIKSsSsESaIS2_ENSt8__detail10_Select1stESt8equal_toISsESt4hashISsENS4_18_Mod_range_hashingENS4_20_Default_ranged_hashENS4_20_Prime_rehash_policyENS4_17_Hashtable_traitsILb1ELb0ELb1EEEE21_M_deallocate_bucketsEPPNS4_15_Hash_node_baseEm+0x49)[0x41f20f]\r\n./test(_ZNSt10_HashtableISsSt4pairIKSsSsESaIS2_ENSt8__detail10_Select1stESt8equal_toISsESt4hashISsENS4_18_Mod_range_hashingENS4_20_Default_ranged_hashENS4_20_Prime_rehash_policyENS4_17_Hashtable_traitsILb1ELb0ELb1EEEED1Ev+0x36)[0x41df10]\r\n/lib64/libc.so.6(__cxa_finalize+0x9a)[0x7f040226c00a]\r\n/home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libtorch.so(+0x9fa9d3)[0x7f040367e9d3]\r\n======= Memory map: ========\r\n00400000-00429000 r-xp 00000000 fd:02 186000054                          /home/yyuser/nlp/codes/bd-nlp/tr/alg-service/build/test\r\n00628000-00629000 r--p 00028000 fd:02 186000054                          /home/yyuser/nlp/codes/bd-nlp/tr/alg-service/build/test\r\n00629000-0062a000 rw-p 00029000 fd:02 186000054                          /home/yyuser/nlp/codes/bd-nlp/tr/alg-service/build/test\r\n021b1000-02aee000 rw-p 00000000 00:00 0                                  [heap]\r\n7f03fc000000-7f03fc021000 rw-p 00000000 00:00 0 \r\n7f03fc021000-7f0400000000 ---p 00000000 00:00 0 \r\n7f04018fa000-7f04019fb000 r-xp 00000000 fd:02 1548                       /usr/lib64/libm-2.17.so\r\n7f04019fb000-7f0401bfa000 ---p 00101000 fd:02 1548                       /usr/lib64/libm-2.17.so\r\n7f0401bfa000-7f0401bfb000 r--p 00100000 fd:02 1548                       /usr/lib64/libm-2.17.so\r\n7f0401bfb000-7f0401bfc000 rw-p 00101000 fd:02 1548                       /usr/lib64/libm-2.17.so\r\n7f0401bfc000-7f0401bfe000 r-xp 00000000 fd:02 1546                       /usr/lib64/libdl-2.17.so\r\n7f0401bfe000-7f0401dfe000 ---p 00002000 fd:02 1546                       /usr/lib64/libdl-2.17.so\r\n7f0401dfe000-7f0401dff000 r--p 00002000 fd:02 1546                       /usr/lib64/libdl-2.17.so\r\n7f0401dff000-7f0401e00000 rw-p 00003000 fd:02 1546                       /usr/lib64/libdl-2.17.so\r\n7f0401e00000-7f0401e07000 r-xp 00000000 fd:02 1570                       /usr/lib64/librt-2.17.so\r\n7f0401e07000-7f0402006000 ---p 00007000 fd:02 1570                       /usr/lib64/librt-2.17.so\r\n7f0402006000-7f0402007000 r--p 00006000 fd:02 1570                       /usr/lib64/librt-2.17.so\r\n7f0402007000-7f0402008000 rw-p 00007000 fd:02 1570                       /usr/lib64/librt-2.17.so\r\n7f0402008000-7f040202d000 r-xp 00000000 fd:02 352611484                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libgomp-7c85b1e2.so.1\r\n7f040202d000-7f040222c000 ---p 00025000 fd:02 352611484                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libgomp-7c85b1e2.so.1\r\n7f040222c000-7f040222d000 r--p 00024000 fd:02 352611484                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libgomp-7c85b1e2.so.1\r\n7f040222d000-7f0402232000 rw-p 00025000 fd:02 352611484                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libgomp-7c85b1e2.so.1\r\n7f0402232000-7f04023f5000 r-xp 00000000 fd:02 1540                       /usr/lib64/libc-2.17.so\r\n7f04023f5000-7f04025f5000 ---p 001c3000 fd:02 1540                       /usr/lib64/libc-2.17.so\r\n7f04025f5000-7f04025f9000 r--p 001c3000 fd:02 1540                       /usr/lib64/libc-2.17.so\r\n7f04025f9000-7f04025fb000 rw-p 001c7000 fd:02 1540                       /usr/lib64/libc-2.17.so\r\n7f04025fb000-7f0402600000 rw-p 00000000 00:00 0 \r\n7f0402600000-7f0402615000 r-xp 00000000 fd:02 1311741                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1\r\n7f0402615000-7f0402814000 ---p 00015000 fd:02 1311741                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1\r\n7f0402814000-7f0402815000 r--p 00014000 fd:02 1311741                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1\r\n7f0402815000-7f0402816000 rw-p 00015000 fd:02 1311741                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1\r\n7f0402816000-7f040282d000 r-xp 00000000 fd:02 1566                       /usr/lib64/libpthread-2.17.so\r\n7f040282d000-7f0402a2c000 ---p 00017000 fd:02 1566                       /usr/lib64/libpthread-2.17.so\r\n7f0402a2c000-7f0402a2d000 r--p 00016000 fd:02 1566                       /usr/lib64/libpthread-2.17.so\r\n7f0402a2d000-7f0402a2e000 rw-p 00017000 fd:02 1566                       /usr/lib64/libpthread-2.17.so\r\n7f0402a2e000-7f0402a32000 rw-p 00000000 00:00 0 \r\n7f0402a32000-7f0402a77000 r-xp 00000000 fd:02 352611503                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libc10.so\r\n7f0402a77000-7f0402c77000 ---p 00045000 fd:02 352611503                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libc10.so\r\n7f0402c77000-7f0402c78000 r--p 00045000 fd:02 352611503                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libc10.so\r\n7f0402c78000-7f0402c79000 rw-p 00046000 fd:02 352611503                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libc10.so\r\n7f0402c79000-7f0402c7a000 rw-p 00000000 00:00 0 \r\n7f0402c7a000-7f0402c84000 rw-p 0005f000 fd:02 352611503                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libc10.so\r\n7f0402c84000-7f040f47f000 r-xp 00000000 fd:02 352611487                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libtorch.so\r\n7f040f47f000-7f040f67f000 ---p 0c7fb000 fd:02 352611487                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libtorch.so\r\n7f040f67f000-7f040f75d000 r--p 0c7fb000 fd:02 352611487                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libtorch.so\r\n7f040f75d000-7f040f854000 rw-p 0c8d9000 fd:02 352611487                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libtorch.so\r\n7f040f854000-7f040f8a6000 rw-p 00000000 00:00 0 \r\n7f040f8a6000-7f040fb5e000 rw-p 0db6f000 fd:02 352611487                  /home/yyuser/nlp/codes/bd-nlp/libtorch/lib/libtorch.so\r\n7f040fb5e000-7f040fb80000 r-xp 00000000 fd:02 1533                       /usr/lib64/ld-2.17.so\r\n7f040fbe9000-7f040fbff000 rw-p 00000000 00:00 0 \r\n7f040fbff000-7f040fca1000 r--p 00000000 fd:02 9122949                    /usr/lib/libstdc++.so.6.0.26\r\n7f040fca1000-7f040fd20000 r-xp 000a2000 fd:02 9122949                    /usr/lib/libstdc++.so.6.0.26\r\n7f040fd20000-7f040fd61000 r--p 00121000 fd:02 9122949                    /usr/lib/libstdc++.so.6.0.26[1]    14117 abort      ./test\r\n```\r\n\r\n\r\n## Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3\r\n - OS (e.g., Linux): CentOS Linux release 7.7.1908 (Core) and CentOS Linux release 7.6.1810 (Core) \r\n - How you installed PyTorch (`conda`, `pip`, source): LibTorch(Pre-cxx11 ABI)\r\n - CUDA/cuDNN version: no GPU\r\n - LibTorch download url: [https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.3.1%2Bcpu.zip](https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.3.1%2Bcpu.zip)\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @yf225"},{"labels":["api",null],"text":"Hello,\r\nI was trying to code example of neural network on CUDA. I have run some examples which they were working on CPU but not on GPU. \r\n\r\nNot sure if it's a bug or my mistake.\r\n\r\nPlease note that CUDA version of [MNIST example](https://github.com/Andrej-sens/libtorch_examples_cpp/blob/master/MNIST_Example_GPU.cpp) is working properly. \r\n\r\nHow to replicate error, just change bias to false. With `bias=true` I am able to move network model to CUDA.\r\n```\r\n#include <torch/torch.h>\r\n\r\n// Define a new Module.\r\nstruct Net : torch::nn::Module {\r\n\tNet() {\r\n\t\ttorch::nn::Conv2d conv = torch::nn::Conv2d(torch::nn::Conv2dOptions(3, 16, 3)\r\n\t\t\t.bias(true)\r\n\t\t\t.stride(1)\r\n\t\t\t.padding(1)\r\n\t\t);\r\n\r\n\t\tmodule->push_back(conv);\r\n\r\n\t\tregister_module(\"Layer\",module);\r\n\t}\r\n\r\n\ttorch::Tensor forward(torch::Tensor x) {\r\n\t\t/**SOME FORWARD**/\r\n\t\treturn x;\r\n\t}\r\n\r\n\ttorch::nn::Sequential module;\r\n};\r\n\r\nint main() {\r\n\tauto net = std::make_shared<Net>();\r\n\tnet->to(at::kCUDA); // It crashes here\r\n}\r\n```\r\n\r\nIt will throw\r\n```\r\nException thrown at 0x00007FFB4A2EA839 in Object_detection.exe: Microsoft C++ exception: c10::Error at memory location 0x000000635F0FDE90.\r\nUnhandled exception at 0x00007FFB4A2EA839 in Object_detection.exe: Microsoft C++ exception: c10::Error at memory location 0x000000635F0FDE90.\r\n```\r\n\r\nThank you very much for any advise.\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @yf225"},{"labels":[null,"api",null,null],"text":"Not only C + + interface on IOS\r\nPython may also call torch on IOS\r\nThis can be used for demonstration or introduction to basic learning\r\n\r\nI made some changes when compiling. At present, most of the examples are available through\r\nIt's just that I don't know if pytorch is compatible with this pattern\r\n\r\nDownload APP:\r\n- US https://apps.apple.com/us/app/id1471351733\r\n- CN  https://apps.apple.com/cn/app/id1471351733\r\n\r\nScreenshots:\r\n- ![](https://github.com/goodclass/PythonAI/raw/master/image/torch1.jpg)\r\n- ![](https://github.com/goodclass/PythonAI/raw/master/image/torch2.jpg)\n\ncc @yf225"},{"labels":["api",null,null],"text":"## üêõ Bug\r\n\r\nLoads of errors during C++ code compilation due to problems with namespaces in LibTorch files.\r\n\r\n## What helped\r\nChanges in files:\r\n\r\n1. libtorch/include/ATen/detail/CUDAHooksInterface.h \r\n26 line:\r\n```\r\nnamespace at {\r\nusing c10::Allocator;\t///Changed manually\r\n```\r\n2. libtorch/include/ATen/core/TensorBody.h\r\n35 line: \r\n```\r\nnamespace at {\r\n using c10::Scalar; ///Changed manually\r\n```\r\n## Environment\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.3.1-14ubuntu2) 5.3.1 20160413\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: Quadro M2000\r\nNvidia driver version: 410.48\r\ncuDNN version: /usr/local/cuda-9.2/lib64/libcudnn.so.7\r\n\r\n\r\n## Additional context\r\n\r\nIt would be great not to have this bug in future versions because original file code change is not the best method of creating robust software. Maybe it is possible to prevent this issue using some commands but I don't know them.\n\ncc @yf225"},{"labels":["api"],"text":"## üêõ Bug\r\nThe following short program does not compile.\r\n\r\n    #include \"torch/torch.h\"\r\n\r\n    int main() {\r\n        torch::Tensor tensor = torch::ones({2, 2});\r\n        tensor.fill_(0); // OK\r\n        tensor = 0; // Gives compile error\r\n    }\r\n\r\n## Expected behavior\r\n\r\nI would expect both the expressions 'tensor.fill_(0)' and 'tensor = 0' to compile and that they would yield the same result.\r\n\r\nLooking at the source code 'aten/src/ATen/TensorOperators.h' the selected assignment operator is defined as follows:\r\n\r\n    inline Tensor & Tensor::operator=(Scalar v) && {\r\n        return fill_(v);\r\n    }\r\n\r\nThat is, it is only defined for rvalues on the lhs of the assignment. I do not understand the rationale behind this restriction.\r\n\r\n## Environment\r\n\r\n```\r\n - PyTorch Version: 1.3.1\r\n - OS: Linux\r\n - How you installed PyTorch:  https://download.pytorch.org/libtorch/cu101/libtorch-cxx11-abi-shared-with-deps-1.3.1.zip\r\n - Build command you used: cmake -DCMAKE_PREFIX_PATH=/absolute/path/to/libtorch .. && make\r\n```\r\nI am building the sample program using the instructions in the [c++ api guide](https://pytorch.org/cppdocs/installing.html)\r\n\n\ncc @yf225"},{"labels":["api"],"text":"I have tried to benchmark and compare a Python script, using a pretrained classification model from the torchvision library with the C++ API implementation in order to make sure it will work with my application.\r\nI have tried the following script in Python:\r\n\r\n```\r\nimport torch\r\nfrom torchvision import transforms\r\nfrom PIL import Image\r\n\r\n\r\n_IMAGE_FILENAME = \"../images/goldfish.jpg\"\r\n_MODEL_JIT_FILENAME = \"../jit_models_bin/traced_mnasnet0_5.pt\"\r\n\r\nmodel = torch.jit.load(_MODEL_JIT_FILENAME)\r\n\r\ntfm = transforms.Compose([ \r\n    transforms.Resize([ 224, 224 ]), \r\n    transforms.ToTensor(), \r\n    transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] ) \r\n    ])\r\n\r\nimage = Image.open(_IMAGE_FILENAME)\r\nimage = tfm(image)\r\nimage = image.unsqueeze(dim=0)\r\n\r\n\r\noutput = model(image)\r\noutput = torch.softmax(output, 1)\r\n\r\nprob_value, index = torch.max(output, 1)\r\n\r\nprint(\"Probability Value: \")\r\nprint(prob_value)\r\nprint(\"ImageNet Index: \")\r\nprint(index)\r\n```\r\n\r\nAnd I used the following code in C++:\r\n```\r\n#include <iostream>\r\n#include <vector>\r\n#include <string>\r\n\r\n#include <torch/torch.h>\r\n#include <torch/script.h>\r\n\r\n#include <opencv2/core.hpp>\r\n#include <opencv2/imgcodecs.hpp>\r\n#include <opencv2/highgui/highgui.hpp>\r\n#include <opencv2/imgproc.hpp>\r\n\r\n\r\nint main() {\r\n\r\n    const cv::String _IMAGE_FILENAME = \"../images/goldfish.jpg\";\r\n    const std::string _MODEL_JIT_FILENAME= \"../jit_models_bin/traced_mnasnet0_5.pt\";\r\n\r\n    cv::Mat img = cv::imread( _IMAGE_FILENAME, cv::IMREAD_UNCHANGED );\r\n    cv::Size rsz = { 224, 224 };\r\n\r\n    cv::resize( img, img, rsz, 0, 0, cv::INTER_LINEAR );\r\n    img.convertTo( img, CV_32FC3, 1/255.0 );\r\n\r\n    at::Tensor tensorImage = torch::from_blob(img.data, { 1, img.rows, img.cols, 3 }, at::kFloat);\r\n    tensorImage = tensorImage.permute({0, 3, 1, 2});\r\n\r\n    //  Normalize data\r\n    tensorImage[0][0] = tensorImage[0][0].sub(0.485).div(0.229);\r\n    tensorImage[0][1] = tensorImage[0][1].sub(0.456).div(0.224);\r\n    tensorImage[0][2] = tensorImage[0][2].sub(0.406).div(0.225);\r\n\r\n    std::vector<torch::jit::IValue> input;\r\n    input.push_back(tensorImage);\r\n\r\n    torch::jit::script::Module model = torch::jit::load( _MODEL_JIT_FILENAME );\r\n\r\n    at::Tensor output = torch::softmax(model.forward(input).toTensor(), 1);\r\n\r\n    std::tuple<at::Tensor, at::Tensor> result = torch::max(output, 1);\r\n\r\n    std::cout << \"Probability Value: \" << std::endl;\r\n    std::cout << std::get<0>(result) << std::endl;\r\n    std::cout << \"ImageNet Index\" << std::endl;\r\n    std::cout << std::get<1>(result) << std::endl;\r\n\r\n    return 0;\r\n}\r\n\r\n```\r\n\r\nThe results I get in the Python script:\r\nProbability: 0.99, Index: 1, Label: GoldFish\r\n\r\nThe results I get in C++:\r\nProbability: 0.4839, Index: 584\r\n\r\nI used PyTorch 1.3 version in Python 3.7 and the latest corresponding libtorch.\r\n\r\nAny ideas?\n\ncc @yf225"},{"labels":[null,"api",null,null,null,null],"text":"`tensor.type()` gives you a DeprecatedTypeProperties object, but we didn't actually deprecate the call.  We should do this ASAP so we can delete DeprecatedTypeProperties.\r\n\r\nCC @ezyang.\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @yf225 @SsnL"},{"labels":["api",null],"text":"`torch::tensor` currently accepts braced-init-list (such as `{{1}, {2}}`) as multidimensional input. It would be great if `at::tensor` supports the same as well.\r\n\r\nThis would involve moving `torch/csrc/api/include/torch/detail/TensorDataContainer.h` to ATen, and using it from `aten/src/ATen/templates/NativeFunctions.h`.\n\ncc @yf225"},{"labels":["api",null,null],"text":"After https://github.com/pytorch/pytorch/pull/28523 is merged, C++ `torch::tensor(scalar)` behaves the same as Python `torch.tensor(scalar)` and creates a 0-dim tensor. However, C++ `at::tensor(scalar)` still creates a 1-dim tensor, and it would be great to change the behavior of `at::tensor(scalar)` to create a 0-dim tensor.\n\ncc @yf225"},{"labels":[null,null,"api",null,null],"text":"In order to make rpc, remote and dist autograd APIs run in torch script mode, we need to provide C++ APIs of them and register them as Prim::ops. \r\n\r\nThese APIs include:\r\n\r\nrpc_sync(), rpc_async(), remote(), dist_autograd.backward()\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @yf225 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @aazzolini @xush6528"},{"labels":["api",null],"text":"Example:\r\nIn C++:\r\n```cpp\r\ntorch::tensor({1., 2., 3.}).dtype() -> double\r\n```\r\n\r\nIn Python:\r\n```python\r\n>>> torch.tensor([1., 2., 3.]).dtype\r\ntorch.float32\r\n```\n\ncc @yf225"},{"labels":[null,"api",null],"text":"Existing documentation / tutorials show only how to train a `torch::nn::Module` https://pytorch.org/cppdocs/frontend.html#end-to-end-example\r\n\r\nI have attempted to make a training loop in the following manner\r\n```\r\n#include <torch/script.h>\r\n#include <torch/torch.h>\r\n#include <iostream>\r\n#include <vector>\r\n// custom loader code\r\n#include \"nets/nets.h\"\r\n#include \"util/runfiles.h\"\r\n\r\nint main(int argc, char** argv) {\r\n  std::cout << \"Nets example\" << std::endl;\r\n\r\n  // Custom code that loads the module on CUDA\r\n  auto runfiles = MakeRunfiles(argv[0]);\r\n  torch::jit::script::Module script_module = LoadSegnetBackbone(*runfiles);\r\n  script_module.train();\r\n  std::cout << \"Loaded script module\" << std::endl;\r\n\r\n  // Pull parameters out of the script module so we can push them into the\r\n  // optimizer.\r\n  std::vector<at::Tensor> parameters;\r\n  for (const auto& parameter : script_module.get_parameters()) {\r\n    parameters.push_back(parameter.value().toTensor());\r\n  }\r\n  torch::optim::SGD optimizer(std::move(parameters), /*lr=*/0.01);\r\n\r\n  constexpr int kBatchSize = 1;\r\n  for (int epoch = 1; epoch <= 1000; ++epoch) {\r\n    optimizer.zero_grad();\r\n\r\n    // The input is a (kBatchSize,3,300,300) tensor filled with ones\r\n    at::Tensor input = torch::ones({kBatchSize, /*channels (rgb) =*/3,\r\n                                    /*height=*/300, /*width=*/300})\r\n                           .to(at::kFloat)\r\n                           .to(at::kCUDA);\r\n\r\n    // Push the input through the script module\r\n    std::vector<torch::jit::IValue> inputs;\r\n    inputs.push_back(input);\r\n    at::Tensor script_module_forward = script_module.forward(inputs).toTensor();\r\n    // The result is an output tensor of size (kBatchSize, 32, 300, 300)\r\n\r\n    // ground truth is a (kBatchSize, 300, 300) tensor filled with ones\r\n    at::Tensor ground_truth =\r\n        torch::ones({kBatchSize, /*height=*/300, /*width=*/300})\r\n            .to(at::kLong)\r\n            .to(at::kCUDA);\r\n\r\n    at::Tensor loss = torch::nll_loss2d(\r\n        torch::log_softmax(script_module_forward, /*dim=*/1), ground_truth);\r\n    loss.backward();\r\n    optimizer.step();\r\n\r\n    if (epoch % 50 == 0) {\r\n      std::cout << \"Loss was \" << loss.item<float>() << std::endl;\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nbut the loss never changes. I have also posted about this on the pytorch forums. https://discuss.pytorch.org/t/jit-module-parameters-are-not-updating-when-training/58945 \r\n\r\ncc @suo @yf225"},{"labels":["api",null],"text":"Currently, `torch.optim` optimizers in PyTorch C++ API behave slightly differently from the Python API. In order to achieve parity, we should check the following aspects of an optimizer:\r\n\r\n1. Make sure the C++ optimizer takes the same set of constructor arguments as the Python optimizer.\r\n2. Make sure the C++ optimizer's constructor has the exact same logic as the Python optimizer's `__init__()` function. Particularly, we need to support `param_groups` in C++ optimizers.\r\n2. Make sure the C++ optimizer's `step()` function has the exact same logic as the Python optimizer's `step()` function.\r\n3. Add `state` to all optimizers, which is equivalent to the `state` dict in Python optimizers. In the `serialize` function of each C++ optimizer, make sure to serialize the `state` field into a list of `at::IValue`s, and have tests to make sure we can deserialize the `state` field successfully.\r\n    - You might ask \"what should we do with the existing serialization logic in the `serialize` function, and would the change break backward compatibility of the user's existing serialized optimizers?\" The answer is that we should put a \"version number\" in the serialized optimizer, and use that to identify the version - if we find no version number, we know that it's the old version, if we find version number \"1.5\", we know that it's the second version (aka. the version after our changes). We need to have tests to cover deserialization of the old version as well.\r\n\r\nFor more detailed discussions on the class structure design, see https://github.com/pytorch/pytorch/pull/29581.\r\n\r\n## Optimizers\r\n- [x] Adagrad (https://github.com/pytorch/pytorch/pull/29335)\r\n- [x] Adam (https://github.com/pytorch/pytorch/pull/33730)\r\n- [x] LBFGS (https://github.com/pytorch/pytorch/pull/34564)\r\n- [x] RMSprop (https://github.com/pytorch/pytorch/pull/33450)\r\n- [x] SGD (https://github.com/pytorch/pytorch/pull/32592)\r\n- [ ] Adadelta\r\n- [x] AdamW\r\n- [ ] SparseAdam\r\n- [ ] Adamax\r\n- [ ] ASGD\r\n- [ ] Rprop\r\n\r\ncc @yf225"},{"labels":["api",null],"text":"## ‚ùì Questions and Help\r\n\r\n# Motivation\r\ni want to implement nms in parallel processing with libtorch library.\r\ni use this cuda code(https://github.com/gdlg/pytorch_nms)\r\n\r\n# Environment\r\nPyTorch version : 1.2.0\r\nCUDA (nvcc compiler ) : 10.0\r\nlibtorch version : 1.2.0\r\nsystem : win10\r\n\r\n# Operation\r\nthe command :`i use nvcc -c nms_kernel.cu -L -lcudart -I D:\\Code-software\\NNF\\libtorch\\libtorch\\include -I D:\\Code-software\\NNF\\libtorch\\libtorch\\include\\torch\\csrc\\api\\include` to compiled it \r\n\r\n# ERROR\r\n`D:/Code-software/NNF/libtorch/libtorch/include\\torch/csrc/jit/argument_spec.h(181): error: member \"torch::jit::ArgumentSpecCreator::DEPTH_LIMIT\" may not be initialized 1 error detected in the compilation of \"C:/Users/Cason/AppData/Local/Temp/tmpxft_00001b28_00000000-10_nms_kernel.cpp1.ii\"`\r\n\r\nas long as i add `#include <torch/extension.h>` or `#include <torch/script.h>` in cuda files,It makes this kind of mistake.\r\n\r\n\r\n \r\n\r\n\r\n\n\ncc @yf225"},{"labels":[null,"api",null,null,null],"text":"When the model does not contain a custom layer, it can be deployed directly on the C + + side using JIT mechanism and libtorch library.\r\n\r\nBut my model contains a custom c++ and CUDA layer. Now that I'm deploying the model on the c++ side, do I need to compile the custom c++ and CUDA layers into the libtorch library?\r\nThank you!!!\n\ncc @suo @yf225"},{"labels":[null,"api",null,null,null],"text":"  here is my code\r\n  torch::Device deviceInfo(torch::kCUDA, 1);\r\n  module->to(deviceInfo);\r\n\r\n  but i got this error\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 10.92 GiB total capacity; 74.36 MiB already allocated; 147.50 MiB free; 11.64 MiB cached) (malloc at ../c10/cuda/CUDACachingAllocator.cpp:267)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6a (0x7ff1cbc4201a in /home/training/pytorch/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0x2009c (0x7ff1c32fd09c in /home/training/pytorch/torch/lib/libc10_cuda.so)\r\nframe #2: <unknown function> + 0x20db3 (0x7ff1c32fddb3 in /home/training/pytorch/torch/lib/libc10_cuda.so)\r\nframe #3: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x282 (0x7ff1ce9fe2b2 in /home/training/pytorch/torch/lib/libtorch.so)\r\nframe #4: <unknown function> + 0x584a765 (0x7ff1d16a0765 in /home/training/pytorch/torch/lib/libtorch.so)\r\nframe #5: at::native::to(at::Tensor const&, c10::Device, c10::ScalarType, bool, bool) + 0x8b9 (0x7ff1cf382889 in /home/training/pytorch/torch/lib/libtorch.so)\r\nframe #6: at::TypeDefault::to(at::Tensor const&, c10::Device, c10::ScalarType, bool, bool) + 0x25 (0x7ff1cf6c7535 in /home/training/pytorch/torch/lib/libtorch.so)\r\nframe #7: <unknown function> + 0x54aca6e (0x7ff1d1302a6e in /home/training/pytorch/torch/lib/libtorch.so)\r\nframe #8: torch::jit::load(std::unique_ptr<caffe2::serialize::ReadAdapterInterface, std::default_delete<caffe2::serialize::ReadAdapterInterface> >, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&) + 0x222 (0x7ff1d1304052 in /home/training/pytorch/torch/lib/libtorch.so)\r\nframe #9: torch::jit::load(std::istream&, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&) + 0x75 (0x7ff1d13041f5 in /home/training/pytorch/torch/lib/libtorch.so)\r\nframe #10: TorchNet::loadModelEncode(char const*, unsigned char*) + 0x169 (0x40cbe9 in ./vehicleFeature)\r\nframe #11: TorchNet::LoadModel(PES_Params_Config) + 0x5e (0x40cdfe in ./vehicleFeature)\r\nframe #12: TorchOperation::LoadModel(PES_Params_Config) + 0x48 (0x418e98 in ./vehicleFeature)\r\nframe #13: VehicleFeature::VehicleFeature(char const*, int) + 0xc2 (0x4192d2 in ./vehicleFeature)\r\nframe #14: test() + 0x217 (0x41a217 in ./vehicleFeature)\r\nframe #15: main + 0x9 (0x4098c9 in ./vehicleFeature)\r\nframe #16: __libc_start_main + 0xf0 (0x7ff1ca45c830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #17: _start + 0x29 (0x409929 in ./vehicleFeature)\r\n\r\nAborted (core dumped)\r\n\r\n\r\n\r\n\n\ncc @yf225"},{"labels":["api",null],"text":"## üêõ Bug\r\n\r\nL-BFGS optimizer will not work if there is one or more registered parameters (tensors) with no grad in the model:\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::view.  This usually means that this function requires a non-empty list of Tensors.  Available functions are [CUDATensorId, QuantizedCPUTensorId, VariableTensorId, CPUTensorId, MkldnnCPUTensorId] (lookup_ at /pytorch/aten/src/ATen/core/dispatch/DispatchTable.h:245)\r\n```\r\n\r\n## To Reproduce\r\n\r\nCode to reproduce the error:\r\n\r\n```\r\n/* A custom dataset for test */\r\nnamespace torch\r\n{\r\n    namespace data\r\n    {\r\n        namespace datasets\r\n        {\r\n            struct TEST_TensorDataset : public Dataset<TEST_TensorDataset>\r\n            {\r\n                private:\r\n                    torch::Tensor data1, data2;\r\n\r\n                public:\r\n                    explicit TEST_TensorDataset(std::vector<torch::Tensor> tensors):\r\n                        data1(tensors[0]), data2(tensors[1]) \r\n                        {}\r\n\r\n                torch::data::Example<> get(size_t index) override\r\n                {\r\n                    return {data1[index], data2[index]};\r\n                }\r\n\r\n                optional<size_t> size() const override\r\n                {\r\n                    return data1.sizes()[0];\r\n                }\r\n            };\r\n        }\r\n    }\r\n}\r\n\r\n/* Test */\r\nint test()\r\n{\r\n    struct Net : torch::nn::Module\r\n    {\r\n        Net():\r\n        fc1(128, 256),\r\n        fc2(256, 1)\r\n        {\r\n            register_module(\"fc1\", fc1);\r\n            register_module(\"fc2\", fc2);\r\n            /* The following line will cause the error */\r\n            tmp = register_parameter(\"tmp\", torch::ones({1,1}), false);\r\n        }\r\n\r\n        torch::Tensor forward(torch::Tensor x)\r\n        {\r\n            x = torch::tanh(fc1->forward(x));\r\n            x = fc2->forward(x);\r\n            return x;\r\n        }\r\n\r\n        torch::nn::Linear fc1;\r\n        torch::nn::Linear fc2;\r\n        torch::Tensor tmp;\r\n    };\r\n\r\n    Net model;\r\n    int batchsize = 16;\r\n    int stop_epoch = 10;\r\n    torch::Tensor data = torch::randn({32, 128});\r\n    torch::Tensor target = torch::randn({32, 1});\r\n    auto dataset = torch::data::datasets::TEST_TensorDataset({data, target}).map(torch::data::transforms::Stack<>());\r\n    auto data_loader = torch::data::make_data_loader<torch::data::samplers::RandomSampler>(std::move(dataset), torch::data::DataLoaderOptions().batch_size(batchsize));\r\n    torch::optim::LBFGS optimizer_lbfgs = torch::optim::LBFGS(model.parameters(), torch::optim::LBFGSOptions(1E-2));\r\n    for (int epoch = 0; epoch <= stop_epoch - 1; epoch++)\r\n    {\r\n        int batch_idx = 0;\r\n        bool output_loss = true;\r\n        for (auto & batch : *data_loader)\r\n        {\r\n            output_loss = true;\r\n            auto closure = [&]()\r\n            {\r\n                model.zero_grad();\r\n                torch::Tensor target_net = model.forward(batch.data);\r\n                torch::Tensor loss = torch::mse_loss(target_net, batch.target);\r\n                loss.backward();\r\n                if (output_loss)\r\n                {\r\n                    std::cout << \"Epoch: \" << epoch << \" Batch: \" << batch_idx << \" loss: \" << loss << std::endl;\r\n                    output_loss = false;\r\n                }\r\n                return loss;\r\n            };\r\n            optimizer_lbfgs.step(closure);\r\n            batch_idx++;\r\n        }\r\n    }\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\n## Expected behavior\r\n\r\nIf line 12 in the function **test()**: `tmp = register_parameter(\"tmp\", torch::ones({1,1}), false);` is commented out, then everything goes fine.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.2.0\r\n - OS (e.g., Linux): Ubuntu 16.04.6 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.5\r\n - CUDA/cuDNN version: 9.2/7.5\r\n - GPU models and configuration: GeForce GTX TITAN\r\n - Any other relevant information:\r\nGCC version: (Ubuntu 4.9.4-2ubuntu1~16.04) 4.9.4\r\nCMake version: version 3.15.2\r\n\n\ncc @yf225"},{"labels":["api",null],"text":"![image](https://user-images.githubusercontent.com/42128459/66465726-f57a7680-eab3-11e9-91f6-92840c57e70f.png)\r\n\r\nThe above is the data preprocessing stage. The batch input to the model is one.\r\nNow, I want to set batch to four, because will process four channels of video at the same time.\r\nHow should I do ? Hope get your help. Thank you !\n\ncc @yf225"}]