[{"labels":[null,"documentation"],"text":"Using the built-in Attention layer shows 0 trainable parameters. Is this just a summary bug? Or does the Attention layer not implement the Q and K matrix calculations? The recently updated documentation is still completely ambiguous.\r\n\r\n![Screenshot_20200608-082722](https://user-images.githubusercontent.com/44982143/84050864-2a9cd880-a963-11ea-80a5-e9182bac65cc.png)\r\n"},{"labels":["documentation"],"text":"**System information**  \r\n- nothing is applicable except:\r\n   - Keras version: 2.3.1\r\n\r\n**Describe the current behavior**  \r\n\r\nThe behavior of the `Concatenate` layer with respect to masks on its inputs [is not documented](https://keras.io/api/layers/merging_layers/concatenate/).  \r\n\r\n**Describe the expected behavior**  \r\n\r\nIt ought to be documented. If I am reading [the code](https://github.com/keras-team/keras/blob/7a39b6c62d43c25472b2c2476bd2a8983ae4f682/keras/layers/merge.py#L406) correctly, the behavior is what you would hope it is: a position is masked out if it is masked out in any of the entries along the final axis of the concatenation result.  However, it would be nice not to need to check the source to confirm this. :-)\r\n\r\nMore generally, it would be useful if behavior with respect to masking were a standard part of layer documentation (especially when it is not trivial)\r\n\r\n**Code to reproduce the issue**  \r\nn/a\r\n\r\n**Other info / logs**  \r\nn/a"},{"labels":[null,"documentation"],"text":"**System information**  \r\n- Have I written custom code (as opposed to using example directory):  No (subclassing callbacks)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04\r\n- TensorFlow backend (yes / no):  YES\r\n- TensorFlow version:  2.1.0\r\n- Keras version:  2.2.4-tf\r\n- Python version:  3.6.10\r\n- CUDA/cuDNN version:  \r\n- GPU model and memory:  \r\n\r\n**Describe the current behavior**  \r\nThe documentation at [Keras Callbacks](https://keras.io/callbacks/) says \r\n> on_epoch_end: logs include acc and loss, and optionally include val_loss\r\n\r\nHowever, '_acc_' throws an error, it seems to be only '_accuracy_'. If you\r\n```\r\nprint(list(logs))\r\n```\r\nIt gives:\r\n```\r\n['loss', 'accuracy']\r\n```\r\n**Describe the expected behavior**  \r\n\r\nDocumentation should say _accuracy_, not _acc_\r\n\r\n**Code to reproduce the issue**  \r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nimport tensorflow as tf\r\nfrom sklearn.datasets import make_blobs\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras import Model, Input\r\nfrom tensorflow.keras.callbacks import Callback\r\n\r\nX, y = make_blobs(n_features=10,\r\n                  n_samples=5000,\r\n                  cluster_std=1.5,\r\n                  centers=5)\r\n\r\n\r\ninputs = Input(shape=(10,))\r\n\r\nx = Dense(8, activation='selu', kernel_initializer='lecun_normal')(inputs)\r\nx = Dense(16, activation='elu', kernel_initializer='he_normal')(x)\r\noutputs = Dense(5, activation='softmax')(x)\r\n\r\nMyModel = Model(inputs=[inputs], outputs=[outputs])\r\n\r\n\r\nclass CustomCallback(Callback):\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        print(list(logs))\r\n        if logs.get('acc') > 9e-1:\r\n            print('\\nAccuracy above 80%, interrupting training.')\r\n            self.model.stop_training = True\r\n\r\n\r\nMyModel.compile(loss='sparse_categorical_crossentropy', \r\n    optimizer='sgd', metrics=['accuracy'])\r\n\r\nhistory = MyModel.fit(X, y, callbacks=[CustomCallback()], epochs=20)\r\n```\r\n>TypeError: '>' not supported between instances of 'NoneType' and 'float'\r\n\r\nWhen getting the documentation in `ipython` for `tensorflow.keras.callbacks.Callback`:\r\n```\r\n    on_epoch_end: logs include `acc` and `loss`, and\r\n        optionally include `val_loss`\r\n        (if validation is enabled in `fit`), and `val_acc`\r\n        (if validation and accuracy monitoring are enabled).\r\n```"},{"labels":["documentation"],"text":"In Keras documentation, glorot_uniform says that the initializer is using Glorot Uniform from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). However, the Keras implementation is totally different from the equation on the paper. Also, there are some arguments such as mode ='fan_avg' is the default. It should be same as the referenced paper. 'fan_sum'. Golort uniform is shown in Equation 1, but Keras implementation is shown in Equation 2. I had hard time to produce the same or close results to Keras using Pytorch. \r\n![Screen Shot 2020-04-23 at 9 44 11 AM](https://user-images.githubusercontent.com/23180823/80107996-ce642d80-8549-11ea-9bc9-a45262ac1f60.png)\r\n\r\n`def __call__(self, shape, dtype=None):\r\n        fan_in, fan_out = _compute_fans(shape)\r\n        scale = self.scale\r\n        if self.mode == 'fan_in':\r\n            scale /= max(1., fan_in)\r\n        elif self.mode == 'fan_out':\r\n            scale /= max(1., fan_out)\r\n        else:\r\n            scale /= max(1., float(fan_in + fan_out) / 2)\r\n        if self.distribution == 'normal':\r\n            # 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\r\n            stddev = np.sqrt(scale) / .87962566103423978\r\n            x = K.truncated_normal(shape, 0., stddev,\r\n                                   dtype=dtype, seed=self.seed)\r\n        else:\r\n            limit = np.sqrt(3. * scale)\r\n            x = K.random_uniform(shape, -limit, limit,\r\n                                 dtype=dtype, seed=self.seed)\r\n        if self.seed is not None:\r\n            self.seed += 1\r\n        return x`\r\n"},{"labels":["documentation"],"text":"1. Make possible to make link to paragraph, i.e. in https://keras.io/layers/recurrent/ we have LSTM paragraph, but I can't get link directly to it. For example like in pytorch docs https://pytorch.org/docs/stable/nn.html#lstm\r\n\r\n2. Link to source becomes outdated, i.e. https://github.com/keras-team/keras/blob/master/keras/layers/recurrent.py#L2081 source code is changed and this line is no longer points to relevant class definition.\r\n"},{"labels":[null,"documentation",null],"text":"Referring to closed log #11690. \r\n(Updating keras to support python 3.7)\r\n\r\nNote that Python 3.8 is now out. \r\n\r\nThe previous log noted that TensorFlow did not yet support python 3.7.\r\n\r\nI note now that TensorFlow DOES support 3.7 (under project details)  : \r\nhttps://pypi.org/project/tensorflow/#data\r\n\r\nSurely other AI backends currently supported by Keras which are no longer maintained need to be dropped, or need to be documented as restricted to older versions of Keras. "},{"labels":[null,"documentation"],"text":"<em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.\r\nIf your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  \r\n\r\n**System information**  \r\n- Have I written custom code (as opposed to using example directory):  unrelated\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  unrelated\r\n- TensorFlow backend (yes / no):  \r\n- TensorFlow version:  unrelated\r\n- Keras version:  unrelated\r\n- Python version:  unrelated\r\n- CUDA/cuDNN version:  unrelated\r\n- GPU model and memory:  unrelated\r\n\r\n<!-- You can obtain the TensorFlow version with:  \r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"  \r\nYou can obtain the Keras version with:  \r\npython -c 'import keras as k; print(k.__version__)'  -->\r\n\r\n**Describe the current behavior**  \r\n\r\nStarted from the \"Convolutional autoencoder\" section, tutorial on the page <https://blog.keras.io/building-autoencoders-in-keras.html> has some mistake about `plt` usage. The code is shown below.\r\n\r\n**Describe the expected behavior**  \r\n\r\nThe usage of `plt` should be correct considering many learners might get frustrated when they imitated the code and got some unexpected errors.\r\n\r\n**Code to reproduce the issue**  \r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.  \r\n\r\n```python\r\ndecoded_imgs = autoencoder.predict(x_test)\r\n\r\nn = 10\r\nplt.figure(figsize=(20, 4))\r\nfor i in range(n):\r\n    # display original\r\n    ax = plt.subplot(2, n, i)\r\n    plt.imshow(x_test[i].reshape(28, 28))\r\n    plt.gray()\r\n    ax.get_xaxis().set_visible(False)\r\n    ax.get_yaxis().set_visible(False)\r\n\r\n    # display reconstruction\r\n    ax = plt.subplot(2, n, i + n)\r\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\r\n    plt.gray()\r\n    ax.get_xaxis().set_visible(False)\r\n    ax.get_yaxis().set_visible(False)\r\nplt.show()\r\n```\r\n\r\nFocus on this code\r\n\r\n```python\r\nplt.subplot(2, n, i)\r\n```\r\n\r\nUnless `i` is started from `1`, this expression is not correct.\r\n\r\n**Other info / logs**  \r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  \r\n\r\nunrelated\r\n"},{"labels":["documentation"],"text":"In the [documentation](https://keras.io/layers/embeddings/) it says:\r\n\r\n> Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\r\n\r\nNeither this explanation nor this example is very clear. I would suggest replacing this with\r\n> Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]] if weights[4] is [0.25, 0.1] and weights[20] is [0.6, -0.2]. Note that float inputs get casted to an integer instead of rounded to the nearest integer."},{"labels":["documentation"],"text":"I was going through the docs in \r\n> examples/README.md\r\n\r\nand many of the links there are broken which would cause problem to the people are new and don't know about the scripts.\r\n\r\nI would like to help on this :))\r\n\r\n"},{"labels":["documentation"],"text":"Steps:\r\n\r\n1. Go to a nonexistent page in Keras docs, e.g. https://keras.io/asdfsada so the 404 page is shown\r\n2. Search something\r\n3. You are redirected to https://search.html/?q=something instead of a relative URL"},{"labels":["documentation"],"text":"Please implement the latest examples in the [examples/ folder](https://github.com/keras-team/keras/tree/master/examples) to the [docs/mkdocs.yml](https://github.com/keras-team/keras/blob/master/docs/mkdocs.yml) to see theses examples on the homepage. Perhaps a subsectioning for that many examples is necessary. See [Homepage](https://keras.io/)\r\n\r\nThank you!"},{"labels":["documentation"],"text":"This readme links to tensorboard_embeddings_mnist.py which doesn't exist. Where did it go?\r\nhttps://github.com/keras-team/keras/blob/master/examples/README.md"},{"labels":["documentation"],"text":"Some lines in the **_code_** block of the keras docs is too long, the result of which is, there will be a horizonal scroll bar at the bottom of the **_code_** block. That is hard to read. The long lines should be rearranged to multiple short lines to improve readibility.\r\n\r\nExample:\r\nThe docs for the SimpleRNN class (https://keras.io/layers/recurrent/#simplernn). The initializer of SimpleRNN has many arguments, but the docs put them in just a single long line. That makes it very hard to read. \r\n![docs line too long](https://user-images.githubusercontent.com/1805467/72451447-ea438f00-37f6-11ea-859b-d5427e444239.JPG)\r\n\r\nThe single long line should be rearranged to multiple short lines, just like what was did in the source code for the __init__ function of SimpleRNN.\r\n\r\n![short lines](https://user-images.githubusercontent.com/1805467/72451555-119a5c00-37f7-11ea-8a41-811d29b52208.JPG)\r\n\r\n"},{"labels":["documentation"],"text":"As indicated in the [FAQ](https://github.com/keras-team/keras/blob/master/docs/templates/getting-started/faq.md), the way to import ResNeXt101 is `from keras.applications.resnext import ResNeXt101`. But in the latest Keras 2.3.1 and Keras-Applications 1.0.8, it gives `ImportError: No module named 'keras.applications.resnext'`. I try to change it to `from keras_applications.resnext import preprocess_input, ResNeXt101`, then it gives the error\r\n ```  \r\nFile \"/usr/local/lib/python3.5/dist-packages/keras_applications/resnet_common.py\", line 575, in ResNeXt101\r\n    **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/keras_applications/resnet_common.py\", line 348, in ResNet\r\n    data_format=backend.image_data_format(),\r\nAttributeError: 'NoneType' object has no attribute 'image_data_format'\r\n```\r\nAfter some Google search, I changed the code into: `base_model = ResNeXt101(input_shape=(\r\n    224, 224, 3), weights='imagenet', include_top=False,backend = keras.backend, layers = keras.layers, models = keras.models, utils = keras.utils)`. It successfully load the model and weights. But when I try to import the `preprocess_input` from `keras_applications.resnext`, I got the similar `AttributeError: 'NoneType' object has no attribute 'image_data_format'`. So I try to apply the same fix to it by wrapping the `preprocess_input` like this:\r\n```\r\ndef _preprocess_input(x):\r\n    preprocess_input(x,backend = keras.backend, layers = keras.layers, models = keras.models, utils = keras.utils)\r\n```\r\nIt fixes the error but results in nan of my training loss. I have to import the preprocess from other models `from keras.applications.resnet import preprocess_input`. Now my training script is running and the loss is decreasing normally. But I think the documentation should be updated so that it won't confuse other people."},{"labels":["documentation"],"text":"https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L1071-L1077\r\nThere are two `validation_steps`."},{"labels":["documentation"],"text":"In the given [documentation](https://keras.io/visualization/), the mentioned key are `acc` and `val_acc`, but actually it is `accuracy` and `val_accuracy`.\r\n\r\nGiven documentation screenshot:\r\n![Screenshot from 2019-12-11 15-05-42](https://user-images.githubusercontent.com/9383897/70609796-70811a00-1c28-11ea-9ff5-29fcad5b2065.png)\r\n\r\nWhereas the actual keys are `dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])`\r\n\r\nChanges to be made in [this](https://github.com/keras-team/keras/blob/master/docs/templates/visualization.md) file. "},{"labels":["documentation"],"text":"Small thing, but costed me several hours to find :)\r\nIn the documentation example of [Siamese mnist ](https://keras.io/examples/mnist_siamese/) .\r\n\r\nWe see a code for contrastive loss, based on a [paper](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf). But the labels in this function are reversed from \r\nthe paper. Meaning in the paper Y=0 if X1,X2 are from same domain, Y=1 otherwise. In the code example in the documentation it is reversed.\r\n\r\nThe code example quote the paper, so I thought it might be confusing :)\r\n\r\nMy suggestion is to change the function to match the paper, and also change the code to use the reverse labels (reverse the 0 and 1)."},{"labels":["documentation"],"text":"Is it a known issue (is it even an issue?) that model.test_on_batch returns the sum of losses of each entry in the batch instead of the average? I looked over the changelog and saw no reference to that.\r\nmodel.train_on_batch does in fact returns the average, but in the docs their return value is documented the same."},{"labels":["documentation"],"text":"I could not find anything in the docs about how to handle different frequencies of time series. I have a Dataset A with monthly data that i want to use to predict the values from Dataset B that contains quarterly based data. So the target value e.g. quarter 1 is based on the values from month 1-3.\r\n\r\nDataset A (Features):\r\n\r\n| Month  | Value1 | Value2 | Value3 |\r\n| ------------- | ------------- | ------------- | ------------- |\r\n| 1  | 91  | 72 | 23 |\r\n| 2  | 93  | 68 | 33 |\r\n| 3  | 92  | 57 | 27 |\r\n| 4  | 91  | 72 | 23 |\r\n| 5  | 93  | 68 | 33 |\r\n| 6  | 92  | 57 | 27 |\r\n\r\nDataset B (Target):\r\n\r\n| Quarter   | Target_Value |\r\n| ------------- | ------------- |\r\n| 1  | 51  |\r\n| 2  | 57  |"},{"labels":["documentation"],"text":"There's a small mistake in the description of the embedding layer. It says\r\n\r\n'Turns positive integers (indexes) into dense vectors of fixed size.'\r\n\r\nbut it should read\r\n\r\n'Turns non-negative integers (indexes) into dense vectors of fixed size.'\r\n\r\nas it expects indexes ranging from 0 to input_dim - 1.\r\n\r\nSee https://keras.io/layers/embeddings/"},{"labels":["documentation"],"text":"Proposing to bundle [Caltech-101 dataset](http://www.vision.caltech.edu/Image_Datasets/Caltech101/) in the next release of Keras (or any future release) as a standard dataset for image-related application tasks.\r\n\r\nIt is small (compared to the whole ImageNet) but more difficult and challenging to train and experiment on than MNIST or CIFAR-10.\r\n\r\nIt was the precursor to Dr. Fei Fei Li's ImageNet program anyway and is nicely organized.\r\n\r\n"},{"labels":["documentation"],"text":"The topic given in the header was discussed for Keras for Python under [(https://github.com/keras-team/keras/issues/2030)], issue closed in the meantime.  \r\n\r\nUsing Keras for R with a Functional API*) I am observing a similar problem which I can't resolve using the advice given above, since the cases refer to Keras for Python and are (for me) not easily transferred to Keras for R. In my case, the output (outpt1=400) after a convolutional layer is reshaped to `outpt1=c(1,400)`, concatenated with two additional inputs `(inpt2=c(1, 6)`, `inpt3=c(1, 6)` to produce a shape (1,412) and this is fed to the first LSTM of a 3-layer LSTM stack, see the following essential lines of code:\r\n\r\n```\r\noutpt1 <- ... \r\nlayer_reshape(target_shape=c(1,4*5*20), name='resh1')\r\n```\r\n```\r\noutpt4 <- layer_concatenate(c(inpt2, inpt3, outpt1)) %>%\r\nlayer_lstm(units = 40,\r\n       return_sequences = T,\r\n       stateful = T,\r\n       name = 'lstm1') %>%\r\n...\r\n```\r\n\r\nWithout the line `stateful = T` the code is working correctly (it only requires an additional reshape just after the concatenation to give shape (412), though). Inserting the line `stateful = T` produces the  following error message:\r\n\r\n```\r\nError in py_call_impl(callable, dots$args, dots$keywords) : \r\nValueError: If a RNN is stateful, it needs to know its batch size. \r\nSpecify the batch size of your input tensors: \r\nIf using a Sequential model, specify the batch size by passing a \r\n`batch_input_shape` argument to your first layer.\r\nIf using the functional API, specify the batch size by passing a \r\n`batch_shape` argument to your Input layer. \r\n```\r\n\r\nDoing as required for the case Functional API, it rejects the `batch_shape = c(Bs,1,412)` argument (with Bs=40) with the following message:\r\n\r\n```\r\nError in layer_lstm(., units = 40, batch_shape = c(Bs, 1, 412), activation = \"tanh\",  :\r\nunused argument (batch_shape = c(Bs, 1, 412))\r\n\r\n```\r\nUsing the command slightly modified to `batch_shape = c(1, 412)` produces the message\r\n\r\n```\r\nError in layer_lstm(., units = 40, batch_shape = c(1, 412), activation = \"tanh\", \r\nunused argument (batch_shape = c(1, 412))\r\n\r\n```\r\nThus, the command is rejected because it is not recognized at all. Using the command  `batch_input_shape = c(1, 412)` or `batch_input_shape = c(Bs, 1, 412)` as recommended in the documentation, produces exactly the same results. Obviously, the error message is clearly misleading. Moreover, the documentation does not mention any peculiarities regarding either Keras Functional API or Keras Sequential Model. \r\n\r\nSince this problem seems to exist for quite a long time by now (it was raised in early 2016 for the first time) - could somebody comment on this problem or perhaps provide some helpful suggestions?\r\n\r\n*) my SW equipment:\r\nWIN10 v1903 \r\nR v3.5.1\r\nRStudio v1.2.1335\r\nkeras v2.2.0.9000 (via Anaconda environment)\r\ntensorflow v1.9\r\nreticulate v1.10\r\n"},{"labels":["documentation"],"text":"If the keras Library had links to examples on Google Colab it will save a lot of time of developers and new comers to the field of deep learning in understanding the code and implementing in with detailed description mentioned."},{"labels":["documentation"],"text":"The documentation for the patience parameter in the callback EarlyStopping/ReduceLROnPlateau may lead one to believe that training is stopped, whenever `patience` epochs have passed and some `quantity` has not improved.\r\n\r\nThis is not the case, when training with `model.fit(validation_freq=5)` and monitoring a validation quantity like `val_loss`. In that case, at least `5*patience` epochs have to pass, because only those epochs are counted that actually produce a `val_loss` value, i.e. every 5th one.\r\n\r\nOne could either change the implementation and count every epoch and not just the ones that produce the measured quantity or improve the documentation and make clear that only those epochs are counted that actually produce the monitored value.\r\n\r\nI'd prefer the latter, as it does not break existing code/expectations."},{"labels":["documentation"],"text":"Is it useful for many users to have some fully executed Jupyter notebooks with key examples and step-by-step explanation?\r\n\r\nFor example, I re-casted the transfer learning MNIST example with CIFAR-10 because, it shows the behavior of 'training on one set of images' and 'transferring the trained ability', more clearly.\r\n\r\nIn this type of illustrations, it is good to have actual images. Therefore, a notebook could be helpful.\r\n\r\nHere is my Notebook: [Transfer learning with CIFAR-10 tutorial example](https://github.com/tirthajyoti/Deep-learning-with-Python/blob/master/Notebooks/Transfer_learning_CIFAR.ipynb)"},{"labels":[null,"documentation",null],"text":"@fchollet Do you think a [focal loss](https://arxiv.org/pdf/1708.02002.pdf) is useful in keras? I've done implementation for my purpose (binary and categorical classification) and I can commit it to keras also.\r\n\r\nIf yes, I'll provide tests, documentation and so on."},{"labels":["documentation"],"text":"The keras Documntation shows evaluate_generator having argument callbacks ,But while im using it it isnt accepting the callbcaks argument.\r\n\r\nI'm using keras version: '2.2.4'\r\n\r\nevaluate_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\r\nhttps://keras.io/models/model/\r\n"},{"labels":[null,"documentation"],"text":"After visiting https://keras.io/preprocessing/text/ , I cannot find explaination for fit_on_texts (and other methods) under Tokenizer class.\r\nOn the other hand if I visit Japanese docs https://keras.io/ja/preprocessing/text/ , I can find the class and the methods listed as expected.\r\nIs it a bug or an expected behavior?\r\n"},{"labels":["documentation"],"text":"The [documentation for the embedding layer](https://keras.io/layers/embeddings/) specifies that the input should be an integer matrix. While the inputs should in fact be indices into the vocabulary (non-negative integers), they need to be cast as floats. If an integer-type matrix is input, the training does not work right, without ever giving an error, which is very hard to debug.\r\n\r\nSuggestions:\r\n- At minimum, update the documentation to specify that the input should be round numbers cast as floats.\r\n- Maybe also check the input for this and all other layers and raise an error if the type is wrong.\r\n- Maybe cast the inputs to floats within the `fit`, `predict`, and other relevant functions.\r\n\r\nI may be able to try taking a stab at this, if you tell me which option you prefer. Thanks!\r\n\r\n\r\n- [x] Check that you are up-to-date with the master branch of Keras. You can update with:\r\npip install git+git://github.com/keras-team/keras.git --upgrade --no-deps\r\n\r\n- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).\r\n\r\n- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:\r\npip install git+git://github.com/Theano/Theano.git --upgrade --no-deps\r\n\r\n- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\r\n"},{"labels":["documentation"],"text":"In the Keras documentation, it says about the convLSTM2D activation parameter:\r\n\r\nactivation: Activation function to use (see activations). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\r\n\r\nbut in the code I see that the default activation is NOT linear:\r\nactivation='tanh'"},{"labels":[null,"documentation"],"text":"<em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.\r\nIf your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  \r\n\r\n**System information**  \r\n- Have I written custom code (as opposed to using example directory):  \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  \r\n- TensorFlow backend (yes / no):  yes\r\n- TensorFlow version:  latest update\r\n- Keras version:  up to date\r\n- Python version:  up to date\r\n- CUDA/cuDNN version:  -\r\n- GPU model and memory: - \r\n\r\nThe keras.io has no documentation on the Generative Adversial Network and No examples too, I guess there should be an section for this too"},{"labels":["documentation"],"text":"There is an issue as title. There supposed to show sub items on the pages. \r\n<img width=\"305\" alt=\"Screen Shot 2019-04-25 at 3 52 46 PM\" src=\"https://user-images.githubusercontent.com/41447049/56773256-3aaf2400-6772-11e9-8e24-dcac1ad80432.png\">\r\n"},{"labels":["documentation"],"text":"There's no documentation for functions mentioned below in keras repo. It's there under keras pre-processing repo.\r\nFind the below functions under `keras/preprocessing/image.py`\r\n\r\n1. `array_to_img`\r\n2. `img_to_array`\r\n3. `save_img`\r\n\r\nThank you!"},{"labels":[null,"documentation"],"text":"I have found the documentation for sample weights in multi-output models to be lacking. It does not tell you of the correct format to include sample weights in, for a multi-output model. \r\n\r\nWhile some trial-and-error helped me figure it out, it would've saved me and my collaborators some anguish if the documentation was clear beforehand.\r\n\r\nPlease add better documentation about this."},{"labels":[null,"documentation"],"text":"Am trying to do some testing of generic optimization capabilities. It feels like there is no reason a graph could not be optimized via a keras model that takes no inputs or outputs but simply takes a loss function and optimizer. \r\n\r\nI ended up in this rabbit hole trying out tf 2.0 and hitting issues doing the same there. Would be good to understand what is the intended workflow between the two in 2.0 for vanilla optimizations."},{"labels":["documentation"],"text":"There are many fixes on master that are not in the latest release (2.2.4, october 2018). Is it possible to push a new release? 2.3.0? Cheers."},{"labels":[null,"documentation"],"text":"This issue is identical to the https://github.com/keras-team/keras/issues/9642. \r\n\r\nThe documentation doesn't include dilation_rate for depthwise convolutions because it was not included as one of the keyword arguments. Instead dilation_rate is passed to _Conv() through **kwargs. Need to add dilation_rate explicitly to the keyword arguments. \r\nThe fix should be pretty straightforward, like https://github.com/keras-team/keras/pull/9844. \r\n\r\n- [+ ] Check that you are up-to-date with the master branch of Keras. You can update with:\r\n`pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps`\r\n\r\n- [+ ] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).\r\n\r\n- [SKIP ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\r\n"},{"labels":["documentation"],"text":"Please make sure that the boxes below are checked before you submit your issue.\r\nIf your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.\r\n\r\nThank you!\r\n\r\n- [. ] Check that you are up-to-date with the master branch of Keras. You can update with:\r\n`pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps`\r\n\r\n- [ .] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).\r\n\r\n- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\r\n\r\nI believe there should be a documentation on\r\nhttps://keras.io\r\nfor letting beginners understand Transfer learning, I spent an entire day figuring out how to do this and i guess many people may have spend months doing this. This stuff reduces time to train and make predictions \r\n\r\nThe documentation at \r\nhttps://keras.io/applications/\r\nis not much clear and needs to be improved for instance there should be and indication upon what needs to be feeded in certain parts of the program and how can one use it using his or her own datasets\r\n"},{"labels":[null,"documentation"],"text":"In the docs section: https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models\r\n\r\nThe example is either incomplete or wrong. The following complete example that generates data fails. Examples, should be complete with random data.\r\n```\r\n\r\nimport numpy as np\r\nfrom tensorflow import keras \r\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense\r\nfrom tensorflow.keras.models import Model\r\n\r\n# Headline input: meant to receive sequences of 100 integers, between 1 and 10000.\r\n# Note that we can name any layer by passing it a \"name\" argument.\r\nmain_input = Input(shape=(100,), dtype='float32', name='main_input')\r\n\r\n# This embedding layer will encode the input sequence\r\n# into a sequence of dense 512-dimensional vectors.\r\nx = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\r\n\r\n# A LSTM will transform the vector sequence into a single vector,\r\n# containing information about the entire sequence\r\nlstm_out = LSTM(32)(x)\r\nauxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)\r\n\r\nauxiliary_input = Input(shape=(5,), name='aux_input')\r\nx = keras.layers.concatenate([lstm_out, auxiliary_input])\r\n\r\n# We stack a deep densely-connected network on top\r\nx = Dense(64, activation='relu')(x)\r\nx = Dense(64, activation='relu')(x)\r\nx = Dense(64, activation='relu')(x)\r\n\r\n# And finally we add the main logistic regression layer\r\nmain_output = Dense(1, activation='sigmoid', name='main_output')(x)\r\n\r\nmodel = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])\r\n\r\nX = np.random.randn(12, 100)\r\nZ = np.random.randn(1, 5, 1)\r\n\r\nmodel.predict({'main_input': X, 'aux_input': Z})\r\n```\r\n\r\nValueError: Error when checking input: expected aux_input to have 2 dimensions, but got array with shape (1, 5, 1)\r\n\r\nI can update the docs if someone knows the correction.\r\n"},{"labels":["documentation"],"text":"#12252 has already been merged, but the changes are not reflected in https://keras.io/examples/conv_filter_visualization/"},{"labels":["documentation"],"text":"Last release was 5 months ago. \r\n\r\nCan you release at least minor versions?"},{"labels":["documentation"],"text":"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.\r\n\r\nThank you!\r\n\r\n- [x] Check that you are up-to-date with the master branch of Keras. You can update with:\r\npip install git+git://github.com/keras-team/keras.git --upgrade --no-deps\r\n\r\n- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).\r\n\r\n- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:\r\npip install git+git://github.com/Theano/Theano.git --upgrade --no-deps\r\n\r\n- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\r\n"},{"labels":[null,"documentation"],"text":"It's great to see half-precision is now supported. Are there any plans to support mixed precision as detailed, for example, [here](https://forums.fast.ai/t/mixed-precision-training/20720)?\r\n"},{"labels":[null,null,"documentation"],"text":" (e.g. for links and images), because some of these examples are now being rendered in the docs.\r\n\r\nAdded by @fchollet in requests for contributions. "},{"labels":[null,null,"documentation"],"text":"\r\n![image](https://user-images.githubusercontent.com/29904255/52306796-e791b600-2966-11e9-82eb-84c1df397683.png)\r\nPlease make sure that the boxes below are checked before you submit your issue.\r\nIf your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.\r\n\r\nThank you!\r\n\r\n- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:\r\n`pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps`\r\n\r\n- [ ] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).\r\n\r\n- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\r\n"},{"labels":[null,null,"documentation"],"text":"In Reuters dataset, there are 11228 instances while in the [dataset's webpage](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection) there are 21578. Even in the [reference paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.3129&rep=rep1&type=pdf) there are more than 11228 examples after pruning. \r\n\r\nUnfortunately, there is no information about the Reuters dataset in Keras documentation. Is it possible to clarify how this dataset gathered and what the topics labels are? It mentioned there are 46 topics but what is the category e.g. for topic number 32?"},{"labels":[null,null,"documentation"],"text":"Please make sure that the boxes below are checked before you submit your issue.\r\nIf your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.\r\n\r\nThank you!\r\n\r\n- [NA ] Check that you are up-to-date with the master branch of Keras. You can update with:\r\n`pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps`\r\n\r\n- [ NA] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).\r\n\r\n- [NA ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\r\n\r\nThe documentation for flow_from_directory() has a spelling error. Under arguments, 'rgb' is misspelled as  'rbg':\r\n\r\ncolor_mode: One of \"grayscale\", \"rbg\", \"rgba\". Default: \"rgb\". Whether the images will be converted to have 1, 3, or 4 channels."},{"labels":[null,null,"documentation"],"text":"This is very minor, but the second link used as reference in Conv2DTranspose seems to be invalid. \r\nIt can be changed from https://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf to https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf."},{"labels":[null,"documentation"],"text":"On the Keras documentation we find the following information for **steps_per_epoch** and **validation_steps** when using the fit_generator() function:\r\n\r\n> It should typically be equal to the number of samples of your validation dataset divided by the batch size.\r\n\r\nI'm writing a [new generator](https://gist.github.com/kleysonr/a41f0d72891afec8a49990c8cc24f5e4) and while debugging my code I didn't get the full dataset that was expected for one epoch.\r\n\r\nBelow my fit_generator:\r\n\r\n```python\r\nkbg = KerasBatchGenerator('/home/kleysonr/Downloads/keras-generator/dataset', batch_size=6, imagesize=(100, 100), test_ratio=0.3)\r\n\r\nmodel.fit_generator(kbg.generate(set='train'), \r\n                    steps_per_epoch=training_steps,\r\n                    epochs=1,\r\n                    verbose=1,\r\n                    validation_data=kbg.generate(set='test'),\r\n                    validation_steps=validation_steps,\r\n                    use_multiprocessing=False,\r\n                    workers=0)\r\n\r\n```\r\n\r\nSetting steps_per_epoch and validation_steps, as:\r\n\r\n```python\r\ntraining_steps = kbg.getTrainingSize() // kbg.getBatchSize()\r\nvalidation_steps = kbg.getTestingSize() // kbg.getBatchSize()\r\n```\r\n\r\nThe batch process was (missing 3 images for the full training dataset):\r\n\r\n```\r\nEpoch 1/1\r\nBatch: 0-0 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class3/c3-10.jpg\r\nBatch: 0-1 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-2.jpg\r\nBatch: 0-2 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class2/c2-1.jpg\r\nBatch: 0-3 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class3/c3-2.jpg\r\nBatch: 0-4 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class2/c2-2.jpg\r\nBatch: 0-5 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-9.jpg\r\n1/3 [=========>....................] - ETA: 1s - loss: 1.1116 - acc: 0.1667\r\nBatch: 1-0 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class2/c2-8.jpg\r\nBatch: 1-1 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-7.jpg\r\nBatch: 1-2 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-8.jpg\r\nBatch: 1-3 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class2/c2-9.jpg\r\nBatch: 1-4 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-1.jpg\r\nBatch: 1-5 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class3/c3-6.jpg\r\n2/3 [===================>..........] - ETA: 0s - loss: 1.1519 - acc: 0.1667\r\nBatch: 2-0 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class3/c3-9.jpg\r\nBatch: 2-1 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-4.jpg\r\nBatch: 2-2 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class2/c2-5.jpg\r\nBatch: 2-3 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class2/c2-6.jpg\r\nBatch: 2-4 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-10.jpg\r\nBatch: 2-5 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class3/c3-1.jpg\r\n3/3 [==============================] - 1s 392ms/step - loss: 1.1632 - acc: 0.2222 - val_loss: 1.0733 - val_acc: 0.3333\r\n```\r\n\r\nSetting steps_per_epoch and validation_steps, as:\r\n\r\n```python\r\ntraining_steps = math.ceil(kbg.getTrainingSize() / kbg.getBatchSize())\r\nvalidation_steps = math.ceil(kbg.getTestingSize() / kbg.getBatchSize())\r\n```\r\n\r\nThe batch process was (got the full training dataset):\r\n\r\n```\r\nEpoch 1/1\r\nBatch: 0-0 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class2/c2-6.jpg\r\nBatch: 0-1 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class2/c2-1.jpg\r\nBatch: 0-2 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-2.jpg\r\nBatch: 0-3 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class3/c3-5.jpg\r\nBatch: 0-4 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-5.jpg\r\nBatch: 0-5 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class3/c3-9.jpg\r\n1/4 [======>.......................] - ETA: 3s - loss: 1.1336 - acc: 0.1667\r\nBatch: 1-0 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-9.jpg\r\nBatch: 1-1 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class2/c2-9.jpg\r\nBatch: 1-2 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-6.jpg\r\nBatch: 1-3 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class2/c2-4.jpg\r\nBatch: 1-4 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-10.jpg\r\nBatch: 1-5 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class2/c2-7.jpg\r\n2/4 [==============>...............] - ETA: 1s - loss: 1.1540 - acc: 0.1667\r\nBatch: 2-0 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-3.jpg\r\nBatch: 2-1 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class2/c2-3.jpg\r\nBatch: 2-2 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class3/c3-3.jpg\r\nBatch: 2-3 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class3/c3-10.jpg\r\nBatch: 2-4 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class3/c3-6.jpg\r\nBatch: 2-5 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class3/c3-8.jpg\r\n3/4 [=====================>........] - ETA: 0s - loss: 1.3066 - acc: 0.2222\r\nBatch: 3-0 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class3/c3-1.jpg\r\nBatch: 3-1 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class1/c1-7.jpg\r\nBatch: 3-2 <<train>> /home/kleysonr/Downloads/keras-generator/dataset/class2/c2-10.jpg\r\n1544964579 --train-- New epoch\r\n4/4 [==============================] - 2s 391ms/step - loss: 1.2351 - acc: 0.3238 - val_loss: 1.1089 - val_acc: 0.2222\r\n```\r\n\r\nIs the second approach - using math.ceil() - the correct way to set the parameters ?\r\nIf yes, I'm wondering if the documentation could be more clear about setting those parameters.\r\n\r\nBelow more useful information:\r\n\r\n```\r\nInformation about dataset:\r\n  Dataset size: 30\r\n  Training size: 21\r\n  Test size: 9\r\n  Classes: 3\r\n  Batch Size: 6\r\n```\r\n```\r\n$ pip3 freeze | egrep \"Keras|tensorflow\"\r\nKeras==2.2.4\r\nKeras-Applications==1.0.6\r\nKeras-Preprocessing==1.0.5\r\ntensorflow-gpu==1.12.0\r\n```\r\n```\r\n$ python3 --version\r\nPython 3.5.2\r\n```\r\n\r\nBest Regards.\r\nKleyson Rios."},{"labels":[null,null,"documentation"],"text":"flow_from_dataframe is a very useful function allowing flexibility that flow_from_directory lacks, for e.g, regression tasks.\r\nBut the problem with this function is it processes the dataframe's (filename) column in sorted way.\r\nfor example\r\nfilename  y\r\na              3.2\r\nd              2.2\r\nc               5.2\r\n\r\nin principle, the generator should process files in this way:\r\na,d,c\r\nbut the current implementation processes file in alphabtic order\r\na,c,d\r\n\r\nthis is not so convenient for predict_generator function"},{"labels":["documentation"],"text":"The link \"Edit on Github\" in the upper right corner of the keras.io documentation is broken and leads to a 404 error page. \r\n\r\nThis is an example of the current link:\r\nhttps://github.com/keras-team/keras/edit/master/docs/index.md\r\n\r\nI guess it should be:\r\nhttps://github.com/keras-team/keras/tree/master/docs \r\n\r\nThis is the same for all links on keras.io."},{"labels":["documentation"],"text":"Are you going to comment each line of code in Keras?  When is this going to happen along with detailed jupyter notebooks with tutorials?  The current stuff on the website is not linear and is missing tons of information.\r\n"},{"labels":["documentation"],"text":"While searching for Keras Callbacks, I found the link as below:\r\n\r\nhttps://keras.io/callbacks/.\r\n\r\nThere, the Arugments for every callbacks is described with couple of sentences. \r\nEspecially The Arguments description for  ModelCheckpoint,Tensorboard is specified with paragraphs. \r\n\r\nIt would be better , if sufficient new lines are added so that the document gives better readability. \r\n\r\nThanks"},{"labels":[null,"documentation"],"text":"DepthwiseConv2D is not listed in the documentation (https://keras.io/layers/convolutional/)"},{"labels":[null,"documentation"],"text":"The current [documentation on callbacks](https://keras.io/callbacks/) isn't showing bullet points correctly under the \"Arguments\" section of a few models. Here's the example for `ModelCheckpoint`:\r\n\r\n> filepath: string, path to save the model file. monitor: quantity to monitor. verbose: verbosity mode, 0 or 1. save_best_only: if save_best_only=True, the latest best model according to the quantity monitored will not be overwritten. mode: one of {auto, min, max}. If save_best_only=True, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_acc, this should be max, for val_loss this should be min, etc. In auto mode, the direction is automatically inferred from the name of the monitored quantity. save_weights_only: if True, then only the model's weights will be saved (model.save_weights(filepath)), else the full model is saved (model.save(filepath)). period: Interval (number of epochs) between checkpoints.\r\n\r\nLooking at the source code, the docstring seems to be organized correctly:\r\nhttps://github.com/keras-team/keras/blob/dc9e510192d0a8a6f6943cd46e9554364d4dcdd2/keras/callbacks.py#L371-L390\r\n\r\nIt is however showing up correctly for other models, e.g. `ProgbarLogger`:\r\n\r\n> # Arguments\r\n> * __count_mode:__ One of \"steps\" or \"samples\". Whether the progress bar should count samples seen or steps (batches) seen.\r\n> * __stateful_metrics:__ Iterable of string names of metrics that should not be averaged over an epoch. Metrics in this list will be logged as-is. All others will be averaged over time (e.g. loss, etc)."},{"labels":[null,null,"documentation"],"text":"The documentation is badly rendered for flow_from_dataframe. See https://keras.io/preprocessing/image/.\r\nThis needs to be modified in `keras-team/preprocessing` in this file: https://github.com/keras-team/keras-preprocessing/blob/052eb78390f91bb1628c59a8b678324f68f9a2b4/keras_preprocessing/image.py\r\n\r\nAlso, a link is given as plain string it would be nice to have it as a hyperlink so that users can click on it.\r\n\r\nTo be clear, the pull request fixing this should not be opened on this repo, but on this repo: https://github.com/keras-team/keras-preprocessing"},{"labels":[null,"documentation"],"text":"In keras, it's possible to load other backends than tensorflow, theano or CNTK. Let's suppose there is a python file called `mxnet_backend.py` (for example), it's possible to load it as a backend by doing \"backend\" : \"mxnet_backend\" in keras.json.\r\n\r\nSee this file for more info: https://github.com/keras-team/keras/blob/161f0a876db4f8aa9d57161c13d366f081369601/keras/backend/__init__.py#L91 \r\n\r\nThis should be documented there: https://keras.io/backend/#switching-from-one-backend-to-another"},{"labels":[null,null,"documentation"],"text":"https://keras.io/constraints/ is badly rendered. There is a lot of text missing. This can come from one of those two things:\r\n\r\nThe file https://github.com/keras-team/keras/blob/master/docs/autogen.py which is responsible for grabbing the docstrings and making a markdown file.\r\n\r\nThe file https://github.com/keras-team/keras/blob/master/keras/constraints.py if something has a strange syntax.\r\n\r\nActually it is also possible to use git bisect (magical tool, incredibly useful) to pinpoint which commit broke this page in the documentation. This is a good opportunity to learn how to use it. Once we know which commit broke the page, it will be easy to find out how to fix it. To learn how to use git bisect, see https://www.metaltoad.com/blog/beginners-guide-git-bisect-process-elimination ."},{"labels":[null,null,"documentation"],"text":"In the documentation, it happens that the references are rendered as plain text instead of a hyperlink. \r\n\r\nSee here a case where it's wrong and rendered as plain text: https://keras.io/layers/advanced-activations/#leakyrelu . Displayed like this: `[Rectifier Nonlinearities Improve Neural Network Acoustic Models] (https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)`\r\n\r\nSee here the case where it's nicely rendered as a hyperlink: https://keras.io/layers/advanced-activations/#prelu\r\n\r\nIt's likely something easy to fix, like a space/newline at the wrong place or something like that. If somebody could fix all those broken references (there are more than one broken in the docs), it'd be cool."},{"labels":[null,null,"documentation"],"text":"In the Keras documentation for `multi_gpu_model`, it is stated:\r\n\r\n> To save the multi-gpu model, use .save(fname) or .save_weights(fname) with the template model (the argument you passed to multi_gpu_model), rather than the model returned by multi_gpu_model.\r\n\r\nHowever in example 2 the template model is overwritten by the multi-gpu model:\r\n\r\n```python\r\n         ..\r\n         # Not needed to change the device scope for model definition:\r\n         model = Xception(weights=None, ..)\r\n         try:\r\n             model = multi_gpu_model(model, cpu_relocation=True)\r\n             print(\"Training using multiple GPUs..\")\r\n         except:\r\n             print(\"Training using single GPU or CPU..\")\r\n         model.compile(..)\r\n         ..\r\n```\r\n\r\nThis means that in this example it would not be possible to save the weights of the template model. I suggest rewritting to something like:\r\n\r\n\r\n```python\r\n         ..\r\n         # Not needed to change the device scope for model definition:\r\n         model = Xception(weights=None, ..)\r\n         try:\r\n             parallel_model = multi_gpu_model(model, cpu_relocation=True)\r\n             print(\"Training using multiple GPUs..\")\r\n         except ValueError:\r\n             parallel_model = model\r\n             print(\"Training using single GPU or CPU..\")\r\n         parallel_model.compile(..)\r\n         ..\r\n```\r\n\r\n(I take this opportunity to except only a specific error)\r\n"},{"labels":[null,null,"documentation"],"text":"Some examples of usage of `multi_gpu_model` appear on the documentation of the function in the [source code](https://github.com/keras-team/keras/blob/master/keras/utils/multi_gpu_utils.py).  However they do not display correctly on the [Keras home page](https://keras.io/utils/):\r\n\r\n```Example 1 - Training models with weights merge on CPU\r\n\r\n$Example_2_-_Training_models_with_weights_merge_on_CPU_using_cpu_relocation$0\r\n\r\nExample 2 - Training models with weights merge on CPU using cpu_relocation\r\n\r\n$Example_2_-_Training_models_with_weights_merge_on_CPU_using_cpu_relocation$1\r\n\r\nExample 3 - Training models with weights merge on GPU (recommended for NV-link)\r\n\r\n$Example_2_-_Training_models_with_weights_merge_on_CPU_using_cpu_relocation$2```"},{"labels":[null,"documentation"],"text":"The keras documentation lacks a page about all the features which make keras integrate well in Tensorflow workflows. Things that we would like to see in this page:\r\n\r\n* Using native TF optimizers\r\n* Using the dataset API and the input tensors.\r\n* How to get the tensorflow tensors out of a keras model\r\n* How to use `session.run` in keras\r\n* Using keras layers in a tensorflow graph\r\n* ... \r\n\r\nHelp would be appreciated. For examples of code, you can look in the keras test directory, all the functions which have the decorator `@pytest.mark.skipif(K.backend() != 'tensorflow') `\r\n\r\n@rohit-gupta "},{"labels":[null,null,"documentation"],"text":"Hi,\r\n\r\nI'm trying to save Keras model that contains Lambda layer for resizing image - \r\n\r\n```python\r\ndef resize_like(input_tensor, ref_tensor): \r\n    H, W = ref_tensor.get_shape()[1], ref_tensor.get_shape()[2]\r\n    return K.tf.image.resize_images(input_tensor, [H.value, W.value])\r\n\r\nx = _inverted_res_block(x, filters=320, alpha=alpha, stride=1, rate=4,\r\n                                expansion=6, block_id=16, skip_connection=False)  #x is 2D tensor object\r\n\r\nb4 = Lambda(resize_like, arguments={'ref_tensor': x})(b4)                 #b4 is 2D tensor object\r\n###some more operation here#####\r\nx = Lambda(resize_like, arguments={'ref_tensor': img_input})(x)           #img_input is 2D tensor object\r\n\r\nmodel = Model(inputs, x, name='model1')\r\nmodel.save(filePath, overwrite=True)\r\n```\r\n\r\n\r\nEach run I get one of those four errors - \r\n\r\n1)  File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 297, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 243, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 174, in deepcopy\r\n    rv = reductor(4)\r\nTypeError: can't pickle SwigPyObject objects\r\n\r\n\r\n  2)File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 243, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 174, in deepcopy\r\n    rv = reductor(4)\r\nTypeError: cannot serialize '_io.TextIOWrapper' object\r\n\r\n  3)File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 243, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 182, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 306, in _reconstruct\r\n    y.__dict__.update(state)\r\nAttributeError: 'NoneType' object has no attribute 'update'\r\n\r\n\r\n  4)File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 155, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 223, in _deepcopy_tuple\r\n    y = [deepcopy(a, memo) for a in x]\r\n  File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 223, in <listcomp>\r\n    y = [deepcopy(a, memo) for a in x]\r\n  File \"T:\\Conda\\Envs\\Kitov-38842-DKtv\\lib\\copy.py\", line 151, in deepcopy\r\n    cls = type(x)\r\nRecursionError: maximum recursion depth exceeded while calling a Python object\r\n\r\nHow can i save the model with Lambda layer?\r\n\r\nThe python version in 3.5, Keras is 2.2.2, tensorflow is 1.10.0\r\n\r\nThank You!"},{"labels":["documentation"],"text":"In documentation for SeparableConv1D data_format parameter explanation includes an extra dimension (width or height) \r\n![image](https://user-images.githubusercontent.com/7511567/43028531-fc815d36-8c45-11e8-9d26-747c67c05d72.png)\r\n)\r\nThis only makes sense for Conv2D layers and is contrary to the corresponding doc from TensorFlow:\r\n![image](https://user-images.githubusercontent.com/7511567/43001926-aa976834-8bec-11e8-99db-b613a6d3ef85.png)\r\n"},{"labels":[null,"documentation"],"text":"Keras : 2.2.0\r\nTensorflow-GPU: 1.8.0/ 1.9.0 (Both versions have the same issue)\r\n\r\nProblem: When using an input with rank greater than 2 in a Dense layer, \"Dimensions must be equal\" exception will be thrown from Tensorflow. \r\n\r\nIf using use_bias=False in Dense layer, it is OK.\r\nIf using Reshape before supply to Dense layer, it is OK.\r\n\r\nWith above symptons , it seems something is wrong when handling the bias with an input of rank size over 2. According to the documents: Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\r\n \r\nHere is the short script to reproduce the issue:\r\n\r\n```\r\nfrom keras import Input, Model\r\nfrom keras.layers import Dense, Reshape\r\n\r\ndef build_model():\r\n    X = Input(shape=(30, 40))\r\n    output = Dense(10, activation='tanh')(X)\r\n    model = Model(inputs=X, outputs=output)\r\n\r\n\r\ndef main():\r\n    build_model()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nHere is the exception:\r\n```\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1567, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 30 and 10 for 'dense_1/add' (op: 'Add') with input shapes: [?,30,10], [1,10,1].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/stiffme/PycharmProjects/Dense_Concate/main.py\", line 15, in <module>\r\n    main()\r\n  File \"/home/stiffme/PycharmProjects/Dense_Concate/main.py\", line 11, in main\r\n    build_model()\r\n  File \"/home/stiffme/PycharmProjects/Dense_Concate/main.py\", line 6, in build_model\r\n    output = Dense(10, activation='tanh')(X)\r\n  File \"/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/engine/topology.py\", line 619, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/layers/core.py\", line 879, in call\r\n    output = K.bias_add(output, self.bias)\r\n  File \"/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3781, in bias_add\r\n    x += reshape(bias, (1, bias_shape[0], 1))\r\n  File \"/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 979, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 297, in add\r\n    \"Add\", x=x, y=y, name=name)\r\n  File \"/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1734, in __init__\r\n    control_input_ops)\r\n  File \"/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1570, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Dimensions must be equal, but are 30 and 10 for 'dense_1/add' (op: 'Add') with input shapes: [?,30,10], [1,10,1].\r\n```"},{"labels":["documentation"],"text":"The default value of said parameter is `None`. As I understand it, this will result in the Lambda layer's discarding of the previous mask. From the source code of the Lambda layer, the mask parameter may or may not be a callable. In the latter case, it takes parameters `mask` and `inputs`, but I do not understand how such a callable would behave or what it would yield. It would be great to have at least an example of a mask callable that simply forwards the mask it receives.  \r\n[Here](https://keras.io/layers/core/#lambda) is the doc for the Lambda layer."},{"labels":[null,null,"documentation"],"text":"This is likely a silly question, but how are activations meant to be handled between CuDNNLSTM/CuDNNGRU layers? The new CuDNNLSTM and CuDNNGRU layers do not contain an activation= parameter to insert activations between GRU cells, and there is no documentation on how activations interact with these new layers. Should they now be created as separate activation layers? \r\n\r\nOr does the optimization/speedup of these constructs come from not having activations between the layers?   Pointers to documentation I may have missed are always appreciated.   \r\n\r\n\r\nPlease make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.\r\n\r\nThank you!\r\n\r\n- [X] Check that you are up-to-date with the master branch of Keras. You can update with:\r\npip install git+git://github.com/fchollet/keras.git --upgrade --no-deps\r\n\r\n- [X] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).\r\n\r\n- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:\r\npip install git+git://github.com/Theano/Theano.git --upgrade --no-deps\r\n\r\n- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\r\n"}]