[{"labels":[null,"enhancement1"],"text":"I'm running a 5 node swarm cluster. None of the services running in that cluster has any placement preferences, placement constraints or resource limits set.\r\n\r\nThe swarm cluster is spread across 3 data centers. One of the data centers participating in the cluster has a somewhat slower link than the two others. I therefore decided to add a placement preference to one service which is particularly sensitive to latency. \r\n\r\nThe layout of the cluster is :\r\n\r\nDC1: **swarm-01**\r\nDC2: **swarm-02**, **swarm-04**, **swarm-05**\r\nDC3: **swarm-03** (slow link)\r\n\r\nI added a `primarynode` label to nodes in DC1 and DC2 :\r\n\r\n**swarm-01**: `primarynode=a`\r\n**swarm-02**: `primarynode=b`\r\n**swarm-04**: `primarynode=c`\r\n**swarm-05**: `primarynode=d`\r\n\r\nThe label was not applied to **swarm-03**.\r\n\r\nI then added a placement preference to before mentioned service to spread tasks based on the `primarynode` label.\r\n\r\nTo my surprise, it appears that tasks for the service are scheduled just as often on the **swarm-03** node as on any of the other nodes - even though **swarm-03** node has no `primarynode` label. This happened even if one of the other nodes with the `primarynode` label set had no other tasks running. In other words, it appeared as if the placement preference had no impact.\r\n\r\nLooking at #35433 , it appears that nodes without a given label will still have that label implicitly set. In this case, the node **swarm-03** would have the label `primarynode` set - but with the value `null`.\r\n\r\n> Note that nodes which are missing the label used to spread, will still receive task assignments. As a group, they will receive tasks in equal proportion to any of the other groups identified by a specific label value. In a sense, a missing label is the same as having the label with a null value attached to it. \r\n\r\nI understand that the placement preference is only a preference. However, I would expect nodes with the label explicitly set to receive higher weight during task scheduling. \r\n\r\nIt makes sense if a task with a placement preference is scheduled on a node without a label if all other nodes are either congested or unavailable. However, in this case that was not the case. \r\n\r\nWould it not be fair to say that nodes with a label explicitly set should receive higher weight than nodes without a label set?\r\n"},{"labels":[null,"enhancement1",null],"text":"`docker pull` command allows download `imagename:tagname` from registry.\r\nIt will be good if there is argument (for example) `--check` that allows checking `imagename:tagname` in a registry (and checks that it can be downloaded if it is needed to provide authentication for example) but without downloading.\r\nSo, as a result, it is expected to see some info about `imagename:tagname` image if it can be downloaded or error in other cases.\r\n\r\nIt will be good if there is an argument `-q` (as in other commands) that allows printing only small info (for example) only `tagname`."},{"labels":[null,"enhancement1"],"text":"There is a --security-opt flag in docker run, Will there be support for a similar option in docker service create?"},{"labels":[null,"enhancement1"],"text":"Resurrecting https://github.com/moby/moby/pull/30786\r\nSorry if there was another discussion that I wasn't able to find but I didn't see any conclusion on the topic. \r\nAlso there are couple of links with questions/solutions that demonstrate that the topic has some demand:\r\nhttps://www.reddit.com/r/docker/comments/7cwfvb/union_fs_inside_docker_container_aufsoverlay/\r\nhttps://serverfault.com/questions/841238/how-to-use-overlayfs-with-docker-volumes\r\nhttps://stackoverflow.com/questions/29550736/can-i-mount-docker-host-directory-as-copy-on-write-overlay\r\nhttps://github.com/meyayl/docker-overlayfs-mount \r\n\r\nSo the idea briefly: to be able to mount volume into container as an overlayfs - host fs is read only, writes are in the volume.\r\n\r\nUse case:\r\nA big dir (for example git repo of >100Gb, so copying is not an option) and one or several containers at the same time working with it.\r\n\r\n`$ docker run -ti -v /my-ro-dir:/data:overlay ubuntu bash`\r\n\r\nThat is definitely achievable on the host using overlayfs for example with https://serverfault.com/a/863428\r\n\r\nBut maybe native implementation with in docker would be better option.\r\nThank you"},{"labels":[null,"enhancement1"],"text":"I am a server admin, and I want to allow everyone on my server able to run docker command.\r\n\r\nIf I add everyone to docker group, it will grant everyone root permission because it's root equivalent.\r\nAnyone can change root's password by following command:\r\n```\r\n$ docker run -it -v /:/rootfs ubuntu bash\r\n# chroot /rootfs\r\n# passwd\r\n```\r\n\r\nIf I ask everyone to use rootless docker, that will cause multiple docker engine running in the server. It's not a problem. \r\nThe problem is same image can't share among different account, it will cause waste of disk space.\r\n\r\n**So I request a feature that a new feature that  non-root user can pull/delete(can't force) image, manage containers which create by himself , run image in rootless mode, bind non-privileged port.**\r\n\r\nBecause the feature of rootless docker, userns-remap will prevent user modify sensitive files in host filesystem:\r\n```\r\n$ docker run -it -v /:/rootfs ubuntu bash\r\n# chroot /rootfs\r\n# passwd\r\nPermission Denied\r\n```"},{"labels":[null,"enhancement1",null],"text":"**Description**\r\nI'm deploying several independent services to swarm cluster.\r\nCurrently `docker service update` command accepts spec change via `xxx-add / xxx-rm` cli flags (see https://github.com/moby/moby/issues/25303).\r\nFrankly, it looks to me not very convenient. Instead of having _declarative_ description of service state I need to provide _imperative_ instructions leading to some state.\r\n\r\nI've tried two workaround to define service in file:\r\n\r\n1. Describe each service in own `docker-stack.yml` file and deploy each service via `docker stack deploy`. Having deployment config in yml file is very convenient. But using `docker stack` has for me following disadvantages:\r\n   - `docker stack deploy` still [does not have](https://github.com/docker/cli/issues/373) `--detach` mode. It means I can't see deploy result and deploy logs - it's blocker for me. In contrast I really like the output of `docker service update` command and it supports `--detach` mode.\r\n   - creating separate stack for every service looks like overhead. Stack assumes several bound services. Using stack just to be able to define config in file is not conceptually correct imho. Also it generates redundant service names `$STACK_$SERVICE` instead of just `myservice`.\r\n\r\n2. Describe service deployment in shell script with `docker service update`. E.g.\r\n```bash\r\n# deploy.sh\r\ndocker service update \\\r\n  --replicas 3 \\\r\n  --network-add traefik \\\r\n  --label-add traefik.enable=true \\\r\n  ...\r\n  myservice\r\n```\r\nBut it does not work, as for example if service already added to network `traefik`, the command fails.\r\n\r\n**Proposal**\r\nAdd cli flag `--compose-file` to `docker service update`:\r\n```bash\r\ndocker service update --compose-file docker-stack.yml myservice\r\n```\r\nWith the following `docker-stack.yml` it should deploy only `myservice`:\r\n```yml\r\nversion: \"3.8\"\r\n\r\nservices:\r\n  myservice:\r\n    image: $IMAGE\r\n    deploy:\r\n      replicas: 3\r\n      labels:\r\n        traefik.enable: \"true\"\r\n      restart_policy:\r\n        condition: on-failure\r\n        delay: 5s\r\n       ...\r\n```\r\nIn related issues several people [were interested](https://github.com/moby/moby/issues/31595) in providing full spec of service instead of delta.\r\n\r\nAnother alternative is to introduce `docker service deploy` command that will deploy particularly one service from stack file. Anyway the purpose is to allow declaratively describe service and deploy it.\r\n\r\nI've investigated the source code of docker-cli - it seems `apiClient.ServiceUpdate` [already accepts](https://github.com/docker/cli/blob/f0a21931c7e323d47ae9447e5e35d3f1b61c6bd1/cli/command/stack/swarm/deploy_composefile.go#L243) full service spec. So I hope it is possible to implement with not much effort.\r\n\r\n**Output of `docker version`:**\r\n<details>\r\n<summary>See below</summary>\r\n\r\n    Client: Docker Engine - Community\r\n\t Azure integration  0.1.7\r\n\t Version:           19.03.12\r\n\t API version:       1.40\r\n\t Go version:        go1.13.10\r\n\t Git commit:        48a66213fe\r\n\t Built:             Mon Jun 22 15:41:33 2020\r\n\t OS/Arch:           darwin/amd64\r\n\t Experimental:      true\r\n\r\n\tServer: Docker Engine - Community\r\n\t Engine:\r\n\t  Version:          19.03.12\r\n\t  API version:      1.40 (minimum version 1.12)\r\n\t  Go version:       go1.13.10\r\n\t  Git commit:       48a66213fe\r\n\t  Built:            Mon Jun 22 15:49:27 2020\r\n\t  OS/Arch:          linux/amd64\r\n\t  Experimental:     true\r\n\t containerd:\r\n\t  Version:          v1.2.13\r\n\t  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429\r\n\t runc:\r\n\t  Version:          1.0.0-rc10\r\n\t  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n\t docker-init:\r\n\t  Version:          0.18.0\r\n\t  GitCommit:        fec3683\r\n</details>\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"It seems like DOCKER-USER can only be used for filter and forward chain.\r\nCan you please establish this to be used in nat with support for PREROUTING and POSTROUTING?\r\n\r\nWe currently have several hosts running VPN software and the containers cannot connect to the encryption domains unless running them with net=host (which is not feasible from our side due to security concerns).\r\n"},{"labels":[null,"enhancement1"],"text":"From my experience, the time IO-intensive work takes (like building large projects) can be improved by running it in `tmpfs` (especially for the writes, like build outputs) instead of a disk-backed filesystem. Others use this trick as well; for example, Chromium notes it [in their build tips](https://chromium.googlesource.com/chromium/src/+/0e94f26e8/docs/linux_faster_builds.md#Using-tmpfs).\r\n\r\nI use Docker for almost everything I build, and for a long while I've been adding `tmpfs` mounts manually to my containers, like `mount -t tmpfs none /var/lib/docker/overlay2/.../merged/some/tmp/dir`. Then I found the `--tmpfs` flag to `docker run` which is indeed very useful :)\r\n\r\nNow, lately I thought - when I run temporary containers, just for build purposes, and when I run any other volatile container (like the containers used for different layers during `docker build`) - is there any reason not to make their entire root filesystem a `tmpfs`? By this, I mean to use a `tmpfs` as the `upperdir` (and `workdir`) of `overlay`. This way, I won't even have to redirect build outputs to a specific folder. Everything I do on the container is really on `tmpfs`, even stuff like `apt-get install`. Very convenient.\r\n\r\nI hacked a quick PoC, you can find it [here](https://github.com/Jongy/moby/tree/overlayfs-upperdir-tmpfs). It forces the `diff` and `work` dirs of executed containers to reside on a `tmpfs`. Works fine on my machine.\r\n\r\nSo basically this is a feature request, because it would be useful to me. I was thinking we can add a `--root-tmpfs` flag to `docker run`. This flag will also require `--rm` to be used (since the container is volatile). It will then use a `tmpfs` for the `upperdir`.\r\n\r\nIf you guys decide it's worth it, I'd be happy to work on this towards mainlining (incorporating it into the API, updating Docker's CLI, writing tests etc...)\r\n"},{"labels":[null,"enhancement1"],"text":"When you build a Dockerfile that has a `FROM` instruction referencing an image that is not stored locally, the image will automatically be pulled.  This kind of automatic pulling is problematic in systems that need to have strict control over how and whether an image should be pulled.\r\n\r\nAs a specific example, the builds for the [.NET](https://github.com/dotnet/docker) Docker images have specific scenarios where we want to ensure the build doesn't happen to pull down an image that we don't expect.  In those scenarios, all dependent images should exist locally on the build agent.  If a `docker build` ends up automatically pulling an image, that means something is wrong and we're ending up with a dependency to an already published image rather than a locally built image -- not what we want.\r\n\r\nOf course, it's always possible to work around this by parsing the Dockerfile to be built, determining which tags it references in its `FROM` instructions, and making sure the tag exists locally with a call to `docker inspect`.  But that's a pretty heavy burden for consumers to bear.\r\n\r\nThe proposal is to add an option to `docker build` that would cause an error to be returned if any of the tags referenced in the Dockerfile's `FROM` instructions do not exist locally."},{"labels":["enhancement1"],"text":"**Description**\r\n\r\nI have a use-case, where I want to create (but not start) a container on a remote docker host, then use `docker cp` to copy files over the network into the container and then start the container. Basically a Dockerfile with FROM, COPY and a RUN, but that does not actually produce a container.\r\n\r\nWhile a COPY instruction in a Dockerfile respects .dockerignore files, the `docker cp` command does not support this, resulting in copying a lot of unnecessary files.\r\n\r\n**Describe the results you received:**\r\nFiles that are matched by .dockerignore files are still being copied.\r\n\r\n**Describe the results you expected:**\r\nFiles that are matched by .dockerignore files are not copied.\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nFor backwards compatibility this should probably require a flag like `--use-ignore` or so.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           19.03.8-ce\r\n API version:       1.40\r\n Go version:        go1.14.1\r\n Git commit:        afacb8b7f0\r\n Built:             Thu Apr  2 00:04:36 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.8\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.12.17\r\n  Git commit:       afacb8b7f0\r\n  Built:            Wed Mar 11 01:24:28 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.2.13\r\n  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429\r\n runc:\r\n  Version:          1.0.0-rc10\r\n  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n\r\n```\r\n\r\n"},{"labels":[null,null,"enhancement1"],"text":"Allow initiating a log rotation by signalling to moby's service.\r\nLike [rsyslog](https://github.com/syslog-ng/syslog-ng/blob/master/contrib/fedora-packaging/syslog-ng.logrotate#L4) and other services allow, sending a signal to the service will trigger a log rotation.\r\n\r\nThis is helpful for 2 reasons:\r\n\r\n1. To re-open the log file if something has changed in the underlying file-system like a mount etc.\r\n2. We'll be able to use linux's `logrotate` cron job to rotate moby's logs, allowing us to use the same system-wide log rotation configuration for all services."},{"labels":[null,null,"enhancement1"],"text":"In light of the Github API changes:\r\nhttps://developer.github.com/changes/2/ (https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/)\r\n\r\nthis won't really work in the near future:\r\n\r\n```Dockerfile\r\nARG token\r\nADD \"<url>?access_token=$token\"  .\r\n```\r\n\r\ninstead we are gonna have to do:\r\n\r\n```Dockerfile\r\nRUN curl -H \"Authorization: token $token\" \"url\"  >  bar\r\n```\r\n\r\nis there a way to use a header with ADD or should we just use curl?\r\n"},{"labels":[null,null,"enhancement1"],"text":"  Problem:\r\nYou have 5 Servers running in swarm mode, running a service `FOO` and a service `BAR` with multiple replicas each. Any FOO container will resolve BAR with all ip addresses on any host.\r\n\r\nThis is a showstopper in production, where multiple services are talking together before returning the first byte (TTFB) to a http request. It adds network latency of >1s in my case.\r\n\r\nOptimally, services would only(preferably!) talk to services running on the same host.\r\n\r\nIf there was an option to the swarm dns resolver, that it either manages to find out the fewest hops from the source service to the target (and caches the answer), or, to hold/cache individual address-lists for every single host so it only returns target services tagged(?) with the source host. \r\nOr something like this, you understand the point... I know there is no fallback a records in dns, but dns sucks anyways...that's where swarm's dns resolver could really shine.\r\n\r\nAlbeit I understand:\r\n- for most users this is not absolutely necessary and\r\n- it diminishes the advantages and simplicity where you just run services so just any container will answer\r\n\r\nBut making this an option, only those would use it who need it.\r\nHow are you approaching lowest latency in production with swarm?\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"Im not sure if there's a proposal or maybe even an ongoing work to get a new logging driver implementation going in docker. Particularly a HTTP driver would be a pretty good general purpose solution to send logs to a HTTP/HTTPS endpoint without need of a sidecar container, especially when running in ECS Fargate. \r\n\r\nDocker comes with a few logging drivers that utilize (directly or not) HTTP delivery method, with Splunk (https://github.com/moby/moby/blob/master/daemon/logger/splunk/splunk.go) being the only option on Fargate. I'm interesting in getting the baseline driver implementation going, however, i'm curious if any similar proposals or PRs have ever been considered? Maybe there's a specific reason for HTTP driver being excluded from the docker distribution.\r\n\r\nThoughts? I'd like to help out. "},{"labels":["enhancement1"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n`docker-compose run` allows spawning a container based on a service template from the compose file, with overrides from command line flags. This is useful for spawning one-off containers(e.g. \"jobs\") using the same environment as a service, without having to specify on the command line the full service specs. Also very useful for debugging purposes(why is this service failing? start a shell in a container using the same template as the service to look around and test). \r\n\r\nWhile one can still use `docker-compose run` with a stack file, useful swarm-specific options won't be supported, like `configs` and `secrets`.\r\n\r\nMaybe the issue is more that `docker-compose` and `docker run` don't support those options. But it would be nice to have a coherent, complete API that covers all use case with `docker stack`.\r\n\r\n \r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.5\r\n API version:       1.40\r\n Go version:        go1.12.12\r\n Git commit:        633a0ea838\r\n Built:             Wed Nov 13 07:50:12 2019\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.5\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.12.12\r\n  Git commit:       633a0ea838\r\n  Built:            Wed Nov 13 07:48:43 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.2.10\r\n  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339\r\n runc:\r\n  Version:          1.0.0-rc8+dev\r\n  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n\r\n```"},{"labels":["enhancement1",null],"text":"- [X] base: https://github.com/moby/moby/pull/40174\r\n- [x] metrics (`docker stats`): https://github.com/moby/moby/pull/40657\r\n- ~~disable cgroupns for privileged containers? (https://github.com/kubernetes/enhancements/pull/1370)~~ (Probably not needed for Moby : https://github.com/containers/libpod/issues/5341)\r\n- [x] systemd cgroup driver for rootless mode: https://github.com/moby/moby/pull/40486\r\n- [x] use \"systemd\" cgroup driver by default when available : https://github.com/moby/moby/pull/40846\r\n- [ ] CI (rootful): https://github.com/moby/moby/pull/41065 https://github.com/moby/moby/issues/41218\r\n    - [ ] add stage to Jenkinsfile, running a distro with cgroups v2 (e.g. 19.10 (kernel 5.3) with `systemd.unified_cgroup_hierarchy=1` enabled)\r\n    - [ ] override (?) containerd and runc binary versions (`CONTAINERD_COMMIT`, `RUNC_COMMIT`)  (related: https://github.com/moby/moby/pull/40094)\r\n    - [ ] ??\r\n- [ ] CI (rootful+systemd)\r\n   - [x] Add systemd to the integration testing image? (PR: https://github.com/moby/moby/pull/40493)\r\n- [ ] CI (rootless+systemd)\r\n\r\n- [ ] docs: https://github.com/docker/docker.github.io/pull/11042\r\n\r\nLow hanging fruits:\r\n- [x] Put cgroup version to `docker info`: https://github.com/moby/moby/pull/40662\r\n-  ~`dockerd --cpu-rt-{period,runtime}`~ (Can't be supported)\r\n\r\nrelated: https://github.com/containerd/containerd/issues/3726 https://github.com/opencontainers/runc/issues/2209"},{"labels":["enhancement1"],"text":"### Question: Why Docker Still doesn't support add device to a running container?\r\nI find this pull request [#8826](https://github.com/moby/moby/pull/8826) been asked 5 years ago.\r\nWhy it wasn't been merged ?\r\nFor what reason？\r\nI'm confused cause I think it's a good feature. \r\n"},{"labels":[null,"enhancement1"],"text":"When searching for images on hub.docker.com, the more popular ones usually have a lot of usage examples on the \"Overview\" tab on their page. When I prepare to go on train by downloading some docker images I might need, I also want to have a copy of the information at docker hub available offline in case I don't have internet on the train."},{"labels":[null,null,"enhancement1"],"text":"Currently, rootless `overlay2` storage driver works only on Ubuntu and Debian 10 hosts.\r\n\r\nSupporting [fuse-overlayfs](https://github.com/containers/fuse-overlayfs) would be highly useful for the users of other distros.\r\n\r\n## System requirement\r\n\r\n* [fuse-overlayfs](https://github.com/containers/fuse-overlayfs) needs to be installed\r\n* Kernel needs to be >= 4.18\r\n\r\n## Implementation\r\n\r\nPodman already implements fuse-overlayfs, but it is implemented as a storage opt (\"overlay.mount_program\") rather than an independent storage driver: https://github.com/containers/storage/blob/01ea27db516e1ea8dd22b63b69d3a66c571effa7/drivers/overlay/overlay.go#L307\r\n\r\nFor Moby, I suggest implementing it as a new independent storage driver \"fuse-overlayfs\".\r\n\r\nMy first plan was to wait until Moby migrates to containerd snapshotters and use [containerd fuse-overlayfs plugin](https://github.com/AkihiroSuda/containerd-fuse-overlayfs), but it is not likely to happen in foreseeable future. "},{"labels":[null,"enhancement1"],"text":"Make it possible to include other Dockerfiles, quite useful for multistage builds, especially if the number of stages is more than 2.\r\n\r\nThere is https://github.com/moby/moby/issues/735 but it's closed, and multistage builds are very common nowadays. Thus, this issue even more relevant than in 2013."},{"labels":["enhancement1"],"text":"Podman already allows creating containers from a `rootfs` directory directly: `podman run -it --rm --rootfs /path/to/rootfs /bin/sh`.\r\n\r\nIt would be good to port over this feature to Moby as well.\r\n\r\nExample usecase: https://github.com/giuseppe/crfs-plugin\r\n\r\n* command will always need to be specified. No default command such as `/bin/sh` or `/bin/init`\r\n* `docker inspect` will show an empty string as Image if we follow Podman implementation. Alternatively, we can consider showing the rootfs path.\r\n"},{"labels":[null,"enhancement1"],"text":"Newest docker 19.03 deprecates nvidia-docker and adds in support for a `--gpus` flag in `docker run`\r\n\r\nWill there be support for a similar option in `docker service create`?"},{"labels":[null,"enhancement1"],"text":"Having access to the source commit for versioning applications at build time is not a new requests. In DockerHub this information is injected as `SOURCE_COMMIT` env variable (https://github.com/docker/hub-feedback/issues/600). It has been requested here in this repo as well #14191. However, the requirement was somewhat satisfied after adding support of `--build-arg`, but that does not cover certain use cases. For example, when building from a [remote Git URL](https://docs.docker.com/engine/reference/commandline/build/#git-repositories), user does not have access to this information, but the build environment first clones the repo and checks out specific refs, so it has access to it and can facilitate it when possible.\r\n\r\nDocker already has numerous [predefined ARGs](https://docs.docker.com/engine/reference/builder/#predefined-args) that include proxies and platform attributes. In addition to these, it will be very useful to provide something like:\r\n\r\n* `SOURCE_COMMIT_HASH`\r\n* `SOURCE_COMMIT_TIMESTAMP`\r\n\r\nARGs in the build context for both local and remote Git repositories that will cover a big portion of build contexts in practice. This will not limit the ability of developers to override these ARGs with their external methods currently used, so this addition is safe when it comes to backward compatibility."},{"labels":[null,"enhancement1"],"text":"Advanced deployments such as gathering core dumps for failing applications, involve making kernel tweaks that are difficult to do with a typical, unprivileged Docker container. The problem of altering the kernel is amplified in Kubernetes, where tenants may not even have access to run privileged containers.\r\n\r\nOne solution would be to explicitly wrap applications in a Vagrant VM inside the container, where the kernel can be tweaked at will.\r\n\r\nI'm requesting that this wrapping become a straightforward option, so that `docker exec` and similar commands pass from the host, through the container, into Vagrant, able to seamlessly interoperate the VM as an normal, albeit slow, container for debugging purposes. As a side effect, this integration would allow for more guest OS's to run at scale in Docker pods, from Illumos to Haiku to macOS."},{"labels":[null,"enhancement1"],"text":"\r\nThere are lots of applications that need a consensus system. For example, even Kafka is looking to move away from Zookeeper into something based on Raft. \r\n\r\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum\r\n\r\nMost applications look to use etcd - which is an added layer of complexity to run and manage. \r\nSwarm already implements Raft and is production tested. It would be great if Swarm can expose the Raft key value store (as a etcd-compatible, built in service) that can be used by other Services. \r\n\r\nThis would be a superb, added benefit of using Swarm over other tools. For end-users, they completely avoid the necessity of writing their own consensus system ...or deploying etcd."},{"labels":[null,null,"enhancement1"],"text":"**Description**\r\n\r\nThe _**COPY**_ command, presently cannot copy/mirror the source directory structure (or pathspec) in the target image.\r\n\r\n**Steps to reproduce the issue:**\r\n- Use `COPY Source/**/*.ext Target/` in your Dockerfile\r\n\r\n**Describe the results you received:**\r\nEvery file matching the path spec is copied into the `Target` folder without the files' original directory structure/layout.\r\n\r\n**Describe the results you expected:**\r\nFiles should be copied along with the directory structure/layout preserved.\r\n\r\n**Solution to the issue:**\r\nAn option should be presented as a secondary switch to `COPY` command to enable recursive/mirror copying. I recommend to have\r\n\r\n```Dockerfile\r\nCOPY [-r|--recursive] <source-path-spec> <target-path>\r\nCOPY [-m|--mirror] <source-path-spec> <target-path>\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           18.09.2\r\n API version:       1.39\r\n Go version:        go1.10.8\r\n Git commit:        6247962\r\n Built:             Sun Feb 10 04:12:31 2019\r\n OS/Arch:           windows/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.2\r\n  API version:      1.39 (minimum version 1.24)\r\n  Go version:       go1.10.6\r\n  Git commit:       6247962\r\n  Built:            Sun Feb 10 04:28:48 2019\r\n  OS/Arch:          windows/amd64\r\n  Experimental:     false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 0\r\nServer Version: 18.09.2\r\nStorage Driver: windowsfilter\r\n Windows: \r\nLogging Driver: json-file\r\nPlugins:\r\n Volume: local\r\n Network: ics l2bridge l2tunnel nat null overlay transparent\r\n Log: awslogs etwlogs fluentd gelf json-file local logentries splunk syslog\r\nSwarm: inactive\r\nDefault Isolation: hyperv\r\nKernel Version: 10.0 18362 (18362.1.amd64fre.19h1_release.190318-1202)\r\nOperating System: Windows 10 Pro Version 1903 (OS Build 18362.239)\r\nOSType: windows\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 31.97GiB\r\nName: WinX-AIO\r\nID: NTRV:UDM2:QK6B:BLI5:RM7C:7US5:SZ2V:LW2Q:HBPN:DE4A:I566:CZ6N\r\nDocker Root Dir: C:\\ProgramData\\Docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: -1\r\n Goroutines: 27\r\n System Time: 2019-07-16T01:28:15.2388227+05:30\r\n EventsListeners: 1\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\nProduct License: Community Engine\r\n```"},{"labels":[null,null,"enhancement1"],"text":"## Summary\r\n\r\nI would like to propose including support for image authors to define how their image interacts with their target package manager to simplify this task for users of the image.\r\n\r\n## Motivation\r\n\r\nWhile reading [a recent post on docker's blog](https://blog.docker.com/2019/07/intro-guide-to-dockerfile-best-practices/) regarding tips for reducing image size, quite a bit of focus was spent highlighting the ways in which a dockerfile author or maintainer can bust cache when installing dependencies in their image. It should be as simple as running a single command that lists the desired packages, with the parent image's dockerfile defining the default behavior controlling:\r\n\r\n  - what the package manager command is\r\n  - where the package repository is located\r\n  - that the install command's repository index is up to date\r\n  - any install options that help minimize the resulting image's footprint\r\n  - any cleanup steps that help minimize the resulting image's footprint\r\n\r\n## The User's Syntax\r\n\r\nAs a user of a given image, I would like to interact with syntax approaching these examples.\r\n\r\nThis example would use `apt-get`.\r\n\r\n```dockerfile\r\nFROM debian\r\nINSTALL openjdk-8-jdk\r\n```\r\n\r\nThis example would use `pip`.\r\n\r\n```dockerfile\r\nFROM python:3.7\r\nINSTALL requests\r\n```\r\n\r\nIf you are required to install a debian package before installing a python image, `--from` should be used to identify which image to use when running an install step. \r\n\r\n```dockerfile\r\nFROM python:3.7-stretch\r\nINSTALL --from buildpack-deps:stretch libmagickwand-dev  # uses apt-get\r\nINSTALL Wand  # uses current FROM's default package manager (pip)\r\n```\r\n\r\nThere are problems with the above example, for instance being unable to reference a parent image by image name in a dockerfile. I go into this in more detail at the end of this proposal.\r\n\r\n## Third-party repositories\r\n\r\nAn image author should also be able to specify how `INSTALL` commands should handle any requests to install from a third party repository location for a given install. This should include the url where the repository is located, the url where the repository's public key is located, and the key-id for local signing. This allows image authors to expose a single interface for users to rely on when moving from one base image to another. \r\n\r\n```dockerfile\r\nFROM archlinux/base\r\nINSTALL sublime-text https://download.sublimetext.com/arch/stable/x86_64 https://download.sublimetext.com/sublimehq-pub.gpg 8A8F901A\r\n```\r\n\r\nThis would use pacman, update `/etc/pacman.conf`, and locally sign the public gpg key for the sublime text user repository so that trusting for image installs is as painless as possible. Achieving a unified interface for all package managers could be a challenge, but I believe this represents a good deal of them here.\r\n\r\n## The Author's Syntax\r\n\r\nI am exclusively a docker user, I haven't authored anything in the project so my input here should be taken lightly. I can highlight a naive approach to demonstrate the concept, and point out some improvements that could be made to provide a high quality experience for users of the feature.\r\n\r\nA potential way forward would be to expose the `INSTALL` command as syntactic sugar on top of the `RUN` command. A dockerfile author would add an executable file to their image that follows a standardized name (`.dockerinstall`), accepts a standardized list of arguments (package name, repository url, public key location, key id), and is invoked by `docker build` whenever a user calls `INSTALL`. This isn't feasible as any images defined using that image as a base would over-write the parent's installer definition.\r\n\r\nA better approach might be to capture each image's name at build time, and associate the install instructions for that image for users who depend on it. This approach still has issues though, since an image a user depends on could change what image they are using. It may not be possible to easily expose a full hierarchy of parent image's internals to end users of the image, but without it you'd be forced to create a custom base image from your platform of choice, run `INSTALL` in that image's dockerfile, and use that custom image as the base of your application's runtime image."},{"labels":[null,null,null,"enhancement1"],"text":"This issue is closed related to issue  - https://github.com/moby/moby/issues/25526\r\n\r\nSwarm ingress should support injecting proxy protocol headers to preserve client information. Proxy protocol is a fully open standard and supported by most known network servers:\r\n1. https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt\r\n2. https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html\r\n3. https://cloud.google.com/load-balancing/docs/tcp/setting-up-tcp#proxy-protocol\r\n4. https://docs.openshift.com/container-platform/3.9/install_config/router/proxy_protocol.html\r\n5. https://docs.traefik.io/configuration/entrypoints/#proxyprotocol\r\n6. https://docs.nginx.com/nginx/admin-guide/load-balancer/using-proxy-protocol/\r\n7. https://aws.amazon.com/about-aws/whats-new/2013/07/30/elastic-load-balancing-now-supports-proxy-protocol/\r\n8. https://avinetworks.com/docs/16.3/proxy-protocol-support/\r\n9. https://blog.digitalocean.com/load-balancers-now-support-proxy-protocol/\r\n10. https://www.brightbox.com/blog/2019/03/12/proxy-protocol-load-balancer-api/\r\n\r\nKubernetes supports proxy protocol because it has pluggable ingress - both haproxy and nginx support it (e.g. https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#use-proxy-protocol)\r\n\r\nIn all fairness, there are some relevant pro and con discussions in Envoy for the same usecase\r\nhttps://github.com/envoyproxy/envoy/issues/4128 and\r\nhttps://github.com/envoyproxy/envoy/issues/1031\r\n\r\nProxy protocol is L4 and is therefore not constrained by L7 solutions like X-Forwarded-For. It is the most widely accepted L4 solution for maintaining source ip through a bunch of proxies.\r\n\r\n"},{"labels":[null,"enhancement1",null],"text":"This issue is a proposal for the implementation of Swarm Jobs. It has already undergone a closed design review, and I am now bringing it to the community for your input. This proposal has been adapted from that format.\r\n\r\nThis issue is a formal feature proposal for docker/swarmkit#2852 and #23880.\r\n\r\n# Problem Statement\r\n\r\nSwarm Jobs are a feature allowing the scheduling of different kinds of one-off Tasks, which run to completion and then exit. Jobs are desired to perform tasks like database migrations, or periodic batch operations. Jobs either run on-demand, or are scheduled to occur at specific times (“cron-jobs”). augmenting Replicated and Global services.\r\n\r\n## Background\r\n\r\nSwarmkit is the Docker Engine’s native container orchestration platform. It provides a way to easily manage distributed services in a cluster. Swarmkit represents workloads as “Services” which spawn “Tasks”. The user inputs the desired state of the Service as a “Service Spec”, which explains what and how a Service should run. Swarm creates a Service object to represent the actual state of the Service, and then spawns Tasks based on the spec.\r\n\r\nCurrently, Swarm supports Services in two modes: Replicated and Global. Replicated Services run a certain number of Tasks of a given workload somewhere on the cluster. Global Services run one Task on each node in the cluster (subject to constraints). Both of these Service modes are designed for long-running daemon-type workloads, like web servers or databases.\r\n\r\nSwarmkit’s model includes the concept of Task States, which track the progression and lifecycle of a given Task of a Service. When a Task exits, it enters one of several terminal states, depending on what caused it to exit. Regardless of the terminal state entered, with existing Service modes, a new Task is created to replace the Task that exited. With some irrelevant states elided, these terminal states include:\r\n\r\n* `COMPLETE`: entered when a task exits with no errors (code 0). \r\n* `SHUTDOWN`: the task is is requested to exit by the orchestrator.\r\n* `FAILED`: the task exits with an error (code non-0).\r\n* `REJECTED`: the task cannot be executed on the given node.\r\n\r\nNotably, the `COMPLETE` state is considered more-or-less equivalent to the other terminal states by most of the swarmkit code, and will result in a new Task being created. However, this state becomes important to the implementation of Jobs, as it allows us to differentiate between successful and unsuccessful job runs.\r\n\r\n## Goals\r\n\r\n* Implement support for one-off on-demand jobs\r\n* Implement support for scheduled, recurring “cron” jobs.\r\n* Allow specifying total number of desired completed jobs, and total number of simultaneously executing Tasks for a job.\r\n* Allow specifying a “global” job, to execute on every node.\r\n* Allow specifying a failure threshold, beyond which job execution will stop being attempted.\r\n* Allowing re-execution of a completed job.\r\n\r\n## Non-Goals\r\n* Provide assistance for coordinating or partitioning work between instances of a Job.\r\n* Implement any sort of dependency between jobs (“job A only proceeds if job B first succeeds”)\r\n* Garbage collection of completed jobs.\r\n\r\n# Functional Elements\r\n\r\n## Behavior\r\n\r\n### Jobs\r\n\r\n“Jobs” is not a technical term. A Job is a kind of Service, and it follows the rules of all other Swarm Services. The most notable difference between job Services and the existing long-running Services is that the Tasks spawned for Jobs are not rescheduled when they enter the `COMPLETE` state. Jobs have two modes, like long-running services: Replicated and Global.\r\n\r\n#### Replicated Jobs\r\n\r\nA Replicated Job is any job for which a certain number of successful runs is expected. Replicated jobs can execute in parallel, and will be scheduled to any node meeting the placement constraints of the Service. Replicated Jobs have two main parameters: maximum concurrent tasks, and total completions.\r\n\r\nMaximum concurrent tasks indicates the maximum number of Tasks to execute at the same time for the job. When a Replicated job is started, Swarmkit will launch Tasks up to the count of maximum concurrent tasks. Like existing Services, if placement constraints do not allow this many tasks to be run simultaneously, then the maximum number of tasks may not be the number actually running.\r\n\r\nTotal completions is the total number of Tasks desired to enter the `COMPLETE` state. Jobs will be fulfilled when the count of `COMPLETE` tasks is equal to the total completions.\r\n\r\nWhen a Task completes, another Task can be scheduled to execute. This will continue until the sum of the total of number of running tasks and the total number of completed tasks is equal to value of total completions. This means a job will never spawn more Tasks than necessary to fulfill the desired total completions.\r\n\r\n#### Global Jobs\r\n\r\nGlobal Jobs are like Global Services; they run on every node matching placement constraints. Global Jobs are much simpler than Replicated Jobs, as they have no special parameters. They simply run on every node.\r\n\r\n### Running and Re-running Jobs\r\n\r\nCreating a job will cause it to be immediately executed. When a job is finished, the Service object will remain until it is manually removed. Users may want to re-execute a job with the same settings many times. To simplify this, users can increment the `ForceUpdate` field of the job’s Task spec, which will cause the job to be re-executed as if it was new. Additionally, any other update to the job’s Spec will cause it to re-execute.\r\n\r\nIf a job has Tasks running when its execution is requested, those tasks will be immediately terminated, and new Tasks will be started. In the case of cron jobs, if execution of the job’s Tasks takes longer than the scheduling interval, then the Tasks for the job may never complete.\r\n\r\n### Cron Jobs\r\n\r\nInstead of manually executing and re-executing a job, users may want to schedule a job to run periodically. While this could be accomplished with clever use of the `cron` utility, because of the difficulty of doing so in the distributed cluster environment, this functionality is provided natively within Swarm. Both Replicated and Global jobs can be configured with `cron` -like specificity to execute at particular times or intervals. Importantly, however, this does not allow for `at` -like one-off scheduling at a future time.\r\n\r\nImportantly, users may try to set up a Cron Job, and then push new image versions, expecting the job to pull the new image each time. Swarmkit pins the image version by using the image digest when a service is created. This means that each execution of a Cron Job will use the exact same image. Even when Swarm is explicitly instructed not to pin images by digest, this leaves selecting the image version up to the engine, which may or may not pull the latest version of the image. Because of these difficulties, this pattern is not supported.\r\n\r\n### Handling Failure\r\n\r\nTo specify how failed Tasks are handled in Jobs, we used the TaskSpec’s existing `RestartPolicy`. The only difference in behavior for Jobs is that the restart condition `RestartOnAny` will have the same effect as `RestartOnFailure`. The whole goal of a Job is to run to completion, and restarting when it does so is contrary to that purpose.\r\n\r\n### Differences from Long-Running Services\r\n\r\nNotably, Jobs do not support setting an `UpdateConfig`. The `UpdateConfig` is used to specify the parameters of a rolling update. Jobs do not need to be rolled out; updating a job will trigger its re-execution. If a Job is currently running when it is updated, all running tasks will be aborted.\r\n\r\n## REST API\r\n\r\nThe Docker REST API will be expanded to encompass the new Jobs functionality.\r\n\r\n### ServiceMode\r\n\r\nThe ServiceMode struct indicates the mode of a service. Jobs will introduce two new fields, corresponding to the new modes, to this struct.\r\n\r\n```go\r\n// ServiceMode represents the mode of a service.\r\ntype ServiceMode struct {\r\n\tReplicated    *ReplicatedService `json:\",omitempty\"`\r\n\tGlobal        *GlobalService     `json:\",omitempty\"`\r\n\tReplicatedJob *ReplicatedJob     `json:\",omitempty\"`\r\n\tGlobalJob     *GlobalJob         `json:\",omitempty\"`\r\n}\r\n```\r\n\r\n### ReplicatedJob\r\n\r\n```go\r\n// ReplicatedJob is a type of one-off job which executes many Tasks in parallel,\r\n// until a certain number of Tasks have succeeded.\r\ntype ReplicatedJob struct {\r\n\t// MaxConcurrent indicates the maximum number of Tasks that should be\r\n\t// executing simultaneously at any given time.\r\n\tMaxConcurrent uint64 `json:\",omitempty\"`\r\n\t\r\n\t// TotalCompletions sets the total number of Tasks desired to run to\r\n\t// completion. This is also the absolute maximum number of jobs that\r\n\t// will be executed in parallel. That is, if this number is smaller\r\n\t// than MaxConcurrent, only this many replicas will be run in parallel.\r\n\tTotalCompletions uint64 `json:\",omitempty\"`\r\n\t\r\n\t// Schedule, if non-empty, specifies the times and intervals at which\r\n\t// to re-run this job.\r\n\tSchedule *Schedule `json:\",omitempty\"`\r\n}\r\n```\r\n\r\n### GlobalJob\r\n\r\n```go\r\n// GlobalJob is a type of one-off job which executes one Task on every\r\n// node matching the Service's constraints.\r\ntype GlobalJob struct {\r\n\t// Schedule, if non-empty, specifies the times and intervals at which\r\n\t// to re-run this job.\r\n\tSchedule *Schedule `json:\",omitempty\"`\r\n}\r\n```\r\n\r\n### Schedule\r\n\r\nThe decision of whether to represent the Schedule intervals as a simple string type or as a more structured datatype is a difficult one.\r\n\r\nIf a cron-like string is used, then it becomes the responsibility of the server to parse that string. Changes to that parser would have to be made very carefully, so as not to break existing strings or change their meaning. On the other hand, new concepts, like non-standard cron characters, could be easily added.\r\n\r\nIf a structured datatype is used, then there is no concern over parsing, and the cron syntax is not baked into the datatype. The downside is that, to be fully expressive, the data may be represented inefficiently.\r\n\r\n```go\r\n// Schedule indicates time intervals. It is a structured datatype which\r\n// represents time intervals as a set of values. Each interval field\r\n// (minutes, hours, day of month, months, day of week) is represented by a\r\n// list of values.\r\n//\r\n// If day of month and day of week are both specified, then only days which\r\n// match *both* values are valid. That is, if the 13th day of the month and\r\n// Friday are both required, then the job will only run on Friday the 13th.\r\n//\r\n// An empty list for any field is invalid (though it may be assigned a valid\r\n// meaning in the future). The equivalent to cron's \"*\" value is a complete\r\n// list of all possible values for that field.\r\n//\r\n// NOTE: Times specified in the Schedule are _always_ in terms of the\r\n// system timezone. If different nodes have different system timezones\r\n// set, this may lead to odd or unexpected behavior.\r\ntype Schedule struct {\r\n\t// Minute is the minute of the hour. Valid values are 0-59 inclusive.\r\n\tMinute []int\r\n\t\r\n\t// Hour is the hour of the day. Valid values are 0-23 inclusive.\r\n\tHour []int\r\n\t\r\n\t// DayOfMonth is the day of the month. Valid values are 1-31 inclusive.\r\n\tDayOfMonth []int\r\n\t\r\n\t// Month is the month of the year. Valid values are 1-12 inclusive.\r\n\tMonth []int\r\n\t\r\n\t// DayOfWeek is the day of the week. Valid values are 0-6 inclusive.\r\n\t// 0 is Sunday and 6 is Saturday. 7 is not a valid value; yoiu only get\r\n\t// one Sunday.\r\n\tDayOfWeek []int\r\n\t\r\n\t// Window represents the window of time in which the job can be executed.\r\n\t// Jobs will be executed, by default, as close to the specified time as\r\n\t// possible. However, in the case of outages, the Swarm may be unavailable\r\n\t// to start execution of the job at precisely the desired time. Window\r\n\t// informs Swarmkit up to how long past the the desired scheduling time\r\n\t// Swarm should attempt to start the job. If empty or 0, Swarm will never\r\n\t// attempt to start the job if its desired execution time has been missed.\r\n\tWindow time.Duration `json:\",omitempty\"`\r\n}\r\n```\r\n\r\n### JobStatus\r\n\r\nThe Service object will be amended to include a JobStatus field, which contains an object indicating when the job was last executed and, if a cron job, the computed time when the job will be next executed.\r\n\r\n```go\r\n// JobStatus indicates the status of the Service, if it is a Job-mode\r\n// service.\r\ntype JobStatus struct {\r\n\t// JobIteration is the count of how many times this job has been\r\n\t// executed, successfully or otherwise.\r\n\tJobIteration Version\r\n\t\r\n\t// LastExecution is the time that the job was last executed. If\r\n\t// the JobStatus is present on the Service, LastExecution will\r\n\t// always be present.\r\n\tLastExecution time.Time\r\n\t\r\n\t// NextExecution is the time that the job will next be executed,\r\n\t// if the job is a cron job. If the job is not a cron job, this\r\n\t// field will be empty.\r\n\tNextExecution *time.Time `json:\",omitempty\"`\r\n}\r\n```\r\n\r\n# Design Elaboration\r\n\r\nThe Design Elaboration contains a lower-level overview of specific details of implementation. It is not necessary to review this section in order to under the feature or its behavior, but it may be useful for providing further detail. \r\n\r\n## Swarmkit Implemention\r\n\r\n### Task Life Cycle\r\n\r\nThe life cycle of a Task in Swarmkit involves the Task passing through several states, and being operated on by specific components concerned with specific states. The life-cycle of job Tasks will be largely identical to that of of service-type tasks, with minor differences.\r\n\r\nTasks are created by the Orchestrator. Each service mode has a corresponding Orchestrator to handle its special requirements. Jobs will likewise need a new Orchestrator to handle the ReplicatedJob and GlobalJob service modes.\r\n\r\nThese Orchestrators will share much in common with the existing replicated and global orchestrators, but will be much simpler because of the lack of need to handle the service update cases.\r\n\r\nWhen a job is processed by the orchestrator, it will have a job iteration value set. This value will increase monotonically with each successive iteration. Each Task spawned as part of a job will have the job iteration value present, in order to distinguish which iteration of the Task the job belongs to. The existing `SpecVersion` field on Tasks will not be repurposed to store job iteration.\r\n\r\n### Scheduling Cron Jobs\r\n\r\nIn order to handle cron jobs, a new component will be needed. Both Orchestrators will share a common component, here referred to as the “cron scheduler”, which will notify them of the need to reschedule a job. During initialization, the cron scheduler will compute the last desired start time for each job. Then, it will determine if the job has been run since that time. If it has not, and the Window duration has not elapsed, then the cron scheduler will immediately signal the orchestrator to launch the job. In either case, the cron scheduler the computes the duration until the next desired run time, and creates a timer to expire at that time. When one of the timers expires, the cron scheduler will signal the orchestrator to begin the specified job.\r\n\r\nThe orchestrator will rely on and trust the cron scheduler to notify it of when to execute a job. It will not double-check whether execution of a cron job is needed. Correctness of the cron scheduler is highly important, as if the cron scheduler signals the orchestrator to execute the job erroneously, the job will be executed without question.\r\n\r\n## Reliability\r\n\r\nReliability of cron jobs is the most difficult problem in this proposal, as it is the only one that has no analogous solution for the existing service modes. The naive solution for cron jobs, to fire when the scheduling interval has occurred, creates a situation where a system outage during a scheduled cron time can cause the job to not be executed. To ameliorate this problem, the “Window” field is included as part of the Schedule type, and allows the user to specify the window in which execution of the job is valid.\r\n\r\nAt startup, the cron scheduler will compute the last desired execution time of the job, and compare this to the last known execution time of the job. If no execution has occurred since the last desired interval, and the duration specified by Window has not elapsed since that desired execution time, the cron scheduler will immediately notify the Orchestrator to start the job.\r\n\r\n“Time” is a difficult concept in distributed systems, specifically when it comes to ordering events. However, as the whole point of cron jobs is to execute at specific times, we must unfortunately contend with it. To simplify as much as possible, the `JobIteration` field acts as a Lamport clock, associating each job execution with the wall-clock time at which it was started. This way, each Task in a cron job is associated not with the clock-time of the cron job execution but with the particular job iteration, and the responsibility for dealing with wall clock times lies solely in the cron scheduler.\r\n\r\n## Scalability\r\n\r\nJob functionality, being an extension of existing Service functionality, is subject to the same scalability constraints as existing services. There are a few new scale problems to contend with, however:\r\n\r\n* Running cron jobs in a tight (~1 minute) interval will cause heavy loading on the cluster, as Tasks are constantly churned. There is no obvious workaround or optimization for this case, and so users should be advised to not run cron jobs with small intervals.\r\n* Cron jobs are scheduled with minute granularity. This means that by default, users will likely choose minute 0 or minute 1 for every job, and heavy users will likely see large cluster loading at the top of the hour, as all cron jobs will be executed simultaneously. In the future, it may be advisable to invest in functionality analogous to Jenkin’s H character, which instructs the scheduler to pick some time during the interval at random, but use the same time every time, which spreads out the start time of workloads automatically.\r\n* Garbage collection of jobs is excluded from this proposal, which means users will need to ensure they remove jobs that are no longer needed."},{"labels":["enhancement1"],"text":"This issue is a sub-task of the work being tracked in #30623\r\nThis issue very specifically tracks the feature to enable `dryRun` mode for the volume prune command.\r\n\r\nExample of desired behavior:\r\n```\r\nroot@ee0fb66c4f18:/var/lib/docker/volumes/178f849c6110911b7a9155a69cfee6863f290f29240c87fa1676d2830a1aea12/_data# docker volume prune -n\r\nWill Delete Volumes:\r\n178f849c6110911b7a9155a69cfee6863f290f29240c87fa1676d2830a1aea12\r\n7aea8a44fc0b213eca32e8255c4985d5a66012f30897579deaa46bbea1c7863c\r\n\r\nEstimated reclaimable space: 1.767MB\r\nroot@ee0fb66c4f18:/var/lib/docker/volumes/178f849c6110911b7a9155a69cfee6863f290f29240c87fa1676d2830a1aea12/_data#\r\n```\r\n```\r\nroot@ee0fb66c4f18:/var/lib/docker/volumes/178f849c6110911b7a9155a69cfee6863f290f29240c87fa1676d2830a1aea12/_data# docker volume prune --filter label=banana -n\r\nWill Delete Volumes:\r\n7aea8a44fc0b213eca32e8255c4985d5a66012f30897579deaa46bbea1c7863c\r\n\r\nEstimated reclaimable space: 0B\r\n```"},{"labels":[null,"enhancement1"],"text":"This is a feature request and/or request for clear rationale for the [current policy to only pre-populate](https://github.com/moby/moby/blob/e511b3be894465d81c21cab35b4f292d9250a4ca/daemon/create_unix.go#L84) named volumes of the type `mounttypes.TypeVolume`.\r\n\r\nWhy is that the policy?\r\n\r\nWill you consider to also pre-populate bind mounts that are empty at container start?\r\n\r\nEspecially for dev, I prefer to be able to have all volumes as subdirectories where the docker-compose file sits.\r\n"},{"labels":[null,"enhancement1"],"text":"Currently Swarm secret and configuration are exposed as files. According to #30866 this makes sense for secret, but the argument in that issue (i.e. security) doesn't apply for configuration, which is not sensitive. It would be nice if Docker Swarm provided a means to transform our configurations to the Linux environment for our applications to consume."},{"labels":[null,"enhancement1"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nI implement project falco (see: https://sysdig.com/opensource/falco/) in my swarm. Now i see all activities of my conainer. For example: the normal running container should not do package updates. The build container must do this. Falco can show all package updates and i want to only allow build container updates. This is possible if they have a defined name. So i need the possibility to set the build container name.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\ndocker build -t some/tag:latest .\r\n\r\n**Describe the results you received:**\r\n\r\ndocker ps\r\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES\r\n14da52ed2797        1a680164cd6f        \"/bin/sh -c 'set -x …\"   24 seconds ago      Up 23 seconds                           ecstatic_heisenberg\r\n\r\n**Describe the results you expected:**\r\n\r\nPlease implement:\r\ndocker build --name=my_container_name -t some/tag:latest .\r\n\r\nI expect:\r\ndocker ps\r\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES\r\n14da52ed2797        1a680164cd6f        \"/bin/sh -c 'set -x …\"   24 seconds ago      Up 23 seconds                           my_container_name\r\n"},{"labels":[null,"enhancement1"],"text":"It'd be very useful to have the capability to change the time in the container without changing the time on the host. This could be very helpful in testing scenarios like:\r\n\r\n- clock drifts in distributed systems\r\n- debugging time-related issues, such as expired SSL certificates\r\n- testing software for year-2038 compliance\r\n\r\nThe alternative right now is [libfaketime](https://github.com/wolfcw/libfaketime), but it has limitations (see libfaketime's compatibility issues) and can cause segmentation faults and unexpected errors."},{"labels":[null,"enhancement1",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nWe would like to be configure the default capabilities to be more limited than the current default. \r\n\r\nWhile working on hardening docker hosts, we found that the default linux capabilities were too open for our security policies. Rather than change the default for everyone we would like the ability to configure the default capabilities in the docker daemon. This will let us achieve our security objectives without changing the defaults for everyone.\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker run alpine\r\n2. Check the linux capabilites of the process.\r\n\r\n**Describe the results you received:**\r\nThe process will have these capabilities:\r\n\r\nCAP_CHOWN\r\n\r\nCAP_DAC_OVERRIDE\r\n\r\nCAP_FSETID\r\n\r\nCAP_FOWNER\r\n\r\nCAP_MKNOD\r\n\r\nCAP_NET_RAW\r\n\r\nCAP_SETGID\r\n\r\nCAP_SETUID\r\n\r\nCAP_SETFCAP\r\n\r\nCAP_SETPCAP\r\n\r\nCAP_NET_BIND_SERVICE\r\n\r\nCAP_SYS_CHROOT\r\n\r\nCAP_KILL\r\n\r\nCAP_AUDIT_WRITE\r\n\r\n**Describe the results you expected:**\r\n We would like to be able to set the default capabilities. (For reference, the policy we are considering to only have CAP_NET_BIND_SERVICE, CAP_KILL, CAP_AUDIT_WRITE on by default.)\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nI am more than happy to implement this change. I opened the ticket first as requested by the contributing guidelines.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n[zach@localhost boot]$ sudo docker version\r\nClient:\r\n Version:         1.13.1\r\n API version:     1.26\r\n Package version: docker-1.13.1-94.gitb2f74b2.el7.centos.x86_64\r\n Go version:      go1.10.3\r\n Git commit:      b2f74b2/1.13.1\r\n Built:           Tue Mar 12 10:27:24 2019\r\n OS/Arch:         linux/amd64\r\n\r\nServer:\r\n Version:         1.13.1\r\n API version:     1.26 (minimum version 1.12)\r\n Package version: docker-1.13.1-94.gitb2f74b2.el7.centos.x86_64\r\n Go version:      go1.10.3\r\n Git commit:      b2f74b2/1.13.1\r\n Built:           Tue Mar 12 10:27:24 2019\r\n OS/Arch:         linux/amd64\r\n Experimental:    false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n[zach@localhost boot]$ sudo docker info\r\n[sudo] password for zach: \r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 0\r\nServer Version: 1.13.1\r\nStorage Driver: overlay2\r\n Backing Filesystem: xfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: journald\r\nCgroup Driver: systemd\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: docker-runc runc\r\nDefault Runtime: docker-runc\r\nInit Binary: /usr/libexec/docker/docker-init-current\r\ncontainerd version:  (expected: aa8187dbd3b7ad67d8e5e3a15115d3eef43a7ed1)\r\nrunc version: df5c38a9167e87f53a9894d77c0950e178a745e7 (expected: 9df8b306d01f59d3a8029be411de015b7304dd8f)\r\ninit version: fec3683b971d9c3ef73f284f176672c44b448662 (expected: 949e6facb77383876aeff8a6944dde66b3089574)\r\nSecurity Options:\r\n seccomp\r\n  WARNING: You're not using the default seccomp profile\r\n  Profile: /etc/docker/seccomp.json\r\n selinux\r\nKernel Version: 3.10.0-957.10.1.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nNumber of Docker Hooks: 3\r\nCPUs: 2\r\nTotal Memory: 5.669 GiB\r\nName: localhost.localdomain\r\nID: WAN2:YHAU:G4C7:V4WM:ZOMC:6MRE:GCXG:7JGR:LOZ3:UQ2F:RNHG:WTYE\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\nRegistries: docker.io (secure)\r\n\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":["enhancement1"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nThere is currently no way to bind mount a directory inside the container over another directory inside the container.\r\n\r\nThis is useful for many things (it's what bind mounts were created for after all, before namespaces and Docker even existed), but in particular I would like to use it to create \"holes\" in a host  bind mount, to alleviate  some of the severe performance problems caused by host bind mounts on Mac and Windows.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Look for a way to bind-mount /hello over /bye inside the container\r\n2. Find there is no way to do this\r\n3. Look again and again because you can't believe such a simple feature would be missing\r\n\r\n**Describe the results you received:**\r\n\r\nThere is  no supported way to do this.\r\n\r\n**Describe the results you expected:**\r\n\r\nA supported way to do this.\r\n\r\n(Misc sections removed as N/A, this is a feature request.)"},{"labels":[null,null,null,"enhancement1",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nWhen building and pushing docker images in a shared environment (e.g. on a jenkins worker) there is always a possibility that between a build command and the following push command that someone else pulls the same tag as the one that is to be pushed.\r\n\r\nThis can result in a push of the other image instead of the one intended. The reason for this issue is that we can't set a source image when pushing, we have to use the same name on the local docker host as the name in the registry.\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. User 1: `docker build -t myregistry/an-image:stable .`\r\n2. User 2: `docker pull myregistry/an-image:stable`\r\n3. User 1: `docker push myregistry/an-image:stable` <- this will push the old image and not the one User 1 just built.\r\n\r\n\r\n**Suggestion**\r\n\r\nAdd an option to `docker push` that allows users to specify which source image they want to push.\r\nExample: `docker push --source <image-id-or-name> NAME[:TAG]`\r\n\r\nThis addition would allow users to minimize the risk of pushing the wrong image to the registry.\r\nAdding an option ensures backward compatibility of the command.\r\n"},{"labels":[null,null,"enhancement1"],"text":"It's unfortunately common for projects to mount the docker socket into (privileged) containers. A configuration option for specifying additional sockets with limited privileges would be fairly simple and would make it easier to do it easily without doing it wrong.\r\n\r\nEssentially, in the docker configuration, users would be able to specify the path of an additional socket with a limited set of permissions:\r\n\r\n`[(socket_path, [permissions,]),]`\r\n\r\nOr maybe also able to specify read-only access:\r\n\r\n`[(socket_path, {principal: 'rw', principal2: 'r'},]`\r\n\r\nI am aware that there are a number of far more complete RBAC solutions to limiting privileges; but I think creating said additional socket(s) with limited docker API privileges would be a fairly simple improvement that could help avoid granting unnecessarily broad privileges (e.g for easy ACME cert reloads and load balancing).\r\n\r\nAn example use case: securing the Traefik docker driver:\r\n\r\n- \"Docker integration: Exposing Docker socket to Traefik container is a serious security risk\" https://github.com/containous/traefik/issues/4174#issuecomment-446600393\r\n  > It seems it only require (read) operations : ServerVersion, ContainerList, ContainerInspect, ServiceList, NetworkList, TaskList & Events. \r\n  - https://github.com/liquidat/ansible-role-traefik\r\n    > This role does exactly that: it launches two containers, a traefik one and another to securely provide limited access to the docker socket. It also provides the necessary configuration.\r\n    - https://github.com/Tecnativa/docker-socket-proxy/issues/13\r\n      - Creates a HAproxy container that proxies limited access to the docket socket\r\n \r\nWith such a proposed configuration option for additional docker sockets with limited privileges, such an additional docker socket proxy container would be unnecessary."},{"labels":[null,"enhancement1"],"text":"[This issue](https://github.com/moby/moby/issues/30310) was closed with the reasoning that `docker config` or `docker secret`, which only work on Swarm nodes, make up for the inability to create a named single file volume and mount it into a container.\r\n\r\nGiven that not everyone is using Docker Swarm, and not everyone has access to administer the Docker host to turn it into a Swarm node (particularly in CI environments), I still think it's worthwhile to fix the volumes feature so that single files can be mounted into a container from a named volume."},{"labels":[null,"enhancement1"],"text":"A number of organisations have adopted PIV smartcard devices, such as Yubikeys for TLS client authentication. It would be beneficial to allow the use of PKCS#11 interfaces when connecting to a registry. \r\n\r\nCurrently Docker allows configuration of the client certificate and key in the `/etc/docker/certs.d/` directory. This requires the certificates and keys to be in PEM format. \r\n\r\nPossible approaches:\r\n1. Specify `MODULE_PATH` and object identifiers in config.json \r\n2. Support a new file type in `/etc/docker/certs.d/` that contains `MODULE_PATH` and PKCS11 object identifiers\r\n3. Allow setting of `MODULE_PATH` and object identifiers as ENV vars.\r\n\r\nSome gotchas \r\n1. Capturing input for user pin?\r\n2. Ensuring permissions of `MODULE_PATH` are valid to prevent privilege escalation. \r\n\r\nWill have a bash at implementing this, but would appreciate some discussion/guidance on the preferred approach. \r\n\r\nMost of the changes will need to be made to: https://github.com/moby/moby/blob/master/registry/registry.go#L57\r\n"},{"labels":[null,"enhancement1"],"text":"Currently there are commands to backup and restore the images, but not to perform a backup and restore of the persistent and mutable volumes."},{"labels":[null,"enhancement1"],"text":"I am suggesting a feature that grants access to _data_ on an already existing image during build time. This might be implemented in a `COPY --from` syntax. This data might be used as a \"secure\" cache for the build process. Currently as far as I know, there is no such functionality, but correct me if I'm wrong.\r\n\r\nThis would enable faster build times and ease the workflow in certain scenarios. Relying on non-mutable data, already precompiled by docker, would be better than relying on the host systems data or volumes - which seems like a quite popular bad practice. If the concept still sounds wrong let me explain a use case:\r\n\r\nConsider an application that requires a resource expensive operation in order to be deployed on the first run. Updates can happen frequently and are also mandatory for the application and are deployed as delta updates.\r\nBut now baking in those updates into an updated image requires the entire image to be rebuild! - without any \"cached\" data. All resulting in high bandwidth usage, processing power and build time even though updates would be trivial in a non-docker workflow.\r\n\r\nCurrently there are several approaches to address this problem:\r\n1) Let the updates happen inside the container without touching the image. Considered bad practice but actually used by many.\r\n2) Use volumes as a storage of \"cached\" data for the build process. A container might even be configured to be responsible to update the volume on demand and serve up-to-date data. This approach is discouraged for a number of reasons obviously, but used by a number of users since it's \"easy to understand\" and can be implemented!\r\n3) Multi-stage builds - using pre-build images as a starting point. Those base images would need to be updated in an arbitrary cycle OR delta updates might get huge over time (updates might be a few gigs), slowing down the process. Also working with already outdated information form the start sounds like a bad practice in a scalable environment over time. Also rather a hassle.. like a workaround for a more simple solution.\r\n4) There might be a new approach with the new buildkit integration? Using `RUN --from` one could create a \"data cache\" directory on the host as far as I understand? `RUN --from` is currently not documented extensively however, so fell free to prove me wrong.\r\nBut if so - why not just use data already existing and up-to-date from the :latest image as suggested here? It would prevent being reliant on the host system and make use of the docker ecosystem.\r\n5) Installing the application on the host system and take the updated data from there using a simple `COPY`. But this circumvents the whole docker ecosystem with all it's downsides attached.\r\n\r\n_Those are the ones I come up with! If there's any better solution that I am not seeing don't hesitate to speak it out_!\r\n\r\n**The goal is to have a simple, fast, secure and efficient solution for the above use case.**\r\n\r\nLet me know what you think! :)\r\n\r\n**EDIT:** Since this is a feature you'd rather want to use dynamically and not statically defined in the Dockerfile, something like a `--mount <image>` <image> flag for buildkit would actually be better I guess."},{"labels":["enhancement1",null],"text":"I would like to ask that the named pipe created by the Windows docker engine be made available remotely (assuming it already isn't, and assuming an option for people who doesn't want the pipe exposed remotely) as well as the Windows docker client able to connect to a remote named pipe, such as `docker -H npipe://remotemachine/pipe/docker_engine`.\r\n\r\nIntroducing this feature would make it very simple to set up docker in an all windows environment, as integrated security could be used to secure the connections, rather than having to manage various X.509 certificates.  Simply creating a group on the container host, modifying the configuration to allow that group access to the docker engine, then adding authorized users/computers/service accounts to the group, then only those users/computers/service accounts can access the docker engine remotely without the need of managing keys or certificates.\r\n\r\nThis can already be done locally, but adding this feature would have remote access managed with similar simplicity."},{"labels":[null,"enhancement1"],"text":"In https://github.com/moby/moby/pull/34319, we are looking to add support for mirrors for private registries, which has brought up the topic of how to deal with auth.\r\n\r\nPersonally speaking, I don't want to see:\r\n\r\n1. Auth credentials stored on the daemon side\r\n2. Universal auth credentials for all clients\r\n3. An even greater mess of the existing auth scenarios (Seriously, read through the code and tell me how long it takes you to figure out all that is happening)."},{"labels":[null,null,"enhancement1"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nThe syslog logging driver currently always specifies `-` for the structured data portion of `rfc5424` formatted messages. I would like to see a `--log-opt` added to be able to specify structured data to be sent with each log line. Perhaps something like `--log-opt \"syslog-structured-data=foo@123 tag=name\"`\r\n\r\nAt Timber, though also common to other log providers that provide a syslog compliant-endpoint (e.g. Loggly), we require the API key to be specified as a structured data field to be used for authentication of incoming messages.\r\n\r\nWe currently maintain our [own logging driver](https://github.com/timberio/timber-docker-logging-driver), but are considering shifting to syslog forwarded messages instead.\r\n\r\nRelated to https://github.com/moby/moby/issues/20354 and https://github.com/moby/moby/issues/26937.\r\n\r\nI'm happy to take a stab at a PR, but wanted to open this first to see if it would be a desirable feature and in-case anyone else is considering it."},{"labels":[null,null,"enhancement1"],"text":"**Description**\r\nOn worker is not available overlay networks. Test cases:\r\n1) created overlay network -> joined worker -> checked overlay network on worker;\r\n2)  joined worker -> created overlay network ->checked overlay network on worker;\r\n\r\n**Steps to reproduce the issue:**\r\n1)  join worker\r\n2) create overlay network\r\n3) check overlay network on worker;\r\n\r\n**Describe the results you received:**\r\n* on manager\r\n```\r\nca-east-doc1 system # docker node ls\r\nID                            HOSTNAME                     STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\r\njbq7vn03c7wmrwwhfg0uv7zxt *   ca-east-doc1.phonebox.work   Ready               Active              Reachable           18.06.1-ce\r\nmiefdbsgg9j81dfxfjlaysg6c     ca-east-doc2.phonebox.work   Ready               Active              Leader              18.06.1-ce\r\na73ewp34001kcz1axmx610f9q     ca-east-doc3.phonebox.work   Ready               Active              Reachable           18.06.1-ce\r\nzrstf6wvgyheffzawpmnvbhtg     us-west-sw1.phonebox.work    Ready               Active                                  18.06.1-ce\r\nca-east-doc1 system # docker network ls\r\nNETWORK ID          NAME                DRIVER              SCOPE\r\n6c2c01201917        bridge              bridge              local\r\n15895052b239        docker_gwbridge     bridge              local\r\naaaa843cb2b4        host                host                local\r\nliwc39jqt5i0        ingress             overlay             swarm\r\nwbkrzdop2alc        kazoo               overlay             swarm\r\na4e079430521        none                null                local\r\nn6vnv0n5zjm5        test                overlay             swarm\r\n```\r\n* on worker\r\n```\r\nus-west-sw1 ~ # docker network ls\r\nNETWORK ID          NAME                DRIVER              SCOPE\r\n2b9c04fed29e        bridge              bridge              local\r\nf5fc7fabe021        docker_gwbridge     bridge              local\r\n2c5b90a85839        host                host                local\r\nliwc39jqt5i0        ingress             overlay             swarm\r\n233d1c75d64e        none                null                local\r\nus-west-sw1 ~ # \r\n```\r\n\r\n**Describe the results you expected:**\r\nOverlay networks on worker must be same to manager.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nus-west-sw1 ~ # docker version\r\nClient:\r\n Version:           18.06.1-ce\r\n API version:       1.38\r\n Go version:        go1.10.4\r\n Git commit:        e68fc7a\r\n Built:             Tue Aug 21 17:16:31 2018\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          18.06.1-ce\r\n  API version:      1.38 (minimum version 1.12)\r\n  Go version:       go1.10.4\r\n  Git commit:       e68fc7a\r\n  Built:            Tue Aug 21 17:16:31 2018\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nus-west-sw1 ~ # docker info   \r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 0\r\nServer Version: 18.06.1-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: zrstf6wvgyheffzawpmnvbhtg\r\n Is Manager: false\r\n Node Address: 192.168.0.16\r\n Manager Addresses:\r\n  192.168.4.16:2377\r\n  192.168.4.17:2377\r\n  192.168.5.16:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 468a545b9edcd5932818eb9de8e72413e616e86e\r\nrunc version: 69663f0bd4b60df09991c08812a60108003fa340\r\ninit version: v0.13.2 (expected: fec3683b971d9c3ef73f284f176672c44b448662)\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\n selinux\r\nKernel Version: 4.14.81-coreos\r\nOperating System: Container Linux by CoreOS 1911.4.0 (Rhyolite)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 1.912GiB\r\nName: us-west-sw1.phonebox.work\r\nID: 6VUN:CLRJ:SSEQ:MJ7C:LATJ:GGGN:SABK:E2M5:GZHU:74QJ:RCSN:DTDS\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nuse CoreOS on Amazon\r\n```\r\nus-west-sw1 ~ # cat /etc/os-release \r\nNAME=\"Container Linux by CoreOS\"\r\nID=coreos\r\nVERSION=1911.4.0\r\nVERSION_ID=1911.4.0\r\nBUILD_ID=2018-11-26-1924\r\nPRETTY_NAME=\"Container Linux by CoreOS 1911.4.0 (Rhyolite)\"\r\nANSI_COLOR=\"38;5;75\"\r\nHOME_URL=\"https://coreos.com/\"\r\nBUG_REPORT_URL=\"https://issues.coreos.com\"\r\nCOREOS_BOARD=\"amd64-usr\"\r\n```"},{"labels":[null,null,"enhancement1"],"text":"**Feature Request**\r\n\r\nIt'd be nice if there was the ability to specify something like `LOGFILE /var/log/my.log` and have docker set up the necessary file descriptors/plumbing to have these logs be included in the typical docker log facilities.\r\n\r\nTypically I've implemented something like this through a sidecar ( to use Kubernetes terms ) that tails the logs messages and prints them out to stdout. While this works, it's often a bit clunky to set up and seems a bit overkill for something as trivial as tailing log files.\r\n\r\nI could envision something like\r\n\r\n```\r\n{\"log\":\"some log message\\r\\n\",\"stream\":\"stdout\",\"time\":\"2018-06-08T20:57:45.538248406Z\"}\r\n{\"log\":\"some other log message\\r\\n\",\"stream\":\"stdout\",\"time\":\"2018-06-08T20:57:45.538987679Z\"}\r\n{\"log\":\"my.log message\\r\\n\",\"stream\":\"/var/log/my.log\",\"time\":\"2018-06-08T20:57:45.538987679Z\"}\r\n```\r\n\r\nI'm unsure how realistic this would be because I think it would involve injecting file descriptors into whatever the entrypoint is, but maybe there's some mo-betta :tm: ways of making this happen. "},{"labels":[null,"enhancement1"],"text":"It would be great to have build stage volumes so that we can copy data while building the container it's self without having to wait for the image to be build.\r\n\r\nProposal:\r\n```\r\nFROM ubuntu\r\n\r\nVOL /mnt/data/ /data/\r\nRUN apt-get install build-essentials\r\nADD . /src\r\nRUN cd /src && make && cp binary /data/\r\n```\r\n"},{"labels":[null,null,"enhancement1"],"text":"**Description**\r\n\r\nDocker service creation should have an option to fail a service creation if a required named volume does not exist rather than create a named volume with the default driver.\r\n\r\nIt is probably a bad idea to implicitly create a volume if the requested volume cannot be found given that this will silently create the service which will in turn store its data on an ephemeral volume. That ephemeral volume will not be available if the service is scheduled and that would cause many issue in a production environment.\r\n\r\nAdding a volume option during service creation which would check if the volume exist before scheduling the service would solve this issue without breaking backwards compatibility. \r\n"},{"labels":[null,"enhancement1"],"text":"**Description**\r\n\r\nNow that [cgroup namespace support has been added to runc](https://github.com/opencontainers/runc/pull/1916), I'd like to add support to docker.  I'm happy to do the work, but I'd like some input as to which of these approaches would work the best:\r\n\r\n1. Always enable cgroup namespaces for all containers, provided that the currently running kernel supports it.\r\n2. Enable for all containers, but only when the docker daemon was started with a flag to enable it (similar behaviour to `--userns-remap`).\r\n3. Enable cgroup namespaces per-container (not for all containers), by passing a flag at container start time.\r\n\r\nThoughts?\r\n\r\nCC: @cpuguy83, @stevvooe, @sargun "},{"labels":[null,"enhancement1"],"text":"**Description**\r\n\r\nSimilar to `--env-file` flag in `docker run` which accepts a file for list of environment variables. Can we have a flag in `docker build` (eg. `--build-args-file`) for passing build arguments with a file.\r\nThis will be useful when there are more than one `--build-args`.\r\n\r\nThanks,\r\nYagnesh"},{"labels":[null,"enhancement1"],"text":"**Description**\r\nI've had a bit of success restricting docker commands via an authz plugin, however there is currently no control when running a docker build to approve or deny based on the contents of the dockerfile.\r\n\r\nIt would be useful if the authz plugin api was also called to validate the contents of a dockerfile being built, so that plugins could do things like\r\n\r\n1. Require base be from a certain repository\r\n2. Force a user to be set\r\n\r\nThanks,\r\nDaniel\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"**Description**\r\n\r\nIt's common to mount the user's work dir (containing project files) as a volume, while some of the sub directories in that work dir should *not* be mounted. The common workaround is to create an anonymous mount for these sub directories.\r\nThe creation of an anonymous volume can be very slow if it contains many files. This is very wasteful when the volume is only used to ignore certain directories. It would be much better if ignores are supported directly.\r\n\r\n**Steps to reproduce the issue:**\r\nImagine a Node project with many dependencies. When the container starts, it mounts the project files from the current directory. E.g. `docker run -v 'pwd':/app`.\r\nThe Docker file has an `ONBUILD` to run the `npm/yarn install`. \r\nThe `node_modules` directory should *not* be mounted to the user's file system, because there might be platform specific dependencies (and other reasons). The only way to accomplish that today is to create a second, anonymous, mount:\r\n\r\n`docker run -v 'pwd':/app -v /app/node_modules`.\r\n\r\nThis works fine, until `node_modules` contains many files. The `volume create 096b2f5e10a8873ac69a93464833e515d109d72b198a351e94f808415a6ae43c (driver=local)` can easily take 30+ seconds. \r\n\r\n**Proposal**\r\nAdd new arguments to the `--mount` syntax to explicitly ignore sub directories. For example:\r\n```\r\ndocker run ... --mount src=pwd,dst=/app,ignores=node_modules,build\r\n```\r\n"},{"labels":[null,"enhancement1"],"text":"**Description**\r\nTypically in multistage build the build stage linter/test.. reports will be lost with the options currently available. \r\n\r\nTagging the intermediate stages would be a nice way to save the build reports. We will not be pushing the images to a registry but will be using the images to extract the reports and publish them to Jenkins.\r\n\r\nArticle with full description and workaround\r\nhttps://medium.com/@cyril.delmas/how-to-keep-the-build-reports-with-a-docker-multi-stage-build-on-jenkins-507200f4007f\r\n\r\n\r\n"},{"labels":[null,null,"enhancement1"],"text":"Often I see projects that require mounting the docker socket into the container. Most of the time it is required, because the container can scale its workers, spin up and down containers to handle workload as needed.\r\n\r\nBut mounting the docker socket into the container basically gives the container full access to all resources of the host and also of all other containers. Therefore I would like to request the implementation of a new feature that allows namespacing containers.\r\n\r\nIf this feature gets implemented, a project that currently requires the docker socket mounted in for scalability would be able to start new containers (but only with equal or less privileges as the current one).\r\n\r\nWhat limitations/implications does this need to have?\r\n\r\n* All resources of that compose project or container get prefixed with a unique namespace, therefore docker can differentiate, which volumes, networks are allowed to be mounted dynamically in addition to the ones mounted to the scheduler container.\r\n* If the namespace already exists, starting the project should fail (the scheduler cannot be started)\r\n* A scheduler that was started without `--privileged` cannot create workers with `--privileged` and is therefore limited to its own privileges\r\n* If user and group ids are namespaced, the workers should automatically be placed into the same namespace.\r\n* If resource constraints are in place for the scheduler, they are shared across the scheduler and all workers.\r\n* If the scheduler is part of a docker-compose setup, it can also create workers with access to other resources specified in that docker-compose setup.\r\n\r\nExample project that has it's own scheduler: [Boinc](https://github.com/marius311/boinc-server-docker/blob/master/docker-compose.yml)"},{"labels":[null,null,"enhancement1"],"text":"I'd like to propose eventually adding a new Engine REST API which exposes buildkit's `Solve` more directly than `/build` which is focused around the dockerfile frontend specific use case and has some quirks due to that (and for legacy/historical reasons).\r\n\r\nI've prototyped running a custom frontend via the `/build` API but it's a bit hacky and not terribly satisfactory (injecting a stub `Dockerfile` with a `syntax` directive in it, passing options by mapping to various `Dockerfile`-ish options etc). \r\n\r\nOther potentially interesting things  to expose are local exporter, debugger, shared session, multiple local sources and the ability to run a frontend client side (cf `Build` rather than `Solve` in the buildkit control API).\r\n\r\nIt's possible we could just extend on the current `/build` API to cover more underlying functionality (e.g. add a `Frontend` field to it), enhance the format of the returned body to remove the legacy framing etc.\r\n\r\nOr we could perhaps expose a new endpoint mapping the underlying `Solve` call a little more directly that the `/build` endpoint, with most of the options passed through or lightly adjusted/filtered (e.g maybe the full set of `Exporter`/`ExporterAttrs` flexibility shouldn't be exposed?). The `Session` side would be the same as with `/build` (an upgraded conn to `/session`). The body returned would be the Status stream, I guess as a direct JSON stream of `StatusResponse` rather than wrapping in a `JSONMessage` like `/build` had to do (for compat reasons I suppose). I'm not entirely sure about `SolveResponse`, so I guess maybe there needs to be a little more structure to the body stream, but ideally not `JSONMessage`, just a simple union type map.\r\n\r\nA possibly simple (but IMHO not at all ideal) would be a simple API end point which opens the entire control API gRPC via a /session-like endpoint. I don't think that is really what we want though (control API is not considered stable, might be too much low level power being exposed).\r\n\r\n/cc @tonistiigi @tiborvass "},{"labels":[null,"enhancement1"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nThis is feature request instead of bug report. The typical usage scenario is the workloads (like DPDK) running as a kubernetes pod needs to take use of the hugetlb-limit support to decrease the possibility of page fault. I noticed that there's already a PR #29911 has been summit by @x1022as from Huawei, I've contacted with him couple weeks ago and he have been busying with other tasks, he suggested we pick it up and re-submit it as a new PR to fulfill the corresponding features. But recently I have some job changes and will not take time to do that, so here I introduce my colleague @lubinsz to re-kick off this topic.\r\n\r\nHello @thaJeztah @justincormack @tonistiigi @dnephin @cpuguy83 and any other relevant maintainers/developers who may be interested in this topic yet I don't loop them in, please help @lubinsz to make it forward in term of this task. \r\n\r\nThanks!\r\n \r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1.\r\n2.\r\n3.\r\n\r\n**Describe the results you received:**\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement1"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\nI have Docker working in a CI setup where each branch and PR has it's own docker container.  This is working wonderfully and each developer is able to make changes to their environment without explicitly needing to come to the OPs team!  Kudos to the docker team for making this awesome software and for all the caching layers you have but into make this work really well!\r\n\r\nHowever, I am facing an issue where I'd like to know whether or not a given CI job will trigger a docker build.  Docker builds in our environment are fairly expensive (source code is built in docker layers) .  We allow this because it's been the fastest way to get developers to continue moving without forcing them to learn how to do proper packaging.\r\n\r\nNow the problem arises in CI where I'd like to limit the number of ongoing docker builds without blocking changes that do not require a docker rebuild.  To this end, I have a docker pipeline which runs docker with a short timeout and then pops it over to a \"docker builder\" node in the case of a docker rebuild.  This works but it's not really ideal because it wastes a couple of minutes waiting for the timeout to occur and even worse I can't tell if the build was fully cached or just completed really quickly.\r\n\r\nIs it possible to implement something similar to \"make -k\" which just simply checks if a rebuild is required?  \r\n\r\n**Steps to reproduce the issue:**\r\n1. Build image\r\n2. Make changes to repo\r\n3. Rebuild image \r\n\r\n**Describe the results you received:**\r\nNo option to check if a rebuild is required\r\n\r\n**Describe the results you expected:**\r\nA --dry-run or -k option similar to Make\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nThis could theoretically be implemented by Make but it would be nice to identify this from docker in order to reduce this to a single source of truth.  \r\n\r\n**Output of `docker version`:**\r\n$ docker version\r\nClient:\r\n Version:           18.06.0-ce\r\n API version:       1.38\r\n Go version:        go1.10.3\r\n Git commit:        0ffa825\r\n Built:             Wed Jul 18 19:11:02 2018\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          18.06.0-ce\r\n  API version:      1.38 (minimum version 1.12)\r\n  Go version:       go1.10.3\r\n  Git commit:       0ffa825\r\n  Built:            Wed Jul 18 19:09:05 2018\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Output of `docker info`:**\r\n$ docker info\r\nContainers: 33\r\n Running: 4\r\n Paused: 0\r\n Stopped: 29\r\nImages: 17\r\nServer Version: 18.06.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc nvidia\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: d64c661f1d51c48782c9cec8fda7604785f93587\r\nrunc version: 69663f0bd4b60df09991c08812a60108003fa340\r\ninit version: fec3683\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-134-generic\r\nOperating System: Ubuntu 16.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 28\r\nTotal Memory: 125.8GiB\r\nName: lab1.smnet\r\nID: RZD3:FFJ6:5HCI:5YGZ:4NUT:RCEO:4U3Z:THKC:CUGF:JLNL:RHDY:KS5P\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: ssicspenserg\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nRegistry Mirrors:\r\n https://dockermirror.smnet:5000/\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nPhysical Ubuntu 16.04 system with Nvidia Docker and Docker installed."},{"labels":[null,"enhancement1"],"text":"Hi! Now that we have a solid way to version secrets using `docker stack deploy` using the `name` property as described in https://github.com/moby/moby/issues/35229, our CI platform can now version secrets when we deploy changes to them and it works great. Thanks!\r\n\r\nHowever, now when we've deployed a few changes to secrets (which we do often for security purposes), we run into an issue where our Swarm has all the old, unused secrets hanging out (for multiple stacks) and it seems to be pretty messy. I'm assuming this isn't a security problem, and it's nice to be able to roll back to an earlier version easily, but would it be possible to add e.g. `docker secrets prune` as a command line option to get rid of old secrets? At this point, unless I've missed something i[n the docs](https://docs.docker.com/config/pruning/), there's no way to do this for secrets or configs, they need to be deleted manually. \r\n\r\nWe can definitely script something, but pruning extraneous containers, volumes, etc. are all CLI features, it seems to be anti-pattern that we don't get that option with secrets and configs.  I guess the command would ideally have an option to limit how many secrets/config versions to keep, to allow rolling back a couple versions, so `docker secrets prune --keep 5` would keep the last 5 by created or updated date. \r\n\r\nThoughts? Not mission critical but seems like it makes sense."},{"labels":[null,null,null,"enhancement1"],"text":"I am working on an authorization plugin for Docker CE using the documented authorization API. I would like to relate the two daemon-to-plugin requests (AuthZReq and AuthZRes) that service the same originating client-to-daemon API call.  In many cases, except at least for certain Image operations, it is useful for authorizing a create operation and then intercepting the newly created object ID. While both authorization plugin phases are provided with the same information (i.e. RequestBody) there is not enough information in that alone to reliably recognize an AuthZReq event's corresponding AuthZRes event when several identical API calls are made rapidly. As with other examples of HTTP-based services, tracking a single user request through processing may be much simpler by introducing a practically unique user request ID accompanying the request's representation in each step. Such a thing might be emitted in each log line generated during the request's processing to facilitate forensics and analytics. In my own case, the request ID would be likewise be passed to both the request's AuthZReq and AuthZRes plugin calls, making obvious their relationship.\r\n\r\nIs this something which can be naturally implemented within the Docker daemon architecture? Would it be a reasonably simple matter to do so?\r\n\r\nCan this be done using a middleware component to add the client request ID as a request header or in the vars map? Although it seems like a more fundamental behavior than would be made optional in middleware."},{"labels":[null,"enhancement1"],"text":"By default names of containers in same network resolves to their ip addresses. It is conveniently.\r\n\r\nIt be useful if would be a similar feature, but for gateway. Then it will be possible to contact with host system simply via gateway name without any workarounds like predefining gateway ip address or anything else horrible.\r\n\r\nFor example:\r\n```\r\n$ docker exec -ti app_1 bash\r\nroot@16ac119cbe94:/# ping default_gateway\r\nPING default_gateway (172.29.0.1): 56 data bytes\r\n64 bytes from 172.29.0.1: seq=0 ttl=64 time=0.076 ms\r\n64 bytes from 172.29.0.1: seq=1 ttl=64 time=0.083 ms\r\n^C\r\n--- default_gateway ping statistics ---\r\n2 packets transmitted, 2 packets received, 0% packet loss\r\nround-trip min/avg/max = 0.076/0.079/0.083 ms\r\n```\r\n\r\nIn my case:\r\n1. A container is connected only to one internal network, but it must able to send email\r\n2. A host system have smtp sever, which listening 25 port on the all interfaces"},{"labels":[null,"enhancement1"],"text":"`docker run -v /:/host:ro` (`docker run --mount type=bind,src=/,dst=/host,ro`) is not really read-only, because there is no way in the kernel to bind-mount a filesystem tree recursively as read-only.\r\n\r\nSo I suggest emulating \"recursive read-only bind-mount\" using FUSE.\r\nCLI would be like `docker run --mount type=bind,src=/,dst=/host,ro,bind-driver=fuse`.\r\n\r\nIf we don't want to use FUSE, an alternative way is to bind the tree as `rprivate` and then bind each of submounts explicitly.\r\nHowever, `rprivate` does not work for `/`, `/var/lib`, and `/var/lib/docker/*`:  https://github.com/moby/moby/commit/589a0afa8cbe39b6512662fd1705873e2d236dd0\r\nAlso,  it doesn't work for those who wants to use the shared propagation.\r\n\r\n@justincormack @cpuguy83 @dmcgowan "},{"labels":[null,"enhancement1",null],"text":"**Description**\r\n\r\nThere appears to be no way to modify `--log-opt` settings on an existing container, running or stopped.\r\n\r\nI can't think of any reason it should be impossible to implement this, since all log drivers are write-only except for `json-file` and `journald`. Most of the options for those two only affect how logs are written, with the exception of the `json-file` log rotation options.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Discover that Docker supports log rotation but it is disabled by default.\r\n2. Configure `/etc/docker/daemon.json` to automatically rotate logs on new containers:\r\n    ```\r\n    {\r\n            \"experimental\": true,\r\n            \"registry-mirrors\": [\"https://dockermirror.local.lubar.me\"],\r\n            \"log-opts\": {\r\n                    \"max-size\": \"100m\",\r\n                    \"max-file\": \"5\"\r\n            }\r\n    }\r\n    ```\r\n3. Run `docker update foo --log-opt max-size=100m --log-opt max-file=5` (`unknown flag: --log-opt`)\r\n4. Attempt to POST `{\"HostConfig\":{\"LogConfig\":{\"Type\":\"json-file\",\"Config\":{\"max-size\":\"100m\",\"max-file\":\"5\"}}}}` to `/containers/foo/update` (returns `{\"Warnings\":null}` and appears to have no effect)\r\n\r\n**Describe the results you received:**\r\nThere appears to be no way to enable log rotation on an existing container.\r\n\r\n**Describe the results you expected:**\r\nThere should be *some* way to modify the logging engine configuration on a container that already exists.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.05.0-ce\r\n API version:  1.29\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:10:54 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.05.0-ce\r\n API version:  1.29 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:10:54 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```"},{"labels":[null,"enhancement1"],"text":"I want to connect to a VPN from within the container during a multi-stage build but can't since there is no way to enable the creation of a tun network interface.\r\n\r\nPlease advise as to whether it would be possible to add this functionality to docker build.\r\n\r\nThanks.\r\n\r\n*Possibly related to #1916 "},{"labels":[null,null,"enhancement1",null],"text":"POC: https://github.com/AkihiroSuda/docker/commits/rootless\r\n\r\n# Rootless mode (Experimental)\r\n\r\nThe rootless mode allows running `dockerd` as an unprivileged user, using `user_namespaces(7)`, `mount_namespaces(7)`, `network_namespaces(7)`, and [VPNKit](https://github.com/moby/vpnkit).\r\n\r\nNo SUID binary is required except `newuidmap` and `newgidmap`.\r\n\r\n## Requirements\r\n* `newuidmap` and `newgidmap` need to be installed on the host. These commands are provided by the `uidmap` package.\r\n\r\n* `/etc/subuid` and `/etc/subgid` should contain >= 65536 sub-IDs. e.g. `penguin:231072:65536`.\r\n\r\n```console\r\n$ id -u\r\n1001\r\n$ grep ^$(whoami): /etc/subuid\r\npenguin:231072:65536\r\n$ grep ^$(whoami): /etc/subgid\r\npenguin:231072:65536\r\n```\r\n\r\n* Some distros such as Debian and Arch Linux require `echo 1 > /proc/sys/kernel/unprivileged_userns_clone`.\r\n\r\n## Restrictions\r\n\r\n* Only `vfs` graphdriver is supported. However, on [Ubuntu](http://kernel.ubuntu.com/git/ubuntu/ubuntu-artful.git/commit/fs/overlayfs?h=Ubuntu-4.13.0-25.29&id=0a414bdc3d01f3b61ed86cfe3ce8b63a9240eba7) and a few distros, `overlay2` and `overlay` are also supported. [Starting with Linux 4.18](https://www.phoronix.com/scan.php?page=news_item&px=Linux-4.18-FUSE), we will be also able to implement FUSE snapshotters.\r\n* Cgroups, AppArmor, and SELinux are disabled at the moment. (FIXME: we could enable Cgroups if configured on the host)\r\n* Checkpoint is not supported at the moment.\r\n* Running rootless `dockerd` in rootless/rootful `dockerd` should be also possible, but not tested yet.\r\n\r\n## Usage\r\n\r\n### Daemon\r\nBefore running `dockerd` you need to unshare userns, mountns, and netns.\r\n\r\nYou may use [RootlessKit](https://github.com/AkihiroSuda/rootlesskit) for unsharing them and [VPNKit](https://github.com/moby/vpnkit) for enabling usermode networking.\r\n\r\nIf your `/etc/resolv.conf` is managed by systemd or NetworkManager, you need to run RootlessKit with `--copy-up=/etc` so as to prevent `/etc/resolv.conf` in the namespace from being unexpectedly unmounted when `/etc/resolv.conf` is recreated on the host.\r\n\r\nAlso, currently you need to mount `/run/docker` as tmpfs before running `dockerd`, because \"/run/docker/libnetwork\" is still hard-coded in `vendor/github.com/docker/libnetwork/sandbox_externalkey_unix.go`.\r\n\r\n```\r\n$ docker-rootlesskit --net=vpnkit --vpnkit-binary=docker-vpnkit --copy-up=/etc \\\r\n  sh -ec \"mount -t tmpfs none /run/docker; dockerd --experimental\"\r\n```\r\n\r\nIf `/run/docker` mount point is not available on your host, you can create the mount point by running RootlessKit with `--copy-up=/run`:\r\n\r\n```console\r\n$ docker-rootlesskit --net=vpnkit --vpnkit-binary=docker-vpnkit --copy-up=/etc --copy-up=/run \\\r\n  sh -ec \"mkdir -p /run/docker; mount -t tmpfs none /run/docker; dockerd --experimental\"\r\n```\r\n\r\nRemarks:\r\n* The socket path is set to `/run/user/$UID/docker.sock` by default.\r\n* The data dir is set to `~/.local/share/docker` by default.\r\n* The exec dir is set to `/run/user/$UID/docker` by default.\r\n* The config dir is set to `~/.config/docker` (not `~/.docker`) by default.\r\n\r\n### Client\r\n\r\nYou can just use the upstream Docker client (without `nsenter`-ing to the `dockerd` namespaces), but you need to set the socket path.\r\n\r\n```console\r\n$ docker -H unix:///run/user/1001/docker.sock run -d nginx\r\n```\r\n\r\n### Exposing ports\r\n\r\nIn addition to exposing container ports to the `dockerd` network namespace, you also need to expose the ports in the network namespace to the host network namespace.\r\n\r\ne.g.\r\n```console\r\n$ docker-rootlesskit --state=/tmp/foo ...\r\n```\r\n\r\n```console\r\n$ docker -H unix:///run/user/1001/docker.sock run -d -p 80:80 nginx\r\n$ socat -t -- TCP-LISTEN:8080,reuseaddr,fork EXEC:\"nsenter -U -n -t $(cat /tmp/foo/child_pid) socat -t -- STDIN TCP4\\:127.0.0.1\\:80\"\r\n```\r\n\r\nIn future, we could integrate RootlessKit into `dockerd` for exposing the namespace ports automatically."},{"labels":["enhancement1",null],"text":"for solving in elegant way the problem of same user in volume mounted with container folder ... \r\ndocker solve automatic the problem: add a instruction USER <username> for exposing a user in this way the user have the same id automatically in the host and in the container. In the host will be generated a user docker_<username>. In this way you can also exposing multiple users .\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"# Add support for multiple (named) build-contexts\r\n\r\n\r\nRelated issues:\r\n\r\n- https://github.com/moby/moby/issues/2745 \"Add with relative path to parent directory fails with \"Forbidden path\"\"\r\n- https://github.com/moby/moby/issues/18789 Dockerfile ADD relative symlinks support\r\n- https://github.com/moby/moby/issues/12886 \"Add support for specifying .dockerignore file with -i/--ignore\"\r\n- probably others\r\n\r\n\r\n# Problem statement\r\n\r\nTake the following directory structure for a project;\r\n\r\n```\r\nproject\r\n ├── .dockerignore\r\n ├── Dockerfile\r\n ├── ginormous\r\n │   ├── big-file-1\r\n │   ├── big-file-2\r\n │   └── big-file-xx\r\n ├── Makefile\r\n ├── service1\r\n │   ├── Dockerfile\r\n │   └── src\r\n │       ├── Makefile\r\n │       ├── source-file-1\r\n │       ├── source-file-2\r\n │       └── source-file-xx\r\n ├── service2\r\n │   ├── Dockerfile\r\n │   └── src\r\n │       ├── Makefile\r\n │       ├── source-file-1\r\n │       ├── source-file-2\r\n │       └── source-file-xx\r\n ├── common\r\n │   └── src\r\n │       ├── 0-various\r\n │       ├── 1-files\r\n │       ├── 2-used-by\r\n │       ├── 3-service-1-and-2\r\n └── common-2\r\n     └── src\r\n         ├── 0-various\r\n         ├── 1-files\r\n         ├── 2-used-by\r\n         ├── 3-service-1-and-2\r\n```\r\n\r\nIn the above;\r\n\r\n- the `Dockerfile` at the root of the project uses `ginormous`, and `shared`\r\n- `service1` has a Dockerfile, and source-files used to build the service in `service1/src`\r\n- `service2` has a Dockerfile, and source-files used to build the service in `service2/src`\r\n- both `service1` and `service2` share some code/resources, located in `common` and `common-2`\r\n- nor `service1`, nor `service2` use `ginormous` (a big directory)\r\n\r\n\r\n## Challenges with this example\r\n\r\nBuilding service1 and 2 is a challenge;\r\n\r\n- When building a Dockerfile, all files used have to be within the build-context. This means that in the project structure above, the only context that can be used is the root `project` directory. Doing so results in the entire project, including `ginormous` to be sent to the daemon (even though it's not used at all). Relative paths outside of the build-context cannot be used (also see https://github.com/moby/moby/issues/2745)\r\n- Similarly; when contructing/sending the build-context, docker won't resolve symlinks, and copy symlinks as-is, so putting symlinks to `common` and `common-2` inside  `project1` and `project2` will not resolve this problem.\r\n- Only a single `.dockerignore` is supported, so it's not possible to \"conditionally\" exclude files (e.g. when building `service1`, exclude the `ginormous` and `service2` directories, and vice-versa)\r\n\r\n\r\n## Proposal: allow multiple (named) build-contexts\r\n\r\nI propose to add support for multiple build contexts, implemented as a `--context` flag on `COPY` and `ADD`, and a `--context <name>=<path>` option on the `docker image build` subcommand.\r\n\r\nFor example, to build `project1`, the Dockerfile could look like this:\r\n\r\n```Dockerfile\r\nFROM baseimage\r\n\r\n# no --context option set: use the default build-context\r\nCOPY . /build/service1/src/\r\n\r\n# use the build-context named \"common\"\r\nCOPY --context=common   . /build/common/src/\r\n\r\n# use the build-context named \"common-2\"\r\nADD --context=common-2 . /build/common-2/src/\r\n\r\nRUN cd /build && make && make install\r\n```\r\n\r\nWhen building the Dockerfile, docker expects two named build-contexts to be provided, in addition to the default (positional) build-context:\r\n\r\nFrom within the `project` directory:\r\n\r\n```bash\r\ndocker build \\\r\n  -f ./service1/Dockerfile \\\r\n  --context common=./common/src \\\r\n  --context common-2=./common-2/src \\\r\n  ./service1\r\n````\r\n\r\nIn the above:\r\n\r\n- `-f ./service1/Dockerfile` is the Dockerfile used to build the image\r\n- the `./common/src` directory is used as build-context \"common\"\r\n- the `./common-2/src` directory is used as build-context \"common-2\"\r\n- the `./service1` directory is used as default build-context\r\n\r\nOnly the `common/src`, `common-2/src` and `service1` directories are uploaded to the daemon. All other directories are not part of the build-context, so won't be uploaded.\r\n\r\n\r\nSimilarly, when building from within the `project/service1` directory:\r\n\r\n```bash\r\ndocker build \\\r\n  --context common=../common/src \\\r\n  --context common-2=../common-2/src \\\r\n  .\r\n````\r\n\r\n- No `-f` is provided, so the Dockerfile in the current directory is used to build the image.\r\n- the `../common/src` directory is used as build-context \"common\"\r\n- the `../common-2/src` directory is used as build-context \"common-2\"\r\n- the current (`.`) directory is used as default build-context\r\n\r\n\r\nBoth relative and absolute paths can be used to specify the location of a build-context, so all of these are valid:\r\n\r\n```\r\n--context foo=~/go/src/github.com/foobar/foo\r\n--context bar=/dev/sdb/share/bar/\r\n--context bar=./some/dir\r\n--context baz=../../../foobar\r\n```\r\n\r\n## Validation\r\n\r\n### Validation: missing build-contexts\r\n\r\nBefore building (and sending the build-context), docker validates if all build-contexts are provided. If a context is missing, an error is produced, and the build is aborted. Trying to build the Dockerfile from the example above without specifying any build-context:\r\n\r\n\r\n```bash\r\ndocker build .\r\nError: missing build-context \"common\"\r\nError: missing build-context \"common-2\"\r\n```\r\n\r\nIn a multi-stage build, only contexts that are required for the stages that are built should be taken into account. For example:\r\n\r\n```Dockerfile\r\nFROM baseimage AS stage-one\r\nCOPY --context=one /subdir/foo /target/dir\r\n\r\nFROM busybox AS stage-two\r\nADD --context=two /foo.tar.gz /target\r\n\r\nFROM scratch AS final\r\nCOPY --from=stage-one /foo /bar\r\nCOPY --from=stage-two /bar /baz\r\nCOPY --context=config /config.ini /config.ini\r\n```\r\n\r\nGiven the Dockerfile above:\r\n\r\nBuilding just `stage-one`:\r\n\r\n```bash\r\ndocker build --target=stage-one .\r\nError: missing build-context \"one\"\r\n```\r\n\r\nBuilding up until `stage-two`:\r\n\r\n```bash\r\ndocker build --target=stage-two .\r\nError: missing build-context \"one\"\r\nError: missing build-context \"two\"\r\n```\r\n\r\nBuilding the whole Dockerfile:\r\n\r\n```bash\r\ndocker build .\r\nError: missing build-context \"one\"\r\nError: missing build-context \"two\"\r\n```\r\n\r\nThe default build context (positional argument) is never optional:\r\n\r\n```bash\r\ndocker build --context one=./one --context two=./two\r\n\"docker build\" requires exactly 1 argument.\r\nSee 'docker build --help'.\r\n\r\nUsage:  docker build [OPTIONS] PATH | URL | - [flags]\r\n\r\nBuild an image from a Dockerfile\r\n```\r\n\r\n\r\n### Validation: unused build-contexts\r\n\r\nSimilar to unused `--build-arg`, specifying a build-context that is not used will produce a warning. When determining which contexts are expected/used in \r\n\r\nGiven the Dockerfile from the previous example:\r\n\r\n```bash\r\ndocker build --target=stage-one --context one=./one --context two=./two .\r\nSending build context to Docker daemon  2.048kB\r\nStep 1/4 : FROM baseimage AS stage-one\r\n.....\r\nRemoving intermediate container edc9bb0f15dc\r\n ---> a77418a9ddad\r\n[Warning] One or more contexts [two] were not consumed\r\nSuccessfully built a77418a9ddad\r\n```\r\n\r\n### Validation: conflicting options\r\n\r\nThe `--context` and `--from` options cannot be combined. Using both will produce an error:\r\n\r\n```Dockerfile\r\nFROM baseimage AS stage-one\r\nRUN echo foo\r\n\r\nFROM busybox\r\nCOPY --from=stage-one --context=foo /foo /bar\r\n```\r\n\r\n```bash\r\ndocker build --context=one .\r\nSending build context to Docker daemon  2.048kB\r\nError response from daemon: Dockerfile error line 5: conflicting options '--from' and '--context'\r\n```\r\n\r\nThe `--context` option _can_ be combined with the `--chown` option.\r\n\r\n\r\n### Validation: context names\r\n\r\nBuild context names follow these rules:\r\n\r\n- lowercase Alphanumeric characters (a-z, 0-9)\r\n- punctuation symbols: dashes and underscores\r\n- must start, and end, with an alphanumberic character\r\n- consecutive punctuation symbols are not allowed\r\n\r\nSpecifying an invalid context-name (either on the command-line, or inside a Dockerfile) produces an error."},{"labels":[null,null,null,"enhancement1"],"text":"https://github.com/opencontainers/runtime-spec/blob/master/config-linux.md#memory\r\n\r\n`.linux.resources.memory.kernelTCP` is defined in runtime spec and supported by runc.\r\n\r\nHowever, Docker/Moby does not expose API/CLI for setting this value currently, unlike [`.linux.resources.memory.kernel`](https://github.com/moby/moby/blob/3a633a712c8bbb863fe7e57ec132dd87a9c4eff7/api/types/container/host_config.go#L331).\r\n"},{"labels":[null,"enhancement1"],"text":"The gelf logger has some great fields that I definitely want in my structured log server (e.g., Graylog), such as (full) container ID, image ID, and creation time.\r\n\r\nMy application uses structured logging and also has some fields that I definitely want in my structured log server, such as userID.\r\n\r\nI can make my application log JSON to stdout, one line per message, pretty easily. What I want is for the gelf logger to take an option that parses `msg.Line` as JSON and merges it with its own fields before sending the log message over GELF.\r\n"},{"labels":[null,"enhancement1"],"text":"This subject has been mentioned multiple times already, starting back in 2015, but I would like to revive this discussion.\r\n\r\n## Current status\r\nToday Docker supports swapping `runc` with another OCI compliant runtime:\r\nhttps://docs.docker.com/engine/reference/commandline/dockerd/#docker-runtime-execution-options\r\n\r\nHowever, this approach is not composable, you can't combine custom runtimes.\r\nIf I want a to customize the behavior of runc for Docker today, I need to fork runc and add my modifications, then ship the whole binary.\r\n\r\n## Proposal\r\nFor this reason, I'm asking the moby community to (re)consider exposing standard OCI hooks to the \"docker run\" API, those hooks are defined in the OCI spec:\r\nhttps://github.com/opencontainers/runtime-spec/blob/master/config.md#posix-platform-hooks\r\n\r\nAs a reference, the OCI runtime spec defines when those hooks must be called:\r\nhttps://github.com/opencontainers/runtime-spec/blob/master/runtime.md#lifecycle\r\n\r\nHooks could be declared at the docker daemon level, similarly to what we have today for OCI runtimes:\r\n```\r\n{\r\n\t\"hooks\": {\r\n\t\t\"prestart\": [\r\n\t\t\t\"my-hook\": {\r\n\t\t\t\t\"path\": \"/usr/local/bin/my-prestart-hook\",\r\n\t\t\t\t\"args\": [\"--debug\"]\r\n\t\t\t},\r\n\t\t...\r\n\t\t],\r\n\t\t\"poststart\": [\r\n\t\t...\r\n\t\t]\r\n\t}\r\n}\r\n```\r\nSimilar to what's proposed here: https://github.com/moby/moby/issues/28837\r\n\r\nAs mentioned in https://github.com/moby/moby/pull/33375#issuecomment-351612832, we could use it the following way:\r\n```\r\ndocker run --exec-opt prestart=hook1,hook2\r\n```\r\nOr, if you want to avoid users having to remember the type of your hook:\r\n```\r\ndocker run --exec-opt hooks=hook1,hook2\r\n```\r\n\r\nThe `exec-opt` option already exists at the daemon level, it would make sense to reuse it.\r\n\r\n\r\n## Related discussions:\r\nWith @3XX0 we work at NVIDIA on enabling GPU applications to run in containers, mostly for machine learning and HPC applications.\r\n\r\n- The initial discussion is here: https://github.com/moby/moby/issues/23917\r\n- I briefly summarized our current approach here: https://github.com/moby/moby/issues/28837#issuecomment-351611903\r\nIt's not ideal because we have to follow the docker releases and ship a full copy of runc (at the same revision).\r\n- For hooks in general, the original discussion is here (2015): https://github.com/moby/moby/pull/17021\r\nThe patch is still applied in Project Atomic's fork of moby: https://github.com/projectatomic/docker/tree/docker-17.03.1-ce#add-dockerhooks-exec-custom-hooks-for-prestartpoststop-containerspatch\r\nGPU support in Docker containers is actually simpler on this fork, instead of having a custom runc, you just drop our hook binary (much smaller) to a folder, and it works fine.\r\n- Note that this approach was carried in Kubernetes for the CRI-O project: https://github.com/kubernetes-incubator/cri-o/blob/master/hooks.md\r\nIt's also a good reference on some of the potential problems with hooks, which mean we might want to prohibit users from registering default hooks.\r\nGPU support in CRI-O works fine too, thanks to this.\r\n- Finally, I believe it would also cover the use cases from this proposal: https://github.com/moby/moby/pull/33375#issuecomment-351612832\r\n\r\n## Next steps\r\nIf core moby maintainers agree on this RFE, I can submit a design doc so we can start discussing the details.\r\n\r\n## cc fest\r\n@3XX0 @RenaudWasTaken: NVIDIA colleagues\r\n@rhatdan: for the initial proposal in 2015\r\n@WeiZhang555: for trying to revive the idea in 2016\r\n@crosbymichael @stevvooe: since you looked at libnvidia-container recently, maybe you are preparing something at the containerd level directly?\r\n@mrunalp: since we discussed this recently\r\n@cmluciano: since you filed an issue against our runtime, asking for other options"},{"labels":[null,"enhancement1"],"text":"For the sake of simplification working with docker (a better UX).\r\n\r\nWhen I need to run some kind of service with docker for the very first time using an existing image, most of the services I use, are coming with a basic configuration files/folders or a clean website files generated, downloaded or created in the container by running the images. So, I always find a workaround and many times ending up making mistakes of copying not the right file and not to the right location and also result with creating a tons of temp files. Which is pretty annoying, consider the fact that I need to do it over and over again, nearly every day.\r\n\r\nTwo examples fo this case:\r\n\r\n1. Create influxdb service, configure and run it.\r\n    ```bash\r\n    docker run --rm influxdb cat /etc/influxdb/influxdb.conf > influxdb.conf\r\n    vim influxdb.conf #modify it accordingly\r\n    docker run --name influxdb \\\r\n      --publish 8086:8086 \\\r\n      --volume $PWD/data:/var/lib/influxdb \\\r\n      --volume $PWD/influxdb.conf:/etc/influxdb/influxdb.conf:ro \\\r\n      --restart=always \\\r\n      --detach \\\r\n      influxdb\r\n    ```\r\n2. Run a clean magento2 application.\r\n   ```bash\r\n   docker create --name magento alexcheng/magento\r\n   docker cp magento:/var/www/html .\r\n   docker rm magento\r\n   docker run --name magento \\\r\n     --publish 80:80 \\\r\n     --volume $PWD/html:/var/www/html \\\r\n     --env-file $PWD/env \\\r\n     --detach \\\r\n     alexcheng/magento\r\n   ```\r\n\r\nInstead, it would be great to accomplish this copy of file by providing some kind of flag to the path of the `--volume` option.\r\nFor example, all mounted volumes ending with `:ch` (\"Copy to Host\" acronym) will not only create a mount point, but also copy the file (or folder's content) into the host, only in case the folder is empty or folder/file not exists on the host machine.\r\nAlternatively, we can change the option from `--volume` to `--volume-copy` that indicates that the following mount point should be copied from container to host.\r\n\r\nIn the first example, the `--volume` row may look like this:\r\n```bash\r\n--volume $PWD/influxdb.conf:/etc/influxdb/influxdb.conf:ro:ch\r\n```\r\nor instead can be changed to `--volume-copy`\r\n```bash\r\n--volume-copy $PWD/influxdb.conf:/etc/influxdb/influxdb.conf:ro\r\n```\r\n\r\nIn the second, all the commands can be summed to a single one:\r\n```bash\r\ndocker run --name magento \\\r\n  --publish 80:80 \\\r\n  --volume $PWD/html:/var/www/html:ch \\\r\n  --env-file $PWD/env \\\r\n  --detach \\\r\n  alexcheng/magento\r\n```\r\nor using `--volume-copy`\r\n```bash\r\ndocker run --name magento \\\r\n  --publish 80:80 \\\r\n  --volume-copy $PWD/html:/var/www/html \\\r\n  --env-file $PWD/env \\\r\n  --detach \\\r\n  alexcheng/magento\r\n```\r\n\r\n\r\nWhat do you think for this suggestion?\r\nDoes anybody also stumbled upon this annoying usage?"},{"labels":[null,null,"enhancement1"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nCurrently managed plugins have to be configured and enabled per swarm node.  Is it possible to deploy the plugin across the swarm?\r\n\r\n**Steps to reproduce the issue:**\r\n1. build a swarm\r\n2. `docker plugin install`\r\n\r\n**Describe the results you received:**\r\n\r\nIt only installs on the node where the command was executed.\r\n\r\n**Describe the results you expected:**\r\n\r\nIt should install across the nodes.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:       18.03.0-ce\r\n API version:   1.37\r\n Go version:    go1.9.4\r\n Git commit:    0520e24\r\n Built: Wed Mar 21 23:06:28 2018\r\n OS/Arch:       windows/amd64\r\n Experimental:  false\r\n Orchestrator:  swarm\r\n\r\nServer:\r\n Engine:\r\n  Version:      18.03.1-ce\r\n  API version:  1.37 (minimum version 1.12)\r\n  Go version:   go1.9.5\r\n  Git commit:   9ee9f40\r\n  Built:        Thu Apr 26 07:23:58 2018\r\n  OS/Arch:      linux/amd64\r\n  Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement1"],"text":"Hello,\r\n\r\nIt would be great to support \"--from\" like COPY but for ADD statement.\r\nI build image in 2 separated images : 1 for compilation that output au rootfs archive, and another which ADD rootfs.tar as / from a scratch image.\r\nI think that MSB is a good idea but ADD con't take \"--from\" parameter.\r\n\r\nSample Dockerfile used (https://gist.github.com/Zenithar/9209968).\r\n\r\nRegards,"},{"labels":[null,"enhancement1",null],"text":"How about implement a new service type for swarm mode\r\ntype: failover\r\nreplicas: only one\r\nand all options from \"docker run\" as --ip  --cap-add etc.\r\nAnd if use macvlan + distributed file system such as ceph can be implemented failover migration for monolithic app for which the license is assigned to ip address."},{"labels":[null,"enhancement1"],"text":"This is a feature request for adding support in docker of AWS Kinesis Streams.\r\nThis will allow to use scalable Kinesis as a \"buffer\" and avoid losing logs when the ingestion agent is not available (down) for some reason."},{"labels":[null,"enhancement1"],"text":"A server is hosting a rootfs-wrapper.tar.xz file which instead of having this structure:\r\n\r\n```\r\n/bin\r\n/sbin\r\n/usr\r\n...\r\n```\r\n\r\nis having a wrapper directory\r\n\r\n```\r\n/wrapper/bin\r\n/wrapper/sbin\r\n/wraper/usr\r\n...\r\n```\r\n\r\nWith regular shell we can do:\r\n\r\n```sh\r\ncurl -O http://site/rootfs-wrapper.tar.xz\r\ntar -xf rootfs.tar.xz --strip-components=1 -C .\r\n```\r\n\r\nHow do we do that with `scratch` docker where we don't have access to `tar` and `curl` and viable option is to use `ADD` command\r\n\r\n```dockerfile\r\nFROM scratch\r\nADD http://site/rootfs-wrapper.tar.xz /\r\nCMD [\"/bin/sh\"]\r\n```\r\n\r\nit builds fine with `docker build -t simple-build .`, but fails to run `docker run -it simple-build` gives:\r\n\r\n```\r\nError response from daemon: OCI runtime create failed: container_linux.go:296: starting container process caused \"exec: \\\"/bin/sh\\\": stat /bin/sh: no such file or directory\": unknown.\r\n```\r\n\r\nIf we use `rootfs.tar.xz` without `wrapper/` then docker run works without modifying the docker.\r\nI tried setting `CMD [\"/wrapper/bin/sh\"]` in docker as well but same result.\r\n\r\nIs there a way to make `ADD` command strip components of tar?\r\n"},{"labels":[null,"enhancement1"],"text":"📃 **Description:**  \r\n\r\nI'd like to see `docker build --profile` exist to generate a report at the end of a build that contains each step, the duration that step took to execute, and the layer size added to the image during that stage.\r\n\r\n:ballot_box_with_check: **Steps to produce the desired outcome:**\r\n\r\n`docker build -t repo/imagename:tag --profile .`\r\n\r\n🎉 **Describe the results you expect:**\r\n\r\n```bash\r\n➭ docker build -t repo/imagename:tag --profile .\r\nSending build context to Docker daemon  2.048kB\r\nStep 1/3 : FROM busybox\r\n ---> f6e427c148a7\r\nStep 2/3 : RUN sleep 10\r\n ---> Running in f4d1ad8b4b75\r\nRemoving intermediate container f4d1ad8b4b75\r\n ---> d3cd6a8b9b14\r\nStep 3/3 : RUN sleep 5 &&  echo \"Profiling in docker build would be d0pe!\"\r\n ---> Running in 86cf9904a391\r\nProfiling in docker build would be d0pe!\r\nRemoving intermediate container 86cf9904a391\r\n ---> ca2771a06dea\r\nSuccessfully built ca2771a06dea\r\nSuccessfully tagged repo/imagename:tag\r\n\r\nSTEP         DURATION          LAYER SIZE\r\n1/3          1 second          1MB\r\n2/3          10 seconds        100MB\r\n3/3          5 seconds         9001MB\r\n```\r\n\r\n🙏 Guidance:\r\n\r\nI'd love to work on this as my first contribution if the maintainers think it's a good idea. I'm thinking it'll at least require a PR here and in [docker/cli](https://github.com/docker/cli). I'm confident I'll need guidance on where to make these changes (assuming they're possible) 😄. \r\n\r\nAdditionally, if this PR is better suited to be opened elsewhere, just let me know!"},{"labels":[null,"enhancement1"],"text":"Support for checkpoint and restore was added some time ago and has stayed marked as experimental.\r\n\r\nWIth the containerd 1.0 integration, some of the checkpoint/restore behavior was broken.\r\nThis is not due to a fault in containerd but rather just a `TODO` in the code to re-implement against containerd.\r\n\r\ncontainerd treats checkpoints a bit differently than as designed in this repo. Basically checkpoints are a special kind of image that get stuffed into the containerd content store and can, theoretically, pushed to a registry or some other distribution mechanism.\r\n\r\nAs designed in moby, checkpoints are just some state dirs stored in the container root dir in Docker and a container can be started with a checkpoint.\r\nAlternatively the a user can request that the checkpoint be stored somewhere else on the host fs.\r\n\r\nThe API provided in moby needs to be re-evaluated. I don't think it's an interface we've ever particularly liked but it has filled the needs of users who are using it.\r\n\r\nI invite those who are using this functionality to comment on the use cases they are solving with the current interfaces.\r\n\r\nThanks!"},{"labels":[null,"enhancement1"],"text":"can we make \"registry-mirror\" support mirror for gci.io?\r\nlike this:\r\n```\r\n{\r\n    \"storage-driver\": \"devicemapper\",\r\n    \"registry-mirror\": [\r\n        \"docker.io\":\"mirror.xx.io\",\r\n        \"gcr.io\":\"gcr.mirror.xx.io\"\r\n    ]\r\n}\r\n```\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"In a lot of Dockerfiles you see WORKDIR followed by `RUN chown 1000:1000 $(pwd)`\r\n\r\nIt only makes sense to combine the two just like was done for ADD/COPY in #34263 to allow creating the WORKDIR with appropriate ownership"},{"labels":[null,"enhancement1"],"text":"**Description**\r\n\r\nIn our organization we would like to never override existing tags. Currently not all registries support disabling this at a registry level. I think it would be beneficial if this would be supported at client level as well. So that I could use `docker push --no-override-tag` and it would succeed only if it had to create a new tag. In some way, this would be an opposite of `git push --force`."},{"labels":[null,"enhancement1"],"text":"Hello All,\r\n\r\nI have been working on an enhancement to the Manager core for the following:\r\n\r\n**Use Case:**\r\nAn existing cluster of authenticated Nodes, many with Placement tags for scheduling Services via spread according to user Preferences. Docker can automatically maintain a set of 37 Cowsay replica container Services spread across my availability zones, but if one of my Manager Nodes fails at 3AM then human intervention is required to select an appropriate Node for Role change. I would like to specify a Replica Set of, say, 5 Manager Roles, and have replacement, authenticated Worker Nodes promoted to Manager automatically promoted to Manager in the event of failure of one of the Replicas.\r\n\r\n**My Solution**\r\nI added a roleScheduler feature that allows the user _to specify a Replica Set of, say, 5 Manager Roles, and have replacement, authenticated Worker Nodes promoted to Manager automatically promoted to Manager in the event of failure of one of the Replicas._ I used as much of the underlying API definitions and Orchestrator/Scheduler/Dispatcher logic as possible to make the user interaction as similar to regular Replicated Service definitions as possible; a new ** RoleManagerService** Mode was made available to allow the user to:\r\n\r\n1. Define the desired number of Manager Replicas to maintain;\r\n2. Define Placement Constraint and spread Preferences to determine how Managers are distributed across network topology.\r\n\r\nroleScheduler does not directly alter Node.Role; it only sets DesiredRole and delegates Role management authority to the existing role_manager.go. Regardless of user-defined Constraints the primary eligibility requirement is that SwarmKit's existing subsystems can change the Role to Manager, so all of the underlying security features of Node registration & authentication and anything else I wouldn't go near are left unmodified and enforced.\r\n\r\nPlease have a look at [https://github.com/foxxxyben/swarmkit](https://github.com/foxxxyben/swarmkit) for more details in README.md. I have run and passed my unit tests, but not any integration tests as I don't actually have a ginormous cluster to really run it on. Please keep in mind I named my file rolescheduler.go because helloworld.go is overused.\r\n\r\nThanks,\r\n-Ben"},{"labels":[null,"enhancement1"],"text":"I'm somewhat of a newcomer to docker, so if this is already done, feasible, or the wrong place to propose this, do point me in the right direction (website, codebase, file, etc.)\r\n\r\nThe following idea primarily comes from [apt-transport-ipfs](https://github.com/JaquerEspeis/apt-transport-ipfs), [apt-transport-tor](https://github.com/diocles/apt-transport-tor), [git-annex](https://git-annex.branchable.com/special_remotes/) and simply git itself with special remotes, which allow retrieving data from sources using different protocols.\r\n\r\nIntroducing the possibility to push and pull from different protocols, would open up a world of possibilities and possibly increase download + upload speeds and reliability. If dockerhub or any other registry were to go down or go under heavy load, I imagine things wouldn't be pretty."},{"labels":[null,"enhancement1"],"text":"As I known, docker will send `kill -9` signal while container OOM and enable its OOMKill option. But it seems that we do NOT support do something while the container is killing due to OOM.\r\n\r\nPlans are:\r\nA: send SIGTERM signal to wait container clean exit;\r\nB: register linux eventfd to bind memory pressure and alert to container process.\r\n\r\nBoth these two plans are tested via [this repo](https://github.com/Colstuwjx/go-oom) and there is no clean way to implement this feature.\r\nThanks."},{"labels":[null,"enhancement1"],"text":"I'm trying to run a RabbitMQ cluster using a Docker stack/compose file. Unfortunately, this means I can't use the `replicas` function to scale the number of containers up or down, I have to use multiple services instead (See below). This is a common scenario for many other clustered applications such as ElasticSearch too.\r\n\r\n```\r\n    version: '3.3'\r\n    \r\n    services: \r\n      rabbit_node_1:\r\n        image: rabbitmq:3.7.0-management-alpine\r\n        environment:\r\n          - RABBITMQ_ERLANG_COOKIE=foo\r\n        hostname: rabbit_node_1\r\n        ports:\r\n          - \"15672:15672\"\r\n          - \"5672:5672\"\r\n        networks:\r\n          - myoverlay\r\n        tty: true\r\n    \r\n      rabbit_node_2:\r\n        image: rabbitmq:3.7.0-management-alpine\r\n        environment:\r\n          - RABBITMQ_ERLANG_COOKIE=foo\r\n        hostname: rabbit_node_2\r\n        depends_on:\r\n          - rabbit_node_1\r\n        ports:\r\n          - \"15673:15672\"\r\n          - \"5673:5672\"\r\n        networks:\r\n          - myoverlay\r\n        tty: true\r\n```\r\n\r\nThe only difference between the services is the `hostname` and I am supplying a custom configuration file like so:\r\n\r\n```\r\n    [ { rabbit, [ \r\n      {cluster_nodes, {['rabbit@rabbit_node_1', 'rabbit@rabbit_node_2'], disc}}, \r\n      { loopback_users, [ ] } ] } \r\n    ].\r\n```\r\n\r\nI'm looking for a way to collapse these two separate Docker services down to a single `rabbitmq` service and somehow use the replica number to drive the differences in each instance? Something like this (See `${replica_ip_address}` and `${replica_number}`):\r\n\r\n```\r\n    version: '3.3'\r\n    \r\n    services: \r\n      rabbitmq:\r\n        image: rabbitmq:3.7.0-management-alpine\r\n        environment:\r\n          - RABBITMQ_ERLANG_COOKIE=foo\r\n          - REPLICA_IP_ADDRESS=${replica_ip_address}\r\n        hostname: rabbit_node_${replica_number}\r\n        ports:\r\n          - \"15672:15672\"\r\n          - \"5672:5672\"\r\n        networks:\r\n          - myoverlay\r\n        tty: true\r\n```\r\n\r\nAdding the ability to use `replicas` would mean much simpler scaling up and down of my cluster."},{"labels":[null,"enhancement1"],"text":"In our environment, we want to set the source value as the tag value using the \"raw\" format. i.e. Take this scenario:\r\nsource= http:LMB_app_Docker_AWS_prod\r\ntag = compassionate_wing/77d485fa6ee70223229a4c71b8540134b210ddc678557272f18909a0593f121e\r\nline = hello splunk\r\n\r\nWe want the entry to look like this:\r\n![image](https://user-images.githubusercontent.com/6536947/33892769-a8689ffa-df27-11e7-9954-489125decff3.png)\r\n\r\nWe think the best approach would be to modify the Splunk logger to be able to pass in a template for the source value (similar to tag). We would then make sure there was nothing set for the prefix. Thoughts?\r\n\r\nNOTE: We did create a custom plugin to try this out, but are running into issues forcing the daemon to use this due to the following issue: https://github.com/moby/moby/issues/35553"},{"labels":[null,"enhancement1",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nWhen a container is running and actively using a volume to persist data, stopping the container seems to kill the app running in it. The application stops abruptly and the state of the data is unknown. The container app needs to perform recovery on being restarted to bring its data to a sane state. The _docker stop_ command description indicates SIGTERM is sent to the process on stop. The issues with this approach are:Every app is required to set up a handler just to handle a stop event and to bring its data to a sane state before exiting. Even this isn't guaranteed if the app is unable to complete its exit in time before docker issues a SIGKILL.\r\n\r\nThis issue is to request adding a STOP/PAUSE directive in the dockerfile so developers have the flexibility to add entry points in their app that will allow control for the app over how its stopped/paused - allowing the app to maintain data in a sane state before it exits.\r\n<!--\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\nN/A\r\n\r\n**Describe the results you received:**\r\nN/A\r\n\r\n**Describe the results you expected:**\r\nThe application needs to be able to complete its exit formalities and ensure data persisted in its volume is sane before it exits.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nNone.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.09.0-ce\r\n API version:  1.32\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:42:38 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.09.0-ce\r\n API version:  1.32 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:41:20 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 17\r\n Running: 11\r\n Paused: 0\r\n Stopped: 6\r\nImages: 20\r\nServer Version: 17.09.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /vol/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 117\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: 2lg225uqlj8qfv7b9qcfghlny\r\n Is Manager: true\r\n ClusterID: p6gss81s1olwcj02cxmtxiek6\r\n Managers: 2\r\n Nodes: 8\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n  External CAs:\r\n    cfssl: https://10.160.10.252:12381/api/v1/cfssl/sign\r\n    cfssl: https://10.160.61.207:12381/api/v1/cfssl/sign\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 10.160.61.207\r\n Manager Addresses:\r\n  10.160.10.252:2377\r\n  10.160.61.207:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 06b9cb35161009dcb7123345749fef02f7cea8e0\r\nrunc version: 3f2f8b84a77f73d38244dd690525642a72156c64\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 4.2.0-27-generic\r\nOperating System: Ubuntu 14.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 3.86GiB\r\nName: sc-rdops-vm02-dhcp-52-237\r\nID: NINK:HV2F:IRM2:6JJU:XBVA:DW7J:CV5S:OARB:J4UL:O7S3:4LBA:7AVE\r\nDocker Root Dir: /vol/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement1"],"text":"**Description**\r\n\r\nOne can start additional processes in a container with `exec` API, one can inspect their status using exec/{id}/json, but one can't kill such a process.\r\nI suggest an API is introduced for this purpose, /exec/{id}/stop + /exec/{id}/kill to mimic the equivalent for container's PID 1.\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"Docs on COPY command state:\r\n```\r\nIf <src> is any other kind of file, it is copied individually along with its metadata. In this case, \r\nif <dest> ends with a trailing slash /, \r\nit will be considered a directory and the contents of <src> will be written at <dest>/base(<src>).\r\n```\r\n\r\nI think it would be useful to have be able to have as a copy result `<dest>/<src>`.\r\n\r\nThis would allow combining in one COPY command files from different folders in the context, this can be useful when trying to optimize for example node apps builds:\r\n\r\n```dockerfile\r\n# would place files to /app/package1/package.json & /app/some/dep\r\nCOPY some/dep package1/package.json /app/ --parents \r\nRUN cd package1 && npm install\r\n\r\n# would place files to /app/package2/package.json & /app/other/dep\r\nCOPY other/dep/ package2/package.json /app/ --parents \r\nRUN cd package2 && npm install\r\n```\r\n\r\nCurrently, have to split COPY commands for this case or do something else less optimal."},{"labels":["enhancement1"],"text":"**Description**\r\n\r\nWhen using the `devicemapper` storage plugin, free/used/total disk space is exposed via the `info` endpoint. The Datadog agent supports extracting these values and reporting `docker.data.*` gauges so our users can monitor it easily.\r\n\r\nFrom my investigation, this is the only storage plugin to expose these metrics, as others are filesystem-backed and a simple `df` on the host gives the information. Unfortunately, `mnt` namespacing makes it impossible to do it from our `docker-dd-agent` container without intrusive bind-mounts.\r\n\r\n**Describe the results you received:**\r\n\r\nFree space available to docker is only monitorable if using the `devicemapper` plugin.\r\n\r\n**Describe the results you expected:**\r\n\r\nFree space available to docker is monitorable for every storage plugin.\r\n\r\n**How should we implement that?**\r\n\r\nWe are willing to contribute code to enable that in future versions, this issue is to open the discussion on how that should be done.\r\n\r\n#### 1. Assume `/` metrics in the container == `/var/lib/docker` metrics on the host\r\n\r\nAlthough that is true for standard use cases with the `overlay2` storage plugin, I think that's pretty fragile.\r\n\r\n#### 2. Patch every plugin to expose these metrics in `docker info`\r\n\r\nWe could add the same \"Data space *\" info lines [as in devicemapper](https://github.com/moby/moby/blob/05ad14fc1bc5626895a419a991fcf8932eb91d27/daemon/graphdriver/devmapper/driver.go#L76-L78) to all drivers. It is probably factorable in a single common function for all filesystem-backed storage plugins.\r\n\r\n#### 3. Add the information to `docker system df`\r\n\r\nIt is pretty counter-intuitive `docker system df` is the equivalent of `du` and not `df`. We could add another section to this payload with the free/used/total stats for `/var/lib/docker`, or even for every subfolder if storing them in different partitions is supported.\r\n\r\n\r\nWhat do you think?"},{"labels":[null,"enhancement1"],"text":"`docker load` attempts to load a specified tar archive and will use an existing tag in case the layers/hashes are identical to an already present image. It is not possible to change that behavior in order to get a unique tag or ID - neither on the CLI nor the API.\r\n\r\nThis is a feature request to add this or a similar functionality in order to make `docker load` usable in automation.\r\n\r\nThe lack of generating a unique ID with `docker load` is currently causing issues in `containerdiff` (see https://github.com/GoogleCloudPlatform/container-diff/pull/149#discussion_r153408770) where a tar archive must be loaded for some analyses and the image must be removed afterwards. However, as we're not sure if the loaded image has been present before, a blind removal is not an option. A 100% complete workaround isn't possible as locking the daemon isn't possible (neither an option). Notice that the described issue doesn't occur with `docker import` due to different semantics and the flat filesystem.  However, even if I `docker import` the same tar archive multiple times in a row, docker will always give a new ID."},{"labels":[null,"enhancement1"],"text":"i have proposal: a combination of global mode with scaling applied.\r\nthis would mean, that in your docker-compose file you could do something like:\r\n\r\n```\r\nweb:\r\n   image: nginx\r\n   deploy:\r\n      replicas: 3\r\n      mode: global\r\n```\r\n\r\nthis would automatically ensure 3 running containers on each added host.\r\nso the total number of containers will grow if the number of machines grows.\r\nand it will decrease if the total number of machines decrease.\r\nthis would greatly help on doing some kind of scaling a service."},{"labels":["enhancement1"],"text":"Hi all,\r\nI am maintaining nginx configs via git with a post-receive-hook informing swarm to do a service update.\r\nEverytime I git push a new configuration, swarm successfully does this update sequentially one after another. so far so good.\r\nNow, everytime a user is connected to an nginx container, the connection is dropped which is fatal if that happens in an order process, but in the nature of a container being restarted, that's logically.\r\n\r\nSo my solution to this would be either to not update the service itself and build something complicated so that the nginx within container pulls the new config and reloads. Well, not my favorite.\r\n\r\nThere's a consul cluster running and I'd like to use the service feature. With that, I was hoping that I can consul maint -disable the specific node on which the next nginx container is being shut down by the swarm service update. \r\n\r\nIs there anything planned to supply or realize hooks before service update?\r\n"},{"labels":[null,null,"enhancement1"],"text":"fluentd v0.14.22 stable has been [released](https://www.fluentd.org/blog/fluentd-v0.14.22-has-been-released), which [supports sub second precision](https://docs.fluentd.org/v0.12/articles/faq#i-have-millisecond-timestamp-log-but-fluentd-drops-subsecond.-why?) for logs.\r\n\r\n[fluent-logger-golang](https://github.com/fluent/fluent-logger-golang) already has an option to enable this called `defaultSubSecondPrecision`; created in May via this PR: https://github.com/fluent/fluent-logger-golang/pull/49\r\n\r\nSince this package is used by the docker fluentd logger under the hood, it could be exposed here: https://github.com/moby/moby/blob/4bc5a5765288c797bb6989a9e08938b5327236dc/daemon/logger/fluentd/fluentd.go#L120\r\n"},{"labels":[null,null,"enhancement1"],"text":"There is already a websocket endpoint to attach to a container.\r\nNow that service logs are available in the API, I'd be happy to be able to use websocket to fetch them.\r\nHijacking a TCP connection is not something we can easily do in a browser.\r\n\r\n"},{"labels":[null,null,null,"enhancement1"],"text":"(relates to https://github.com/moby/moby/issues/34565)\r\n\r\nMulti stage builds are a better alternative to the experimental `--squash` option for most use cases. However, there is one use-case that's not addressed by this feature (see https://github.com/moby/moby/issues/34565#issuecomment-338924048). Some silly examples below are just to illustrate the issue.\r\n\r\nCopying all files from a build-stage to the final image (from `scratch`) produces a single-layer image:\r\n\r\n```Dockerfile\r\nFROM alpine AS build-stage\r\nRUN apk add --no-cache man\r\nRUN apk add --no-cache nginx\r\nRUN apk del --no-cache man\r\n\r\n\r\nFROM scratch\r\nCOPY --from=build-stage / /\r\n```\r\n\r\nWhile works correctly if the final image is built from scratch (and may be the desired result), the resulting image is _fully_ squashed, including the base-image, and therefore does not take advantage of sharing the base image layer with other images.\r\n\r\nWhen attempting to use the same base-image for both stages;\r\n\r\n```Dockerfile\r\nFROM alpine AS build-stage\r\nRUN apk add --no-cache man\r\nRUN apk add --no-cache nginx\r\nRUN apk del --no-cache man\r\n\r\n\r\nFROM alpine\r\nCOPY --from=build-stage / /\r\n```\r\n\r\nThe entire base image is included _twice_ in the final image (once as the \"base\" layer, and once as part of the squashed / copied layer):\r\n\r\n\r\n```\r\nREPOSITORY    TAG         IMAGE ID            CREATED             SIZE\r\nsquashed2     latest      ad22751e06e6        About an hour ago   9.28MB\r\nsquashed      latest      94080bdf30e4        About an hour ago   5.31MB\r\n```\r\n\r\n\r\n_Ideally_ the builder would automatically skip files that are already present in the base image (based on their checksum/metadata). Doing so would resolve cases where the same base image is used for both stages.\r\n\r\nIn many cases this won't be enough though; the build-stage could use a different base image (containing build-tools), and only artifacts produced by the build-stage itself should be copied; for example in the following dockerfile:\r\n\r\n\r\n```Dockerfile\r\nFROM buildpack-deps:stretch AS build-stage\r\nCOPY . /usr/src/\r\nWORKDIR /usr/src/\r\nRUN gcc -g .......\r\n\r\n\r\nFROM debian:stretch\r\nCOPY --from build-stage / /\r\n```\r\n\r\nWhile it is best practice to be specific _what_ to copy from a stage (i.e., only the artifacts you need), however, keeping track of exact paths and files to copy can be cumbersome, and error-prone (for example, determining which files are installed by an `apt-get install foo` may not be known by the user). Artifacts can also be distributed over many (sub)directories, some of those containing files that were inherited from the base image.\r\n\r\n### Proposal: add option to skip files from the base image\r\n\r\nThe `COPY` command should have an option to ignore any file that was already present in the base image to address the issues mentioned above. I don't have a proper name for this option yet, so just using `--skip-base` as a placeholder. Example uses would look something like:\r\n\r\nCopy every file that was added/modified in build-stage to the final image:\r\n\r\n```Dockerfile\r\nFROM alpine AS build-stage\r\nRUN apk add --no-cache nginx\r\n\r\nFROM alpine\r\nCOPY --skip-base --from=build-stage / /\r\n```\r\n\r\nCopy every file _inside `/usr` that was created in build-stage to the final image\r\n\r\n```Dockerfile\r\nFROM buildpack-deps:stretch AS build-stage\r\nCOPY . /usr/src/\r\nWORKDIR /usr/src/\r\nRUN gcc -g .......\r\n\r\n\r\nFROM debian:stretch\r\nCOPY  --skip-base --from=build-stage /usr /usr\r\n```\r\n\r\n\r\n/cc @ijc @dnephin  @tonistiigi PTAL"},{"labels":[null,"enhancement1"],"text":"This is a **feature request**. I try to keep it as simple as possible, but don't hesitate to ask if something is unclear.\r\n\r\nI'm using docker *multi-stage builds* to build my go binaries and create small sized containers. To get the code from my local filesystem into the build environment, I'm using two commands (assuming `WORKDIR` is set correctly, etc.):\r\n\r\n```bash\r\nCOPY . /tmp/code/myproject\r\nRUN git clone /tmp/code/myproject .\r\n```\r\n\r\nI prefer this method, because I only want code which is under version control in my build environment. Example: When using GRPC, I'm not keeping the generated code under version control. I explicitly want the build env to generate the GRPC code from the `*.proto` definitions before building the go binaries.\r\n\r\nI suggest to add a `--clone` flag (or `--git`?) to the `COPY` command, which modifies its behaviour. Instead of copying from the local filesystem, it performs a `git clone`.\r\n\r\nPlease provide me with some feedback."},{"labels":[null,"enhancement1"],"text":"We have a use-case, where containers part of swarm, need to access host system driver, for hardware support on the target node, where it can get deployed, as part of swarm.\r\n\r\nThis is a feature request, to support, passing --devices=/dev/device-name, just like how it is supported in docker run command.\r\n\r\nJust like --network=host option was recently added to swarm mode, it would also make sense to add support, to leverage hardware support by allowing access to specific devices on the target node, part of swarm.\r\n\r\nSo, for example:\r\nsudo docker service create --name new_service --device=/dev/device-name --network=host image:latest\r\n"},{"labels":[null,null,null,"enhancement1",null],"text":"How to use COPY --chown without losing portability (recently merged from this [issue](https://github.com/moby/moby/pull/34263))? I'll try to explain with an example wherein I find --chown causing portability issues.\r\n\r\nLet's say I got a base image (mybase) which I use for running other containers as non-root user (let's say with UID 1000) using [gosu](https://github.com/tianon/gosu) in ENTRYPOINT.\r\n`exec /usr/local/bin/gosu ${USER_NAME} \"$@\"`\r\n\r\nI use this base image (mybase) for building other container (let's say mycontainer) Dockerfile where I COPY some files from host to container using --chown\r\n```\r\nFROM mybase\r\nRUN mkdir -p /tmp/build\r\nCOPY --chown=1000 . /tmp/build\r\n```\r\n\r\nNow I want the files generated via my build tool (from mycontainer) to be shared on a Docker Volume mapped with my host. Generated files would be available at specified host location but the owner of those files would be user with UID 1000. Doesn't this mean I'll need to have a user with UID 1000 on host machine for generated files to be accessible? Isn't there a way to provide value for --chown dynamically? I tired using ENV and ARG but they are not accessible in --chown.\r\n```\r\nARG owner\r\nCOPY --chown=$owner . /tmp/build\r\n```\r\nit gives me an error \r\n\r\n> unable to convert uid/gid chown string to host mapping: can't find uid for user $owner: no such user: $owner\r\n\r\n**Steps to reproduce the issue:**\r\n1. Add this to Dockerfile\r\n```\r\nFROM openjdk:8\r\n\r\nARG owner\r\nADD --chown=$owner . /tmp/build/platform3\r\n\r\nCMD echo 'test'\r\n```\r\n2. Build image\r\n`sudo docker build -t test -f Dockerfile .`\r\n\r\n**Describe the results you received:**\r\nResult of running docker build\r\n\r\n> unable to convert uid/gid chown string to host mapping: can't find uid for user $owner: no such user: $owner\r\n\r\n**Describe the results you expected:**\r\nExpected result should be to get owner information from build arguments\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.09.0-ce\r\n API version:  1.32\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:42:38 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.09.0-ce\r\n API version:  1.32 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:41:20 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 0\r\n Paused: 0\r\n Stopped: 1\r\nImages: 15\r\nServer Version: 17.09.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /docker/pd0/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 18\r\n Dirperm1 Supported: false\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 06b9cb35161009dcb7123345749fef02f7cea8e0\r\nrunc version: 3f2f8b84a77f73d38244dd690525642a72156c64\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 3.13.0-129-generic\r\nOperating System: Ubuntu 14.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 14.69GiB\r\nName: ip-172-31-35-149\r\nID: KR63:RIDQ:SQNY:3ESC:5OA6:4DFT:QXSB:YE6M:727G:H7O2:REIS:RSFA\r\nDocker Root Dir: /docker/pd0\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: mavericksid\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n\r\n```\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"Would there be any way to limit the disk IO for `docker build` ?\r\nSomething like we have in `docker run`\r\n\r\n```\r\n--device-read-bps Limit read rate from a device \r\n--device-write-bps Limit write rate to a device\r\n--device-read-iops Limit read rate (IO per second) to a device  \r\n--device-write-iops\tLimit write rate (IO per second) to a device \r\n```"},{"labels":["enhancement1"],"text":"Something like this has already been discussed [here](https://github.com/moby/moby/issues/18701), but I would like to bring it up again as I have found a new use case for it.\r\n\r\nSo, about my use case. I'd like to build an immutable Arch Linux image for all my machines. I'd like to build my images (gaming/development/children) using the layering model of Docker. I will have a script that exports a layer to a squashfs for ```/``` booting on bare-metal, with a ```tempfs``` overlayed for immutable, but writable changes during a boot session.\r\n\r\nWhen creating a new layer, I'd like to mount a Docker image on my host, make my changes, and create a new Docker image (with inherited layers preserved).\r\n\r\nI will be deploying these Docker images to my registry for history/backup/rollback support and sharing the images across my machines in different regions.\r\n\r\nI have no intention of ever running any Docker images I create, but I'd like to leverage the image/layering/registry features it provides.\r\n\r\nSo, is it possible?\r\n\r\nIf not, I will have to run a container with the image I intend to inherit from, access the mounted file system (/var/lib/docker/somewhere...), and then commit the container changes in a new image. Rinse repeat for other layers."},{"labels":[null,"enhancement1"],"text":"I do not currently see any method (nor issues discussing such a method) of adding metadata such as labels or a name to a Swarm cluster.\r\n\r\nFor instance, I would like to be able to run `docker swarm init --name sandbox` and `docker swarm update --label-add site=$LOCATION`.\r\n\r\nThis would ease management of services that are duplicated across Swarms (eg an application deployed across multiple sites) as well as making it simple to indicate the purpose of a Swarm.\r\n\r\nAs an example, I have a monitor pushing Swarm/node info from each of my cluster members to a service that normalizes it to information useful for our orchestration/monitoring tooling. Right now I have to derive the site/name/etc from hostnames... Ideally, I would be able to refer to a swarm (or set of swarms) based on name or labels. Any time I bring up a new Swarm I would simply need to create my monitor service and suddenly all of my tooling is aware of the new sandbox or site, etc."},{"labels":[null,"enhancement1"],"text":"The daemon has various built-in drivers (logging, networking, graph-drivers). A long time goal has been to move those drivers out of the monolith, and use plugins instead.\r\n\r\nSome motivations for using plugins:\r\n\r\n- drivers may only target a small subset of users/installations, due to them requiring a specific environment to run in.\r\n- drivers may not be (fully) functional on all distros, architectures or builds (e.g. static builds are problematic for some drivers).\r\n- drivers bring in dependencies, in some cases this is a substantial amount of code (see the AWS logging driver; https://github.com/moby/moby/pull/15495 https://github.com/moby/moby/pull/27822)\r\n- most drivers are contributed by contributors that are not a maintainer, and knowledge about these drivers may not be available among the Moby maintainers themselves, making it difficult to maintain those drivers, causing quality and maintenance concerns.\r\n- bugfixes and/or improvements to logging drivers are tied to (Docker) releases, which is not ideal.\r\n\r\n\r\nWith the implementation of logging plugins in docker 17.05 (https://github.com/moby/moby/pull/28403), we now have most bits in place to start looking at moving drivers out of the daemon, and use plugins instead.\r\n\r\nSimply _removing_ the drivers and requiring users to manually install them as a plugin would be a breaking change, so we need a migration path, and (at least initially) ship the plugins together with the daemon binaries.\r\n\r\nUnfortunately, that's not currently possible, because installing plugins is a manual action, and tricky to automate.\r\n\r\n\r\n### Automatic installation\r\n\r\nI'm proposing to add functionality to automatically install plugins when the daemon is started. For this to work, plugins are put in a specific directory on the host (as a `.tar.gz`) before the daemon is started. Upon start, the daemon scans this directory, and loads/installs any plugin it finds.\r\n\r\n\r\n### Other uses\r\n\r\nBesides assisting with the migration to externalise the built-in drivers, this functionality is useful for other purposes, for example:\r\n\r\n- It allows users to provision machines with plugins they need. Currently, installing plugins is a manual action, and can only be performed _after_ the daemon itself is installed. This complicates (for example) adding nodes to a swarm (think of a volume plugin that's used in a swarm, and must be available the moment a node joins the swarm).\r\n- For Docker, it allows bundling plugins with specific versions of the packages.\r\n\r\n\r\n### Challenges\r\n\r\n- When pulling plugins from a registry, the content is automatically verified (content addressable store). Loading a plugin from a local `.tgz` does not perform the same validation. Perhaps notary/docker content trust can help with this (@n4ss @endophage ?)\r\n- The daemon needs to keep track which plugins were installed, and not attempt to re-install the plugins when restarting (move the tar's to a different directory after installation? add a (hidden) file to track state?)\r\n- If a plugin was installed from a local tar, is it possible to associate it with a registry (e.g. Docker Store), for later upgrading the plugin?\r\n\r\n\r\nping @cpugu83 @rogaha @vieux "},{"labels":[null,"enhancement1",null],"text":"**Description**\r\nI've bumped into #34263 being merged and it hit me that `--chmod` would be nice in conjunction with this feature.\r\n\r\nI frequently build Linux images from a Windows machine and I'm kind of annoyed with the 755 mode the added/copied files end up with.\r\n\r\nADD/COPY with `--chown` and `--chmod` together will be a readable and maintainable notation delivering precise control over permissions.\r\n\r\nI'm aware of #29853, but I don't think this issue is a duplicate (that one seems to be stale).\r\n"},{"labels":[null,"enhancement1"],"text":"I propose a feature for `Dockerfile` aliases, by which ANY parameter of `docker build`, `docker run`, and `docker create` can refer to default values defined within a Dockerfile.  This feature request was inspired by #33315. \r\n\r\nTo avoid confusion with other conventions, we should consider a new Dockerfile directive (`ALIAS`?) and a way to refer to those aliases from the command line.\r\n\r\nSo what's the true purpose of an alias?  To allow Dockerfile authors to provide suggested defaults, and configuration touch points, in a way that subsequent container builders and runners can override from the command line, and in a way that allows container runners to, by default, not enable.\r\n\r\nIdeally, aliases would sit somewhere between `ARG` and `ENV`.\r\n\r\nSimilar to `ARG`:\r\n- default alias values are defined during Dockerfile authoring\r\n- alias values can be overridden during `docker build`\r\n- alias values do not pollute a container's `ENV` namespace\r\n  - alias values do not provide the opportunity for bad actors to inject values into environment variables for malicious purposes\r\n\r\nSimilar to `ENV`:\r\n- alias values can be referenced and overridden during `docker run` and `docker create`\r\n\r\nAn example:\r\n\r\n```\r\nFROM docker.io/centos:7.3.1611 AS test\r\n\r\nALIAS APP_HOME /var/app\r\nVOLUME ${APP_HOME}\r\n```\r\n\r\n`docker run -v /path/on/host:\\$APP_HOME` OR\r\n`docker run -v /path/on/host:{{APP_HOME}}`\r\n\r\nTo override an alias:\r\n\r\n`docker run --alias APP_HOME=/var/new_app -v /path/on/host:{{APP_HOME}}`\r\n\r\nYou could even use this for other parameters as well:\r\n\r\n`docker run -v '{{APP_VOL_NAME}}:{{APP_HOME}}:{{APP_VOL_OPTS}}' --cpus '{{RECOMMENDED_CPU}}' --add-host '{{APP_SERVER_HOST}}' --dns '[{{HOSTED_APP_DNS}}]'`\r\n\r\nIn this way, `Dockerfile` authors can provide suggested usage and suggested values without overriding the user's ability to decide whether to actually use them."},{"labels":[null,"enhancement1"],"text":"In my own containers and others I have seen that rely on a db/backend container in compose, we specify depends_on to wait till that container is started before our app container starts. As is stated in the documentation, docker does not wait for the container to initialize to move to the next dependency. I propose taking things a step further by adding the ability to wait for a specified port to open on the db/backend service before starting the app container which specifies depends_on.\r\n\r\nTake my compose file for example: https://github.com/garvincasimir/csharp-datatables-parser/blob/master/docker-compose-test.yaml.\r\n\r\nI would add ports 1433 and 3306 to the depends_on config so docker can do what this file is doing:\r\nhttps://github.com/garvincasimir/csharp-datatables-parser/blob/master/test/DatatablesParser.Tests/test-runner.sh\r\n\r\nIf this functionality existed, my Entrypoint would simply be *dotnet test* and there would be no need for a shell script.\r\nhttps://github.com/garvincasimir/csharp-datatables-parser/blob/master/Dockerfile-test\r\n\r\nI originally created this issue in the [Compose repo](https://github.com/docker/compose/issues/5121#issuecomment-327287878) but was asked to created it here. "},{"labels":[null,null,"enhancement1"],"text":"This is something I’m working on to add to swarm. I brought this up a few times with your team over email, and the more scenarios I work with on swarm, I so far haven’t been dissuaded after sleeping over this for three months. I wanted to discuss this and see if this resonates or holds value for anyone else. I will contribute the code towards this, but want to see if this feels like the right direction.\r\n\r\n## The Problem\r\n\r\nA lot of “stacks” on Docker are really not scalable by adding “n” instances as a service, but also not quite custom enough that they can’t be automated at all. I’ll list the scenarios down below, but I take “Etcd Cluster of n instances” as my representative scenario.\r\n\r\nIt’s not “docker service scale etcd=n”, but it’s also not impossible to automate the cluster (with each instance requiring a different parameter). Every time the scaling changes there are some hooks/actions that need to be performed for member addition/removal as well. Again, 100% deterministic, but not simply launch-and-go.\r\n\r\nThe current problem, in a crux, is that *\"docker service scale foo=n\" doesn't quite do everything needed, while simultaneously having a severely restricted feature set for others to write automation.*\r\n\r\n## Ecosystem Win\r\n\r\nAs good citizens and to ensure that we design something that works for everyone, I started with more than just this scenario. \r\n\r\nPart of this motivation is technical — it ensures we build the right thing. Part of it is social — if more people find it easier to deploy on Docker swarms, then we get more buy-in.\r\n\r\nI had the following scenarios that needed to pass my sniff test:\r\n* Etcd cluster management as a single unit.\r\n* Elastic Search cluster management as a single unit — with support for rebalancing, shard-draining, scaling up and scaling down.\r\n* A stateful distributed system like MySQL or Oracle that can be deployed with sharing and failover and all config settings done right.\r\n* Wordpress or Wikipedia being able to be run as a distributed service on a Swarm.\r\nIf all these can be enabled as a single component that can be managed, etc. then other companies can build awesome aggressive scenarios that really would allow for killer stickiness!\r\n\r\n## User Experience\r\nNext up, I wanted to capture the User Experience for how this would work. I considered everything from CLI plugins to Swarm plugins, and here’s the model in my head, that maintains one of the most beautiful aspects of Swarm  — API, CLI and Compose symmetry.\r\n\r\n### Symmetry for users\r\nBasically let’s say that there is a plugin called “Etcd”, then in order to be fully accessible, this is what three endpoints would look like:\r\n\r\nAPI-based Etcd launch example:\r\n'GET http://<dockerhost>/v2/extensions/etcd/subcommand/?param1=value1&param2=value2&….'\r\ntells the Etcd extension to do something — and expose extra commands (listing etcds, scale up, scale down, change config, etc.)\r\n\r\nCLI generic example:\r\n`docker extensions etcd <foobar>`\r\n\r\nMakes a /v2/extensions/etcd/foobar call over the API.\r\n\r\nCompose generic example:\r\n```\r\nversion: 4\r\nservices:\r\n   ….\r\nnetworks:\r\n   ….\r\nextensions:\r\n  etcd:\r\n    foobar:\r\n      ….\r\n```\r\n\r\n### Need for extensions\r\n\r\nThe symmetry aspect is tempting, and I fully expect it to help a LOT of scenarios and while a bit difficult to do at first, I think has the potential to wildly open up the entire ecosystem. \r\n\r\nWhat this symmetry needs is a special “type” of plugin. They extend the Swarm with newer types of abstractions that you can compose over.\r\n\r\nThis means you can create compose stacks that themselves use extensions to define large constructs like Elastic Search or Etcd or MySQL to be launched with high-level parameters.\r\n\r\n## Implementation Proposal\r\n\r\nIf we buy into the first four scenarios and how they help the ecosystem, and then look at why the API symmetry is desirable, it leads to a rather simple implementation that is not particularly difficult, but requires some additional features.\r\n\r\n### Special plugin type\r\nThe first feature we’d want is a new “type” of plugin, which I’m calling extensions (we'll need a better word). I’m a terrible namer-of-things. This plugin would fundamentally add new constructs to swarm that are comprised of basic components such as services, networks, secrets, etc.\r\n\r\n### Swarm-wide plugin availability\r\nThis plugin must be a “swarm” plugin, not an “engine plugin”. This feature was already implemented a couple of weeks ago and is in experimental in the latest release.\r\n\r\n### Special Swarm API Endpoint\r\nOne key missing component in plugins today is the ability to talk to the swarm it’s on. This change would mean that the plugin can now talk to the swarm over an API with possibly a special account (for monitoring/auditing) and even perhaps restricted to access things it started itself. This feature has been in discussion/implementation for a bit. We'd just need to inform it from this scenario.\r\n\r\n### Special monitoring side-channel pipe\r\nThis is a big one, and one that would be the game-changer. \r\n\r\nDocker Logs are great when a person is running those containers. They are terrible when an automated supervisor is trying to command-and-control sub-services. Either they have to be moved across the internal network and the controller has to parse them and make sense of them, or they have to be sent elsewhere through a driver.\r\n\r\nIn general logs are passive. What is needed crucially is the ability to side-channel connect to  services through this plugin, and form supervision-trees. \r\n\r\nHere too, when you think about all scenarios it makes sense to just expose a bidirectional socket between containers that they can use to send status/reports/messags.\r\n\r\nA poor-persons implementation of ability to \"tee\" logs would also be okay for me. But we can't build additional components if we have to pick between json-file logging (this losing ability to store logs), and no visibility.\r\n\r\n### Exposure in the Remote API\r\nI think this one is trivial. Once the plugin is swarm-wide, really all that you need to do is, any API calls under:\r\n`/extensions/<pluginname>/foo/bar…`\r\n\r\nSimply need to be passed to the plugin’s socket at:\r\n`/foo/bar/….`\r\n\r\nAnd that means plugins can extend the remote API.\r\n\r\n### Exposure in Compose and CLI\r\nWith this API, there should be a simple and fixed and unambiguous mapping between CLI command and parameters to HTTP URLs.\r\nSo for instance:\r\n`docker extension <pluginname> foo bar something –flag1 –flag2=value2`\r\n\r\nmight become:\r\n`/extensions/<pluginname>/foo/bar/something?flag1&flag2=value2`\r\n\r\nAnd given that CLI<->Compose already has symmetry, this would naturally extend to compose (the compose<->CLI mapping does require more thought):\r\n\r\n```\r\nversion: 4\r\nextensions:\r\n  pluginname:\r\n    foo:\r\n    bar:\r\n      something: flag1 flag2=value2\r\n```\r\n\r\nEnd result: This would open up a whole new world of rich and complex plugins to do things on Swarms that you couldn’t do before. And all with the power of:\r\ndocker plugin install etcd/elasticsearch/mysql."},{"labels":[null,"enhancement1"],"text":"**Description**\r\nBefore I go about implementing anything, I'd like to get some folks opinion on how to implement VFS Driver improvements. The following two issues frustrate me:\r\n\r\n1. The VFS driver does not support filesystem quota\r\n2. The VFS driver does a naive copy, which doesn't take advantage of techniques like zero-copy, and reflinks. \r\n\r\nAs a solution, I propose the following:\r\n\r\n- [ ] We adopt the techniques that I've added to the overlay driver in https://github.com/moby/moby/pull/34670. This gracefully degrades, and allows the user to take advantage of reflinks (FICLONE), zerocopy (copy_file_range / splice), and then only falls back to a naive copy. Although there is still an O(N) cost for layer setup in the ideal case, N for the number of files is likely not a great number. Naive tests show that copying ubuntu:latest using these techniques can be done in 600 milliseconds.\r\n- [ ] We adopt the projectquota code, after the improvements laid out in https://github.com/moby/moby/issues/34701\r\n\r\nAgree? Disagree?\r\nCC: @cpuguy83 @dmcgowan"},{"labels":["enhancement1"],"text":"It would be great if we could copy a file from an image to the local filesystem in the same way that we can copy to/from a running container.\r\n\r\nEg.\r\n```\r\ndocker cp SOMECONTAINERREFERENCE:/foo/bar /local/foo/bat\r\n```\r\n\r\nI'm not sure what the best syntax would be for referencing the container (since it would need to handle `/`'s, which might be confused with filesystem) Perhaps even just a `--fromContainer` type flag would be sufficient?\r\n\r\nI know that it can currently by done by making a temporary container, but that doesn't work well for super-minimal images (eg. `FROM scratch`)\r\n\r\n```\r\n# From: https://forums.docker.com/t/is-there-a-way-to-copy-a-file-from-an-image-to-the-local-filesystem-without-running-a-container/2841\r\n\r\ndocker run --name temp-container-name image-name /bin/true\r\ndocker cp temp-container-name:/path/to/manifest /local/path/to/manifest\r\ndocker rm temp-container-name\r\n```\r\n\r\nCurrently it can be worked around by doing something like this, though it's obviously less than ideal:\r\n```\r\nTEMPCONT=\"fooTemp$(date +%s)\"\r\ndocker run --name $TEMPCONT devalias/gobuster\r\ndocker export $TEMPCONT | tar -xvzf - gobuster\r\ndocker rm $TEMPCONT\r\n```"},{"labels":["enhancement1",null,null],"text":"**Description**\r\n\r\nWhen I try to copy anything from mounted volumes I get error:\r\n`GetFileAttributesEx \\\\?\\Volume{063aa9af...: The system cannot find the path specified.`\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a directory and files for farther mounting\r\n```\r\nmkdir C:\\tmp_test_docker_cp_host\r\necho \"Test content\" > C:\\tmp_test_docker_cp_host\\file.txt\r\nmkdir C:\\tmp_test_docker_cp_host\\testdir\\\r\necho \"Testdir content\" > C:\\tmp_test_docker_cp_host\\testdir\\testdirfile.txt\r\n```\r\n2. Run a container which mounts created directory\r\n```\r\ndocker run --name test_docker_cp -v C:\\tmp_test_docker_cp_host:C:\\test_docker_cp_container -d microsoft/nanoserver ping -t localhost\r\n```\r\n3. Try to copy file or directory from mounted volumes.\r\n```\r\ndocker cp test_docker_cp:C:\\test_docker_cp_container\\file.txt .\r\ndocker cp test_docker_cp:C:\\test_docker_cp_container\\testdir .\r\n```\r\n\r\n**Describe the results you received:**\r\nI expect to have that files locally\r\n\r\n**Describe the results you expected:**\r\nI got following errors\r\n\r\n```\r\nPS C:\\Users\\Administrator> docker cp test_docker_cp:C:\\test_docker_cp_container\\file.txt .\r\nError response from daemon: GetFileAttributesEx \\\\?\\Volume{063aa9af-5cbe-11e7-a962-ac9d30da39d1}\\ContainerMappedDirectories\\7DF7DB6C-BD61-4C4E-A1F4-39C3F0D85748\\file.txt: The system cannot find the path specified.\r\n\r\nPS C:\\Users\\Administrator> docker cp test_docker_cp:C:\\test_docker_cp_container\\testdir .\r\nError response from daemon: GetFileAttributesEx \\\\?\\Volume{063aa9af-5cbe-11e7-a962-ac9d30da39d1}\\ContainerMappedDirectories\\7DF7DB6C-BD61-4C4E-A1F4-39C3F0D85748\\testdir: The system cannot find the path specified.\r\n```\r\n\r\nI see following debug records in this moment\r\n```\r\ntime=\"2017-08-20T17:25:07.896867000Z\" level=debug msg=\"Calling GET /v1.30/containers/test_docker_cp/archive?path=C%3A%2Ftest_docker_cp_container%2Ffile.txt\" \r\ntime=\"2017-08-20T17:25:07.896867000Z\" level=debug msg=\"WindowsGraphDriver Get() id 92a8f538792c903dbcc34cbe18c1fd5cdae931c87e2920330f19794ba1198f2a mountLabel \" \r\ntime=\"2017-08-20T17:25:07.896867000Z\" level=debug msg=\"container mounted via layerStore: \\\\\\\\?\\\\Volume{063aa9af-5cbe-11e7-a962-ac9d30da39d1}\" \r\ntime=\"2017-08-20T17:25:07.897898000Z\" level=debug msg=\"WindowsGraphDriver Put() id 92a8f538792c903dbcc34cbe18c1fd5cdae931c87e2920330f19794ba1198f2a\" \r\ntime=\"2017-08-20T17:25:33.615821900Z\" level=debug msg=\"Calling GET /_ping\" \r\ntime=\"2017-08-20T17:25:33.616829600Z\" level=debug msg=\"Calling GET /v1.30/containers/test_docker_cp/archive?path=C%3A%2Ftest_docker_cp_container%2Ftestdir\" \r\ntime=\"2017-08-20T17:25:33.616829600Z\" level=debug msg=\"WindowsGraphDriver Get() id 92a8f538792c903dbcc34cbe18c1fd5cdae931c87e2920330f19794ba1198f2a mountLabel \" \r\ntime=\"2017-08-20T17:25:33.616829600Z\" level=debug msg=\"container mounted via layerStore: \\\\\\\\?\\\\Volume{063aa9af-5cbe-11e7-a962-ac9d30da39d1}\" \r\ntime=\"2017-08-20T17:25:33.617789500Z\" level=debug msg=\"WindowsGraphDriver Put() id 92a8f538792c903dbcc34cbe18c1fd5cdae931c87e2920330f19794ba1198f2a\" \r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nActually, I tested a bug with `docker cp` which create a hanging container because of windows symlink docker panic and force docker ps to hang https://github.com/moby/moby/issues/30605#issuecomment-309093247 . Since Docker 17.06 was build with Go 1.8 which has got a fix for some Symlink bugs, I decided to test it. \r\n\r\nMay be it is stil one of Golang Symlink isssue which were fixed in Go 1.9\r\nhttps://github.com/golang/go/issues/17541\r\nhttps://github.com/golang/go/issues/17540\r\nhttps://github.com/golang/go/issues/19870\r\nhttps://github.com/golang/go/issues/20064\r\n\r\n\r\nMay be this bug has similar nature with https://github.com/moby/moby/issues/34428\r\n\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.06.1-ee-1\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   4dd6e94\r\n Built:        Sat Aug 12 01:34:13 2017\r\n OS/Arch:      windows/amd64\r\n\r\nServer:\r\n Version:      17.06.1-ee-1\r\n API version:  1.30 (minimum version 1.24)\r\n Go version:   go1.8.3\r\n Git commit:   4dd6e94\r\n Built:        Sat Aug 12 02:14:08 2017\r\n OS/Arch:      windows/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 4\r\n Running: 1\r\n Paused: 0\r\n Stopped: 3\r\nImages: 4\r\nServer Version: 17.06.1-ee-1\r\nStorage Driver: windowsfilter\r\n Windows:\r\nLogging Driver: json-file\r\nPlugins:\r\n Volume: local\r\n Network: l2bridge l2tunnel nat null overlay transparent\r\n Log: awslogs etwlogs fluentd json-file logentries splunk syslog\r\nSwarm: inactive\r\nDefault Isolation: process\r\nKernel Version: 10.0 14393 (14393.1358.amd64fre.rs1_release.170602-2252)\r\nOperating System: Windows Server 2016 Datacenter\r\nOSType: windows\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 16GiB\r\nName: EC2AMAZ-N2GM0M9\r\nID: VZIV:QABT:63VB:KUOW:4FOR:BTTB:QULZ:U7KS:CIZK:QWTU:63SI:65EG\r\nDocker Root Dir: E:\\docker_storage_1_13\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: -1\r\n Goroutines: 26\r\n System Time: 2017-08-20T18:12:00.9915256Z\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nAWS EC2 instance\r\n\r\nThanks!"},{"labels":[null,"enhancement1"],"text":"When using docker-in-docker, or in general when running several daemons on one host, only one daemon can have `--iptables=true`, otherwise the DOCKER and related iptables chains will be flushed when the other daemon starts.\r\n\r\nHaving the option to specify the chain names (or the prefix, e.g. DOCKER1 to have chains named DOCKER1, DOCKER1-ISOLATION, etc), would allow having several daemons on a single host without having to do manual container iptables rules, wouldn't it?\r\n\r\nThis would require some changes in libnetwork where chain names are specified as consts, and in moby/dockerd to allow passing something like `--iptables-chain-prefix` (default `DOCKER`)."},{"labels":[null,"enhancement1"],"text":"This is a **feature request**. This is usually for development and test purpose.\r\n\r\n**Description**\r\n\r\nThe term `NAT` is for address translation, while `NAPT`  is for address and port translation.  \r\n\r\nCommand `docker run -p ip:port2:port1` actually does NAPT instead of NAT. The request is for NAT without port mapping. This is to say, the container IP is mapped to the outside IP.  All ports of the container are mapped to the same ports (so we also need an `EXPOSE all` in dockerfile). \r\n\r\n** Step to produce **\r\n1.  create a cluster with N containers with bridge network.  \r\n2.  create N IPs with eth0  (e.g, 192.168.1.100-110)\r\n3.  bind each IP of eth0 to each container.   \r\n\r\n**Why is desired and other options not desirable:**\r\n1. Normally, the port range is wide (e.g., 22-57000, ssh to hdfs), so port range approach does not work well.\r\n2. Some port are opened at run time. We do not know which port to map.  \r\n3. Map to host machine's 0.0.0.0 (or using `host` network) makes the cluster look like an single machine (not wanted).  \r\n4. Map to host's 0.0.0.0 may cause port conflict and thus shutdown of the server.  \r\n5. There are hidden ports we are not aware of (we access them via web links). \r\n6. To map ports one by one is tricky (also make the command long and the `docker ps` table not aligned.  \r\n7. Create swarm/VM is too tricky for the test purpose.\r\n\r\n**Security is not an issue**  \r\n\r\nThe server (container) 's security is protected by firewall, application and OS's mechanism, not blocking port by docker. Also this is for test and dev only, so security is not an issue at all.\r\n\r\n**Other option**\r\nAn alternative is a network like virtualbox's bridged work. \r\n"},{"labels":[null,"enhancement1",null],"text":"**Description**\r\n\r\nOn Linux daemons, we can get the amount of disk space available in `/var/lib/docker` by either running `df -h` inside a container (I believe this is the case for overlay FS systems) or by running `docker info` (I believe this is the case for devicemapper). In Windows, however, disk space checks inside containers do not correspond to the host machine's disk space, and `docker info` does not contain any disk space info.\r\n\r\nIt'd be nice to add something like this for Windows so that we can get a node's disk usage through the Docker API.\r\n"},{"labels":[null,"enhancement1"],"text":"I want to deploy a stack.yml with services apache+mysql of 1 replica each on a swarm.\r\n\r\nI want swarm to decide on which node the stack will run, but with the requirement that swarm must deploy both/all services on the **same node** and **keep them together** there forever, so I want to **prohibit** that swarm will ever **move** a service to another node, even if a service fails. I think, that could be done within one constraint like:\r\n\r\n```\r\ndeploy:\r\n  replicas: 1\r\n  placement:\r\n    constraints:\r\n      - node == bind\r\n```\r\n\r\nIs that requirement anyhow possible without tricky hacking? :-)"},{"labels":[null,null,null,"enhancement1"],"text":"When writing Dockerfiles, often layering is important, and it's best practice to group shell commands in logical units (e.g., combine `apt-get update` and `apt-get install` in a [single `RUN` ](https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#run)).\r\n\r\nUnfortunately, the Dockerfile syntax can become complicated if many instructions have to be combined in a `RUN` instruction.\r\n\r\n- empty lines [should be avoided](https://github.com/moby/moby/pull/33719)\r\n- comment lines are handled by the Dockerfile parser, not by the shell, and as a result [don't work the same as in a regular shell](https://github.com/moby/moby/pull/34333)\r\n- escape characters are handled by the Dockerfile parser, not by the shell, and by default is a back-slash, which is fine for Linux, but [awkward for Windows Dockerfiles](https://github.com/moby/moby/pull/22268)\r\n\r\nBasically, to write a script / commands to run in a Dockerfile will require them to be rewritten in most cases.\r\n\r\n### Proposing a here doc notation for Dockerfiles\r\n\r\nHaving support for multi-line `RUN` instructions has been discussed in the past (for example, https://github.com/moby/moby/issues/1799, https://github.com/moby/moby/issues/1554, and https://github.com/moby/moby/issues/16058#issuecomment-138011204, possibly others), and was partly addressed by adding support for line-continuation symbols (`\\`), later enhanced with the `escape=` directive to assist in writing Dockerfiles targeting Windows.\r\n\r\nFurther changes were put on hold, pending a major refactor of the builder; now that the Dockerfile syntax is no longer frozen, and there's a clearer [roadmap for the builder](https://github.com/moby/buildkit), I'm opening this proposal to start the discussion again :) \r\n\r\nMy proposal is to add support for heredoc-style notation in the Dockerfile, similar to what's implemented in @jlhawn's Dockramp (https://github.com/jlhawn/dockramp#tokens). Having this notation makes writing multi-line (`RUN`, possibly extending to other Dockerfile instructions as well)  commands easier to write and, even though heredoc is not a known concept on Windows, benefits writing Windows Dockerfiles as well.\r\n\r\nThe full definitiona of here documents [can be found here](https://www.gnu.org/software/bash/manual/bashref.html#Here-Documents\r\n), but I'll provide some examples below.\r\n\r\nThe basic notation is;\r\n\r\n```\r\nRUN <<[-]word\r\n(run instructions)\r\nword\r\n```\r\n\r\nWhere\r\n\r\n- `<<` marks the start of the here document\r\n- `-`, if set, strips leading tabs from the here document\r\n- `word` can be any word, and is used as delimiter\r\n- if `word` is quoted (`'` or `\"`), no (variable) expansion is performed inside the here document.\r\n\r\nTo see this in action, create a shell-script containing the following;\r\n\r\n```bash\r\n#! /bin/bash\r\ncat <<EOF\r\n\t# example 1\r\n\techo $PWD\r\n\techo \\$PWD\r\n\techo `pwd`\r\n\r\nEOF\r\ncat <<'EOF'\r\n\t# example 2\r\n\techo $PWD\r\n\techo \\$PWD\r\n\techo `pwd`\r\n\r\nEOF\r\ncat <<\"EOF\"\r\n\t# example 3\r\n\techo $PWD\r\n\techo \\$PWD\r\n\techo `pwd`\r\n\r\nEOF\r\ncat <<-EOF\r\n\t# example 4\r\n\techo $PWD\r\n\techo \\$PWD\r\n\techo `pwd`\r\n\r\nEOF\r\ncat <<-'EOF'\r\n\t# example 5\r\n\techo $PWD\r\n\techo \\$PWD\r\n\techo `pwd`\r\n\r\nEOF\r\ncat <<-\"EOF\"\r\n\t# example 6\r\n\techo $PWD\r\n\techo \\$PWD\r\n\techo `pwd`\r\n\r\nEOF\r\n```\r\n\r\nWhich produces something like:\r\n\r\n```bash\r\n\t# example 1\r\n\techo /Users/sebastiaan/projects/docker-proposals/heredoc\r\n\techo $PWD\r\n\techo /Users/sebastiaan/projects/docker-proposals/heredoc\r\n\r\n\t# example 2\r\n\techo $PWD\r\n\techo \\$PWD\r\n\techo `pwd`\r\n\r\n\t# example 3\r\n\techo $PWD\r\n\techo \\$PWD\r\n\techo `pwd`\r\n\r\n# example 4\r\necho /Users/sebastiaan/projects/docker-proposals/heredoc\r\necho $PWD\r\necho /Users/sebastiaan/projects/docker-proposals/heredoc\r\n\r\n# example 5\r\necho $PWD\r\necho \\$PWD\r\necho `pwd`\r\n\r\n# example 6\r\necho $PWD\r\necho \\$PWD\r\necho `pwd`\r\n```\r\n\r\n### Implementation in the Dockerfile syntax\r\n\r\nThe heredoc notation in the Dockerfile should largely follow the behavior as described above;\r\n\r\nIf `word` is not quoted, environment variables that are known in the builder's context are expanded (as is done today when using the `shell` syntax);\r\n\r\n```Dockerfile\r\nENV FOO=hello\r\nRUN <<EOL\r\n\techo $FOO\r\nEOL\r\n```\r\n\r\nIs expanded _by the Dockerfile parser_ to;\r\n\r\n```Dockerfile\r\nENV FOO=hello\r\nRUN <<EOL\r\n\techo hello\r\nEOL\r\n```\r\n\r\nOr (in the image's configuration);\r\n\r\n```bash\r\n/bin/sh -c '\techo hello\\n'\r\n```\r\nor in JSON format:\r\n\r\n```Dockerfile\r\nRUN [\"/bin/sh\", \"-c\", \"\techo hello\\n\"]\r\n```\r\n\r\nIf `word` _is_ quoted, no expansion takes place, other than expansion by the shell, when executing the command:\r\n\r\n```Dockerfile\r\nENV FOO=hello\r\nRUN <<'EOL'\r\n\techo $FOO\r\nEOL\r\n```\r\n\r\nProduces\r\n\r\n```bash\r\n/bin/sh -c '\techo \\$FOO\\n'\r\n```\r\nor in JSON format:\r\n\r\n```Dockerfile\r\nRUN [\"/bin/sh\", \"-c\", \"\techo $FOO\\n\"]\r\n```\r\n\r\nThe quoted syntax can be usefull for Windows Dockerfiles as well, think of:\r\n\r\n```Dockerfile\r\nENV FOO=hello\r\nRUN <<'EOL'\r\n\tdir C:\\some\\directory\r\nEOL\r\n```\r\n\r\nWhen using the `<<-` syntax, all leading tabs are removed.\r\n\r\n### Limitations\r\n\r\nNote that, due to the way the builder works;\r\n\r\n- only environment variables known by the builder are expanded\r\n- instructions, such as ```pwd``` or `$(pwd)` are not expanded by the builder (but will be executed by the shell) \r\n\r\n### Here-doc and the `escape` directive\r\n\r\nMy original intent was to have here-documents _ignore_ the escape-directive, basically, pass anything inside the here-document as-is to the shell (which could be `bash`, `CMD.exe` or `PowerShell`).\r\n\r\nWhile this would solve many use-cases, there are some caveats;\r\n\r\nIf `word` is not quoted, _all_ environment variables would be expanded; there is no way to have _some_ environment variables expanded, and others unexpanded. For example:\r\n\r\n```dockerfile\r\nENV FOO=hello\r\nENV BAR=baz\r\nRUN <<-'EOL'\r\n\techo $FOO;\r\n\techo \\$BAR\r\nEOL\r\n```\r\n\r\nWould result in;\r\n\r\n```bash\r\n/bin/sh -c 'echo hello; echo \\baz\\n'\r\n```\r\n\r\nWe also need to take into account possible expansion of this syntax to Dockerfile instructions, other than just `RUN` (see below).\r\n\r\n### Support for other Dockerfile instructions\r\n\r\nAlthough we could start with just supporting this syntax for `RUN`, the here-doc syntax could also be implemented for other Dockerfile instructions. Here are some examples that came up in a discussion I had with @tonistiigi;\r\n\r\n```Dockerfile\r\nCOPY  <<EOF /dest\r\nthis is contents\r\nEOF\r\n```\r\n\r\n```Dockerfile\r\nARG myscript=<<EOF\r\nstuff\r\nEOF\r\n\r\nRUN $myscript\r\nCOPY $myscript /\r\n```\r\n\r\nFinally this example came up as well;\r\n\r\n```Dockerfile\r\nRUN <<EOF | sh\r\necho aa\r\nEOF\r\n```\r\n"},{"labels":[null,"enhancement1",null],"text":"Currently, managed plugins aren't supported on Windows environment running docker daemons.\r\nReference: https://docs.docker.com/engine/extend/\r\n\r\nThis is a feature request for the same."},{"labels":[null,null,"enhancement1"],"text":"**Description**\r\n\r\nThis is a bit of a combination of https://github.com/moby/moby/issues/10693 and https://github.com/moby/moby/issues/13331 (and was discussed briefly during the maintainer meeting today with @thaJeztah and @cpuguy83, where we determined that we ought to have a new, clean proposal).\r\n\r\nThe TL;DR of the problem is that currently, the Docker CLI will automatically perform a `docker pull` if the image does not exist, and will not ever perform a pull if the specified image _does_ exist.  This is especially irritating because `docker build` has a `--pull` flag which will always-pull, and `docker service` (and friends) will do digest-resolution, which results in the same general effect as doing a pull-before-run (although in a cluster-wide deterministic way).\r\n\r\n**Use Cases**\r\n\r\nWe discussed a few use cases, and in the context of `docker run`, there are three main use cases that we feel would be useful to accommodate:\r\n\r\n1. pull if missing (current behavior, likely what users expect at this point, and necessary for backwards compatibility anyhow)\r\n2. do not pull (useful for systems where \"pulling\" is something that administrators often want tight control over to ensure they don't ever run an image from an unexpected source, or as an atomic \"run-if-available-but-not-otherwise\" which is complicated to get right without something like this)\r\n3. always pull (just like `service update`, etc -- usefulness here speaks for itself)\r\n\r\n**Proposed Solution**\r\n\r\nWhat we discussed is a new `--pull` flag on `docker run`, but with a tri-state instead of a simple boolean like `docker build`'s (perhaps choosing a new name so that `docker build` can grow the new flag and eventually phase out `--pull` as well?):\r\n\r\n1. `--pull=missing`\r\n2. `--pull=never`\r\n3. `--pull=always`\r\n\r\nSince this pulling behavior isn't currently part of the API, this _should_ be reasonably simple to implement, and would be a CLI-only feature (just like the existing pulling behavior is).\r\n\r\nThere is some precedent for this with `docker stack deploy`'s `--resolve-image` flag, which is one of `always`, `changed`, or `never`.  We also discussed the possibility of doing digest resolution during `docker run` (as `docker service` does), but determined that it's likely going to be too big of a surprise for users to take that leap as well (since then the digest ends up in the container metadata, where users are likely already fairly used to doing things like `docker run some-image:some-version` and then `docker ps | grep some-image:some-version`, or similar).\r\n\r\n**Picture of a Cute Animal**\r\n\r\n![image](https://media1.giphy.com/media/26gs72bOpaZuCHiOk/giphy.gif)"},{"labels":[null,"enhancement1",null],"text":"It is currently impossible to specify a container isolation mode when configuring a swarm service. This isn't relevant for Linux containers, but Windows containers can run with either `process` (shared kernel) or `hyperv` (VM) isolation. When using `docker swarm` to deploy to a mixed-version Windows cluster, it is possible for a container to be scheduled on a node with a different kernel version [1]. A container kernel mismatch isn't significant when running with `hyperv` isolation but will cause containers using `process` isolation to fail to start.\r\n\r\nAs the default isolation mode is `process`, deployments to mixed-version Windows clusters currently fail if a mismatch occurs. Even if a cluster initially consists of a single version, mixed-version support is necessary to support online upgrades. Setting `\"exec-opts\":[\"isolation=hyperv\"]` in `C:\\ProgramData\\docker\\config\\daemon.json` on each node resolves the issue at the cost of additional overhead.\r\n\r\nThe Docker CLI supports an `isolation` parameter to control the setting when starting containers locally, but not for Swarm services. Docker-Compose supported this option in versions 2.1-2.3 but dropped it with v3.0's Swarm integration, presumably due to this issue. I've opened issues in each to reinstate the option but neither can proceed unless it is added to the Swarm Service API [2].\r\n\r\n[1]: I had trouble finding a concrete answer for this, but [MSDN](https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/swarm-mode#linuxwindows-mixed-os-clusters) indicates that labels should be used to constrain deployment.\r\n\r\n[2]: Related issues: docker/cli#414, docker/compose#5069"},{"labels":[null,"enhancement1"],"text":"We should be able to create `plugins` with `labels` which would allow us to filter them based on those filters. e.g. \r\n\r\n```\r\ndocker plugin ls --filter custom_type=my_plugin_type\r\n```\r\n\r\n/cc @cpuguy83 @anusha-ragunathan @tiborvass "},{"labels":[null,"enhancement1"],"text":"have the ability to configure webhook endpoints when deploy actions happen.  Perhaps defined like:\r\n\r\n```yaml\r\nversion: '3.x'\r\n services:\r\n   name:\r\n     deploy:\r\n       notification: http://this.host/asdf/this-service\r\n```\r\n\r\nWith metadata similar to the event stream info, sent as a json object."},{"labels":[null,null,"enhancement1"],"text":"To help push the ephemeral nature of containers, it would be amazing to have a 'maxage' or 'recycle' time as part of the deploy block.  When this time is reached, the scheduler reaps the container and replaces it (following normal deploy rules).  For example:\r\n\r\n    version: '3.x'\r\n    services:\r\n      name:\r\n        deploy:\r\n          lifecycle: 8 hours"},{"labels":[null,null,"enhancement1"],"text":"I am deploying a compose file using `stack deploy` which has 18 services in it, of which about 10 are Java Spring Boot services (all global)\r\n\r\nWhen trying to start all of these services at the same time, the host is unable to cope with the load. Consequently the services fail their health checks after 90s, and start over - increasing the problem.\r\n\r\nOne solution to this would have been of course the ability to deploy a single service from within the stack file. However this still requires manually doing starting each one.\r\n\r\nI think the ideal solution is that the rolling logic should not be limited to within the same service, but also consider the stack as a whole. Starting each service in turn."},{"labels":[null,"enhancement1"],"text":"I have a docker image which sets up a cross-compile environment. A full sdk + toolchain are part of the docker image, adding up to >5GB already. Additionally, we use volume mount to mount a _targetfs_ directory for the cross-compiling, which is used for finding all the libraries and headers for the cross-compilation. This _targetfs_ is >10GB in size. Then there is another volume mount containing another part of the SDK, adding another 20 GB or so...\r\n\r\nThere are currently about 5 client systems using this setup. The problem is however, that every now and then we need to add a few libraries to the _targetfs_, requiring us to update the volumes manually on all clients.\r\n\r\nIt would be incredibly useful if there would be a way to store such volumes in a central registry and have them roll-out automatically. Or if there is a way to mount directories from one image into another image (or at runtime as an image directory mounted in a container or a container directory mounter in a container).\r\n\r\nI want to avoid adding all this data to the image itself, as the image would become huge (>35GB).\r\n\r\nI'm curious if others might have other ideas of use cases too. E.g. to be able to mount applications that are installed nicely in their on folder (e.g. under /opt, though that could give problems with shared lib dependencies that don't match). This way you could for example add a whole Matlab (about 10GB) installation as a volume mount to an image.\r\n\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"## Summary\r\n\r\nThere are numerous high-value Docker use cases that could be satisfied if the Docker Volume engine supported volume snapshots and clones.  Examples include:\r\n\r\n* Rapidly create identical copies of Docker volumes\r\n  * Test & training environments\r\n  * Experimentation on temporary copies of data sets\r\n\r\n* Create periodic snapshots of Docker volumes\r\n  * Best practice of keeping regular snapshots of critical data\r\n  * Backup tools that copy snapshots to a separate location\r\n\r\n* Create a Docker volume initialized to the contents of an earlier snapshot\r\n  * Recovery of deleted files or earlier file versions\r\n\r\n* Rollback a Docker volume to an earlier snapshot\r\n  * Rapid recovery from accidental or malicious deletion or corruption\r\n\r\n## Plan\r\n\r\nWe propose to modify Docker (and have already done the work!) to add support for volume snapshots and clones.  The following Docker CLI commands are needed and are designed to “feel” identical to the existing volume commands:\r\n\r\n* docker volume clone\r\n  * Creates a new volume by cloning an existing volume, either directly or from a specified snapshot\r\n  * `volume clone [OPTIONS] VOLUME SOURCE_VOLUME [SOURCE_SNAPSHOT]`\r\n\r\n* docker volume snapshot create\r\n  * Creates a new snapshot of a volume\r\n  * `volume snapshot create [OPTIONS] VOLUME SNAPSHOT`\r\n\r\n* docker volume snapshot inspect\r\n  * Returns details of one or more snapshots, including volume name, snapshot name, and snapshot creation time\r\n  * `volume snapshot inspect [OPTIONS] VOLUME SNAPSHOT [SNAPSHOT...]`\r\n\r\n* docker volume snapshot ls\r\n  * Lists all the snapshots of a specific volume in tabular form, including snapshot name and creation time\r\n  * `volume snapshot ls [OPTIONS] VOLUME`\r\n\r\n* docker volume snapshot rename\r\n  * Renames an existing snapshot\r\n  * `volume snapshot rename [OPTIONS] VOLUME SNAPSHOT NEW_NAME`\r\n\r\n* docker volume snapshot restore\r\n  * Reverts the contents of a volume (in place) to an earlier snapshot\r\n  * `volume snapshot restore [OPTIONS] VOLUME SNAPSHOT`\r\n\r\n* docker volume snapshot rm\r\n  * Deletes one or more volume snapshots\r\n  * `volume snapshot rm [OPTIONS] VOLUME SNAPSHOT [SNAPSHOT...]`\r\n\r\nAdding this feature requires modification of several components: the Docker client/CLI, the engine, and the volume plugin library.\r\n\r\nNot only have we developed this feature, but we even demonstrated it on stage during a DockerCon 2017 session.  Please see a video demo here: ([link](http://netapp.io/2017/06/22/snapshots-clones-docker-volume-paradigm/)).\r\n\r\n## Notes\r\n\r\nWe're posting this proposal to gauge interest, and we are glad to follow up with code, unit tests, etc.\r\n\r\nIt's worth noting that the Kubernetes community is actively building support for volume snapshots, and volume clones are also a roadmap item, so supporting such basic storage features will soon become table stakes for container orchestrators.\r\n\r\nNote also that multiple filesystems used by Docker today, such as btrfs and zfs, support snapshots and could be enhanced to support this feature.\r\n\r\nWe can leverage extensive experience developing similar functionality in OpenStack ([link](https://www.youtube.com/watch?v=yQpg99L7Vno)).  For example, when restoring a volume to a snapshot, some storage systems automatically delete all snapshots newer than the one being restored.  That may be considered a form of data loss, so the safest approach is to only allow restoring a volume to the newest snapshot available.\r\n"},{"labels":[null,"enhancement1",null],"text":"[Current log driver message](https://github.com/moby/moby/blob/master/daemon/logger/copier.go#L102) is hard coded with `UTC` time, how about add `--log-timezone` for `log driver` to config message time zone?"},{"labels":[null,"enhancement1"],"text":"Hello people !\r\n\r\nThat would be really nice to be able to specify a delay when we scale a docker service.\r\nThere is multiple service out there that are just not ready to face racing conditions and scaling multiple containers at once can make a mess of them.\r\n\r\nHere's some examples:\r\n\r\nCassandra: You can't have multiple cassandra node rejoining a cluster simultaneously or the bootstrap process will fail.\r\n\r\nApplications running Serf in background for clustering features : Serf join command will sometimes hang if it tries to reach another container not fully booted yet, scaling containers too fast can cause that.\r\n\r\nSometimes applications runs migrations while bootstraping, but doesn't if the migrations are already there. Launching the whole stack of containers at once could cause all the containers to try to run the migrations simultaneously and crash.\r\n\r\nI tried to workaround the problem using service update --update-parallelism and --update-delay but it doesn't work. So finally I found a workaround with ansible to manage the scaling one container at a time, and using \"wait\" role in between. I believe we could do better and add a proper way to do it\r\n\r\nThe idea is launched, any comments or objections ?\r\n\r\nThx !"},{"labels":[null,"enhancement1"],"text":"I have some volumes that have contents that are automatically generated. I'd like to keep the MD5 of the contents in the label on the volume. Unfortunately, I can't change the labels on a volume after its created. It would be nice to be able to change volume labels after creation time, or to know why its not feasible to do so"},{"labels":[null,"enhancement1"],"text":"Currently neither 'docker info' nor 'docker node inspect' returns any info related to the diskspace allocated to docker daemon( /var/lib/docker):\r\n\r\n1. Total diskspace allocated\r\n2. Total available/used.\r\n\r\nThis makes it difficult to schedule  containers to nodes that  have sufficient diskspaces at the risk of getting \"disk quota exceeded\" error message."},{"labels":[null,"enhancement1"],"text":"**Description**\r\nSometimes it will be very useful to be able to squash last n layers of an docker image.\r\n\r\nWe use the following workflow:\r\n1. A container is started from basic image (e.g. ubuntu or centos)\r\n2. It works some time, some changes are made to FS.\r\n3. In the end of a session the container is committed, new image is saved.\r\n4. Next session starts container from previously committed image.\r\n5. Repeat from 2-nd item.\r\n\r\n**Describe the results you received:**\r\nWith each iteration it becomes slower and slower, because of increasing layers number from which new image consists (one commit - one new layer). And after some period of time it becomes dramatically slow (and, of course, image becomes huge), which makes usage of this very painful, then practically impossible.\r\n\r\n**Describe the results you expected:**\r\nTo be able to squash (to base image) layers on/after a container commit.\r\n"},{"labels":["enhancement1",null],"text":"Exporting a container on Windows seems to be explicitly disallowed:\r\n```go\r\nif runtime.GOOS == \"windows\" {\r\n    return fmt.Errorf(\"the daemon on this platform does not support export of a container\")\r\n}\r\n````\r\nhttps://github.com/moby/moby/blob/master/daemon/export.go#L16-L18\r\n\r\nIt would be good if this functionality were also available to Windows users."},{"labels":[null,"enhancement1"],"text":"If you run docker with default settings `--iptables=true` makes it nearly impossible to make custom security settings. I must accept that each container can call everything outside or nothing (`--icc=false`).\r\n\r\nI would suppose to create more chains to allow special options:\r\n\r\n- PRE_DOCKER_IN which is registered before the DOCKER and DOCKERISOLATION-chain and used registered at FORWARD -o docker0\r\n- PRE_DOCKER_OUT which is registered before the DOCKER and DOCKERISOLATION-chain and used registered at FORWARD -i docker0\r\n- POST_DOCKER_IN which is registered after the DOCKER and DOCKERISOLATION-chain and used registered at FORWARD -o docker0\r\n- POST_DOCKER_OUT which is registered after the DOCKER and DOCKERISOLATION-chain and used registered at FORWARD -i docker0\r\n\r\nafter that chains you can add ACCEPT all or DROP all depending on `--icc=true|false` .\r\n\r\nThe logic must be if PRE_DOCKER_* oder POST_DOCKER_* exists it will not be created. Docker will not put any rules to the chains. So a normal admin can use them to have full control over the forward chain.\r\n\r\nWhat do you think?\r\n"},{"labels":[null,"enhancement1",null],"text":"Currently there is no option to add build-time storage opts for e.G. increasing disk-space during build when building large images. Please add it! :)\r\n\r\nThis ticket results from a post on the windowscontainers forum:\r\n[https://social.msdn.microsoft.com/Forums/en-US/07dfc61e-f5ef-40dc-b75e-f98725a1cef1/missing-storageopts-on-buildtime?forum=windowscontainers](https://social.msdn.microsoft.com/Forums/en-US/07dfc61e-f5ef-40dc-b75e-f98725a1cef1/missing-storageopts-on-buildtime?forum=windowscontainers)\r\n\r\nrequested tag: @mylesbkeating1993\r\n\r\n**Description**\r\ncheck the linked forum post"},{"labels":[null,null,"enhancement1"],"text":"Description:  I have 2 Swarm Mode 1.13 clusters: Cluster A in staging and Cluster B in production. I created some secrets in the Staging cluster A and now I want to migrate those set of secrets from A to production cluster B. I  do not see how it can be done in a safe way except to replicate everything manually. Needs an secret EXPORT/IMPORT feature:\r\n\r\ndocker secret export --file  \"/some/file\"\r\ndocker secret import --file  \"/some/file\"\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"**Description**\r\n\r\nThis is a suggestion for a new feature:\r\ndocker stats is a very useful command but in docker swarm mode it is currently required to SSH into each node and to start docker stats on each node to check the swarm status.\r\nThe suggestion is to extend the docker stats command to display stats to consider the tasks / containers from all nodes across the swarm rather than displaying only the information from the local node.\r\nAlternatively, a docker service stats could be implemted - just like the docker service logs.\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"(Related issue #33392)\r\n\r\nThe replicated orchestrator scales down a service by removing tasks in a way that balances the resulting tasks between nodes. But some stateful services may need to be scaled down by removing specific tasks to minimize the impact of scaling down.\r\n\r\n#### Use Case\r\n- An elasticsearch cluster can reallocate shards from tasks to be ended to another tasks if it can select tasks for termination when scaling down.\r\n- Some applications may not want their elected master to be killed when scaling down.\r\n\r\n---\r\n### docker service rm-replica\r\n#### Description\r\nThe `rm-replica` command enables you to scale down one or multiple replicated services by removing replicas referenced by specific tasks. \r\n#### Usage\r\n```\r\n$ docker service rm-replica TASKNAME [TASKNAME...]\r\n```\r\n#### Usage Example\r\n```\r\n$ docker service create --name redis --replicas 3 redis:3.0.6\r\n\r\n$ docker service ps redis\r\n\r\nID             NAME      IMAGE        NODE      DESIRED STATE  CURRENT STATE          ERROR  PORTS\r\n0qihejybwf1x   redis.1   redis:3.0.5  manager1  Running        Running 8 seconds\r\nbk658fpbex0d   redis.2   redis:3.0.5  worker2   Running        Running 9 seconds\r\n5ls5s5fldaqg   redis.3   redis:3.0.5  worker1   Running        Running 9 seconds\r\n\r\n$ docker service rm-replica redis.2\r\n\r\nredis scaled to 2\r\n\r\n$ docker service ps redis\r\n\r\nID             NAME      IMAGE        NODE      DESIRED STATE  CURRENT STATE          ERROR  PORTS\r\n0qihejybwf1x   redis.1   redis:3.0.5  manager1  Running        Running 30 seconds\r\n5ls5s5fldaqg   redis.3   redis:3.0.5  worker1   Running        Running 31 seconds\r\n```"},{"labels":[null,"enhancement1"],"text":"At the moment, the overlay network use the vxlan default port 4789 as the cross host container communication.\r\n\r\nBut in some use-case, alibaba cloud classic network for example,  the cluster's nodes maybe behind firewall/NAT device, or 4789/udp port be blocked. It's required that the overlay network's vxlan host/port can be configurable and advertisable."},{"labels":[null,null,"enhancement1"],"text":"In a maintainers meeting last Thursday, there was a discussion how to move forward with Build secrets.\r\n\r\nThe previous PR https://github.com/moby/moby/pull/30637 is closed atm but we need to make sure that the issue is still tracked. The issue was closed because of design issues(listed below) and possible changes/features coming with #32507. This issue is mainly for keeping the secrets discussion from blocking #32507.\r\n\r\n\r\n### Open questions:\r\n\r\n#### Secret sources:\r\n\r\nIn #30637 secrets are sent from the client with the context tar. There were concerns if these should be loaded from swarm secrets instead. The use cases seem quite different but it does feel weird to have 2 secrets implementations. Also, the current \"build-secrets\" use cases are not as secure as swarm ones.\r\n\r\n#### Sending secrets:\r\n\r\nIn #30637 secrets are injected into context tar on the client side and extracted on daemon before being used. With #32677 this could be done independently from context.\r\n\r\n#### Dockerfile UI:\r\n\r\nIn #30637 user specifies target path for the secrets in cli command with `--build-secret` flag. All data exposed like this will become available in `/run/secrets` for every `RUN` operation. Normally it is the image/dockerfile author who knows where specific secrets were expected. `SECRET` Dockerfile command was one of the options considered. #32507 lets image author specify the mount path. With https://github.com/docker/swarmkit/pull/2118 regular swarm secrets do not need to be in `/run/secrets` either. #32507 allows exposing mounts to a specific command that needs a secret, not to everything at once.\r\n\r\n#### Build cache:\r\n\r\n#30637 ignores build cache, #32507 uses it. It is probably more correct to ignore cache to avoid chosen plaintext attacks so #32507 would need to allow that.\r\n\r\n#### Other solutions:\r\n\r\nMost examples that show build secrets use it for SSH keys. There are other ways for exposing this specific feature. #32677 allows ssh forwarding(poc https://github.com/tonistiigi/docker/commit/a175773938b73937b417c7d322961d53ceab359e). By exposing git sources as build stages we could use any auth(ssh, oauth) for cloning git repos.\r\n\r\n@ehazlett @thaJeztah @dnephin @cpuguy83 @diogomonica "},{"labels":[null,null,"enhancement1",null],"text":"## Problem\r\n\r\nContainer bind mounts cannot be intercepted to perform additional setup and teardown (e.g. in the case of cross-platform Docker products like Docker for Mac).\r\n\r\n## Proposal: Mount Point Plugins\r\n\r\nMount point plugins enable end-users, systems developers, and Docker Inc to develop plugins that interpose on mount point setup and teardown in containers. In particular, a number of mount point plugins can be installed in the engine that are consulted, in order, to confirm or deny mounts and potentially change their source location. On container shutdown or stop, relevant mount point plugins would also be called, in reverse order, to teardown the mounts. These teardown or detachment transactions block shutdown until they complete and may change the container exit code if the teardown fails (e.g. synchronizing state fails). Mount point plugins will be able to consume the [new consistency flags](https://github.com/moby/moby/pull/31047).\r\n\r\nInitially, only bind mounts and volume mounts will be supported (i.e. not tmpfs, network mounts, secrets, or container layers). Mount point plugins will register filters on initialization so that only\r\napplicable plugins are consulted for any given mount. This improves performance by reducing the number of plugin round trips for un-interposed mounts.\r\n\r\nThis functionality is necessary to fix [bind mount inotify events not delivered after container\r\nrestart](https://github.com/docker/for-mac/issues/681) and [fs events not working for services](https://github.com/docker/for-mac/issues/148) as well as [Cannot add, remove, add overlapping directory](https://github.com/docker/for-mac/issues/139). It will remove one of the use cases of the current Docker API proxy in Docker for Mac and make componentization of that product more tractable.\r\n\r\nFinally, by enabling container file system virtualization, mount point plugins will enable a number of interesting use cases like:\r\n\r\n * Seamless cross-platform bind mounts\r\n * Sensitive data write auditing\r\n * Unused file analysis\r\n * Access pattern recording\r\n * Unix domain socket file snooping\r\n * Copy-on-write mount points\r\n * Process-based access control\r\n * Non-swappable tmpfs mounts (with tmpfs support)\r\n * Generic file system caching (e.g. deferring flush until container shutdown)\r\n * Special path-dispatched file systems\r\n * Arbitrary ownership mapping and JIT ownership transfer\r\n\r\nI've drafted a(n unpublished) patchset implementing this functionality as a way to explore the design space and familiarize myself with the relevant Docker subsystems. I'm interested in your thoughts.\r\n\r\n/cc @yallop @cpuguy83 @dnephin "},{"labels":[null,"enhancement1"],"text":"Anyone know if there is anything being done to query the statistics of a service?\r\n\r\nSo do we have the docker stats container today?"},{"labels":["enhancement1"],"text":"Docker 17.04.0-ce\r\n\r\nIt would be nice if I could create stack-scoped secrets:\r\n\r\n```bash\r\ndocker secret create --stack=test_my_product mysql_password <foo>\r\ndocker secret create --stack=staging_my_product mysql_password <foo>\r\ndocker stack deploy -c my-product.yaml test_my_product\r\ndocker stack deploy -c my-product.yaml staging_my_product\r\n```\r\nThus a single `my-product.yaml` can reference a `mysql_password` secret who's value happens to vary based on the name of the stack.\r\n\r\nThis reduces the need to chain multiple YAML files together as we did with `docker-compose` to overlay individual environment overrides. It is not possible to make a YAML key interpolate environment variables either, which is part of how we arrived at this idea.\r\n"},{"labels":[null,"enhancement1"],"text":"For instance, when we works with applications like Elasticsearch, would be very interesting can setup each new service task with differents parameters/variables... elastic node name, host addresses of the others nodes which form the cluster and so on...\r\n\r\nThe service tasks of those kind of applications, are almost equal about their setup, but there're a few parameters which have to be setup concretly in each task to allow them to work as a only one service."},{"labels":[null,"enhancement1"],"text":"Currently the only way to run docker containers with only an IPv6 stack is to run with host networking on a host running only IPv6.  For any other network type, if no IPv4 addresses are specified, they are automatically generated and assigned to the containers.\r\n\r\nWhile it may be reasonable to default to operating in dual-stack mode, it would be good to have an option to create docker networks without an IPv4 stack by supplying a flag like `--no-ipv4` or something similar.  I'm working in an IPv6-only environment and the automatic v4 addresses are causing problems with things like name resolution.\r\n"},{"labels":[null,"enhancement1"],"text":"**This issue supersedes PR https://github.com/moby/moby/pull/32767**\r\n\r\n## Why is this change needed or what are the use cases?\r\n\r\nSometimes after an outage, a user may want to access logs through a specific\r\ntime window. Right now, `docker logs` only allows you to retrieve logs after\r\na specific timestamp, but it lacks an `--until` flag.\r\n\r\nMany other logging tools like journalctl allow this kind of relativistic\r\nlogging, so it'd be great if Docker could too.\r\n\r\n## What are the requirements this change should meet?\r\n\r\n- Should allow users to inspect logs within a time window\r\n- Should not affect any existing functionality, such as `--since`\r\n\r\n## What are some ways to design/implement this feature?\r\n\r\nAdd a new `--until` flag which operates in a similar way to `--since`.\r\n\r\nUsers can use `--until` by itself (all logs older than a specific date), or\r\nin tandem with `--since` (all logs in a time window).\r\n\r\n## Which design/implementation do you think is best and why?\r\n\r\nThere are two steps in the process: adding it to the daemon and to the client.\r\n\r\n### 1. Daemon\r\n\r\n- New `string` Until field in [ContainerLogsOptions](https://github.com/moby/moby/blob/d40a17ffc2f6592396a3dfc0f5ebe396c2107536/api/types/client.go#L73-L81)\r\n- New `time.Time` Until field in [ReadConfig](https://github.com/moby/moby/blob/d40a17ffc2f6592396a3dfc0f5ebe396c2107536/daemon/logger/logger.go#L84-L88)\r\n- Refactor all logging adaptors to use until logic:\r\n  - [`daemon/logger/adapter.go`](https://github.com/moby/moby/blob/d40a17ffc2f6592396a3dfc0f5ebe396c2107536/daemon/logger/adapter.go#L81)\r\n  - [`daemon/logger/journald/read.go`](https://github.com/moby/moby/blob/d40a17ffc2f6592396a3dfc0f5ebe396c2107536/daemon/logger/journald/read.go#L413)\r\n  - [`daemon/logger/jsonfilelog/read.go`](https://github.com/moby/moby/blob/d40a17ffc2f6592396a3dfc0f5ebe396c2107536/daemon/logger/jsonfilelog/read.go#L41)\r\n  - [`daemon/logger/proxy.go`](https://github.com/moby/moby/blob/d40a17ffc2f6592396a3dfc0f5ebe396c2107536/daemon/logger/proxy.go#L99)\r\n  - [`daemon/logger/ring.go`](https://github.com/moby/moby/blob/d40a17ffc2f6592396a3dfc0f5ebe396c2107536/daemon/logger/ring.go#L28)\r\n\r\n### 2. Client/CLI\r\n\r\n- Add conditional logic to [`client/container_logs.go`](https://github.com/moby/moby/blob/d40a17ffc2f6592396a3dfc0f5ebe396c2107536/client/container_logs.go)\r\n- Populate correct options in [`cli/command/container/logs.go`](https://github.com/moby/moby/blob/d40a17ffc2f6592396a3dfc0f5ebe396c2107536/cli/command/container/logs.go)\r\n\r\n### 3. Docs\r\n\r\n- Update docs and any examples"},{"labels":[null,null,"enhancement1"],"text":"# Entitlements in Moby\r\n\r\nThis issue captures a draft design proposal for an entitlement mechanism that can be leveraged by Moby and other container management platforms to describe what additional permissions a specific service should be allowed to have when executing.\r\n\r\nAn entitlement is a single right granted to a particular service/container that gives it additional permissions above and beyond what it would ordinarily have. An entitlement is a piece of configuration information included in the service spec, telling the container engine that executes a service allow access to certain resources or perform certain operations. In effect, an entitlement extends the sandbox and capabilities of your service to allow a particular operation to occur.\r\n\r\nThe Docker CLI currently supports over 100 command-line flags. By implementing an entitlement mechanism we plan to allow downstream consumes of moby such as docker to unify all of the security related flags into a single mechanism that is granular enough to be useful, platform independent, and understandable by a non-expert user. This mechanism can be seen as Moby's equivalent of Apple’s App Store permission model, where apps are granted capabilities to operate beyond the normal privileged of an application, such as access to the keychain or the ability of enabling push-notifications.\r\n\r\n## Goal\r\n\r\nThe goal is to simplify the way downstream users ask for permissions for their containers/services.\r\n\r\nIn order for a service to use a specific entitlement, access to that entitlement has to be granted. The objective is to have a grant mechanism that would look like this:\r\n\r\nSpecifying 1+ entitlement on the command-line:\r\n> docker run --entitlements=[entitlement1] --entitlements=[entitlement2] alpine\r\n> docker service create --entitlements=[entitlement] alpine\r\n\r\n## Current proposal for entitlements\r\n\r\n|Entitlement|Privileges|Capabilities|Blocked syscalls|On Windows|\r\n|:-:|:-:|:-:|:-:|:-:|\r\n|api.access| API access management, defaults provided for the Engine and Swarm APIs||||\r\n|host.processes.none|Do not share host's PID namespace||| N/A |\r\n|host.processes.admin|Shares host's PID namespace||| N/A |\r\n|host.devices.none| RO for sysfs, No additional non-default mounts; No RW on /proc/kcore||| N/A |\r\n|host.devices.view| RO on non-default mounts ||| N/A |\r\n|host.devices.mount|Add SYS_ADMIN and allow a device to be mounted in.||| N/A |\r\n|network.none| No access to /proc/pid/net, /proc/sys/net; No access to /sys/class/net| No NET_ADMIN, NET_BIND_SERVICE, NET_RAW, NET_BROADCAST| socket, socketpair, setsockopt, getsockopt, getsockname, getpeername, bind, listen, accept, accept4, connect, shutdown,recvfrom, recvmsg, sendto, sendmsg, sendmmsg, sethostname, setdomainname, bpf||\r\n|network.user| |CAP_NET_RAW, CAP_NET_BIND_SERVICE?, CAP_NET_BROADCAST?|sethostname, setdomainname bpf, setsockopt(SO_DEBUG)||\r\n|network.proxy||Add: CAP_NET_RAW, CAP_NET_BROADCAST, CAP_NET_BIND_SERVICE, CAP_NET_BIND_SERVICE, CAP_NET_RAW|||\r\n|network.admin||CAP_NET_ADMIN, CAP_NET_BROADCAST, CAP_NET_BIND_SERVICE, CAP_NET_RAW|||\r\n|security.confined|Block access to sentitive paths: /sys/kernel/security , /sys/kernel/debug (ftrace), /sys/kernel/livepatch, /sys/fs/selinux, /sys/fs/cgroup, debugfs, securityfs, selinuxfs, /proc/sys/kernel/, /proc/config.gz, /boot, /proc/{mem,cpu,kcore,kmem,sysrq-trigger,bus} <br> No MAC/DAC policy read/write or configuration/state change <br> NoNewPrivileges activated|Drop:No CAP_MAC_\\*, CAP_DAC_\\*, CAP_SETPCAP, SYS_PTRACE, CAP_SET_\\*, CAP_FSETID, CAP_SYS_ADMIN|bpf, ptrace, seccomp, arch_prctl, personality, setuid/setgid?, madvise, prctl(PR_CAPBSET_DROP, PR_SET_\\*, ..)||\r\n|security.view|Read Only rights on sensitive filesystems / fs directories and MAC/DAC policies|Add:CAP_MAC_\\*, CAP_DAC_\\*, CAP_SETPCAP <br>Drop: CAP_LINUX_IMMUTABLE?, CAP_SET_\\*, CAP_FSETID, , SYS_PTRACE, CAP_SYS_ADMIN|||\r\n|security.admin||Add: CAP_MAC_\\*, CAP_DAC_\\*, CAP_LINUX_IMMUTABLE,  CAP_SYS_MODULE, CAP_SYS_PTRACE, CAP_SYSLOG,  CAP_FSETID, CAP_SYS_BOOT|||\r\n|security.read-only| Mounts the container's filesystem as read-only||||\r\n|resources.limit=`value`| `value` is a percentage of available resources in the container at launch context for: Pids, perf_event, blkio, hugetlb, freezer, net_cls, net_prio, cpuset, memory, systemd. Set ulimits properly||||\r\n|debug|security.unconfined|Add:CAP_SYS_ADMIN, CAP_SYS_PTRACE, CAP_SYSLOG|||\r\n\r\n\r\n## Examples of usecases\r\n\r\nFollowing examples are meant to show how to use entitlements for the most downloaded images on Docker Hub. They will probably be edited as new entitlements will probably be added and/or adjusted.\r\n\r\nPeople used to run the following command in various use-cases:\r\n> docker run --privileged imagename:label\r\n\r\nWe impact privileges by reducing the amount of rights granted, let’s see some examples:\r\n\r\nPeople who need to use raw sockets to control link discovery and aggregation\r\nBefore:\r\n > docker run --privileged image:label\r\n\r\nAfter:\r\n > docker run --entitlements=network.proxy image:label\r\n\r\nDocker in Docker would probably look like:\r\nBefore:\r\n > docker run --privileged docker:dind\r\n\r\nAfter:\r\n > docker run --entitlements=network.admin --entitlements=host.devices.admin --entitlements=security.admin docker:dind\r\n\r\nLong term, you should be able to tie a set of entitlements to an image if you want to as a publisher so the more example, the better.\r\n\r\n## Open Questions\r\n\r\nCan the same privilege name mean different things for docker version to docker version? We would be treating entitlements as we treat default profiles.\r\n\r\nDo all these entitlements also make sense on windows?\r\n\r\nShould we provide a way to create custom entitlements?\r\n\r\n## What we need from the community\r\n\r\n- Validation that these entitlements are: high-level enough, non-overlapping and correctly implemented using the lower level primitives available from Linux\r\n- Examples of use-cases where the entitlements fit/don't fit, and if they add too much privilege over what users are usually doing #33422 "},{"labels":[null,"enhancement1",null],"text":"It would be very helpful if `docker stack COMMAND` did its logic not inside the CLI, but rather on the Client or on the API.\r\n\r\nThis would make integration with other tools much better, because you can just use the API and not have to switch between an SDK and shell commands.\r\n\r\nAny good reason not to do that?\r\n\r\nthanks : )\r\n\r\ndocker version:\r\n\r\n```\r\nClient:\r\n Version:      17.05.0-ce-rc1\r\n API version:  1.29\r\n Go version:   go1.7.5\r\n Git commit:   2878a85\r\n Built:        Tue Apr 11 20:55:05 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      17.05.0-ce-rc1\r\n API version:  1.29 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   2878a85\r\n Built:        Tue Apr 11 20:55:05 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\ndocker info:\r\n```\r\nContainers: 15\r\n Running: 4\r\n Paused: 0\r\n Stopped: 11\r\nImages: 122\r\nServer Version: 17.05.0-ce-rc1\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 9048e5e50717ea4497b757314bad98ea3763c145\r\nrunc version: 9c2d8d184e5da67c95d601382adf14862e4f2228\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.21-moby\r\nOperating System: Alpine Linux v3.5\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 1.952GiB\r\nName: moby\r\nID: QI6M:5L7Q:2EL3:OOB5:4ZHV:Z5OF:QDWN:KUD3:J2ND:C4WM:MBOS:N6IS\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 56\r\n Goroutines: 78\r\n System Time: 2017-04-22T20:15:34.575559217Z\r\n EventsListeners: 3\r\nNo Proxy: *.local, 169.254/16\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```"},{"labels":["enhancement1"],"text":"## Feature request\r\n\r\nI think it would be nice to have a cli for `cd` into docker volume mounpuint (from host), like:\r\n```bash\r\n$ pwd\r\n/tmp\r\n$ docker volume ls\r\nlocal    vol1\r\n$ docker volume inspect vol1\r\n[\r\n    {\r\n        \"Driver\": \"local\",\r\n        \"Labels\": null,\r\n        \"Mountpoint\": \"/var/lib/docker/volumes/vol1/_data\",\r\n        \"Name\": \"vol1\",\r\n        \"Options\": {},\r\n        \"Scope\": \"local\"\r\n    }\r\n]\r\n$ docker volume cd vol1\r\n$ pwd\r\n/var/lib/docker/volumes/vol1/_data\r\n```\r\n\r\nFor local volumes it is useful. I am just wondering, will it work for any other type of volumes?"},{"labels":[null,"enhancement1"],"text":"\r\n**Description**\r\nDocker volume plugin v2 doesn't support tcp communication to docker engine (in v2 only unix socket is supported)\r\nTalked about it with Tibor Vass (Docker Engineer) @DockerCon17 and he asked me to open this ticket so tcp will also be supported.\r\n\r\nIn DockerCon Tibor considered about generic proxy plugin that just pass the unix socket to tcp request to the real plugin but they we may have a problem to certify the proxy and the real plugin.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n\r\n**Describe the results you received:**\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n\r\n**Output of `docker info`:**\r\n\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement1"],"text":"That's a feature request/inquiry.\r\n\r\nI am using a separate rsync container to overcome the issue https://github.com/docker/for-mac/issues/77 (suppose that one is so common, that everybody knows about it.)\r\n\r\nThat's no problem for now, but every time I start a new developer's project I need to modify and restart my rsync container, adding new volumes to it like\r\n\r\n```\r\nrsync:\r\n   volumes:\r\n       project1-src:/project-rsync/project1\r\n       project2-src:/project-rsync/project3\r\n       project3-src:/project-rsync/project3\r\n...\r\n\r\nvolumes:\r\n    project1-src:\r\n    project2-src:\r\n    project3-src:\r\n...\r\n```\r\n\r\nIt would be nice if starting a new project, I could simply do:\r\n```\r\ndocker volume create new-project\r\ndocker restart rsync --addvolume new-project:/project-rsync/new-project\r\n```\r\n\r\nSo it restarts the container with the same parameters it was running, plus adding one volume.\r\nSuch kind of parameter managing during container restart.\r\n\r\n\r\n"},{"labels":[null,null,"enhancement1"],"text":"Make docker in swarm nodes to be able to share local images between nodes.\r\n\r\n# Why\r\nSetting up docker in swarm mode is seamless compared to [docker swarm](http://docs.master.dockerproject.org/swarm/overview/) because there is no need to set up the security infrastructure and key value store. But when one want to use any of local image in swarm mode one have to\r\n* Spin up private registry\r\n* Manually configure each node to use the registry\r\n* Push local images to registry\r\nI believe use of local images is frequent enough that adding embedded registry to swarm would be valuable.\r\n\r\nI also think it would enable support of **build** feature from **compose** by **stacks**.\r\n\r\n# How (suggestion)\r\n\r\n- Add flag to run embedded registry to **swarm init** and **swarm update** \r\n- Add flag to **push** to target embedded registry instead of docker hub when pushing images without registry specified\r\n- Make swarm nodes to look for images in embedded registry before checking with remote registry. \r\n\r\nThis request may be related to #25997\r\n"},{"labels":[null,"enhancement1"],"text":"It would be more of a doubt but I do not know where to ask this:\r\n\r\nWould there be any way to limit the disk IO of the containers with \"docker service\"?\r\n\r\n\r\nSomething like this \"docker create\"?\r\n\r\n      --device-read-bps list       Limit read rate (bytes per second) from a device (default [])\r\n      --device-read-iops list      Limit read rate (IO per second) from a device (default [])\r\n      --device-write-bps list      Limit write rate (bytes per second) to a device (default [])\r\n      --device-write-iops list     Limit write rate (IO per second) to a device (default [])\r\n\r\n\r\nThanks"},{"labels":[null,"enhancement1"],"text":"Similar to how bind mounts let you mount subfolders i.e `-v /host/path:/container/path`, is it possible for the same functionality to be available for named volumes? i.e `-v namedvolume/path:/container/path`. Right now in order to mount a named volume, you must mount the entire volume. \r\n\r\nI apologize in advance if this has been discussed somewhere already, but I couldn't find anything saying this isn't doable.  \r\n\r\n"},{"labels":[null,"enhancement1"],"text":"With the macvlan network, it is possible to create container applications which work exclusively at layer 2, and ignore all IP-based routing.\r\n\r\nHowever, a subnet & gateway are always allocated to every network, and an IP (both v4 and v6) are always allocated to any containers on the network.\r\n\r\nDownsides of this approach:\r\n* Unnecessary IPs allocated.\r\n* Subnet exhaustion/conflicts.\r\n* Various \"network discovery\" packets sent into L2 network.\r\n\r\nRecommendations:\r\n* Allow the existing container option to disable IP assignment. (`--ip=none`, `--ipv6=none`)\r\n* Disable ipv6 IP assignment by default.\r\n* Add a network option to disable IPv4 (possibly only for the macvlan network type).  This should disable subnet/gateway allocation.  Any connected containers should not have default IPs assigned. (`--ipv4=false`)\r\n* Ensure the equivalent options are ported to compose."},{"labels":["enhancement1"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\nIf a user named user1 logs in to the host and launches a container, the container is hosted by the docker engine. The user1 need not be in any special group or a sudo user. While issuing the docker run command user1 may provide -u switch to let the process within the container run as a non-root user.\r\n\r\nNow if user2 does a docker ps and sees the container started by user1, he can docker exec into the container without using -u and get root access of the container.\r\n\r\nThis needs to be prevented.\r\n\r\n1. Login to the Host as user1\r\n2. Run the command : docker run busybox\r\n3. Login as user2 on another terminal\r\n4. Enlist the containers started by all users using : docker ps\r\n5. Connect to the container started by user1 using: docker exec -it bash\r\n6. Kill the process in the conatiner\r\n\r\nThe container started by user1 was killed by user2.\r\n\r\n\r\nThe docker engine should provide a way to:\r\nPrevent user2 from firstly seeing the containers started by other users.\r\nEven if user2 gets to know the container ID, he should not be allowed to exec into the container started by user1.\r\n\r\n\r\nThis is on docker 1.13."},{"labels":[null,"enhancement1"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nIs there a way to RUN each command independently rather than re-execute simply because the order of RUN command is changed in docker file?\r\n\r\n**Describe the results you received:**\r\n\r\nFor example, `RUN wget url-one` and  `RUN wget url-two`, apparently those two lines does not depend on each other as far as for the one who wrote the Dockerfile, but docker will re-download everything if the order changed. This becomes a big problem when the run command is time consuming, such as build compiler from source code. \r\n\r\n**Describe the results you expected:**\r\n\r\nIs that possible to run a command independently in a Docker file that is only depend on base image rather than previous command?? So the layer can be cached and reused. \r\n\r\n"},{"labels":[null,"enhancement1"],"text":"Dear all,\r\n\r\nI'd like to raise a feature request regarding the logging/log driver. Since I find the json logger very valuable for having a quick look at what is going on, but on the same time for digging trough the logs I prefer to have a log server, it would be really nice to have somehow both. \r\n\r\nThis means that docker would need to allow to have multiple logging driver for a container at the same time.\r\n\r\nWhat do you think about this? Would it be possible and valuable to implement this in a future version?\r\n\r\nbest\r\np"},{"labels":["enhancement1"],"text":"Sometimes when docker try doing many operations (build containers or download images) looks like docker has stoped and doing nothing (and responces from docker service have very long time).  \r\nThere is comand ```docker system events``` which shows log of events which already happend. But if current operation is very long I cannot say it continues or docker stoped due to something failures.  \r\nSo it will be good if some command will appear like ```docker system status``` or ```docker system current``` which can show current operation and their status and approxymately percent of execution.  \r\nFor example: ```Downloading image name:tag. 45% completed``` or ```Build container name. Current build step 4 of 11```. So in that case I can invoke this command in 5 minutes and can see that downloading image completed on 50% and I can understand that docker is working and I need to patiently wait."},{"labels":[null,null,"enhancement1"],"text":"Following up on @sirlatrom's comment \r\nhttps://github.com/docker/docker/pull/31144#issuecomment-291461132,\r\nwe do have interactive `service create` and `service update` commands (https://github.com/docker/docker/pull/31144) but not `service scale`.\r\n\r\nWe should consider supporting scale as well.\r\n\r\nThis is somewhat related to https://github.com/docker/docker/issues/32367 since `scale` supports multiple services (e.g. `scale web=5 db=2`), therefore we should provide per-service overall status rather than per-task.\r\n\r\n/cc @aaronlehmann @thaJeztah "},{"labels":[null,"enhancement1"],"text":"Since we now do support synchronous and interactive service creation using `-d=false` (added in #31144), we should probably support the same in docker stack deploy.\r\n\r\nThe output would be slightly different:\r\nWhile for services we should the progress of every single task, for stacks we'd display the overall progress of each service.\r\n\r\n/cc @aaronlehmann @dnephin @vieux "},{"labels":[null,"enhancement1"],"text":"This is very low priority, but it would be pretty cool to be able to run:\r\n\r\n```\r\n$ docker stack deploy mystack -c https://raw.github.com/.../docker-compose.yml\r\n```\r\n\r\n/cc @vieux @dnephin @thaJeztah "},{"labels":[null,null,"enhancement1"],"text":"It would be awesome to have a `docker stack diff` command that would print the difference between the local file and the server.\r\n\r\nIt's useful to do a dry run and see exactly what is going to change between.\r\n\r\nAll we need to add is serialization **back** to the compose format and then there are plenty of diffing libraries available natively (we don't need to spawn `diff` on the system).\r\n\r\n/cc @aaronlehmann @vieux @dnephin @thaJeztah "},{"labels":[null,"enhancement1"],"text":"I have been searching online for anyone to have mentioned this ever so slightly, but I can't seem to find the `-i` option when running `docker service create`.  I am not sure if this is a deprecated feature, but you can do the following `docker run -it`, and when running `docker inspect`, you can see that the container `Tty` and `OpenStdin` are set to `true`.  `docker service create` [documentation](https://docs.docker.com/engine/reference/commandline/service_create/) only has `-t`, but  missing its `-i`. Nothing saying this feature is being deprecated, or maybe it's simply forgotten.\r\n\r\nIn other words, how should one enable `OpenStdin` when running from `docker service create`?"},{"labels":[null,"enhancement1"],"text":"Is there any plan to add some “trigger function” to docker events? I think it should be very useful.\r\nfor example:\r\nIf healthcheck report the status of one container is \"unhealthy\", then this event trigger docker to restart the container automatically.\r\nSorry for my poor english, but i hope you can understand what i said above :)\r\n"},{"labels":["enhancement1"],"text":"Sometimes you will wish to transfer a volume from one PC to another for any number of reasons.\r\n\r\nCurrently this must be done manually, and I believe it would be a worthwhile feature to include a saving ability for data volumes. Not only would this allow for easy transfer of volumes, it would also become a simple way to backup persistent parts of a data volume.\r\n\r\nI would expect that the containers using the volume would need to be paused during this action, which is where I'm a little concerned, but otherwise I don't see an issue.\r\n\r\nSimple:\r\n\r\n`docker volume save > volumeName.tar`\r\n\r\nor\r\n\r\n`docker volume --output  volumeName.tar save`\r\n\r\nSimilarly:\r\n\r\n`docker volume load < volumeName.tar`\r\n\r\n(potentially with renaming ability)."},{"labels":[null,"enhancement1"],"text":"I did a browse of the issues and couldn't find anything particular to this. Apologies if it has been answered before though.\r\n\r\nI may be misunderstanding, but I believe Swarm mode currently uses a KV-store to share important state information amongst workers/masters and generally keep the cluster in a good state. I'm wondering if it's feasible to expose a \"general\" KV-store to all swarm nodes that could be used for storing configuration information and queried by applications running in containers on the cluster, sort of like Amazons EC2 metadata service, or a standalone implementation of etcd/consul.\r\n\r\nUnderstandably this is kind of out-of-scope for docker, as you could run your own KV-store system in the cluster... but I guess it'd just be nice to have it built in to docker as a standard (but optional) feature.\r\n\r\nFeasible? Out of scope?\r\n\r\nCheers."},{"labels":[null,"enhancement1",null],"text":"**Description**\r\n\r\nI want to edit my docker container source code in host IDE so I want to expose a folder in a docker container (via a share) in a named host folder. I dont want to mount a host folder into a container, but the other way around. I cant find any online documentation. If its not possible this seems like a serious limitation of docker and impairment to development on the host.  If its not possible, how may i best use my host machine to make live changes to source code in a container? Or am i limited to editing code in a container folder from the containers command line?\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n**Output of `docker version`:**\r\n```\r\nClient:\r\n Version:      17.03.0-ce\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   3a232c8\r\n Built:        Tue Feb 28 07:52:04 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.0-ce\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   3a232c8\r\n Built:        Tue Feb 28 07:52:04 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 2\r\n Running: 2\r\n Paused: 0\r\n Stopped: 0\r\nImages: 14\r\nServer Version: 17.03.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /mnt/sda1/var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 172\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 977c511eda0925a723debdc94d09459af49d082a\r\nrunc version: a01dafd48bc1c7cc12bdb01206f9fea7dd6feb70\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.52-boot2docker\r\nOperating System: Boot2Docker 17.03.0-ce (TCL 7.2); HEAD : f11a204 - Thu Mar  2 00:14:47 UTC 2017\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 995.8 MiB\r\nName: default\r\nID: CCJC:TFPL:MEEY:QXM3:JQLP:SGOM:H5YR:MLY2:P5DK:P3MF:F2SF:5ZGM\r\nDocker Root Dir: /mnt/sda1/var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 34\r\n Goroutines: 70\r\n System Time: 2017-03-31T10:47:19.838409976Z\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\n provider=virtualbox\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nVirtualBox 5+"},{"labels":[null,"enhancement1"],"text":"### Description\r\nIt will be great if docker stack deploy command can do targeted service deployments using compose file."},{"labels":["enhancement1"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\nOnce I create a bunch of volumes the docker volume rm command requires specifying the volume name(s) to remove these. Can a pattern be allowed so all volumes that match the pattern are removed by the volume provider (plugin).\r\n\r\nThis is a feature request to allow patterns to match volume names when removing a set of volumes. Presently dockz \r\n\r\n**Describe the results you received:**\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.03.0-ce\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 10:57:47 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.0-ce\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 10:57:47 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 7\r\nServer Version: 17.03.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 147\r\n Dirperm1 Supported: false\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local vsphere\r\n Network: bridge host macvlan null overlay\r\nSwarm: pending\r\n NodeID: tszk3hjhbc79g5cmspfv6h3pz\r\n Is Manager: false\r\n Node Address: 10.110.48.54\r\n Manager Addresses:\r\n  10.110.48.49:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 977c511eda0925a723debdc94d09459af49d082a\r\nrunc version: a01dafd48bc1c7cc12bdb01206f9fea7dd6feb70\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 3.13.0-85-generic\r\nOperating System: Ubuntu 14.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1.955 GiB\r\nName: hte-1s-eng-dhcp98\r\nID: H3SC:SAIV:HFHW:WYI5:WC4O:GEVQ:S6AJ:VIJ2:4CVT:UXDQ:T4GW:7MCA\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: govin1\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement1"],"text":"Add the possibility to have a \r\n`docker service create/update --min-replicas 1 --max-replicas 5` \r\nupon creating or updating a service, based on cpu limit or service response latency etc... \r\n"},{"labels":["enhancement1",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nIf I create an exec instance in detached mode, there is no API to retrieve its output.\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. POST /containers/(id or name)/exec\r\n2. POST /exec/(id)/start\r\n{\"Detach\": true}\r\n\r\n**Describe the results you received:**\r\nI can use the API \"GET /exec/(id)/json\" to inspect the status of exec instance, but I couldn't retrieve its output.\r\n\r\n**Describe the results you expected:**\r\nThere could be an API like \"GET /exec/(id)/logs\" to retrieve the output of the exec instance.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nNone\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.12.6\r\n API version:  1.24\r\n Go version:   go1.6.4\r\n Git commit:   78d1802\r\n Built:        Tue Jan 10 20:26:30 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.12.6\r\n API version:  1.24\r\n Go version:   go1.6.4\r\n Git commit:   78d1802\r\n Built:        Tue Jan 10 20:26:30 2017\r\n OS/Arch:      linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 60\r\n Running: 13\r\n Paused: 0\r\n Stopped: 47\r\nImages: 97\r\nServer Version: 1.12.6\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 268\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge overlay host null\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: apparmor\r\nKernel Version: 4.2.0-27-generic\r\nOperating System: Ubuntu 14.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 48\r\nTotal Memory: 251.5 GiB\r\nName: ubuntu\r\nID: BFVC:B5JB:GZZ6:ZS66:6GWY:NXYE:IY2V:KKUH:F6KO:ZLQP:R3JR:6SCA\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nInsecure Registries:\r\n 127.0.0.0/8\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nNone"},{"labels":[null,"enhancement1"],"text":"\r\n**Description**\r\n\r\nIt would be nice to be able to say on the docker run command line or from docker-compose what protocols/ports are allowed or denied outbound access from the container, rather than resorting to Host OS specific firewall solutions like IPTABLES\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"Is is possible to add a feature that lets you create layers for the build process that get removed immediately after the build finishes.  There should be no reason to keep all your build dependencies in a published container.  Maybe something like the following  . . .\r\n\r\n```\r\nRUN_TMP apt-get install build-essential   # layer removed after build is finished\r\nRUN apt-get install nginx                 # layer kept after build is finished\r\nCOPY_TMP /tmp/file /tmp/file              # layer removed after build is finished\r\n```"},{"labels":[null,"enhancement1"],"text":"Hello folks!\r\n\r\nThis is a feature request. \r\n\r\nI have a solid Docker Swarm Mode cluster a this point and now I am looking to update docker images on the fly. (_I still do docker service remove / create ..._) I read the doc https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/ and the logic makes plenty of sense when you update a service that have 2 or more replicas. \r\n\r\n**What about services with 1 replicas ?**\r\n\r\nAt the moment, the scheduler applies rolling updates as follows by default:\r\n* Stop the first task.\r\n* Schedule update for the stopped task.\r\n* Start the container for the updated task.\r\n* ... ...\r\n\r\nOf course this is bad when you have one instance.\r\n\r\nIMHO, Swarm shall be smart enough to know there is only one instance. In this case, let's use this process:\r\n* scale the service with 2 replicas\r\n* Stop the first task.\r\n* Schedule update for the stopped task.\r\n* Start the container for the updated task.\r\n* ... ...\r\n\r\nMake sense ?\r\n"},{"labels":[null,null,"enhancement1"],"text":"Debugging the daemon can be quite gruesome work.\r\nEven one of the simplest things like collecting a goroutine stack dump (most common request) is difficult to explain and even more difficult to collect.\r\n\r\nWe already have some API endpoints to aid in debugging but it's only available if debug mode is actually enabled.... forcing a daemon reload (also difficult to explain and perform) to enable it can even mess up the results.\r\n\r\nI propose:\r\n\r\n1. Enable debug endpoints (namely `/debug/pprof/*`) by default.\r\n2. Add a new CLI subcommand `docker debug` which includes a suite of subcommands for pulling the desired information\r\ni. `docker debug stack` -> generates a goroutine stack dump\r\nii. `docker debug profile <blah>` -> generates pprof formatted reports (whatever happens to be supported by the language runtime)\r\nii. `docker debug profile ls` -> List available profiles (this requires a call to the daemon, otherwise would include in `--help` output)\r\n\r\nThese commands either send output directly to stdout or optionally to an output file.\r\n\r\nPotentially also interesting is grabbing a trace or a cpu profile (explicitly not supported above), perhaps by adding a `--trace` or `--cpu-profile` to a `docker` command.... but this may be left as another exercise.\r\n\r\n**note**: not tied to the layout/naming of those commands\r\n\r\nAnything under `/debug` or `docker debug` should be considered non-formal API that is subject to change outside the scope of API versioning.\r\n\r\nIn the future these endpoints can be used to generate something like a support tarball via `docker debug support`."},{"labels":[null,"enhancement1"],"text":"Background\r\n=========\r\nSome distributed applications(MS,bigdata and so on)have CPU intensive workload, which need bind each container/replica to a specific NUMA node to gain a better performance.\r\n\r\nCurrently\r\n=========\r\n1.can't find cpuset and cpuset-mem options in swarm service create/update command\r\n2.can't assign different options for different replica within the same service\r\n\r\nAs a PAAS tool, I think Docker should enable such infrastructure performance features."},{"labels":[null,"enhancement1",null],"text":"**Description**\r\nI thought I could add `-H tcp://0.0.0.0:2375 --tlsverify=false` (swapping the order made no difference) to the end of the command line in /etc/systemd/system/docker.service.d/10-machine.conf, but both ports 2376 and 2375 still end up requiring TLS. \r\n\r\nIt would be good if dockerd could listen on two ports, one TLS and the other non-TLS.\r\n\r\nI'm using this as a workaround but would like to do this without another container. Also, the socat workaround _might_ be triggering this: https://github.com/docker/docker/issues/31438.\r\n```\r\ndocker run -d --restart always -p 2375:2375 --volume=/var/run/docker.sock:/var/run/docker.sock --name=docker-http -m 50m sequenceiq/socat\r\n```\r\n\r\nI want this because I'm running jenkins in a container and want jenkins jobs to be able to run docker commands. I can't map /var/run/docker.sock because the container runs as user jenkins, not root. And I'd rather not write the docker server's ca.pem, key.pem, cert.pem into the container.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.03.0-ce\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 11:02:43 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.0-ce\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 11:02:43 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 94\r\n Running: 12\r\n Paused: 0\r\n Stopped: 82\r\nImages: 2596\r\nServer Version: 17.03.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 977c511eda0925a723debdc94d09459af49d082a\r\nrunc version: a01dafd48bc1c7cc12bdb01206f9fea7dd6feb70\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-67-generic\r\nOperating System: Ubuntu 16.04.2 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 31.31 GiB\r\nName: 61\r\nID: PZJW:CVPM:HPPS:7LRV:UZXE:RQPI:4ALK:7HR3:OSX2:OTAN:T4SJ:A7WM\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nLabels:\r\n provider=generic\r\nExperimental: false\r\nInsecure Registries:\r\n 192.168.2.61:5000\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```"},{"labels":[null,"enhancement1"],"text":"I suggest adding support for forwarding the API socket over SSH: `docker -H ssh://me@host/var/run/docker.sock`. (or in short form: `docker -H ssh://host`)\r\n\r\ni.e. it is a user-friendly equivalent of the following script:\r\n```sh\r\n#!/bin/sh\r\ntmp=$(mktemp -d)\r\nchmod 700 $tmp\r\nssh -N -L $tmp/docker.sock:/var/run/docker.sock -l $USER $HOST &\r\nsshpid=$!\r\ndocker -H unix://$tmp/docker.sock $@\r\nstatus=$?\r\nkill $sshpid\r\nrm -rf $tmp\r\nexit $status\r\n```\r\n\r\nMotivations:\r\n\r\n* No mess of setting up TLS properly (`ssh-keygen` vs a bunch of [`openssl blah -blah -blah -blah ...`](https://docs.docker.com/engine/security/https/#create-a-ca-server-and-client-keys-with-openssl), #6817)\r\n* Single user account management for both regular SSH session and Docker session\r\n* Single login monitoring for both\r\n\r\ncc @diogomonica\r\n"},{"labels":[null,"enhancement1"],"text":"Docker Services are great for stateless applications (e.g., web servers, workers) but most real application stacks revolve around one or more distributed stateful services (e.g., databases, message queues, etc). Some more modern examples include key-value stores like etcd, Consul, and Zookeeper, document-oriented databases like RethinkDB and MongoDB, and even some newer/experimental SQL databases like CockroachDB. Some of these are a building block for other distributed stateful services - as is the case with Apache Zookeeper and Kafka. These were designed *from the beginning* as distributed systems which manage replication of data and failover *automatically*.\r\n\r\nThis sets them apart from other widely used yet monolithic stateful services such as PostgreSQL and MySQL which typically provide a mechanism for replication of data (usually primary->secondary configurations) but failover still requires manual operator intervention or some other system to be in place which detects failure of the primary and handles promotion of a secondary. So, for the purposes of this feature request, *do not consider these types of stateful services*. These systems simply do not fit well into the model of Docker Services.\r\n\r\nThe kinds of stateful services which **do** fit into this feature request typically have the following properties:\r\n\r\n- Each peer in the system has a name or address that does not change, even if it restarts. This name or address is advertised to other peers.\r\n- Any peer in the system is capable of handling a client's request just as well as any other task in the system *or* is capable of routing it to the appropriate peer.\r\n- Each peer, once assigned a location, stays in that location (physical or virtual machine, i.e. \"node\" in Docker).\r\n- These systems are capable of distributing and replicating their own data across peers (i.e., you don't need any fancy volume plugins; The local volume driver will do) and gracefully handle failover in the case that one or more peers becomes unavailable.\r\n- As a consequence of the previous 2 points, no two peer tasks should ever be scheduled to the same Docker node in order to not compromise the availability of that system.\r\n- An operator must manually intervene to scale the system up or down (add or remove peers). While in some systems (like etcd) simply adding a new peer automatically replicates data to that peer, other systems also require a configuration change to replicate any data to the new peer. This could be made easier with Docker if Swarmkit provided custom hooks which can be performed when adding or removing a peer task.\r\n- When a new peer is added, it is provided with a method for discovering and connecting to all other peers. Once a peer task is running, it is not necessary for this information to be updated when another peer joins as that new peer will be responsible for connecting to the existing peers.\r\n\r\nIf Docker could provide a set of features which makes the above items possible then these distributed/replicated/auto-failover systems could easily be made into Docker services.\r\n\r\n*NOTE*: I am not requesting a special distributed/replicated Docker volume feature since these systems can distribute/replicate their own data. I explicitly want to use the local storage on a specific node. This feature request is more about having consistent peer discovery and node pinning for these kinds of services.\r\n\r\n@docker/core-swarmkit-maintainers \r\n\r\nSo far, I have been able to kind-of make this work with RethinkDB as an example stateful service but it's far from perfect. To get more control of placement of the peers, I have to run a service in \"global\" mode to limit to at most 1 task per node with a placement constraint like `node.labels.db.replica == true`. So in order to scale the service up or down I cannot use `docker service scale rethinkdb=N` but instead must manage node labels with `docker node update ...`. I actually like this method as it gives a lot of control around placement and I know that the swarm scheduler will never put a database task anywhere else - even if the node it's on becomes unavailable. The real difficulty is in connecting the peers together. My service spec includes an overlay network so each of the peers can communicate over that but it is difficult to know how to connect them. I can't use the `service_name.network_name` because that resolves to a virtual IP and from within the service usually only resolves to your own task endpoint. It turns out that resolving `tasks.service_name.network_name` returns a DNS record for each of the peer tasks but the DNS results are eventually consistent and so I cannot rely on this to make sure that the peers are fully connected. This also requires that I build into the image an entrypoint script which does this DNS lookup and prepairs `--join` arguments before execing into the server process. Another problem that I have is that these peers need to advertise their address to each other and if a task restarts it's given a different name. That's not a big deal with RethinkDB but it may be with other systems."},{"labels":[null,"enhancement1"],"text":"When do database migration, before migration, need backup database like following:\r\n\r\n```sh\r\ndocker run --rm -it -v source_volume:/from -v dest_volume:/to debian:jessie bash -c \"cd /to ; cp -a /from/* .\"\r\n```\r\n\r\nI think docker volume maybe could support this feature, make this process more comfortable."},{"labels":["enhancement1",null],"text":"\"docker update\" does not provide the way to update hostname of existing container.\r\nYes, I can commit the container to a image, and run new container from the image with new hostname.\r\nBut, it would be nicer if I can directly update hostname using \"docker update\".\r\n\r\n"},{"labels":[null,"enhancement1",null],"text":"'docker node/services' doesn't work for Docker swarm workers which belongs to a current design.\r\nHowever, every swarm worker is able to look up the address of swarm managers. So I think swarm worker should proxy this request to a random swarm manager (just like what etcd does), thus, making 'docker node/services' work for swarm worker nodes.\r\n"},{"labels":[null,"enhancement1"],"text":"**Proposal**\r\nAdd `--isolation` as a parameter to `docker service create`\r\n\r\n**Background**\r\nWindows supports running containers with two different levels of isolation - process (shared kernel), and hyperv (separate kernel per container). Customers are deploying a mix of these based on their security needs. For example, trusted internal apps may be run with process level isolation to favor higher density, while others may require a separate kernel. Therefore this setting should be service-specific and not host-wide.\r\n\r\nI mentioned this at https://github.com/docker/docker/issues/25209#issuecomment-270221448 but want to track this as a separate specific ask."},{"labels":[null,null,null,"enhancement1"],"text":"Currently, dockerd itself supports a \"single\" and \"static\" way to protect /run/docker.sock using a tlskey file. But it is not suitable for swarm.\r\n\r\nJust like swarm token has its expired time, tlskey should also have, especially when the key has been exposed to public.\r\n\r\n\"single\" means the key takes effect for only 1 node, thus, swarm cluster requires people to update key on each of active swarm nodes once the key is changed.\r\n\r\n\"static\" indicates we have to restart all daemons once the key is changed to apply for changes.\r\n\r\nTaking advantage of built-in etcd storage, Swarm node should record a key-value pair in it which enables a \"non-single\" way to solve the problem. For example, assume there are 5 swarm nodes, post a request to 1 node about to change the password, swarm stores the new password, and then all other swarm nodes will get the updated automatically. Besides, all swarm nodes needn't restart to apply for changes if we provide standard WWW-Auth like Basic-Auth.\r\n\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"I think it would be great to make possible passing an env file to the docker service update command, just like with docker service create. \r\n\r\nWhen upgrading a service probably there are also environment variables that need to change too.\r\n\r\nThanks! \r\n"},{"labels":[null,"enhancement1"],"text":"I am aware that #18812 and #18799 have been rejected, but I don't think that, with respect, those '-1' them are aware of all the use cases to which Docker Images can be / are being put.\r\n\r\nIt is necessary for us to create an Image upon which it is prohibited to add another layer. All that is permitted is to spin up a Container from it.\r\n\r\nSince Docker is more and more advertised as 'the unit of distribution of code which supersedes DLLs', I don't want people changing my DLLs. Nor do I want to permit people from overriding classes in those DLLs and changing behaviour in inheritance.\r\n\r\nIn Dockerland: I don't want people to take my Images and add/change behaviour by adding a layer on top."},{"labels":[null,"enhancement1",null,null],"text":"Hello there,\r\n\r\nI am running my test \"microsoft/iis\" container with following command with \"--cpus 16\" limitation:\r\n```\r\ndocker run -d  --name gt_test_iis --cpus 16 microsoft/iis ping -t localhost\r\n```\r\n\r\nLooks like \"--cpus\" works fine and when I run my `simulate_cpu_load.ps1` script:\r\n```\r\nForEach ($Number in 1..16){\r\n   start-job -ScriptBlock{\r\n      $result = 1; foreach ($number in 1..2147483647) {$result = $result * $number}\r\n   }\r\n}\r\n```\r\n\r\nI got only about 16% CPU on my 64 cpus virtual machine. \r\n\r\nWhen I wanted to change CPU limits online I tried `docker update` and got expected error (as --cpus flag is not described there https://docs.docker.com/engine/reference/commandline/update/#options):\r\n```\r\nPS> docker update --cpus 1 gt_test_iis\r\nunknown flag: --cpus\r\nSee 'docker update --help'.\r\n```\r\n\r\nBut on the API documentation page for \"docker update\" ( https://docs.docker.com/engine/api/v1.26/#operation/ContainerUpdate ) we have following option `CpuCount`, so from my understanding \"docker update\" should have \"--cpus\" parameter.\r\n\r\nAnother thing I found is that changing the value of `CpuCount` by Web API does not affect running container.\r\n\r\nWhen I tried to invoke it in Powershell console:\r\n\r\n```\r\nInvoke-WebRequest -Uri http://hostname:2375/containers/gt_test_iis/update -Method POST -Body '{\"CpuCount\":1}' -ContentType 'application/json'\r\n```\r\nit returned 200\r\n```\r\nStatusCode        : 200\r\nStatusDescription : OK\r\nContent           : {\"Warnings\":[]}\r\n\r\nRawContent        : HTTP/1.1 200 OK\r\n                    Api-Version: 1.26\r\n                    Docker-Experimental: false\r\n                    Content-Length: 16\r\n                    Content-Type: application/json\r\n                    Date: Mon, 06 Mar 2017 12:32:40 GMT\r\n                    Server: Docker/1.13.1-cs1 (windows)\r\n\r\n                    {\"Warnin...\r\nForms             : {}\r\nHeaders           : {[Api-Version, 1.26], [Docker-Experimental, false], [Content-Length, 16], [Content-Type, application/json]...}\r\nImages            : {}\r\nInputFields       : {}\r\nLinks             : {}\r\nParsedHtml        : mshtml.HTMLDocumentClass\r\nRawContentLength  : 16\r\n```\r\n\r\nbut the container continued to consume 16% of CPU load.\r\n\r\nI also found that \"docker inspect\" always shows `\"CpuCount\": 0,` even it was run with \"--cpus 16\" or without \"--cpus\" at all, and it was not changed when I run `Invoke-WebRequest` above.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.1-cs1\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   8709b81\r\n Built:        Thu Feb  9 02:05:36 2017\r\n OS/Arch:      windows/amd64\r\n\r\nServer:\r\n Version:      1.13.1-cs1\r\n API version:  1.26 (minimum version 1.24)\r\n Go version:   go1.7.5\r\n Git commit:   8709b81\r\n Built:        Thu Feb  9 02:05:36 2017\r\n OS/Arch:      windows/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 248\r\n Running: 111\r\n Paused: 0\r\n Stopped: 137\r\nImages: 1201\r\nServer Version: 1.13.1-cs1\r\nStorage Driver: windowsfilter\r\n Windows:\r\nLogging Driver: json-file\r\nPlugins:\r\n Volume: local\r\n Network: l2bridge l2tunnel nat null overlay transparent\r\nSwarm: inactive\r\nDefault Isolation: process\r\nKernel Version: 10.0 14393 (14393.693.amd64fre.rs1_release.161220-1747)\r\nOperating System: Windows Server 2016 Datacenter\r\nOSType: windows\r\nArchitecture: x86_64\r\nCPUs: 64\r\nTotal Memory: 1.906 TiB\r\nName: skipped\r\nID: 62EZ:P3MP:SQ3U:3UET:3JAP:QLAU:X2O4:E5NA:7WAH:BUGN:CAQT:VZ5V\r\nDocker Root Dir: E:\\docker_storage_1_13\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: -1\r\n Goroutines: 721\r\n System Time: 2017-03-06T13:49:13.9906709Z\r\n EventsListeners: 1\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nIt is x1.x32large AWS instance. But the issue was reproduced on all our Docker 1.13.1-cs1 installations."},{"labels":[null,"enhancement1"],"text":"**Description**\r\n\r\nWhen I run containers by hand I was able to run afterwards `docker exec` to run some special setup scripts. Currently this doesn't seem to be possible as if container is run at different node, I would need to ssh to that node and run command.\r\n\r\n**Describe the results you expected:**\r\n\r\nThere should be an option to run `docker exec` on a service, and upon doing it the command would get propagated to all containers distributed on different nodes.\r\n\r\n~~Another option that could solve the problem would be to allow prepend `Command` in docker-compose.yml. Currently you can only overwrite cmd which makes it really complicated.~~ (edit: after re-thinking such change in compose wouldn't solve much as you still wouldn't be able to run commands with local context. However this still could be useful in some cases where only guest env is needed)\r\n"},{"labels":[null,null,null,"enhancement1"],"text":"**Description**\r\n\r\nThis is a request for a `docker stack logs` which can show the logs for a `docker stack` much like `docker service logs` work in 1.13. docker-compose works similarly today, showing the interleaved logs for all containers deployed from a compose file. This will be useful for troubleshooting any kind of errors that span across heterogeneous services.\r\n\r\ncc @dperny "},{"labels":[null,"enhancement1"],"text":"This is a feature request. \r\n\r\n#### Current status\r\n\r\nIt is only possible to specify the docker volumes/host files/host directories to mount into a container at creation time, with `docker run` or `docker create`.\r\n\r\n#### Suggestion\r\n\r\nThe new `docker mount` command accepts three arguments:\r\n\r\n* the container name\r\n* a source path (a docker volume name or an absolute path of the host's filesystem\r\n* a destination path (where to mount the source)\r\n\r\n#### Usecase example\r\n\r\nYou have your development container, let's say a PostgreSQL container, started with `docker run --rm postgres`. You want to execute a script into it. Unfortunately, the script is on the host's filesystem.\r\n\r\nYour options:\r\n\r\n* stop the container and restart it with `docker run --rm -v /path/to/script:/script postgres`\r\n* copy the script: `docker cp /path/to/script container_name:/script` and redo the copy everytime the script changes on the host\r\n* or, thanks to `docker mount`, run `docker mount /path/to/script container_name:/script`\r\n\r\n#### Thoughts\r\n\r\nAfter a short discussion on IRC, `docker mount` could be seen as something breaking a container's immutability/\"ephemerality\", but:\r\n\r\n- `docker cp` achieves the same result, it's just not as efficient\r\n- Specifying a volume at container creation achieves the same result, but it's not extensible (you can't add/remove volumes dynamically) and requires to restart the container anyway.\r\n\r\n"},{"labels":[null,"enhancement1",null],"text":"This issue tracks work on adding **automatic multi-arch support** to Swarm mode. In other words, for services that run images with fat manifests, tasks will be scheduled on only those nodes which can actually run the image.\r\n\r\n### What does this give us?\r\nThe ability to automatically schedule service tasks on nodes that are capable of running them, without having to do do it manually.\r\n\r\n### How to do it?\r\nThe proposed way of achieving this works in two steps:\r\n1. The daemon looks at the manifest of the service image. If it is a fat manifest, the platform information is pulled out and passed to the Swarm manager.\r\n2. The manager uses a platform filter to filter out nodes that cannot support a given image.\r\n\r\nA proposal SwarmKit PR to implement the second step is here: https://github.com/docker/swarmkit/pull/1981\r\n\r\nHere's a little more detail about the specifics:\r\n- `service create` -- the daemon pulls manifest at the time it pulls image digest. If this is a fat manifest, then platform information is extracted and passed on to the Swarm manager. If there is no platform information to pass, the Swarm manager does not enable the platform filter\r\n  - A pertinent question is whether we will allow the user to pass platform information in the future.\r\n-  `service update` -- this is a little more involved, because we don't want to unnecessarily poll the registry to get the manifest.\r\n  - In cases where the `--image` is not supplied with `service update`, we don't need to do anything, because we just assume that the image and all its platform-related requirements stay the same.\r\n  - If the user does update the image, and it is required to reach the registry to obtain the digest, then we also get platform information, since we're reaching the registry anyway.\r\n  - The interesting case is when the digest is provided (say by the user). Right now, we will not contact the registry in such a situation. For platform information, we do know at the manager level whether the digest of the new service spec is different from the existing one. If it isn't different, we don't need to do anything. If there is a change, then we want to contact the registry to get platform information. The only difference is that before, the user could update the service by providing a new digest, and skip the registry lookup, whereas now it would entail the registry lookup.\r\n\r\n### What will it cost?\r\nThe only cost is an addition registry round-trip during `service create` and sometimes during `service update`. While in my tests, this cost has been minimal, it is something to be aware of."},{"labels":[null,"enhancement1"],"text":"I want to use a secure registry on localhost, but currently there does not appear to be any way to achieve this (due to https://github.com/docker/docker/pull/9100/commits/6aba75db4e7b0151aeb48f450bb43e659ce0ec82).\r\n\r\nI have encountered at least two situations in which this is required.\r\n\r\nFirst, I want to be able to test my (secure) registry locally.  If it weren't for this issue, I could achieve this using a modified /etc/hosts file (i.e. add 127.0.1.1 mysecureregistry.domain.com) or using a TLS certificate issued for multiple domains and IP addresses (i.e. mysecureregistry.domain.com and 127.0.1.1).\r\n\r\nSecond, I want to be able to use an SSH tunnel to allow a machine on a different network to use my secure registry (i.e. I forward localhost:5000 on my docker host to mysecureregistry.domain.com:5000 on my machine on the same network as my registry).  Again, I could achieve this if it weren't for this issue.\r\n\r\nCould we only add 127.0.0.0/8 to the list of insecure registries if no other insecure registries are configured?  This would allow me to achieve what I need by adding a dummy insecure registry (e.g. --insecure-registries dummy).  Another alternative would be to add an explicit option to configure this behaviour (e.g. --secure-localhost-registries or something like that), but I guess this would involve more work."},{"labels":[null,"enhancement1"],"text":"Currently we have to SSH into each node and run `docker system prune` to clean up old images / data. This seems fairly impractical for large swarms. It would be great to have command that can be run from the leader that instructs all the other nodes to clean up old data."},{"labels":["enhancement1"],"text":"Sometimes I find if I have 10 containers running I find I want to run a command on all containers while they are running. I can achieve this via ansible kinda but it is not ideal. It would be useful if docker supported this.\r\n\r\nFor example say I have 10 containers running:\r\n\r\ndocker ps\r\nCONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMES\r\nbc2c540d224b        el632-build         \"linux32 init\"      About a minute ago   Up 55 seconds                           crash7\r\n1be1d003b13c        el632-build         \"linux32 init\"      8 minutes ago        Up 8 minutes                            crash6\r\n480dbd255045        el632-build         \"linux32 init\"      13 minutes ago       Up 13 minutes                           crash5\r\n170431727bf4        el632-build         \"linux32 init\"      18 minutes ago       Up 18 minutes                           crash4\r\nde9cbc1c1f06        el632-build         \"linux32 init\"      22 minutes ago       Up 22 minutes                           crash3\r\n68dfa15d4f4e        el632-build         \"linux32 init\"      27 minutes ago       Up 27 minutes                           crash2\r\nd0cf29ae6228        el632-build         \"linux32 init\"      31 minutes ago       Up 31 minutes                           crash1\r\n7ba320f34897        el632-build         \"linux32 init\"      36 minutes ago       Up 36 minutes                           crash0\r\n\r\nThat I could run:\r\n\r\ndocker exec -it crash* some command\r\n\r\nAnd it would run on all ten containers. Or say if I had 6 containers named:\r\n\r\ncrashpro0\r\ncrashpro1\r\ncrashpro2\r\ncrashitg0\r\ncrashitg1\r\ncrashitg2\r\n\r\nand I could run:\r\n\r\ndocker exec -it crash*0 some command\r\n\r\nto run on crashpro0 and crashitg0. Just an idea, I know I would find this quite useful. Maybe this is just me! And if by default it executed this command on a max of 5 containers in parallel by default it would be ideal for my common use case."},{"labels":[null,"enhancement1"],"text":"I miss a command which I can use to rename a volume.\r\n\r\nOnce I have created a lot of modules and forget to name the volumes at the start, it gets really difficult to identify which volume contains what.\r\n\r\nA command like\r\n`docker volume mv 27722c3065e2f074bc7f29963fe62f73669cb369617482596e222f9991daee57 my-nice-volume-name`\r\nwould be nice.\r\nIf that is not possible (because the id is used in a definition), it would be nice to have the ability to add a short comment:\r\n`docker volume set-comment 27722c3065e2f074bc7f29963fe62f73669cb369617482596e222f9991daee57 \"This is the volume /var/logs/ for my creat app\"`\r\nThis comment should be visible in the list `docker volume ls`."},{"labels":[null,"enhancement1"],"text":"As can be seen in https://github.com/docker/compose/issues/4315 , the `extends` feature that exists in `docker-compose` seems to be popular among users despite its flaws. However, it has so far not been added in the Engine's implementation of the Compose format. So far, we have advised users to simply flatten their Compose file structure when using v3, but is this the long-term solution we want to go with? How can we provide a clear upgrade path for users who have come to rely on this feature?\r\n\r\ncc @dnephin @vdemeester "},{"labels":[null,"enhancement1"],"text":"#resurrects #7149\r\n\r\nWe've been going back-and-forth among some maintainers to provide a way to provide capabilities for users to produce sleek images without the cruft of the intermediate build artifacts.\r\n\r\nWe see a lot of requests from the community for this feature and different ways how people try to work around it, most commonly with `docker cp` and re-tarring a new context or trying to combine the whole build into a single `RUN` instruction.\r\n\r\nAmong the things we discussed were rebasing to a different rootfs path, mounting or copying data from other images, using cache storage between images, squashing, subblocks inside dockerfile, invoking builder inside of dockerfile etc.\r\n\r\nEventually, we ended up on the #7149 proposal that allows switching context of a build to a directory from an existing image. The benefits of this proposal are that it least conflicts with the current design principles of Dockerfile like self-consistency, build cache, returning single target etc. while elegantly solving the small images problem\r\n\r\nWhile this proposal can be considered as a \"chained-build\" and has some limitations for describing complicated build graphs with multiple branches we have concluded that it would be best to solve that problem in a more higher level and we continue to investigate possible improvements.\r\n\r\n#### The proposal:\r\n\r\nedit: this has been updated to new syntax\r\nedit2: `s/--context/--from/`\r\n\r\n`--from=n` flag allows to access files from rootfs of previous build block. Every build block starts with a `FROM` instruction(multiple `FROM` instructions already work in Docker today). `n` specifies an incrementing index for every block. In the future we want to extend it to human readable labels.\r\n\r\n```\r\nFROM ubuntu\r\n\r\nRUN apt-get install build-essentials\r\nADD . /src\r\nRUN cd /src && make\r\n\r\nFROM busybox\r\nCOPY --from=0 app /usr/local/bin/app\r\nEXPOSE 80\r\nENTRYPOINT /usr/local/bin/app\r\n```\r\n\r\nBenefits for this syntax are that when files from the user context are required both for building some artifact and also for the final image they don't need to be copied to the first environment. That also means that it doesn't invalidate cache for the first environment if the file is not used there. This syntax can also be used for including content from other images with just extra `FROM` command.\r\n\r\nold proposal:\r\n\r\n#### The proposal:\r\n\r\n`BUILD /path/to/context` instruction in the `Dockerfile` that switches the current build context to `/path/to/context` from the current image's rootfs.\r\n\r\n`docker build docker://image-reference[::/subdir]` that invokes a new build using the data from a specified image as a build context.\r\n\r\n#### Notes:\r\n\r\n- No previous metadata carries over to the new image after `BUILD`. The next instruction after this command needs to be `FROM`.\r\n- The other way to think about the `BUILD` instruction is as `SETCONTEXT`\r\n- The build from docker reference syntax is useful when the build is described by multiple Dockerfiles and dependencies are controlled by Makefile like utility.\r\n- Only the layers after the last `BUILD` instruction end up in the final image.\r\n- `docker build -t` would tag the last image defined at the end the Dockerfile \r\n- Some features like auto-tagging and specifying/loading a Dockerfile from new context directory have been left out and can be considered as future additions.\r\n\r\n#### Example:\r\n\r\n```\r\nFROM ubuntu\r\n\r\nRUN apt-get install build-essentials\r\nADD . /src\r\nRUN cd /src && make\r\n\r\nBUILD /src/build\r\nFROM busybox\r\nCOPY app /usr/local/bin/app\r\nEXPOSE 80\r\nENTRYPOINT /usr/local/bin/app\r\n```\r\n\r\n\r\n@icecrime @vikstrous @fermayo"},{"labels":[null,"enhancement1"],"text":"Let's say you want to debug an exited container without actually running its default `CMD`.\r\n\r\nCurrently, you can achieve this by `docker commit ...` and a subsequent `docker run --entrypoint ...`. While this is possible, it's not the easiest to understand, a lot to type and leaves new images around.\r\n\r\nWhy not simply allow `docker start --entrypoint ...` or even `docker run --stopped-container=...`?\r\n"},{"labels":[null,"enhancement1"],"text":"This is about PR #29911 \r\n\r\n\r\nSome containerized apps may need hugetlb resources in it ( some DB, for example).  Users can map the hugetlb page file info container to guarantee hugetlb access within the container. And thus there comes a new requirement that users should restrict the hugetlb limit container can consume, in case one container uses up all the hugetlb page provided by the host.\r\n\r\nSo here I think a new option `--hugetlb-limit` in docker create/run would help. You can restrict 2MB huge page size (please make sure you host do have 2MB huge page, otherwise docker would error out) limit to 1G with:\r\n\r\n` $ docker run -ti --hugetlb-limit 2m:1g busybox cat /sys/fs/cgroup/hugetlb/hugetlb.2MB.limit_in_bytes`\r\n\r\n**Please note that command line above do no have hugetlb file mapper, just check the hugetbl cgroup setting.**"},{"labels":[null,"enhancement1"],"text":"Volume API should provide default mount approach for those volumes which are not specified from execution command, such as: docker volume create -d xxx --default-opt type=tmpfs,o=ro.\r\nOtherwise, lots of local volumes will be created locally and containers can easily hack the local storage space with `dd` as well as reducing local storage performace. All containers may suffer from disk out of space before it is going to prune."},{"labels":[null,null,"enhancement1"],"text":"\r\n**Description**\r\n\r\nThis is a feature request for Swarm Mode. There is a plan to support topology-aware task scheduling in 1.14 (https://github.com/docker/swarmkit/pull/1512). This feature request is to take same model and ensure that task affinity is taken into consideration when resolving DNS queries for services. For example, if we deploy a stack made up of two services ( web and k/v store ) each with two replicas across two availability zones. It would be optimal for `web_1` task to talk to `kv_1` when both are deployed on same AZ and fall back to talk to `kv_2` if `kv_1` is down.\r\n\r\nFor routing mesh's `ingress` network, this feature can ensure that local tasks are prioritized when an external request comes in. Instead of current mode where requests for an exposed service can be forwarded to tasks running on other nodes even if there is a task running on the node that receives the request.\r\n\r\n"},{"labels":[null,null,"enhancement1"],"text":"**Feature Request: Plugins should be able to leverage secrets**\r\n@thaJeztah describes the best practices for secrets in issue #13490.  The current implementation of secrets requires swarm and the use of a service.  This pattern would be useful for Plugins, which are inherently insecure today.  The 'docker plugin inspect' command shows Args and Env values which are commonly used to pass credentials.  In addition, these credentials are not encrypted at rest.\r\n\r\nThis request is to allow the container providing a Plugin to have access to a named Docker secret.\r\n"},{"labels":[null,"enhancement1",null],"text":"Run docker containers with foreign binarys \r\n( @** wish list enhancement) \r\n\r\nProot , qemu static sometimes works sometimes  don't in image , image updates , \r\nIe gentoo arm64 tarball updates , and it breaks. \r\n\r\nIf the system is native,  then ignore emulator.  \r\nIf not have option to call for qemu plug-in etc. \r\n\r\n\r\nHowever mere mortals can't obtain  arm64  (for now ) servers , over a rassbery-pi 3's , but likewise slow. \r\nHowever  can call a wrapper for qemu or proot etc \r\nFrom docker , on your average intel i7 etc. \r\n\r\nHowever, one can pull in static qemu or proot... however,  prone to breakages...  \r\nNot as long term reliable. \r\n\r\nAnyhow,  having a wrapper or plug-in would be a plus.\r\n\r\n\r\n\r\n\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"Docker already hides experimental options and commands in the help messages. To get to this information, it queries the server for its experimental property.\r\n\r\nThis feature could be extended to also hide operation system specific commands and options. At present, this would be Windows specific options, e.g. `docker run --cpu-count --cpu-percent -credentialspec --io-maxbandwidth --io-maxiops --isolation`. But this could as well be Mac or Solaris specific ones in the future.\r\n\r\nBash completion already has this feature, see #30438. It issues `docker version --format '{{.Server.Os}}'` to get the required information."},{"labels":[null,null,"enhancement1"],"text":"The new `prune` commands are potentially dangerous and I find it hard to predict what actually would be deleted. The user might use `docker system df` and `docker image ls --filter dangling=true` to get to this information, but this seems like an unneccessary step.\r\n\r\nI propose to add a new operating mode where the normal output is produced without actually deleting the objects.\r\n\r\n```bash\r\n$ docker system prune --dry-run\r\nWarning: Running in dry run mode, not actually deleting anything.\r\nDeleted Volumes:\r\ndockerdev-go-pkg-cache-gopath\r\ndockerdev-go-pkg-cache-goroot-linux_amd64_netgo\r\n\r\nTotal reclaimed space: 112.1 MB\r\n```\r\n\r\nThe user could then decide to run the same command again without `--dry-run`."},{"labels":[null,"enhancement1",null],"text":"**Description**\r\n\r\n`docker inspect --type task abc123 def456` fails fast if `abc123` is not a valid task.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n1. Run a container. Observe the task attached, and for this example assume it is has identifier `def456`.\r\n2. Run `docker inspect --type task abc123 def456` (where `abc123` is not a task identifier in any container)\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\n[]\r\nError: No such task: abc123\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\n```\r\n[\r\n    {\r\n        \"ID\": \"def456\",\r\n        ...\r\n    }\r\n]\r\nError: No such task: abc123\r\n```\r\n\r\nI expect this result because switching the order of tasks does not fail fast:\r\n\r\n```\r\ndocker inspect --type task def456 abc123\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nThis happens occasionally when diagnosing a failing swarm service: First I need to get the task identifiers, and then I need to inspect them. If it happens that the first task identifier in my list is invalid, the `docker inspect --type task ...` command fails and returns nothing about the otherwise correctly identified tasks.\r\n\r\nAs a result, I have to resort to calling `docker inspect --type task ...` with one task at a time which is less efficient.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.24 (downgraded from 1.25)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.12.6\r\n API version:  1.24 (minimum version )\r\n Go version:   go1.6.4\r\n Git commit:   78d1802\r\n Built:        Tue Jan 10 20:38:45 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 5\r\n Running: 0\r\n Paused: 0\r\n Stopped: 5\r\nImages: 17\r\nServer Version: 1.12.6\r\nStorage Driver: devicemapper\r\n Pool Name: docker-202:1-524732-pool\r\n Pool Blocksize: 65.54 kB\r\n Base Device Size: 10.74 GB\r\n Backing Filesystem: xfs\r\n Data file: /dev/loop0\r\n Metadata file: /dev/loop1\r\n Data Space Used: 4.758 GB\r\n Data Space Total: 107.4 GB\r\n Data Space Available: 23.58 GB\r\n Metadata Space Used: 6.644 MB\r\n Metadata Space Total: 2.147 GB\r\n Metadata Space Available: 2.141 GB\r\n Thin Pool Minimum Free Space: 10.74 GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: false\r\n Deferred Deletion Enabled: false\r\n Deferred Deleted Device Count: 0\r\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\r\n WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\r\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\r\n Library Version: 1.02.110 (2015-10-30)\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: overlay null host bridge\r\nSwarm: active\r\n NodeID: 1qutd93aptnd2us8wnbypjhyc\r\n Is Manager: true\r\n ClusterID: 9a8q3p4qg01vo386rsueqtk33\r\n Managers: 3\r\n Nodes: 7\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: xx.xx.xx.xx\r\n Manager Addresses:\r\n  xx.xx.xx.xx:2377\r\n  xx.xx.xx.xx:2377\r\n  xx.xx.xx.xx:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary:\r\ncontainerd version:\r\nrunc version:\r\ninit version:\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\nKernel Version: 4.4.0-38-generic\r\nOperating System: Ubuntu 16.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 15.67 GiB\r\nName: xxxxxx\r\nID: xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":["enhancement1"],"text":"Hi,\r\n\r\nI was wondering if there is the possibility to use /home/user/.docker/config.json file to configure the tls authentication to a remote docker engine ( running in swarm mode).\r\n\r\nRight now if we type docker help, we get this list of option ( on MacOS )\r\n\r\n```\r\n--config string      Location of client config files (default \"/Users/yourname/.docker\")\r\n-D, --debug Enable debug mode\r\n-H, --host list Daemon socket(s) to connect to (default [])\r\n-l, --log-level string Set the logging level (\"debug\", \"info\", \"warn\", \"error\", \"fatal\") (default \"info\")\r\n--tls Use TLS; implied by --tlsverify\r\n--tlscacert string Trust certs signed only by this CA (default \"/Users/yourname/.docker/ca.pem\")\r\n--tlscert string Path to TLS certificate file (default \"/Users/yourname/.docker/cert.pem\")\r\n--tlskey string Path to TLS key file (default \"/Users/yourname/.docker/key.pem\")\r\n--tlsverify\r\n```\r\n\r\nThe idea is to have those tls option ( `--tls`, `--tlscacert`, `--tlscert`, `--tlskey`, `--tlsverify`) being configured inside the `config.json` file for convenience. I am well aware of other options like  passing tlsverify on every call like this\r\n\r\n```\r\n#docker --tls --tlsverify ...etc  \r\n```\r\nor using environment variable on every shell\r\n\r\n```\r\n#export DOCKER_TLS_VERIFY=1 \r\n#export DOCKER_CERT_PATH=~/.docker/zone1/\r\n```\r\n\r\nBut i would like to know if the third option of using config.json to specify all TLS information would be possible now or in the future as a feature request. I find it very elegant and convenient to have such info in a config file, just like the kubernetes community has with the \"kubeconfig\" file which is yaml file containing all the authentication info to a \"kubernetes deployments\" which is roughly equivalent to a swarm stack ( deployment) based on a docker-compose.yml (Version 3 format) of my micro service application.\r\n\r\nthe `/Users/yourname/.docker/config.json` file would look like this :\r\n\r\n```json\r\n{\r\n      \"HttpHeaders\": {\r\n           \"MyHeader\": \"MyValue\"\r\n      },\r\n     \"StackContext\": {\r\n           \"myStack1\": {\r\n                  \"User\": \"UserProd\",\r\n                  \"tlsverify\": \"true\",\r\n                  \"tlscacert\": \"/Users/yourname/.docker/ca.pem\",\r\n                  \"tlscert\": \"/Users/yourname/.docker/cert.pem\",\r\n                  \"tlskey\": \"/Users/yourname/.docker/key.pem\"\r\n           },\r\n           \"myStack2\": {\r\n                 \"User\": \"UserStaging\",\r\n                  \"tlsverify\": \"true\",\r\n                  \"tlscacert\": \"/Users/yourname/.docker/ca.pem\",\r\n                  \"tlscert\": \"/Users/yourname/.docker/cert.pem\",\r\n                  \"tlskey\": \"/Users/yourname/.docker/key.pem\"\r\n            }\r\n      },\r\n\r\n      \"psFormat\": \"table {{.ID}}\\\\t{{.Image}}\\\\t{{.Command}}\\\\t{{.Labels}}\",\r\n\r\n      \"auths\": {\r\n\t          \"https://index.docker.io/v1/\": {\r\n\t\t               \"auth\": \"khsfdfhsjdskhlfhdlfjfjdj=\"\r\n\t          },\r\n\t          \"https://registry-1.docker.io/v2/\": {\r\n\t\t               \"auth\": \"qlfldhflqshmfdhsdnferi=\"\r\n\t          }\r\n       }\r\n}\r\n```"},{"labels":[null,null,"enhancement1"],"text":"Not a bug, a feature, the facility to access a host by service.id in the dns tools.\r\n\r\n #Example :  with a service mongo with 3 replicas : \r\n\r\nEach replica should be accessible by : \r\n- {servicename}.{replica.id}                           \r\n   - ex: mongo.1 \r\n- {servicename}.{nodename}                         \r\n  - ex: mongo.node1\r\n- {servicename}.{nodename}.{replica.id}                        \r\n  - ex: mongo.node1.1\r\n- {nodename}.{servicename} \r\n  - ex: node1.mongo\r\n- {nodename}.{servicename}.{replica.id} \r\n  - ex: node1.mongo.1\r\n- {serviceid} \r\n  - ex: g1xlvn56l30k\r\n\r\nhttps://github.com/docker/libnetwork/pull/1372\r\nhttps://github.com/docker/docker/pull/25420"},{"labels":[null,null,null,null,null,null,"enhancement1"],"text":"This is an umbrella ticket for tracking the implementation status of the `--format` option.\r\n\r\n\r\n- [X] docker events\r\n- [x] docker history #30962 @TheHipbot \r\n- [X] docker images\r\n- [X] docker info\r\n- [X] docker inspect\r\n- [X] docker ps\r\n- [ ] docker search https://github.com/docker/docker/pull/31539 @TheHipbot \r\n- [X] docker stats\r\n\r\n- [X] docker network ls\r\n- [x] docker node ls #30424 @yongtang\r\n- [x] docker node ps #28213 @yongtang\r\n- [X] docker plugin ls\r\n- [X] docker plugin inspect\r\n- [x] docker secret ls https://github.com/docker/docker/pull/31552 @ripcurld0 \r\n- [X] docker secret inspect\r\n- [X] docker service inspect\r\n- [x] docker service ls #28199 @yongtang\r\n- [x] docker service ps #28213 @yongtang\r\n- [x] docker stack ls https://github.com/docker/docker/pull/31557 @ripcurld0 \r\n- [x] docker stack ps #28213 @yongtang\r\n- [x] docker stack services #28199 @yongtang\r\n- [x] docker system df https://github.com/docker/docker/pull/31482 @ripcurld0 \r\n- [X] docker volume ls\r\n- [X] docker volume inspect\r\n\r\n\r\nFor following commands, maybe worth adding `--format`, but either is fine.\r\n- docker top (difficult because currently invoking external `top`)\r\n\r\nFor following commands, no need to implement the flag\r\n\r\n:x: docker port https://github.com/docker/docker/pull/31859\r\n:x: docker diff https://github.com/docker/docker/pull/31599 @ripcurld0  (PR for internally using formatter pkg has been merged)\r\n:x: docker checkpoint ls https://github.com/docker/docker/pull/31800 @ripcurld0 (ditto)\r\n\r\ncc @albers @sdurrheimer\r\n"},{"labels":[null,"enhancement1"],"text":"`docker run` has `--add-host=[]` which is very handy, especially in the network environment without internal DNS. However, `docker build` don't have `--add-host=[]` option.\r\n\r\nDuring the image building process, sometimes, we need to have specific `name`, `ip` mappings. Such as:\r\n\r\n* accessing internal Git server via HTTP with internal domain name;\r\n* accessing some websites (such as, GitHub repo) from inside of GFW, which often contaminated certain domain names;\r\n* accessing internal HTTP API server with internal domain name;\r\n\r\nThere is a workaround by adding `echo 1.2.3.4 >> abc.com /etc/hosts` in the begining of each `RUN` command which has the problems, however it would be handy to have `--add-host=[]` option in the `docker build` command.\r\n\r\nRelated issues:\r\n\r\n* https://github.com/docker/docker/issues/1916#issuecomment-40007129\r\n* https://github.com/docker/docker/issues/2267\r\n* https://github.com/docker/docker/issues/3851\r\n* https://github.com/docker/docker/issues/5779\r\n* https://github.com/docker/docker/issues/7966\r\n* https://github.com/docker/docker/issues/10171\r\n* https://github.com/docker/docker/issues/10324\r\n* https://github.com/docker/docker/issues/24928\r\n"},{"labels":[null,null,"enhancement1"],"text":"Docker 1.12 swarm's native load balancing does not provide an option to print the pool members for a given Docker service deployed to the swarm. This blackbox approach becomes problematic when debugging or troubleshooting errors that may or may not be related to routing.\r\n\r\nCan we add a load balancing pool member print out to docker service inspect?\r\n\r\nSomething along the lines of:\r\n\r\n```\r\n$ docker service inspect xyz\r\n[\r\n    {\r\n        \"ID\": \"dmu1ept4cxcfe8k8lhtux3ro3\",\r\n        \"Version\": {\r\n            \"Index\": 12\r\n        },\r\n        \"CreatedAt\": \"2016-06-17T18:44:02.558012087Z\",\r\n        \"UpdatedAt\": \"2016-06-17T18:44:02.558012087Z\",\r\n        \"Spec\": {\r\n            \"Name\": \"xyz\",\r\n            \"TaskTemplate\": {\r\n                \"ContainerSpec\": {\r\n                    \"Image\": \"xyz:3.0.6\"\r\n                },\r\n                \"Resources\": {\r\n                    \"Limits\": {},\r\n                    \"Reservations\": {}\r\n                },\r\n                \"RestartPolicy\": {\r\n                    \"Condition\": \"any\",\r\n                    \"MaxAttempts\": 0\r\n                },\r\n                \"Placement\": {}\r\n            },\r\n            \"Mode\": {\r\n                \"Replicated\": {\r\n                    \"Replicas\": 1\r\n                }\r\n            },\r\n            \"UpdateConfig\": {},\r\n            \"EndpointSpec\": {\r\n                \"Mode\": \"vip\"\r\n            }\r\n        },\r\n        \"Endpoint\": {\r\n            \"Spec\": {}\r\n        },\r\n        \"LoadBalancer\" : {\r\n            \"PoolMembers\" : { \"xyz1\", \"xyz2\", \"xyz3\", \"xyz4\" }\r\n        }\r\n    }\r\n]\r\n```\r\n\r\n\r\n"},{"labels":[null,"enhancement1"],"text":"Hello,\r\n\r\nIt's could be great to have possibility to change running container `log-opts` like `max-size` and `max-file`.\r\nI have running containers that I can't recreate and that containers produce gigabytes of logs. logrotate with copytruncate doesn't works right.\r\n\r\nThanks!"},{"labels":[null,"enhancement1"],"text":"Now that secret managment is available, shouldn't we be able to reference them from a compose file? I guess secrets should always be external and the stack should fail to deploy when some secrets are missing.\r\nI don't see anything about that point on the bug trackers."},{"labels":[null,"enhancement1"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nThere's support for `ONBUILD` commands, that trigger before starting the execution of the current (child) Dockerfile.\r\n\r\nIt would be great to **add `ONBUILDEND`, that trigger after building** the child Dockerfile.\r\n\r\nThis would be useful for cases where you expect somebody to `COPY` some folder somewhere before actually i.e. compiling the code:\r\n\r\nParent Dockerfile:\r\n\r\n```\r\nFROM alpine\r\nRUN apk add --no-cache python\r\nONBUILDEND pip install --no-cache-dir --editable /usr/src/app\r\n```\r\n\r\nChild Dockerfile:\r\n\r\n```\r\nFROM previousimage\r\nCOPY mypackage /usr/src/app\r\n```\r\n\r\nThis way you let the subproject have the directory structure they desire, [instead of e.g. forcing them to put the `requirements.txt` file in the project root folder](https://github.com/docker-library/python/blob/7663560df7547e69d13b1b548675502f4e0917d1/2.7/onbuild/Dockerfile#L6)."},{"labels":[null,"enhancement1"],"text":"Based on my current understanding, it appears that any custom build arguments provided to 'docker build' (ie: using the --build-arg parameter) must have an associated 'ARG' declaration in the dockerfile for the parameter to be used, except for several specific parameters which are somehow 'whitelisted' and thus do not require an associated declaration in the dockerfile. This is fine for one or two build arguments, or when managing just a small number of dockerfiles, but it quickly becomes unmanageable at scale.\r\n\r\n**Example Use Case:**\r\nSuppose a company has a mirror of a public package repository such as Python PyPI. The URL to the internal mirror can be set using an environment variable - PIP_INDEX_URL in this case - so that all Python package operations \"just work\" without further customizations (ie: pip install, etc.). Now, if one wants to create a Docker container which is able to install packages from this repository it would be beneficial to pass the relevant environment variable along to the container so it can be used at build time (ie: to be able to do a \"RUN pip install\" within a dockerfile, for example). To facilitate this behavior we can provide the relevant URL to docker at build time using \"docker build --build-arg PIP_INDEX_URL=http://my_mirror\", but in order for the dockerfile to make use of this environment variable we then need to add an associated ARG command to the dockerfile indicating that the argument is to be used. Further, since this is a site-wide customization, **every** dockerfile created must have this ARG command within it.\r\n\r\nNow, suppose a company has 5, 10, 15 or more parameters used by automation and development alike, all customized on a site-by-site basis. As these permutations increase, trying to keep track of which dockerfiles use which variables where and when becomes tedious at best, and fragile at worse.\r\n\r\nAs a small step towards simplifying these more complex build environments, it would be helpful if there were some way to allow arguments to be pass along to a docker build without having to make modifications to the dockerfile being built.\r\n\r\nDrawing some information from the extensive comment thread found [here](https://github.com/docker/docker/pull/15182#issuecomment-270158246), maybe we could define a command line option, config file, or some such on the host used by the docker dameon to list one or more build arguments to inherit from the --build-arg parameter regardless of whether the dockerfile contains an ARG declaration for it or not. \r\n\r\nAlternatively, maybe there could be a way to add a single boolean parameter to the docker build command to allow all build-arg params provided on the command line to be used regardless of the existence of ARG commands in the dockerfile being built - basically a flag that says \"whitelist every build argument I'm giving you\". This would allow us to override the default behavior of docker build on a case-by-case basis and would avoid having to make changes to the global environment. In our particular case we have  a bootstrapper script that developers and automation uses to run docker operations providing an assortment of shortcuts and checks already. With a change like this we could simply update our bootstrapper script so the appropriate parameter gets passed along to the docker command and our users could reap the benefits without further intervention on their part."},{"labels":[null,"enhancement1"],"text":"originally proposed here: https://github.com/docker/docker/pull/28499#issuecomment-270050669\r\n\r\nafter #28499 gets merged, next step would to support file/dir permissions.\r\n\r\ni'm proposing `DEFATTR` so that if you use multiple `COPY` or `ADD` you don't have to repeat the options:\r\n\r\n```\r\nDEFATTR chown=USER\r\nDEFATTR chown=USER:GROUP\r\nDEFATTR chown=UID\r\nDEFATTR chown=UID:GID\r\n```\r\n\r\n`chmod` is a bit tricky because you definetely want different permissions for files and dirs:\r\n```\r\nDEFATTR chmod=a+rX\r\nDEFATTR chmod=a+X\r\n```\r\n\r\nthese would rise the `x` bit only if `x` bit is present for any of the users: `chmod(1)`: \r\n> execute/search  only if the file is a directory or already has execute permission for some user (X)\r\n\r\nor more clearer directives for different kinds:\r\n```\r\nDEFATTR dir_chmod=0755 file_chmod=0644\r\n```"},{"labels":[null,null,"enhancement1"],"text":"Following up to with a dedicated issue for:\r\n```\r\nhttps://github.com/docker/docker/issues/25303#issuecomment-269867258\r\n```\r\n\r\nThis is a feature request/suggestion for adding a static IP option on the VIP overlay (ingress network) for Docker Services.\r\n\r\n## Real example to summarize need:\r\n* You have 50 VMs, each running Docker. They are all on 10.0.0.0/24, that is:\r\n10.0.0.101\r\n10.0.0.102\r\n10.10.0.103\r\n...etc...\r\n\r\n* You want to run different web services for customers and for each at least one overlay net. Let's assume you have:\r\ncust01\r\ncust02\r\ncust03\r\n...etc...\r\n\r\n* Each customer has multiple public IPs dedicated to them:\r\n\r\n** cust01 has:\r\n1.2.3.4\r\n5.6.7.8\r\n\r\n** cust02 has:\r\n9.10.11.12\r\n13.14.15.16\r\n17.18.19.20\r\n21.22.23.24\r\n\r\n** cust03 has:\r\n25.26.27.28\r\n29.30.31.32\r\n\r\n...etc...\r\n\r\n* Each PUBLIC IP is mapped to specific RFC1918 customer IPs.\r\nex:\r\n1.2.3.4 -> 10.0.0.251\r\n5.6.7.8 -> 10.0.0.252\r\n...etc..\r\n\r\n* Now you want to distribute each customer's services (let's assume a simple web server).\r\nYou create an overlay net for each customer, and you deploy their web containers.\r\n\r\n* Let's assume that:\r\n** each container runs on tcp/80\r\n** you will have some sort of a LB/haproxy/nginx solution that will backend to tcp/80 by service name (overlay DNS), and then expose tcp/80 externally\r\n\r\n\r\nSo you have:\r\ncust01-webservice (10 containers, each running on tcp/80, with a publish port of tcp/80 via HAproxy)\r\ncust02-webservice (10 containers, each running on tcp/80, with a publish port of tcp/80 via HAproxy)\r\n...etc...\r\n\r\n## Here's the problem\r\nYou deploy a LB/Proxy (haproxy, nginx, etc) for each customer. You publish/expose it on tcp/80. That points to the service VIP (cust01-webservice-container01, cust01-webservice-container02, etc...)\r\n\r\nNow you need to publish that tcp/80 as a serviceport so that you can reach it externally. You are now essentially limited because you cannot have more than 1 customer/service.\r\n\r\nSo now you have:\r\n1.2.3.4 -> 10.0.0.101 -> (haproxy/nginx) -> VIP(webservice-container01, webservice-container02, webservice-container03,etc...)\r\n\r\n\r\nAs you publish the first customer, that takes over all 0.0.0.0:80.\r\nYou can't specify only to publish it on 10.0.0.101\r\n\r\nSo now you have published cust01's web service (via their LB/proxy/nginx) to ALL of your local IPs in the swarm (since all nodes)\r\n\r\n^ This is a very very common scenario, and I think everyone who is not using public cloud resources, but instead using private datacenters or AWS VPC with full control of the subnet like setups is running into this.\r\n\r\n## Additional Information/Explanation\r\nA few uses cases as examples:\r\n\r\n* If \"docker service create\" is designed to replace \"docker run\", then this simply makes sense as a \"one to one\" feature of being able to port the functionality of adding a static IP.\r\n\r\n* In datacenters where the setup includes existing blocks of routed RFC1918 subnets, network plugins like macvlans for the overlay network make perfect sense. At that point, to bridge into the existing routed subnet, it's difficult to have a \"dynamic\" service VIPs. You cannot pre-configure your infrastructure. Generally you will open a few IPs as the \"load balancer/externally available\" IPs, or a few that one a 1-1 NAT.\r\n\r\nIn addition to this -- generally you want to utilize multiple internal IPs. You have two options as of now:\r\n1.) Publish and utilize a single IP\r\nor\r\n2.) Alias different IPs on the host nterfaces, and publish -- which will publish that port to ALL of the IPs -- which is no good.\r\n\r\nYou are essentially limited to having 1 docker host PER each non-routed internal IP. This is a huge limitation.\r\n\r\nAn example use case would be to run multiple services on the same port (in the example of a web service or a load balancer for different apps). As soon as for example you run a service on tcp/80, all of your nodes in your cluster will use up that port. You are now limited on running other services on tcp/80. With the ability of providing the IPs, this is very easy and portable. Currently, without static IPs, you would have to assign the IPs as aliases on the docker server interfaces, and then configure each load balancer to listen to that particular IP. This is messy, and it also makes it much harder to add more IPs. It also makes it harder to move across servers/environments.\r\n\r\nAlso related to #2, but technically dedicated to LBs - if you are using a LB (ex: nginx, haproxy, etc), you would need to have it run as a service onto the swarm distributed overlay network, in order to plug into the existing services. At that point, you will need to know those IPs ahead of time in order to map them as destination NATs. Without SDNs, this becomes a much more complicated task, as you cannot pre-map your infrastructure/network side ahead of time. There is also the potential that it will dynamically change on a re-create/re-deploy.\r\n\r\n* Certain applications require IPs to be statically configured into the config files\r\n(ex: https://github.com/docker/docker/issues/24170)"},{"labels":[null,"enhancement1"],"text":"Right now the syslog driver takes paths to TLS certs. See [docs](https://docs.docker.com/engine/admin/logging/overview/#/syslog-options). This works fine for a container because you know exactly where it'll run and you can prepare that one machine ahead of time, but when used with a service, you need a separate mechanism for distributing TLS certs across the cluster. It would make using the syslog log driver in swarm mode much easier if you could put the cert directly in the cli argument like so:\r\n\r\n```\r\ndocker service create --log-driver syslog --log-opt syslog-address=tcp+tls://url:11111 --log-opt syslog-tls-ca-cert-pem=\"$(cat cert.pem)\" --name registry --publish 127.0.0.1:5000:5000 registry:2\r\n```"},{"labels":[null,"enhancement1"],"text":"To reduce the number of layers, combining technique used commands through the operator \"&&\".\r\n\r\nBut this operator can not be combined arbitrary commands, only the commands in RUN.\r\n\r\nI propose to introduce a new unit construction in Dockerfile syntax, which allows to combine several commands in one commit.\r\n\r\nFor example:\r\n\r\nInstead\r\n```Dockerfile\r\nRUN apt-get update && \\\r\n    apt-get dist-upgrade --no-install-recommends -y\r\n```\r\nYou can write\r\n```Dockerfile\r\nBEGIN\r\nRUN apt-get update\r\nRUN apt-get dist-upgrade --no-install-recommends -y\r\nEND\r\n```\r\nBut such a structure in the current syntax in general is impossible:\r\n```Dockerfile\r\nBEGIN\r\nADD binary /binary\r\nRUN /binary\r\nEND\r\n```"},{"labels":[null,"enhancement1"],"text":"There are cases in dockerfile that we want to do something while the entrypoint is running so it has the the effect of exec after running container. some example of these cases are:\r\n1) in MySQL doing any mysql command like executing a .sql file\r\n2) in Solr doing config with solr command\r\n\r\ncurrently the approach is to use something like this: (as suggested in http://stackoverflow.com/questions/25920029/setting-up-mysql-and-importing-dump-within-dockerfile)\r\n```\r\nRUN /bin/bash -c \"/usr/bin/mysqld_safe &\" && \\\r\n    sleep 5 && \\\r\n    mysql -u root -e \"CREATE DATABASE mydb\"\r\n```\r\n\r\nbut it has some disadvantages:\r\n1) finding entrypoint and cmd is not straightforward.\r\n2) entrypoint or cmd might change.\r\n3) sleep part is not safe as it might needs more time based on host performance.\r\n3) it's not a clean code.\r\n\r\nMy suggestion is to add EXEC command to Dockerfile so above smaple will be like this:\r\n\r\n`EXEC sleep 5 && mysql -u root -e \"CREATE DATABASE mydb\"`\r\n\r\nor maybe we could remove \"sleep 5\" by using healthcheck feature:\r\n\r\n`EXEC mysql -u root -e \"CREATE DATABASE mydb\"`\r\n"},{"labels":[null,"enhancement1"],"text":"Docker has ARG, and has ENV, the issue I have with these is that they exposed to users. Sometimes you just want an internal variable that can be passed around to multiple commands, but not exposed to the user. Personally like ARG I don't really see this needing to be baked into the image. Suggested keyword would be VAR."},{"labels":[null,"enhancement1"],"text":"In some scenario, a service is developed by several teams. For example, the os team develop the base image, the platform team develop the platform image base on the base image, and the app team develop the app image base on the platform image, so the final image release to the user is `app<-platform<-base ` .  The issue is if there is some CVE or bug in the base and have to release a new base image to fix these, all the `platform` image and `app` image have to rebuild and release a new image.   The platform team and app team are not quite happy about this,  they want to decouple from the base image's release and release their image only if there is some changes in their own developed software.   They provide two approach: \r\n\r\n* 1  image combine\r\nThey suggest the os team release a base image, the platform release platform image, and the app team release the app image, all the image only contain the stuff developed by themselves without the base image, and the end user combine these image to an final image.  For example, os team release `base1`, platform team release `jvm1`,  app team release `app1`, the end user use something like `docker combine base1 jvm1 app1` and this create a `base1-jvm1-app1` image.  after some days, a CVE occur in `base1`, so the os team release `base2`, then the end user use `docker combine base2 jvm1 app1` to create a `base2-jvm1-app1` image.\r\n \r\n* 2  image patch\r\nIf the base image release a patch, the end user get the patch use something like `docker patch` or other tools to apply the patch to the image and create a new image.  \r\nThis is similar to https://github.com/SUSE/zypper-docker\r\n\r\nThis  break the immutable infrastructure philosophy a little bit,  I want some feedback from the community and see if these are acceptable, or there is some other better solution, or convince they not to do this because this is not the future. \r\n"},{"labels":[null,"enhancement1"],"text":"**Description**\r\n\r\nDocker can collect logs, but only as _primary_ and _error_ logs.  It would be nice to be able to collect more logs, especially from child worker processes.\r\n\r\nFor example:  `php-fpm` emits its own main log on `stdout`; while its children try to open an `access.log` and `error.log`.  That's three separate logs; docker can't collect them separately.\r\n\r\nIt would be nice if Docker allowed an option to name a log file, at which Docker will create a named pipe.  For example:  `/var/log/nginx/error.log` could be specified, and Docker would create an `error.log` named pipe which it then recognizes as that file.\r\n\r\nBy naming these logs, it would be possible to pull that specific log via docker, or to inform log drivers that a particular log is in use.  At the very least, it becomes possible to collect additional logs, and to read them by calling `docker logs --name LOGALIAS MYCONTAINER`."},{"labels":[null,"enhancement1",null],"text":"**Description**\r\n\r\nDocker provides a way to capture all the events logs using \"docker events\" command.\r\nBut docker currently, don't save these events logs in any file.\r\nDocker should store all these events log in a separate file, so that as a user I can analyse all the events taking place in my containerized plaform.\r\nSince docker don't provide this, I need to write a separate systemd service that depends on docker service and it redirects the \"docker events\" logs to a separate file.\r\n\r\n**Describe the results you received:**\r\n\"docker events\" don't logs in any file.\r\n\r\n**Describe the results you expected:**\r\n\"docker events\" should store all the events logs in a separate file.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.12.1\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   23cf638\r\n Built:        \r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.12.1\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   23cf638\r\n Built:        \r\n OS/Arch:      linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 97\r\nServer Version: 1.12.1\r\nStorage Driver: devicemapper\r\n Pool Name: docker-253:1-650320-pool\r\n Pool Blocksize: 65.54 kB\r\n Base Device Size: 10.74 GB\r\n Backing Filesystem: xfs\r\n Data file: /dev/loop0\r\n Metadata file: /dev/loop1\r\n Data Space Used: 5.223 GB\r\n Data Space Total: 107.4 GB\r\n Data Space Available: 44.74 GB\r\n Metadata Space Used: 6.988 MB\r\n Metadata Space Total: 2.147 GB\r\n Metadata Space Available: 2.14 GB\r\n Thin Pool Minimum Free Space: 10.74 GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: false\r\n Deferred Deletion Enabled: false\r\n Deferred Deleted Device Count: 0\r\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\r\n WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\r\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\r\n Library Version: 1.02.107-RHEL7 (2016-06-09)\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: null overlay host bridge\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: seccomp\r\nKernel Version: 3.10.0-327.13.1.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1.954 GiB\r\nName: dockerhost.cisco.com\r\nID: BLND:CG2Q:6TZJ:KBUU:LPON:F6CG:LZBO:OVKT:P67J:YHQO:XD7M:HE77\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nInsecure Registries:\r\n 127.0.0.0/8\r\n```\r\n"},{"labels":[null,null,"enhancement1"],"text":"I spent a bunch of time debugging a [bearer token](https://github.com/docker/docker/issues/29257) issue.  A lot of it was wasted because I hadn't introspected the bearer token to discover that my `access` section was empty.  Perhaps there's an easier way, but I ended up downloading the [JWT Debugger](https://jwt.io/) and manually copying the long tokens.\r\n\r\nTo reduce the complexity of debugging, it'd be nice if there was an API endpoint that returned a simple JSON structure showing the access rights granted by the request."},{"labels":[null,"enhancement1"],"text":"We run containerized high-performance applications in my team, some of them have a multi-process approach to parallelism, for instance the [Microsoft Cognitive Toolkit](https://github.com/Microsoft/CNTK).\r\nThe default size for `/dev/shm` is [64MB](https://github.com/docker/docker/blob/1.13.x/container/container_unix.go#L26-L27), which is often largely insufficient for our applications running on high-end machines. We need to remember to set `--shm-size` for each `docker run` we do.\r\n\r\nThus, it would be very helpful to have a daemon configuration option to set the default value.\r\n@justincormack: you mentioned implementing this [here](https://github.com/docker/docker/issues/24743#issuecomment-233322278). Below, you also mentioned doing the same for [sysctl](https://github.com/docker/docker/issues/24743#issuecomment-248582403), that would also be helpful for us."},{"labels":[null,"enhancement1"],"text":"# Adding support for Qemu/KVM containers\r\n\r\n\r\n## What?\r\n\r\n\r\nThis proposal is about adding support  for docker container execution on Linux using KVM hypervisor.\r\n\r\n\r\n\r\n\r\n## Why?\r\n\r\n\r\nThe conventional way of running docker containers on Linux leveraging namespaces and cgroups (runc) works great and we absolutely love it! But there are situations where we would like to have a stronger isolation between the container and the host kernel. \r\n\r\n\r\n\r\n\r\n## Here is how!\r\n\r\n\r\nThe basic idea here is to isolate the running container from the host kernel by running it inside a virtual machine. Traditional hypervisors are good at having a separation between the `guest` kernel and `host` kernel. So in a nutshell, we propose to use the KVM hypervisor to run docker containers! (But, what about performance??? see the section **'who is it for?**')\r\n\r\n\r\n\r\n\r\nThere are alternate approaches like those from Hyper, Clear Containers. However one of our motives was to make this happen transparently without requiring users to alter their docker 'experience'. \r\n\r\n\r\n\r\n\r\nOur proposal is to add a qemu option to the existing `--isolation` flag allowing the container to be run as a KVM VM rather than a default namespace based container. Further there is 1:1 mapping between a container and a VM. This is similar to Hyper-V containers approach (https://github.com/docker/docker/pull/16412). Our approach allows the user to execute containers with default as well as QEMU isolation simultaneously without changing the docker runtime.\r\n\r\n\r\nAnyway, this is a sample invocation with the isolation=qemu option.\r\n\r\n\r\n`docker run --isolation=qemu busybox /bin/ls`\r\n\r\n\r\nThe output of above command looks similar to the default docker container execution on Linux. i.e. without `--isolation=qemu`\r\n\r\n\r\n`docker run busybox /bin/ls`\r\n\r\n\r\nWhich is,\r\n```\r\n$docker run busybox /bin/ls\r\nbin   dev   etc   home  proc  root  sys   tmp   usr   var\r\n$\r\n$docker run --isolation=qemu  busybox /bin/ls\r\nbin   dev   etc   home  proc  root  sys   tmp   usr   var\r\n$\r\n```\r\n\r\n\r\nExcept for the fact that with `--isolation=qemu` the container actually ran inside a virtual machine.\r\n\r\n\r\nAlso, `docker ps` shows container running while in the background the task is getting executed inside a virtual machine.\r\n\r\n\r\n\r\n\r\n## But exactly how?\r\n\r\n\r\nOur main objective here is to execute the container inside a virtual machine on linux. Following steps will help you understand how we go about it,\r\n\r\n\r\n\r\n\r\n1. When the user says, `docker run busybox /bin/ls` docker essentially gets the `busybox` image and runs `/bin/ls` within the confinements of container started from that `busybox` image. When we move this logic to a virtual machine we have to do the following:\r\n   - Boot a the virtual machine image.\r\n   - configure the VM networking by using networking info provided by docker engine (e.g. IP, Mac address etc)\r\n   - Mount a file system consisting of the `busybox` image.\r\n   - chroot to that filesystem\r\n   - execute the command `/bin/ls`\r\n   - Redirect the stdout/stderr to docker cli so that user gets a consistent experience with or without qemu isolation driver.\r\n\r\n\r\n2. So we need to have a virtual machine that can configure itself and execute the command intended for a container (in this case, `/bin/ls`). Here is how we went about it,\r\n   - cloud-init (https://cloudinit.readthedocs.io/en/latest/) - VMs installed with cloud-init software mount a config-drive during their bootup and parse a `yaml` file to configure the VM(e.g. networking). One can also embed commands in cloud-init yaml file to be executed by VM after bootup. \r\n   - We start with creating the config-drive on the fly whenever user executes the ‘run’ command with the flag, `--isolation=qemu`. \r\n   - A Yaml file is injected into this ISO image with the information from docker daemon about it's networking and command that needs to be executed.\r\n   - We also mount the docker image (e.g. busybox) into this image as a passthrough disk from the host filesystem. Once the VM completes the boot we do the chroot to this drive and execute the command (both using cloud-init)\r\n   - VM stdout/stdin is redirected to the docker cli.\r\n   - If user does `docker stop`, in the background this will be mapped to stopping a corresponding virtual machine, or if the user decides to do `docker rm` on that container we will map it to undefining the virtual machine apart from the standard flow. Same approach can be extended to other docker commands as well (like, `docker logs`)\r\n \r\n**Here is the [sample implementation](https://github.com/harche/docker/tree/qemu-isolation) for anyone to try it out.**\r\n\r\n\r\n## Who is it for?\r\n\r\n\r\nPlaces where applications don't wish to share kernel with hosts can take advantage of such approach. This gives them the flexibility of running any docker image with the isolation of a virtual machine on linux without compromising the docker workflow they are used to and love.\r\n\r\n\r\nThe obvious drawback of this approach is the speed. We are trying to boot a VM and then execute the command. This will have overheads when compared to the default approach of using namespaces.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"},{"labels":["enhancement1"],"text":"After some searching, I have chosen to open this issue.  Due to the nature of Docker and its open issues, there are many issues that show up in searches, and refined searching may reject related issues; if this is a duplicate, please mark and close.\r\n\r\nSuggestions welcome.\r\n\r\nKeywords:  outgoing firewall, iptables, addresses, access, network\r\n\r\n**Description**\r\n\r\nFor security purposes, it is useful to control the outgoing access of Docker containers.  This prevents attacks on the network from within Docker.\r\n\r\nFor example:  you may run a `Proftpd` service configured to use auth-pam and auth-nss, and expose the Winbind socket to the container.  This allows the host to access your Active Directory server, while the `Proftpd` container only needs to access a local socket file for Winbind.  Because the `Proftpd` container can access the network freely across the bridge (e.g. `docker0`), an exploit in `Proftpd` allows probing and attacking of the Active Directory server itself.\r\n\r\nTo prevent this, you can block traffic from the bridge to the Active Directory server:\r\n\r\n```\r\niptables -A DOCKER-ISOLATION -d ad-domain-controllers.example.com \\\r\n -i docker0 -j REJECT --reject-with icmp-port-unreachable\r\n```\r\n\r\nThis blocks all traffic to `ad-domain-controllers.example.com` from all containers.  If another container uses a domain controller in the A records returned by that DNS lookup, it will also be blocked.  Thus the above `iptables` line will prevent your `Winbind` container from communicating with your Active Directory domain controller.\r\n\r\n**Suggestion**\r\n\r\nDocker could supply a set of simple firewalling options to apply to the container itself.  These would apply in order, such that the first rule is evaluated first, the second afterwards, and so on; a system with `iptables` will insert later rules physically further down the chain.\r\n\r\nI suggest the following `docker run` switch:\r\n\r\n```\r\n-F, --outgoing-firewall=[!][destination]:state:[!][state]:[policy]:[policy-setting]\r\n-F, --outgoing-firewall=[!][destination]::[policy]:[policy-setting]\r\n-F, --outgoing-firewall=[!][destination]:tcp:[!][ports]:[policy]:[policy-setting]\r\n-F, --outgoing-firewall=[!][destination]:udp:[!][ports]:[policy]:[policy-setting]\r\n```\r\n\r\n`Destination` can be empty (any address), a CIDR range (10.0.0.0/8), a single address, or a DNS name (resolved at time of rule implementation--in `iptables`, you can just pass the DNS name).\r\n\r\n`State` is a state list including one or more comma-separated values of `new`, `established`, `related`, or `invalid`.\r\n\r\n`Ports` is a port list in a form such as `80,443,8080-8088`.\r\n\r\n`Policy` is a policy, such as `accept`, `reject`, `drop`, or `log`.  Reject selects a sane default (tcp-reset) or approximates the requested method.  Example syntax::\r\n\r\n```\r\n-F :state:new:reject:icmp-admin-prohibited\r\n```\r\n\r\nThe second rule shows a default rule, such as a final deny-all:\r\n\r\n```\r\n-F ::reject\r\n```\r\n\r\nAs an example:\r\n\r\n```\r\ndocker run -d -F :state:established,related:accept -F dns.example.com:udp:53:accept \\\r\n  -F !10.0.0.0/8:tcp:80,443,21:accept -F ::reject wordpress:fpm`\r\n```\r\n\r\nThe above would run a Docker image called `wordpress:fpm`.  On an `iptables` system, the following rules would be generated:\r\n\r\n```\r\niptables -A DOCKER-ISOLATION -i docker0 -s 172.17.0.15 \\\r\n  -m state --state ESTABLISHED,RELATED -j ACCEPT\r\niptables -A DOCKER-ISOLATION -i docker0 -s 172.17.0.15 \\\r\n  -d dns.example.com -p udp --dport 53 -j ACCEPT\r\niptables -A DOCKER-ISOLATION -i docker0 -s 172.17.0.15 \\\r\n  ! -d 10.0.0.0/8 -p tcp -m multiport --dports 80,443.21 -j ACCEPT\r\niptables -A DOCKER-ISOLATION -i docker0 -s 172.17.0.15 \\\r\n  -j REJECT\r\n```\r\n\r\nIt may make some sense to instead create a chain for each container and simply jump to that chain if matching:\r\n\r\n```\r\niptables -A DOCKER-ISOLATION -i docker0 -s 172.17.0.15 -j WP-WORDPRESS-1\r\niptables -A WP-WORDPRESS-1 -m state --state ESTABLISHED,RELATED -j ACCEPT\r\niptables -A WP-WORDPRESS-1 -d dns.example.com -p udp --dport 53 -j ACCEPT\r\niptables -A WP-WORDPRESS-1 ! -d 10.0.0.0/8 -p tcp -m multiport --dports 80,443.21 -j ACCEPT\r\niptables -A WP-WORDPRESS-1 -j REJECT\r\niptables -A WP-WORDPRESS-1 -j RETURN\r\n```\r\n\r\nThis reduces the number of rules evaluated (evaluate until find the correct source, jump to its chain, evaluate rules) and allows Docker to just drop the jump rule and the chain, or to update the jump rule when restarting the container.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.12.1\r\n API version:  1.24\r\n Go version:   go1.6.2\r\n Git commit:   23cf638\r\n Built:        Tue, 27 Sep 2016 12:25:38 +1300\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.12.1\r\n API version:  1.24\r\n Go version:   go1.6.2\r\n Git commit:   23cf638\r\n Built:        Tue, 27 Sep 2016 12:25:38 +1300\r\n OS/Arch:      linux/amd64\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nUsing Docker on Ubuntu 16.04 on VMware, so not expecting an immediate distribution update."},{"labels":["enhancement1"],"text":"We have some server application packed into docker image. Now customers could download it from privat docker registry and configurate docker-compose to expose server in a right way. But there is two more problems.\r\n1. Customer could extract server code from image (it's an issue if server written in nodejs)\r\n2. Customer may copy image to multiple hosts. It would be nice to allow only one deployment for container (and tag them with customer names)\r\n\r\nThose scenarios both connected with distribution of proprietary applications using docker, not by admins, but by end-customers."},{"labels":[null,"enhancement1"],"text":"See https://github.com/docker/swarmkit/issues/1412 for background.\r\n\r\nProposal:\r\n That Swarm Mode have the ability to accept some pluggable means for a RAFT-compatible external KV store:\r\n1. User may already use a store like Consul for storing other metadatas and would like to reuse the **SAME** store for Swarm Mode.\r\n2. Allows other third-party RAFT-compatible KV store to be in the play instead of just using ETCD by default.\r\nAvoids violation of the \"Batteries Included\" principle espoused by Docker and promotes innovation and best-of-breed softwares.\r\n(See https://raft.github.io/ for various implementation)"},{"labels":[null,null,"enhancement1"],"text":"Hi guys,\r\n\r\nIt would be awesome to be able to do that:\r\n\r\nParent Dockerfile:\r\n```\r\nENTRYPOINT [\"/app/entrypoint.1.sh\"]\r\n```\r\n\r\nDockerfile:\r\n```\r\nFROM <parent>\r\nADD entrypoint.0.sh /app/entrypoint.0.sh\r\nENTRYPOINT_PREPEND [\"/app/entrypoint.0.sh\"]\r\n```\r\n\r\nEquals to [\"/app/entrypoint.0.sh\", \"/app/entrypoint.1.sh\"]\r\n\r\nIt would be helpful, in order to use parent images without erasing entrypoints !"},{"labels":[null,"enhancement1"],"text":"## 1. Introduction \r\n\r\nThis proposal is for adding hooks support to docker daemon, currently OCI spec defined 3 kinds of hooks: `prestart`, `poststart` and `poststop` (see https://github.com/opencontainers/runtime-spec/blob/master/config.md#hooks) and `runc` has implemented this, so everything is ready, what we really need to do is expose an appropriate API from `docker`.\r\n\r\n## 2. Scenarios\r\n\r\nFrom our side we have two special requirement that better to be done with a hook support from docker:\r\n\r\n1. We want to integrate our own network solution into docker, our own network project can support cross host communication and it manages its network interfaces and rules by itself. What we need to do is insert the existing network card created by it into the container. Also, a special need is we need to insert the nic before container CMD executed so that the user application can detect the network once it started. So `prestart` hook is exactly what we need.\r\n\r\n2. We want to  run systemd as pid 1 in our OS container, then we find two components from redhat: [oci-systemd-hook](https://github.com/projectatomic/oci-systemd-hook) and [oci-register-machine](https://github.com/projectatomic/oci-register-machine), they can satisfy our need, with new feature of hook support. Also I read about their docker patch for this: https://github.com/docker/docker/pull/17021 , the function is good, though I also agree with the idea that this is dangerous, BUT, we can find a safer way with a better UX and API. \r\n\r\n## 3. Proposed Design and API\r\n\r\nTo make this happen, I have two design idea currently in my mind:\r\n\r\n### (a) new `--hook-spec STRING` flag for `docker create` and `docker run` \r\nThe idea is simple, allow user to specify a json file containing hooks definition, `docker` read the file and register the hooks in the stage of container creating, and run the hook program when start. We have already done this in our private repo in this way, it works perfectly. \r\n\r\nFor example, to support redhat `oci-systemd-hook` and `oci-register-machine`, we only need a custom hook spec file in this format:\r\n\r\n``` hookspec.json\r\n{\r\n        \"prestart\": [\r\n            {   \r\n                \"path\": \"/usr/libexec/oci/hooks.d/oci-systemd-hook\",\r\n                \"args\": [\"oci-systemd-hook\", \"prestart\"],\r\n                \"env\": [\"container=runc\"]\r\n            },  \r\n            {   \r\n                \"path\": \"/usr/libexec/oci/hooks.d/oci-register-machine\",\r\n                \"args\": [\"oci-register-machine\", \"prestart\"]\r\n            }   \r\n        ],  \r\n        \"poststop\":[\r\n            {   \r\n                \"path\": \"/usr/libexec/oci/hooks.d/oci-systemd-hook\",\r\n                \"args\": [\"oci-systemd-hook\", \"poststop\"],\r\n                \"env\": [\"container=runc\"]\r\n            },  \r\n            {   \r\n                \"path\": \"/usr/libexec/oci/hooks.d/oci-register-machine\",\r\n                \"args\": [\"oci-register-machine\", \"poststop\"]\r\n            }   \r\n        ]   \r\n}\r\n```\r\nThe flag is simple and powerful, but maybe someone will be worried that it exposed too much low level stuff to user, that's why I bring a second design.\r\n\r\n### (b) new `docker hook` command with some subcommands\r\n\r\nI'll list all the commands which I think necessary one by one:\r\n\r\nAll subcommands:\r\n```\r\nUsage: docker hook [COMMANDS]\r\n\r\nCommands:\r\ncreate: create a hook\r\ninspect:  return low level information on a hook\r\nls:  list hooks\r\nrm:  remove a hook\r\n```\r\n\r\n`docker hook create` accepts two args, `HOOK-NAME` and `PATH`, path is the binary/script path you'll execute in this hook, `PATH` is mandatory(see definition https://github.com/opencontainers/runtime-spec/blob/master/specs-go/config.go#L109-L115) so it should be a REQUIRED field.\r\n\r\n```\r\nUsage: docker hook create [OPTIONS] HOOK-NAME PATH\r\n\r\nOptions:\r\n--type STRING:  the hook type(prestart, poststart, poststop), default is \"prestart\"\r\n--env []STRING:  environments passed to hooks\r\n--timeout INT:  timeout before aborting hook execution\r\n--args []STRING:  args of the hook, default to []\r\n```\r\n\r\n`docker hook inspect ` would inspect hook definition, `docker hook list` will list all the hooks, and `docker hook rm` will remove one/more hooks.\r\n\r\nAnd for `docker create`, `docker run`, we add a new `--add-hook HOOK-NAME:[extra-args]` flag, to allow register a new hook into container configuration, and of course it will keep hooks created by docker(currently only `libnetwork` prestart hook), it will simply append new hooks after libnetwork prestart hook.\r\n\r\nExample workflow:\r\n```\r\n# docker hook create --type prestart --env [\"platform=linux\"] --timeout 5 --args [\"ls\", \"/tmp/a\"]  myFirstHook /bin/ls\r\n# docker run --add-hook myFirstHook:[\"/tmp/b\", \"/tmp/c\"] busybox top\r\n/\r\n```\r\n\r\nSo before `top` command executed in busybox container, myFirstHook will be executed first, the hook command is `ls /tmp/a /tmp/b /tmp/c`. (I know this is a useless hook, just for demonstration)\r\n\r\n```\r\n# docker hook inspect myFirstHook\r\n{\r\n  \"Name\": \"myFirstHook\",\r\n  \"Type\": \"prestart\",\r\n  \"Path\": \"/bin/ls\",\r\n  \"Args\":  [\"ls\", \"/tmp/a\"],\r\n  \"Timeout\": 5,\r\n  \"Env\": [\"platform=linux\"] \r\n}\r\n\r\n# docker hook ls\r\nName          Path        Args\r\nMyFirstHook  \"/bin/ls\"   [\"ls\", \"/tmp/a\"]\r\n\r\n# docker hook rm myFirstHook\r\nmyFirstHook\r\n\r\n# docker hook ls\r\nName          Path        Args\r\n```\r\n\r\nI think I should already make it clear. This will need some efforts to implement, but it hides low level structure and expose a better UX to user.\r\n\r\n## 4. Reference\r\n\r\nOther related topics: https://github.com/docker/docker/issues/14542 "},{"labels":[null,"enhancement1"],"text":"Looked around a bunch but can't find how to do this:\r\n\r\n```\r\nENVFILE ./env\r\n```\r\n\r\nIf that's not possible, is there a workaround to build a docker image that will contain all those environment variables when run? \r\n\r\nI see that there's a way to do it when you start a container with docker-run, but I'd like to be able to do it when I build the image."},{"labels":[null,"enhancement1",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Docker network create doesn't inherit the MTU set in the Docker deamon**\r\n\r\nThe Docker deamon dockerd on some of our hosts has the mtu set to 1450 using the flag \"--mtu=1450\".\r\nWhen a network is created using \"docker network create <name>\" the mtu for the bridge is set at 1500.\r\nIt would make sense for the network to use the mtu setting specified as   \r\n\r\n**Steps to reproduce the issue:**\r\n1. run docker with a custom mtu \"dockerd --mtu=1450\"\r\n```\r\n/usr/bin/dockerd --mtu=1400 --debug=true -H unix:///var/run/docker.sock\r\n```\r\n2. docker create a network  called test\r\n```\r\ndocker network create test\r\n8fbf416d37582ee1f0352625cb0cdff786e3516d10b8540e5bac458bebf53d95\r\n```\r\n3. list the networks\r\n```\r\ndocker network ls\r\nNETWORK ID          NAME                     DRIVER              SCOPE\r\ne9c21cb405c9        bridge                   bridge              local               \r\nbd0f75218012        host                     host                local               \r\n3d81c322c790        none                     null                local               \r\n8fbf416d3758        test                     bridge              local\r\n```\r\n4. ip link show | grep 8fbf416d3758\r\n```\r\n543: br-8fbf416d3758: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN \r\n```\r\n\r\n**Describe the results you received:**\r\n\r\nI expected the network created to honour the MTU set at the daemon level\r\n\r\n**Describe the results you expected:**\r\n\r\nAny networks created the MTU is set to 1500 \r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nI am aware that I can set the MTU manually. This means I need to have knowledge about the docker I'm running on. I just want to be unaware of the underlying hardware and network configuration\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\ndocker version\r\nClient:\r\n Version:      1.12.2\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   19fddc2\r\n Built:        \r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.12.2\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   19fddc2\r\n Built:        \r\n OS/Arch:      linux/amd64\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\ndocker info\r\nContainers: 15\r\n Running: 7\r\n Paused: 0\r\n Stopped: 8\r\nImages: 37\r\nServer Version: 1.12.2\r\nStorage Driver: btrfs\r\n Build Version: Btrfs v4.2.2\r\n Library Version: 101\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options:\r\nKernel Version: 4.1.12-61.1.17.el6uek.x86_64\r\nOperating System: Oracle Linux Server 6.7\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 3.858 GiB\r\nName: **********.novalocal\r\nID: P5PR:YBOT:SLNB:IPES:KTV6:GO26:6UU3:FUWC:UKU5:TFY6:P2VI:Z3MP\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 59\r\n Goroutines: 87\r\n System Time: 2016-11-11T12:03:02.740206819-05:00\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nOpenstack, physical mix\r\n"},{"labels":[null,"enhancement1"],"text":"I already know that it is not supported in swarm   from #25274.\r\n\r\nBut I wish the swarm mode support the internal k/v store connection to read and write just in swarm container.  \r\nIn many case we have to run Etcd/Consul cluster outside, if the k/v store cluster state is the synchronize  with swam mode,  it will simplify the user's project.\r\n\r\nFor security, I think we can split the k/v store into system and user data space.\r\n"},{"labels":[null,"enhancement1"],"text":"Hi\n\n_I'm almost sure this might be already discussed, but I haven't been able to find an issue with keywords like: service depends, dependency, order, precedence. Sorry for any inconvenience._\n\n**Is there any plan to add something like --startorder, --depends, --after... to services to (https://github.com/docker/docker/issues/25303) in order to sequence service deployments?**\nI have readed about using swarm mode+compose or bundled applications, but I haven't played with it yet.\n\nActually I'm having an issue with nginx-ELK caused by \"container boot order\".\n\nNginx upstream configuration requires hostname to be resolved on start, otherwise application stops. http://stackoverflow.com/questions/32845674/setup-nginx-not-to-crash-if-host-in-upstream-is-not-found\n\nIf any of the main (non backup) upstream servers can't be resolved, nginx won't start. So, to make it run, I have to start all kibana (proxied service) instances. It doesn't matter if I add 127.0.0.11 as DNS resolver, as long as the container is not run, name won't be registered.\n\nIMHO nginx shouldn't crash when upstream is down, but probably there are other scenarios when this feature is also required.\nWith current behaviour, nginx service is launched several times, with their respective impact on system, until all upstream server names can be resolved.\n\nAs I'm sure I'm not the only one, neither the first having this issue, I'll like to ask how you solved this using swarm mode, and asking for the inclusion of such feature\n\nThanks.\n"},{"labels":[null,"enhancement1"],"text":"### Feature idea\n\nI belive docker service is missing a way to exec on all replicas of service. \n\nThe reason is, that if you want to for example reload config file you updated in volume, you have to restart service inside container. Without this feature you have to remove the service and create new, which cause downtime...\n\n**Steps to reproduce the issue:**\n\n```\ndocker service update --exec ...\n```\n\nThis command would exec command on all replicas of service. `--update-parallelism` and `update-delay` would affect parallelism and delay of this action \n"},{"labels":[null,"enhancement1",null],"text":"Ubuntu 16.10 was released https://wiki.ubuntu.com/YakketyYak/ReleaseSchedule, so we should add this to the list of supported versions for 1.13\n\nAlso should be added to the installation docs in the \"vnext-engine\" branch; https://github.com/docker/docker.github.io/blob/vnext-engine/engine/installation/linux/ubuntulinux.md\n\n/cc @vieux @andrewhsu \n"},{"labels":[null,"enhancement1"],"text":"Today, newly-scheduled tasks always try to pull the image indicated in the ContainerSpec, even if that exact image tag is available on the node where the task is scheduled (as indicated in the comment at https://github.com/docker/docker/issues/25211#issuecomment-236158843)\n\nIn order to make this behavior more configurable, I would like to propose a \"Service Pull Policy\" feature, featuring three policies:\n- \"always\": The image is always pulled from a remote registry, even if an image with the same name and tag exists on the node where the task is scheduled. This would be the default behavior, as\n- \"missing\": Only pull the image from the remote registry if the image is not already present on the node\n- \"never\": Never attempt to contact a remote registry to pull an image. If the image does not exist, the task exits with an error code\n\nThis policy would effectively be a new field in the ServiceSpec (or the TaskTemplate, if more appropriate) which could be manipulated through the `service create` and `service update` operations. \n\nA hand-wavy CLI example:\n\n```\ndocker service create --pull=missing <image:tag>\ndocker service update <service_name> --pull=always\n```\n## Cute animal photo\n\n![](http://static.boredpanda.com/blog/wp-content/uploads/2015/09/cute-hamsters-23__880.jpg)\n"},{"labels":[null,"enhancement1",null],"text":"This is a tracking issue to make sure we prepare RPM's for Fedora 25 as it will be released during 1.13 code freeze; https://fedoraproject.org/wiki/Releases/25/Schedule (2016-11-15)\n\nDuring code freeze, we can use the \"rawhide\" tag (https://hub.docker.com/_/fedora/) which is \"fedora 25 in the making\"\n\n@andrewhsu is going to look into this\n\n/cc @runcom @vieux @andrewhsu \n"},{"labels":[null,null,"enhancement1"],"text":"I tried to search for an existing issue with no luck, sorry if this is a duplicate\n\n**Description**\n\nCurrently if we have to get list of available tags of an image, we have to use the raw API. and if we have to get the list of available tags in a private image, we first have to fake a web login, generate a jwt token and then use it to do the API calls to get available tags.\n\nWe have to do this again and again everywhere we want to get tags.\n\n**Describe the results you received:**\nDescribed above\n\n**Describe the results you expected:**\nIt would be wonderful to have a `docker get-tags $image` and get list of available tags for that image. Doing it in docker cli would mean the user auth will be used by default, which is greaat\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.7.1\n Git commit:   6f9534c\n Built:        Thu Sep  8 10:31:18 2016\n OS/Arch:      darwin/amd64\n```\n"},{"labels":[null,"enhancement1"],"text":"In order to help facilitate a more self-service approach with some basic resilience after engine restarts, reboots, etc, it would be helpful to be able to set the default restart policy at the daemon level.  This would allow administrators to better control what the expected state should be out of the box.\n\n/cc @pvnovarese\n"},{"labels":[null,"enhancement1"],"text":"I propose that `docker build` accept `-f -` to indicate that the Dockerfile to be used is read from the STDIN of the `docker` client process.\n### Use cases\n- Shared engine CI systems can now stack anonymous image builds easily without requiring intermediate directories or tar archives\n- Dockerfiles in local directories, tar archives, and git repos can now be overridden\n- `docker build` is now a versatile shell scripting command as Dockerfiles can participate in shell pipelines decoupled from file system contexts\n### New capabilities\n- Uncooperative remote tar archives and git repos can now have out-of-band Dockerfiles applied to them\n- Local directory builds can now have Dockerfiles imported for _just_ the build via I/O redirection\n### Design considerations\n- A `-` stdin reader interface already exists in the context path argument so any `-f -` implementation will have to accommodate that\n- Accepting `-` in place of file paths is a common UNIX idiom to indicate that what would have been read from the path specified should instead be read from stdin; this makes having `docker` participate in UNIX pipelines much easier\n- This feature could be abused to avoid Best Practice so all advice regarding application/system design should be maintained and users should be encouraged to place Dockerfiles in repositories and tarballs so builds are straightforward and unambiguous when possible\n### Cases\n- `builder.GetContextFromReader` (`builder/context.go`) is used to read a context (or Dockerfile only) from stdin: nothing will change, a file called `-` will still be sourced from the stdin tarball (or still ignored in the case of `Dockerfile` only on stdin)\n- `builder.GetContextFromGitURL` (`builder/context.go`) is used to read a context from a git URL and the resultant context will have its `Dockerfile` rewritten and used for the build\n- `builder.GetContextFromURL` (`builder/context.go`) is used to read a context from a remote URL and the resultant context will have its `Dockerfile` rewritten and used for the build (or ignored in the case of a `Dockerfile` only context)\n- `builder.GetContextFromLocalDir` (`builder/context.go`) is used to read a context from a local directory and the resultant context will have its `Dockerfile` rewritten and used for the build\n### Implementation\n\nI've prototyped this functionality by rewriting the tar stream of the context before the trusted pull rewriting. If there is interest in this functionality, I will clean up my prototype and submit a PR. I welcome your ideas regarding this or similar features. Thanks!\n"},{"labels":[null,"enhancement1"],"text":"Hi Guys,\n\nIn my code, I want to take image tag as an input from user and then validate the tag against images present in my private repository. Unfortunately, I do not find any command which does that for me. I need do a docker pull and then check if that fails to know the provided image tag was correct or not. Can we have some kind of facility to check if the image tag is present before we try  to pull the image directly.\n"},{"labels":[null,null,null,"enhancement1",null],"text":"`docker service create` explicitly takes in `--network` option to let user select a globally scoped network for which swarmkit can allocate and manage resources.\n\nBut `--network` is optional and if the user doesn't specify one then swarmkit doesn't setup any network or allocate network resources ( _one exception is -p where swarmkit uses the default `ingress` network_ ).. In such a scenario docker daemon uses the node-local default network, `docker0` bridge in Linux or `nat` in Windows, etc... \n\nThis proves that swarmkit and services are robust enough to work in node-local network setup. Infact, there is absolutely no special case to support this scenario and that means the same logic must work for other node-local networks such as `--host`, `--none` or other user-defined `bridge` network or `macvlan` or `ipvlan` drivers. (there has been a few requests to enable this  as well)\n\nBut, we don't have the UX/API to enable this functionality & this is a proposal to enable that support.\n\nnode-local networks (aka `local-scoped` networks) needs resource allocation and management done locally by the engine in the same traditional ways. Hence swarm manager cannot be involved in such a node specific resource allocation. Hence it is not appropriate to overload the `--network` keyword in `docker service create` to convey the same functionality. Infact, (as per @mrjana) this feature can be considered more of a `network constraint` applied by the swarm manager and schedule the task only on the nodes on which the corresponding `node-local` network is present.\n\nIn order to achieve this, we can consider either of the following UX -\n- overload the `--network` API in `docker service create` that will take in `node-local` networks as well and make a decision to use a `global-scoped` network if present or apply the `network constraint` to filter schedule the tasks only to those nodes where the `node-local` network is present.\n- introduce a new `--net-constraint`flag (apologies on poor choice for the flag name), and that is explicitly meant for such a constraint mechanism and scheduling the task on appropriate nodes.\n\nIn either case, the only role of swarmkit is to schedule the task on a particular node and the rest of the functionality is handled at the engine level just as it is done today for the `default` case.\n\nping @mrjana @icecrime @al @stevvooe @cpuguy83 \n"},{"labels":[null,"enhancement1"],"text":"Both documentations ( [Docker Engine](https://docs.docker.com/engine/reference/commandline/kill/) as well as [Docker Compose](https://docs.docker.com/compose/reference/kill/) ) describe a useful feature which unfortunately seems to be unavailable through `docker service`.\n\nIt would be great to have the possibility to send signals to all tasks of a service, e.g. for making them reload their configuration.\n"},{"labels":[null,null,"enhancement1"],"text":"I would appreciate the ability to pass some options to volume driver on `docker volume rm` (like `docker volume create -o foo=bar`).\n\nI've some use cases:\n1. make a secure erase (shred)\n2. remove the volume and archive it somewhere\n3. I'm currently implementing a ceph storage docker plugin, and I'd like the ability to remove a volume along and with all its snapshots.\n4. Or the opposite : remove a volume and flatten its snapshots (making them independent of the volume they originally depend on)\n"},{"labels":[null,null,"enhancement1"],"text":"Now that Docker with v1.12 and the swarm mode enabled can transparently handle TLS for its swarm, I think it would be great if this could be extended to services as well.\n\nMost clusterable solution leverage TLS infra and ask the user to provide the CA cert as well as the node cert and this is just as tedious as settings TLS for the swarm manually.\n"},{"labels":[null,null,"enhancement1",null],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\n**Description**\nThe checkpoint feature for containers needs to handle checkpointing storage volumes. Storage volumes used by the containers may be provisioned by volume plugins and those plugins should be given a chance to create a snap of the volume. This is critical to be able to restore a container to a specific state w.r.t its data.\n\nThis issue is to request adding a volume plugin API to checkpoint a storage volume.  The plugin does whatever to create a point in time snap of the storage volume. Similarly for a restore to allow the plugin to revert to a prior PIT snap.\n\n<!--\nBriefly describe the problem you are having in a few paragraphs.\n-->\n\n**Steps to reproduce the issue:**\n1. NA\n2.\n3.\n\n**Describe the results you received:**\nNA\n\n**Describe the results you expected:**\nNA\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.0-dev\n API version:  1.24\n Go version:   go1.6.2\n Git commit:   ae816ba\n Built:        Wed Jun 15 07:20:09 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.0-dev\n API version:  1.24\n Go version:   go1.6.2\n Git commit:   ae816ba\n Built:        Wed Jun 15 07:20:09 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 1\n Running: 0\n Paused: 0\n Stopped: 1\nImages: 93\nServer Version: 1.12.0-dev\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 226\n Dirperm1 Supported: false\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: vmdk local\n Network: host overlay bridge null\nSwarm: active\n NodeID: 51wq28fmqek5c2a7ur50fd3w5\n IsManager: No\nKernel Version: 3.13.0-85-generic\nOperating System: Ubuntu 14.04.1 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 1.955 GiB\nName: hte-1s-eng-dhcp98\nID: H3SC:SAIV:HFHW:WYI5:WC4O:GEVQ:S6AJ:VIJ2:4CVT:UXDQ:T4GW:7MCA\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 30\n Goroutines: 70\n System Time: 2016-09-23T02:32:04.388872503-07:00\n EventsListeners: 0\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n"},{"labels":[null,"enhancement1"],"text":"Docker seems to be optimized for iptables at the moment. Are there any plans to support nftables in future versions of Docker?\n\nMy workaround at the moment is do deactivate the iptables integration via `--iptables=false` and then set the right rules for nftables by hand.\n"},{"labels":[null,null,"enhancement1"],"text":"In one of the very earliest Docker issues, [#37: Add ability to throttle bandwidth by container](https://github.com/docker/docker/issues/37), Ken proposes that Docker should allow per-container bandwidth throttling support. I think this idea should be reconsidered.\n\nThe current state of affairs is that people who want to do this are told to use `tc` on the `veth` for a given container. I believe this to not great place to be for a few reasons:\n1. `tc` is notoriously difficult to learn, so much so that various different suites of software have tried to paper over its rough edges. All of them are unsuited for use with docker and the large number of `veth`s it creates.\n2. `tc` has to be configured at container creation time. If you are deploying at scale, containers will be brought up or down constantly, which will force you, a developer or operations person, to constantly bookkeep to keep `tc` up to date. In practice this means you will give up. This problem is especially bad if you create many short lived containers per minute (_ahem, me_).\n3. As the Docker-centric ecosystem expands, more and more critical ops behavior will need to apply to specific containers. Right now, cluster management is focused on the allocation and distribution of containers across physical nodes, resource allotment, and load balancing / discovery. Rate limiting and more fine-grained resource allotment (_ahem, disk quota_) are logical next steps, and the network throttling side of all this has been neglected.\n\nI propose that Docker manage underlying network rules (however they are represented on the underlying system), to accept (at minimum) maximum bandwidth limits for inbound and outbound traffic, per-container. This should probably be implemented as an attribute on container creation.\n\nTHOUGHTS?\n"},{"labels":[null,"enhancement1",null],"text":"ping @jhowardmsft @tagomoris @darrenstahlmsft \n"},{"labels":[null,"enhancement1"],"text":"Feature request\n\nCurrently the experimental plugins can only expose endpoints via unix sockets and access (as a client) to other network services via the host's networking stack (via `host` mode in the networking section of the manifest).   There is no way for plugins to bind to a host port and listen, even though Docker containers all have the ability to expose / publish their services via the `--publish` option in `docker run` and `EXPOSE` in Dockerfile. \n\nThe ability to expose/publish will allow plugins to listen on host ports and be comparable to Docker containers and will enable many use cases.  Please consider implementing this in 1.13.\n\ncc @anusha-ragunathan \n"},{"labels":[null,"enhancement1"],"text":"In Compose (specifically `docker-compose run`), instead of duplicating all of Docker's considerable client-side attach logic, we'd like to start [shelling out to `docker attach`](https://github.com/docker/compose/issues/3194#issuecomment-220292194) instead, so we have better parity across all platforms (especially Windows).\n\nHowever, there's currently [client-side code](https://github.com/docker/docker/blob/0640a14b4fcba3715f7cc3bc9444f3c7f4827edd/cli/command/container/attach.go#L56-L58) which prohibits a user from `docker attach`-ing to a non-running container. This is a showstopper: to avoid race conditions, the order of operations must be _create, attach, start_.\n\nI'd like to either remove this logic or add a flag to skip it.\n"},{"labels":[null,"enhancement1",null],"text":"I understand Docker Swarm is using strategy spread by default.\nhttps://docs.docker.com/swarm/scheduler/strategy/\n\nHow can I configure the strategy=binpack? I don't see a way to update this behaviour.\n\nThank you!\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:33:38 2016\n OS/Arch:      linux/amd64\n"},{"labels":[null,"enhancement1"],"text":"**Description**\n\nNow that the pull request for checkpoint/restore https://github.com/docker/docker/pull/22049 has been merged, I am creating a new issue as state in the pull request for additional functionality.\n\nWe could perhaps introduce callbacks into the container for each of these actions performed from the docker CLI / API\n\n```\ndocker start\ndocker stop\ndocker checkpoint\ndocker restore\n```\n\nThese could as simple as the entry point executable, like the one specified in the compose file. This can aid in the application within the container do some pre/post processing before the docker management layers perform operations on the container. For e.g. prior to a checkpoint the application can perhaps do a sync of its in memory data structures to persistent storage. Similarly a restore could load back the stuff into memory.\n"},{"labels":[null,"enhancement1"],"text":"### Feature idea\n\nI belive docker service is missing a way to restart all replicas of service and way to exec on all replicas of service. Also logs available on one place would be great.\n\nThe reason is, that if you want to for example reload config file you updated in volume, you have to restart service inside container (reason of exec) or restart whole container (reason of restart). Without this features you have to remove the service and create new, which cause downtime...\nThe reason why logs would be great is obvious...\n\n**Steps to reproduce the issue:**\n1. docker service update --restart ...\nThis command would send HUP command to all replicas of service. `--update-parallelism` and `update-delay` would affect parallelism and delay of this action \n2. docker service update --exec ...\nThis command would exec command on all replicas of service. `--update-parallelism` and `update-delay` would affect parallelism and delay of this action \n3. docker service logs ...\nThis would show logs, same way as [docker-cloud](https://docs.docker.com/docker-cloud/getting-started/deploy-app/8_view_logs/) does.\n"},{"labels":[null,"enhancement1"],"text":"As a follow-up to #22049, @mlaventure [made a request](https://github.com/docker/docker/pull/22049#issuecomment-244442884) to move the metadata location for criu checkpoints to a non-single-container specific directory, such as `/var/lib/containers/checkpoints`.\n\nThis issue will track that work and the request that this happen prior to the 1.13 release.\n\ncc: @boucher \n"},{"labels":[null,"enhancement1"],"text":"I opened another issue (#26222) to use HEALTHCHECKs for communicating load information back to the DNS load balancer. After thinking about it more, I think that this capability would best be implemented as its own feature, separate from HEALTHCHECK'ing.\n\nSo, I propose that Docker add a LOADCHECK feature to the Dockerfile which runs a script within a container and returns a status between 0 and 255. A status of 255 would indicate that the container is fully loaded and cannot handle any further connections. A status of less than 255 would give an indication of how loaded the container thinks it is.\n\nWhen used with a service that specifies `--entrypoint-mode=dnsload`, the internal DNS load balancer would regularly invoke the LOADCHECK script for each replica/task to determine the overall load of the service. The replica with the lowest load would receive the next connection. If there are more than one replica with the lowest load status, the load balancer would use round-robin to these replicas.\n\nAn auto-scale feature could also be implemented to scale a service based on its LOADCHECK statuses. The `docker service create` should allow defining `--minimum-replicas=N` and`--maximum-replicas=N` for the service. The service would be started with `--minimum-replicas` and auto-scale. If the current lowest load status of all healthy replicas is 128 or greater, Docker would add another replica for the services (up to `--maximum-replicas`).  If the load status is 127 or lower for all replicas, one replicas would be stopped/removed from the service (down to `--minimum-replicas`).\n\nIf such an auto-scale feature were implemented, maybe use `--entrypoint-mode=auto-scale` to enable this feature in conjunction with LOADCHECK entrypoint being required in the service images.\n\nThe important part of this proposal is that the containers themselves should be able to communicate their load (as defined by the container and not the load balancer) back to the load balancer and Docker should manage the service automatically using the load balancer to determine when to scale up/scale down a service.\n"},{"labels":[null,"enhancement1"],"text":"on OSX it's very easy to install docker via brew (`brew install docker`)\n\non linux, using linuxbrew, `brew install docker` works but doesn't install the docker daemon. \nSo on linux, it's a lot more complex, depends on which flavor of linux (and which flavor of ubuntu), eg https://docs.docker.com/engine/installation/linux/ubuntulinux/ and also requires sudo access (and messes up with system libraries)\nproviding a linuxbrew way (as on OSX) would make things much nicer.\n"},{"labels":[null,null,"enhancement1"],"text":"Sometimes people have sporadic problem with swarm-mode overlay networking, and it is difficult to debug these issues (even with logs) because we don't have a lot of information on the states of their clusters.\n\nBelow are a few ideas from a recent meeting on code checks and output we could add to make these situations easier to debug:\n\nFor routing mesh not working on startup:\n- check ipvs actually enabled on all nodes' kernels\n- check firewalld, ufw\n- generally adapt overlay so that it works correctly with common firewalls\n\nFor routing mesh just failing over time for no reason:\n- surface the gossip state in `docker info`\n- log dump of service-level information (what nodes its on, each node's ipvs state, etc)\n\nOther ideas:\n- `docker service --loadbalancer ps` - would show all the backing instances behind a VIP\n"},{"labels":[null,"enhancement1"],"text":"Due the lack of any way to ignore possible known errors on COPY fe. the official node.js images don't include COPY command for `npm-shinkwrap.json` file (only `package.json`) as the builds that don't have said file would obviously break, a file which only exists once something \"production ready\" is produced and what is pretty damn important at that for repeatable builds (as in, locking version numbers of modules).\n\nThis leads to situations where fe. you might have `npm-shrinkwrap.json` locally and using _official_ node.js Docker images and yet you could be producing less secure/stable Docker images because of lack of something so simple. And I'm pretty sure this can't be the only usecase for things like that.\n\nThus, something like `COPY npm-shrinkwrap.json /usr/src/app/ --ignore-errors` would be extremely useful as a whole, to help the community produce better quality images.\n"},{"labels":[null,"enhancement1"],"text":"Currently as I known there is no function to sharing(mounting) data(whole dirs) from image to host. \nThis feature could be **very useful for developers** environments via IDE;\n\nExample case:\nnode_modules or vendors are installed within image (like src app) but this dirs there is not in git repo.\nInstalling this dirs take a lot of time, so this function will be very useful for developers\n"},{"labels":[null,"enhancement1"],"text":"I need to ensure that each task/replica of my docker service run on separate nodes for HA (so if one server fails or is shutdown it does not take my service offline). Currently, I am running 3 separate services with constraints allocating each service to a different node, but I'd really like to construct my image such that it can run as a single service with 3 replicas.\n\nI can think of several ways to support such a feature, but the most general, straight forward way might be to simply add an option called `--max-replicas-per-node=<n>` on the `docker service create` command where `<n>` is a positive integer. So `--max-replicas-per-node=1` would ensure that the swarm scheduler places at most 1 replica on a node.\n"},{"labels":[null,"enhancement1"],"text":"I need to pass host specific information to my swarm mode services so the tasks may configure themselves during container start up.\n\nI suggest that you add `--env` and `--env-file` options to `docker node update`. All containers started on the node would run with these environment variables added. If an ENV is added twice (possibly through different `docker node update` calls), the most recently added value would be used for subsequent tasks (including setting an ENV to the empty string \"\").\n"},{"labels":[null,null,"enhancement1"],"text":"I would like to be able to connect to a specific task in a service from some client containers rather than just using the load balanced service name.\n\nI suggest that the tasks for a service have implicit aliases added in the internal DNS. Such an implicit alias should follow a pattern. I suggest `<service name>-<n>` where `<n>` is the task number (e.g. a service named `mysrv` with 3 replicas would have its tasks have aliases of `mysrv-1`, `mysrv-2`, `mysrv-3`.\n\nWithin the task container, the assigned task alias should be available via an ENV such as `SERVICE_TASK_NETWORK_ALIAS` so the container knows how others can reach it and to do specific task configurations (such as task 1 for a mysql service would configure itself as a Master and task 2 and 3 would configure themselves as Slaves of task 1).\n"},{"labels":["enhancement1"],"text":"**Problem Statement**\n\nI would like to make sure that a restarting zookeeper service reuses a certain ID after the task is rescheduled. For this the task has to hold on a piece of information that determines his current host.\n\n**Current Workaround**\nI achieve this currently by passing in `/etc/hostname:/etc/docker-hostname:ro`, which allows me to grab the hostname of the node the container is running on and use this as a key in a key/value store which holds all Zookeeper-IDs.\n\nThe script to create a unique ID (https://github.com/qnib/consul-content/blob/master/consul/bin/get-uniq-id.sh#L6-L7), will use `/etc/hostname` as a key, if it is present.\n\n```\nelif [ -f /etc/docker-hostname ];then\nHOSTNAME=$(tail -n1 /etc/docker-hostname)\n```\n\nI then start a new service like this:\n\n```\ndocker service create --name zookeeper -e DC_NAME=dc1 -e CONSUL_CLUSTER_IPS=consul -e CONSUL_SKIP_CURL=true -e ZK_USE_CONSUL=true \\\n                      -e CONSUL_GETHOSTNAME=true --mode global  --network consul-net --publish 2181:2181 \\\n                      --mount type=bind,source=/etc/hostname,target=/etc/docker-hostname:ro \\\n                      --mount type=bind,source=/tmp/,target=/data/ \\\n                      qnib/alpn-zookeeper\n```\n\nThis breaks on hosts not providing `/etc/hostname`, for which I have to determine before hand, where they store the hostname.\nAnd it is an semi-ugly workaround anyway...\n\n**Proposal**\nI would like to be able to define environment variables in the `dockerd` startup, which are passed to each container started by that given docker-engine.\n\n```\ndockerd --global-env=DOCKER_HOSTNAME:physical1 --global-env=RACK:corner2\n```\n\nBy doing so I do not have any constraints in my `docker run`, `docker-compose up`, `swarmctl creat` or `docker service create` command, as this will be magically introduced by the docker-engine on each host of my swarm cluster.\n"},{"labels":[null,"enhancement1"],"text":"This is a feature request.\n\nIn golang , we can using chan/goroutine to parallel execute command.\n\nSo i request Dockerfile provide us a Parallel RUN command. Like PRUN\n\n```\nPRUN cd public && node run build\nPRUN go get -v ./...&&go build ....\n```\n\nThese two kind build progress were very common on mordern web development. We need build web frontend stack , like using npm + webpack.\nAnd we need build backend code , from golang source code to binary executable file.\n\nIf we can run them parallel. our building time will getting more shorter.\n"},{"labels":["enhancement1"],"text":"**Output of `docker version`:**\n\n[root@adc01klb tmp]# docker --version\nDocker version 1.12.1, build 23cf638\n**Output of `docker info`:**\n[root@adc01klb tmp]# docker info\nContainers: 3\n Running: 0\n Paused: 0\n Stopped: 3\nImages: 19\nServer Version: 1.12.1\nStorage Driver: btrfs\n Build Version: Btrfs v3.19.1\n Library Version: 101\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: null bridge host overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 4.1.12-37.6.3.el7uek.x86_64\nOperating System: Oracle Linux Server 7.1\nOSType: linux\nArchitecture: x86_64\nCPUs: 4\nTotal Memory: 14.57 GiB\nName: adc01klb\nID: VU4C:BYLG:4PY5:645N:A3EF:BY75:JMVU:XHLW:56H2:WQ2R:XARC:K7CN\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n[root@adc01klb tmp]# uname -a\nLinux adc01klb 4.1.12-37.6.3.el7uek.x86_64 #2 SMP Fri Aug 12 15:44:33 PDT 2016 x86_64 x86_64 x86_64 GNU/Linux\n\n**Steps to reproduce the issue:**\n1.  Run command like \"docker build  --shm-size=512K  -t test  -f test1.dockerfile .\"\n2. After build, run into the container using the image just built:\n[root@adc01klb tmp]# docker run -it ff24885393fa bash\n[root@e0405006caee /]# df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/loop0       45G  196M   42G   1% /\ntmpfs           7.3G     0  7.3G   0% /dev\ntmpfs           7.3G     0  7.3G   0% /sys/fs/cgroup\n/dev/loop0       45G  196M   42G   1% /etc/resolv.conf\n/dev/loop0       45G  196M   42G   1% /etc/hostname\n/dev/loop0       45G  196M   42G   1% /etc/hosts\nshm              64M     0   64M   0% /dev/shm\n\n**Describe the results you received:**\nThe mount size of /dev/shm is still 64M\n\n**Describe the results you expected:**\n\nThe mount size of /dev/shm should be the same as we specified.\n"},{"labels":[null,"enhancement1"],"text":"Would like to have ability to mount directory (which has application binary in it) into container (`container1`) from other container (`container2`).\n\nHere is `docker-compose.yml` that was tried:\n\n```\nversion: '2'\nservices:\n  container1:\n    build:\n      context: container1\n    depends_on:\n      - container2\n    volumes_from:\n      - container2:ro\n  container2:\n    build:\n      context: container2\n    volumes:\n      - /dir_with_app\n```\n\nBut whenever binary app changes and `container2` image gets built the `container1` still sees the old binary app.\n"},{"labels":[null,null,"enhancement1"],"text":"Sometimes when sharing front facing services in a docker swarm it is necessary to be able to restrict on which nodes a given service should be available. This is quite different of the --constraint flag, which only restricts in which nodes a given service is allowed to run.\n\n(see old discussion here: #26007)\n"},{"labels":[null,null,null,"enhancement1"],"text":"Stack yaml should support replicas. Example of yaml code:\n\n```\nlb:lb:\n  image: dockercloud/haproxy\n  ports:\n    - \"80:80\"\n  replicas: 5\n```\n\nThis would be equal to:\n\n```\ndocker service create -p 80:80 --name lb:lb --replicas=5 dockercloud/haproxy\n```\n"},{"labels":[null,"enhancement1",null],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.11.2\n API version:  1.23\n Go version:   go1.5.4\n Git commit:   b9f10c9\n Built:        Wed Jun  1 21:47:50 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.11.2\n API version:  1.23\n Go version:   go1.5.4\n Git commit:   b9f10c9\n Built:        Wed Jun  1 21:47:50 2016\n OS/Arch:      linux/amd64\n\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 5\n Running: 0\n Paused: 0\n Stopped: 5\nImages: 19\nServer Version: 1.11.2\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 29\n Dirperm1 Supported: false\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins: \n Volume: local\n Network: bridge null host\nKernel Version: 3.13.0-86-generic\nOperating System: Ubuntu 14.04.4 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 4\nTotal Memory: 7.8 GiB\nName: janonymous\nID: MOM4:ICMJ:RC7F:MJPI:K35I:64VL:2CD6:LADA:2B6E:FLCQ:QAAK:JB46\nDocker Root Dir: /var/lib/docker\nDebug mode (client): false\nDebug mode (server): true\n File Descriptors: 12\n Goroutines: 32\n System Time: 2016-08-30T11:25:25.255022521+05:30\n EventsListeners: 0\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nphysical machine\n\n**Steps to reproduce the issue:**\n1.   Make a Dockerfile\n2.   docker build .\n\n**Describe the results you received:**\nMake a docker file:\n_Dockerfile_\nFROM alpine:3.3\nMAINTAINER Jaivish\nWORKDIR /\nRUN apk add --no-cache \\\n        bash\n\nCMD [\"ls\"]\n\n_Commads_:\n$ docker build .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM alpine:3.3\n ---> 47cf20d8c26c\nStep 2 : MAINTAINER Jaivish\n ---> Running in afc90b4e39bd\n ---> 4d0a4153ef49\nRemoving intermediate container afc90b4e39bd\nStep 3 : WORKDIR /\n ---> Running in 094fb47c755c\n ---> 8dc7c8d84681\nRemoving intermediate container 094fb47c755c\nStep 4 : RUN apk add --no-cache         bash\n ---> Running in 9965699efe14\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.3/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.3/community/x86_64/APKINDEX.tar.gz\n(1/5) Installing ncurses-terminfo-base (6.0-r6)\n(2/5) Installing ncurses-terminfo (6.0-r6)\n(3/5) Installing ncurses-libs (6.0-r6)\n(4/5) Installing readline (6.3.008-r4)\n(5/5) Installing bash (4.3.42-r3)\nExecuting bash-4.3.42-r3.post-install\nExecuting busybox-1.24.2-r0.trigger\nOK: 13 MiB in 16 packages\n ---> c3cbb0a8fe7a\nRemoving intermediate container 9965699efe14\nStep 5 : CMD ls\n ---> Running in 3daea69b0ae0\n ---> 7afa38ea3317\nRemoving intermediate container 3daea69b0ae0\nSuccessfully built 7afa38ea3317\n\n**_Output_**\n$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n<none>              <none>              7afa38ea3317        2 minutes ago       8.239 MB\n<none>              <none>              85f0fa9f1190        28 minutes ago      158.7 MB\nubuntu              latest              f8d79ba03c00        2 weeks ago         126.4 MB\nalpine              3.3                 47cf20d8c26c        9 weeks ago         4.797 MB\nhello-world         latest              94df4f0ce8a4        4 months ago        967 B\nlarsks/thttpd       latest              a31ab5050b67        20 months ago       1.058 MB\n\n**Describe the results you expected:**\nInstead of REPOSITORY+TAG getting none there should be an option to specify it within dockerfile itself if command line arguments are not passed.\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n**If someone can confirm that this issue is valid i would like to solve this.**\n"},{"labels":[null,"enhancement1"],"text":"This proposal was originally from https://github.com/docker/docker/pull/24711#issuecomment-237666954.\n\nWith several changes to the image format, the scope of build caching has been limited to local nodes. This is problematic for architectures which dispatch builds to arbitrary nodes, since pulling new images and data will not fulfill the build cache.\n\nThe main concern here is [Cache poisoning](https://www.owasp.org/index.php/Cache_Poisoning). The worst part about is that it is not at all obvious that you are affected or protected. It can only be mitigated by limiting the horizon of data that one trusts.\n\nAnything that circumvents that protection, even `docker save/load`, is going to open your infrastructure up to injection of malicious content. The proposal in #20316 and previous proposals have not addressed this problem. While we all want fast builds (and I really do), introducing cache poisoning to the build step of the infrastructure must be avoided. Could you imagine the impact if someone could just inject a malicious layer into `library/ubuntu` or `library/alpine`?\n\nThe other aspect to this is the misapplied assumption about the idempotence of shell commands. `apt-get update` run twice is _never_ guaranteed to have the same result. Ever. That is just not how it works. If you have a build cache that is never purged, you will never update your upstream software. That may or may not be the intent. Even worse, if this build cache gets filled in with remote data, you probably have no visibility into when that command was run.\n\nThe underlying problem here is that with 1.10 changes in the image format, we no longer restore the parent image chain when pulling from a registry. As such, a proper solution to this problem involves something that can control the level of trust for content to a distributed build cache.\n\nLet's look at how we build an image, with `FROM alpine` at the top:\n\n```\ndocker pull mysuperapp:v0\ndocker build -t mysuperapp:v1 .\n```\n\nIn this simple case, we cannot assume that a remote `mysuperapp:v0` and the ongoing build are related, since that would possibly introduce the cache poisoning scenario that we need to avoid. However, one may have local registry infrastructure that they know they can trust. While we can infer parentage (despite other assertions, this is still possible), we may not be able trust that parentage from a build caching perspective from _all_ registries. But, this build environment is special.\n\nWhat better way than to tell the build process process that you can trust a related image?\n\n``` console\ndocker build --cache-from mysuperapp:v0 -t mysuperapp:v1 .\n```\n\nThe above would allow `Dockerfile` commands to be satisfied from the entries of `mysuperapp:v0` in the build of `mysuperapp:v1`. Job done!\n\nNo! We still have a problem. Now, my build system has to know tag lineage (mysuperapp:v0`<`mysuperapp:v1`). Let's modify the meaning the tagless reference to mean something slightly different:\n\n```\ndocker pull -a mysuperapp\ndocker build --cache-from mysuperapp -t mysuperapp:v1 .\n```\n\nIn the above, we pull _all_ the tags from `mysuperapp`, any layer of which can satisfy the build cache. In practice, this probably is a little wide for most scenarios, so we can allow multiple `--cache-from` directives on the command line:\n\n```\ndocker build --cache-from mysuperapp:v0 docker build --cache-from mysuperapp:v1 -t mysuperapp:v2 .\n```\n\nThere are many possibilities here to make this more flexible, such as running a registry service specially for the purpose of build caching `mybuildcache.internal/mysuperapp`. Did you know that you can just run a registry and rsync the filesystem around without locking? You can also rsync from multiple sources and merge the result safely (kind of). Such a registry can be purged periodically (or some one could submit a PR to purge old data).\n\nWe can take this even further, but I hope the point is brought home. This is probably less convenient that the original behavior, but it is a good trade off. It leverages the existing infrastructure and has the possibility of being extended as use cases change.\n\nCloses #18924.\n"},{"labels":[null,"enhancement1"],"text":"There are a number of systems including kubernetes that provide the ability to use pod-style networking:\n\n> Each Pod in a Kubernetes cluster is assigned an IP in a flat shared networking namespace. This allows for a clean network model where Pods, from a networking perspective, can be treated much like VMs or physical hosts.\n\nThis allows containers in the pod to share the same network and communicate over `localhost`\n\nThis can be achieved today by creating and starting a dummy container for the sole purpose of providing a shared network. The group of containers in the pod use `--net=container:id` to share the dummy container's network. This is certainly a viable approach, however, there seems to be room for improvement.\n\nThe proposal is to implement a new network driver for creating a flat, shared networking namespace that can be used without creating a dummy container:\n\n```\ndocker network create -d shared my-shared-network\n```\n\nThe creation of a network driver is not something I have much experience with, so I apologize if the proposal is light on technical detail. I wanted to first gauge interest.\n\n**note** there is an existing GitHub issue for making pods first class citizens in Docker (see https://github.com/docker/docker/issues/8781), however, the scope seems pretty large. The purpose of creating this as a separate issue is to propose incremental / smaller scale changes that could improve how we work with pods today.\n"},{"labels":[null,null,"enhancement1"],"text":"When running a large cluster of docker hosts with large docker images, the weight of pulling images onto newly provisioned/reprovisioned machines can put a lot of network and disk IO load on the docker registry server.\n\nIt would be convenient if the docker daemon's of peer servers in a swarm could somehow serve as registry endpoints, now that content-secured images have been implemented, to help spread the load of image pulls without requiring a dedicated layer of servers to handle it.\n\nThis would require two extra bits of functionality that I can see - the ability to store and retrieve image layers directly into the docker daemon, and support in docker-registry to use a docker daemon as the image backing store (but probably not the metadata store).\n"},{"labels":["enhancement1"],"text":"I can set container_name=\"foo\" in my docker-compose.yml to control the name of my container. This is great. But then I can't scale it anymore.\n\nWhy can't docker just take my name and use it as a \"prefix\" and concatenate to it some random string when it in scale mode? If you scale with docker-compose then the container name will always start with the same.\n"},{"labels":[null,null,"enhancement1"],"text":"This is not an issue, it is a feature request.  \nI have been searching and looking but not found any solution.\n\nI would like a \"tag\" to prevent docker volumes to be mounted so the date stay inside the container. This sounds strange but I will explain.\n\nI want to use the percona image from the docker hub. That mounts the datadir for the sql data. But as I make a dev box I want to have a test DB inside the image and I do not want to add / update data on user level when the DB is updated. I want to deploy an image with the DB data inside.  \n\nTo achive this I must build my own percona image just to remove the VOLUME part in the Dockerfile. It should not be nessecary. In for example the docker-compose or the docker run command I should be able to set --novolume or somthing. Then volumes from this container would not be mounted.\n\nThen I could use the same image in production but remove the --novolume this saves me to maintain two different images.  \n\nIt might be a solution to fix this but I have not found it.\n"},{"labels":[null,"enhancement1"],"text":"Are you open to a PR which adds a simple pass-through logging driver? The driver would push an application's standard out and standard error to files without any formatting or modification whatsoever.\n\nI've followed as much of the logging and plugin discussion as I can find (especially #18001 and #18604), and I realize that you don't want to support every logging format under the sun. However, this seems like a useful escape hatch for logging systems that don't have a dedicated driver; it allows developers to log to standard out/err in their organization's preferred format (JSON, msgpack, Splunk, etc.) and lets operators configure local persistence, post-processing, and forwarding.\n"},{"labels":[null,"enhancement1"],"text":"To validate support for the [OCI Image Spec](https://github.com/opencontainers/image-spec/), we are going to add support for image layout to `docker save` and `docker load`.\n\nThe [OCI Image Format](https://github.com/opencontainers/image-spec/blob/master/image-layout.md) is a content-addressable package roughly equivalent to the docker save/load format. It contains manifests with references to them.\n\nWhen OCI images are loaded into docker, they become regular docker images. When they are exported again as OCI images, they will be serialized to the OCI Image Layout. The main additions here will be provisions to control which references become which images when imported to docker. Because the OCI Image Spec lacks naming, we include simple parameters for safely importing images into desired names.\n\nThis is best demonstrated by example.\n\nThe following would generate an image `ubuntu:latest` into an OCI Image Layout:\n\n```\ndocker save --oci ubuntu > ubuntu-oci.tar.gz\n```\n\nThis would create a ref \"latest\" pointing to the relevant OCI structures.\n\nWe can also support multiple tags:\n\n```\ndocker save --oci ubuntu:15.10 ubuntu:16.04 > ubuntu-oci-multi.tar.gz\n```\n\nThe above would create an image layout where the refs \"15.10\" and \"16.04\" would point to their respective manifests. This can also work with images from different repositories:\n\n```\ndocker save --oci redis:3.2-alpine alpine > ubuntu-redis-oci.tar.gz\n```\n\nThe above would create refs for `latest` and `3.2-alpine`. However, the following would create a collision:\n\n``` console\n$ docker save --oci redis alpine\nerror: unable to include unique references in oci image\n```\n\nIn this case, both `redis` and `alpine` have a latest, so we would have to map them. We introduce the `--ref` flag to handle these cases:\n\n```\n$ docker save --oci --ref redis=redis-latest --ref alpine=alpine-latest redis alpine > ubuntu-redis-refs-oci.tar.gz\n```\n\nAt this point, it's important to note that mapping tags to references is merely a convention. The user can then use the `--ref` flag to map from the unconstrained ref space into the docker image namespace.\n\nLet's introduce our first `docker load`, using the tars generated above:\n\n```\ndocker load --oci --name ubuntu < ubuntu-oci.tar.gz\n```\n\nBy default, this command will import the provide OCI image layout, into an image named `ubuntu`, translating all refs directly into tags. The same command from above can be used with the `ubuntu-oci-multi.tar.gz`:\n\n```\ndocker save --oci --name ubuntu < ubuntu-oci-multi.tar.gz\n```\n\nBecause `ubuntu-oci-multi.tar.gz` has refs \"15.10\" and \"16.04\", those tags are created in docker.\n\nLet's move to the multi-image example:\n\n```\n$ docker load --oci --ref redis=redis-latest --ref alpine=alpine-latest < ubuntu-redis-refs-oci.tar.gz\n```\n\nBecause we had multiple images, we have to tell docker how to correctly import each image based on the refs in the image layout file. First thing to note is the structure of the `--ref` command. It is always `--ref <image>=<ref>`, whether using it with `docker save` or `docker load`. For example, the above is really equivalent to `--ref redis:latest=redis-latest`. The other thing to note is the rough equivalence of the `--name` command and `--ref` command. The `--name` command is the same as listing all the refs in an image layout creating a `--ref <image>:<ref>=<ref>` where `<image>` is the argument to `--name`.\n"},{"labels":[null,"enhancement1"],"text":"When an image is built for the first time, the `Created` time is set.  If another build is performed, and the context hasn't changed, the result of the build will be a no-op. The old image will be used, and the `Created` time wont change.\n\nNormally this is fine, but there are some cases when it is valuable to know the time the last build was performed, not just when the original cached image was created.\n\nThe use case that I've encountered is creating a build automation tool that uses docker ([dobi](https://github.com/dnephin/dobi)). Using `make` to build images would have the same problem. Repeating the build (and hitting the cache) is fine for the \"build image\" step of the automation, but if there are steps that depend on the \"build image\" step, there is no way to determine if the dependency is stale or not.\n\nIf there was a field that stored the latest \"build\" time, that time could be compared against the modified time of files in the context, to determine if the build step is stale.\n\nRelated to #4237 in that it's mutable metadata that would not be sent to the registry. It would stay local to the daemon.\n"},{"labels":[null,"enhancement1"],"text":"I'm currently using gelf log driver to ship the logs from my Docker containers to Logstash, then I use gelf plugin as input for Logstash and output the logs into Elasticsearch.  \n\nAs I haven't seen any issue about this, I've already put together a proof of concept that sent logs directly to Elasticsearch cluster, basically it works like this:\n\n``` bash\ndocker run --rm -p 8080:80 \\\n    --log-driver elasticsearch \\\n    --log-opt elasticsearch-address=\"http://172.17.0.2:9200,http://172.17.0.3:9200\" \\\n    --log-opt elasticsearch-username=\"tester\"    \\\n    --log-opt elasticsearch-password=\"encrypted\" \\\n    --log-opt elasticsearch-index=\"docker\"       \\\n    --log-opt elasticsearch-type=\"logs\"          \\\n    --log-opt elasticsearch-id=\"1\"               \\\n    --log-opt elasticsearch-max-retry=\"3\"        \\\n    --log-opt elasticsearch-timeout=\"60s\"        \\\n    tutum/hello-world\n```\n\nMy plan though is to develop it further, bring in SSL support and different log options, but I first wanted to make sure that there was some interest and a willingness to incorporate this feature into Docker.  \nAnd how could I assign this issue for myself?\n"},{"labels":[null,"enhancement1",null,null],"text":"Syslog was disabled in https://github.com/docker/docker/commit/655a58e27bfcfd2494bb5d46f95cb49d655ad17a by @jhowardmsft because that time it was using \"log/syslog\" package (not supported on windows).\n\nNow [syslog.go](https://github.com/docker/docker/blob/master/daemon/logger/syslog/syslog.go#L17) is using \"github.com/RackSec/srslog\" so it's possbile to use it on windows. (I was able to compile and run docker on windows with syslog enabled).\n\nWould you consider to enable this driver for windows?\n"},{"labels":["enhancement1"],"text":"We run a HA web server(2 servers in LB setup) and another external monitoring server.\n\nThe external IP on both servers is added as an A record to the DNS (Cloudflare)\nIf the external monitoring server detects a problem with one of the server it uses the Cloudflare API to disable the DNS records for this server so no clients are sent to this bad server.\nThis is how it looks like:\n\n![Schema](https://raw.githubusercontent.com/prioactiveinvestors/fmServer/master/schema.png)\n\nNow with the new Docker health check and swarm mode it would be really cool if we could implement all this without relying on a external health check service.\n\nI am thinking something that will extend the idea for the container health check and maybe allow adding commands on a container or node failure. \n\nIt will run some curl command that will adjust the external DNS in our case.\n"},{"labels":[null,"enhancement1",null],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.0\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   8eab29e\n Built:        \n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.0\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   8eab29e\n Built:        \n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 53\n Running: 0\n Paused: 0\n Stopped: 53\nImages: 13\nServer Version: 1.12.0\nStorage Driver: devicemapper\n Pool Name: docker-253:0-1032817-pool\n Pool Blocksize: 65.54 kB\n Base Device Size: 10.74 GB\n Backing Filesystem: xfs\n Data file: /dev/loop0\n Metadata file: /dev/loop1\n Data Space Used: 1.458 GB\n Data Space Total: 107.4 GB\n Data Space Available: 37.56 GB\n Metadata Space Used: 6.697 MB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 2.141 GB\n Thin Pool Minimum Free Space: 10.74 GB\n Udev Sync Supported: true\n Deferred Removal Enabled: false\n Deferred Deletion Enabled: false\n Deferred Deleted Device Count: 0\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\n WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\n Library Version: 1.02.107-RHEL7 (2015-10-14)\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local nimble\n Network: null bridge host overlay\nSwarm: active\n NodeID: 8857oygvwnu30na829rdb7y2j\n Is Manager: true\n ClusterID: cbxpmne2d14dzvgnfwuh9w65i\n Managers: 3\n Nodes: 3\n Orchestration:\n  Task History Retention Limit: 5\n Raft:\n  Snapshot interval: 10000\n  Heartbeat tick: 1\n  Election tick: 3\n Dispatcher:\n  Heartbeat period: 5 seconds\n CA configuration:\n  Expiry duration: 3 months\n Node Address: 192.168.59.21\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 3.10.0-327.el7.x86_64\nOperating System: CentOS Linux 7 (Core)\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 489.4 MiB\nName: ping\nID: CYLJ:J4E3:LVIE:FVPH:BW2Z:FHQU:6DLC:7QT7:O4NO:GEFC:Q6LU:WESM\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nVagrant on VMware Fusion\n\n**Steps to reproduce the issue:**\n1. Run any image that requires a TTY as a \"docker service\" (I've been using jenserat/samba-publicshare)\n2. Watch it fail indefinitely \n\n**Describe the results you received:**\nsmbd dies \n\n**Describe the results you expected:**\nsmbd dies unless there's a TTY\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\nPlease add a -t flag to \"docker service create\"\n"},{"labels":[null,"enhancement1",null],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.11.2\n API version:  1.23\n Go version:   go1.5.4\n Git commit:   b9f10c9\n Built:        Wed Jun  1 21:47:50 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.11.2\n API version:  1.23\n Go version:   go1.5.4\n Git commit:   b9f10c9\n Built:        Wed Jun  1 21:47:50 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 2\n Running: 2\n Paused: 0\n Stopped: 0\nImages: 22\nServer Version: 1.11.2\nStorage Driver: overlay\n Backing Filesystem: extfs\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: host bridge null\nKernel Version: 4.4.13-040413-generic\nOperating System: Ubuntu 14.04.3 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 2\nTotal Memory: 3.675 GiB\nName: ip-10-97-0-69\nID: UWJX:34OB:JFBN:7R4G:33IC:IWVW:SOBE:AM3B:B5BU:L2RL:LU32:ZVQB\nDocker Root Dir: /var/lib/docker\nDebug mode (client): false\nDebug mode (server): true\n File Descriptors: 22\n Goroutines: 55\n System Time: 2016-08-10T17:38:55.300602957Z\n EventsListeners: 0\nRegistry: https://index.docker.io/v1/\n```\n\nAs far as I can tell, there's no way to enable automatic container removal for containers run through the docker create/start CLI instead of docker run. Neither the create or start CLI commands accepts \"--rm\" like run does. This seems like it'll be a problem when Docker integrates with C/R (https://github.com/docker/docker/pull/22049), which plans on using docker start to restore a container, and which won't integrate with docker run.\n\nI see https://github.com/docker/docker/pull/20848 has been merged which changes some things about --rm, so I'm not sure if that changes anything. It looks like with that change the API supports an AutoRemove flag for create, but not the CLI.\n\nIs there a way to support auto-removal in the create/start CLI workflow?\n"},{"labels":[null,"enhancement1"],"text":"**Output of `docker version`:**\n\n```\n$ docker --version\nDocker version 1.12.0-rc4, build e4a0dbc, experimental\n```\n\n**Output of `docker info`:**\n\n```\n$ docker info\nContainers: 64\n Running: 0\n Paused: 0\n Stopped: 64\nImages: 18\nServer Version: 1.12.0-rc4\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 232\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge null host overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 4.4.15-moby\nOperating System: Alpine Linux v3.4\nOSType: linux\nArchitecture: x86_64\nCPUs: 2\nTotal Memory: 1.954 GiB\nName: moby\nID: 5ZV2:TTQ6:2YLF:JZ7B:VAAD:QICF:TWHK:3WGF:UZR7:OT6W:K27Q:NZL6\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 20\n Goroutines: 32\n System Time: 2016-08-09T14:24:58.396697939Z\n EventsListeners: 1\nNo Proxy: *.local, 169.254/16\nRegistry: https://index.docker.io/v1/\nExperimental: true\nInsecure Registries:\n registry.example.com\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n\nRunning on **OS X 10.11.6** (El Capitan)\n\n**Steps to reproduce the issue:**\n1. Create `Dockerfile` with custom DNS settings (`RUN echo ... > /etc/resolv.conf`)\n2. Try to build image with command `docker build .` (In same directory as `Dockerfile`)\n\n**Describe the results you received:**\n\nDNS Settings are ignored\n\n**Describe the results you expected:**\n\nDNS Settings should be respected. They work fine when working on an attached image using `docker run -it debian bash`\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n\nThe full story follows:\n\nI use the base Debian image for all my Docker projects, and since we're hitting the package repository multiple times a day now, I decided to host a local apt mirror using `apt-cacher`.\n\nRunning an image with `--dns` and `--dns-search` flags works as expected:\n\n```\n$ docker run -it --dns 10.1.1.1 --dns-search example.com debian bash\nroot@d1aa4e361f96:/# echo \"deb http://apt-cache:3142/ftp.debian.org/debian/ jessie main\" > /etc/apt/sources.list\nroot@d1aa4e361f96:/# apt-get update && apt-get upgrade -y\nIgn http://apt-cache:3142 jessie InRelease\nGet:1 http://apt-cache:3142 jessie Release.gpg [2373 B]\nGet:2 http://apt-cache:3142 jessie Release [148 kB]\nGet:3 http://apt-cache:3142 jessie/main amd64 Packages [9032 kB]\nFetched 9182 kB in 9s (967 kB/s)\nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nCalculating upgrade... Done\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\nroot@d1aa4e361f96:/# apt-get install npm -y\n[...]\nSetting up npm (1.4.21+ds-2) ...\n[...]\nroot@d1aa4e361f96:/# npm --version\n1.4.21\n```\n\nI can also manually edit `resolv.conf` to get the same results:\n\n```\n$ docker run -it debian bash\nroot@f6cc7b964169:/# echo \"search example.com\" > /etc/resolv.conf\nroot@f6cc7b964169:/# echo \"nameserver 10.1.1.1\" >> /etc/resolv.conf\nroot@f6cc7b964169:/# echo \"deb http://apt-cache:3142/ftp.debian.org/debian/ jessie main\" > /etc/apt/sources.list\nroot@f6cc7b964169:/# apt-get update && apt-get upgrade -y\nIgn http://apt-cache:3142 jessie InRelease\nGet:1 http://apt-cache:3142 jessie Release.gpg [2373 B]\nGet:2 http://apt-cache:3142 jessie Release [148 kB]\nGet:3 http://apt-cache:3142 jessie/main amd64 Packages [9032 kB]\nFetched 9182 kB in 4s (2005 kB/s)\nReading package lists... Done\nroot@f6cc7b964169:/# apt-get install npm -y\n[...]\nSetting up npm (1.4.21+ds-2) ...\n[...]\nroot@f6cc7b964169:/# npm --version\n1.4.21\n```\n\nHowever, there's no way of setting the DNS settings for a built image. There are no `--dns` or `--dns-search` flags (ref. #24928), and setting up `resolv.conf` from the `Dockerfile` appears to be ignored.\n\nDockerfile:\n\n```\nFROM debian\nRUN echo \"search example.com\" > /etc/resolv.conf\nRUN echo \"nameserver 10.1.1.1\" >> /etc/resolv.conf\nRUN echo \"deb http://apt-cache:3142/ftp.debian.org/debian/ jessie main\" > /etc/apt/sources.list\nRUN apt-get update && apt-get upgrade -y\nRUN apt-get install npm -y\nRUN npm --version\n```\n\nAttempt to build:\n\n```\n$ docker build .\nSending build context to Docker daemon 185.2 MB\nStep 1 : FROM debian\n ---> 1b01529cc499\nStep 2 : RUN echo \"search example.com\" > /etc/resolv.conf\n ---> Running in c90dbbfd7b26\n ---> ee6bf727209c\nRemoving intermediate container c90dbbfd7b26\nStep 3 : RUN echo \"nameserver 10.1.1.1\" >> /etc/resolv.conf\n ---> Running in bc32375c98c9\n ---> b7778d569bb8\nRemoving intermediate container bc32375c98c9\nStep 4 : RUN echo \"deb http://apt-cache:3142/ftp.debian.org/debian/ jessie main\" > /etc/apt/sources.list\n ---> Running in 853ea5594991\n ---> 6d3f724575b6\nRemoving intermediate container 853ea5594991\nStep 5 : RUN apt-get update && apt-get upgrade -y\n ---> Running in d62b971844fa\nErr http://apt-cache:3142 jessie InRelease\n\nErr http://apt-cache:3142 jessie Release.gpg\n  Could not resolve 'apt-cache'\nReading package lists...\nW: Failed to fetch http://apt-cache:3142/ftp.debian.org/debian/dists/jessie/InRelease\n\nW: Failed to fetch http://apt-cache:3142/ftp.debian.org/debian/dists/jessie/Release.gpg  Could not resolve 'apt-cache'\n\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nReading package lists...\nBuilding dependency tree...\nReading state information...\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n ---> 97492dcd5a95\nRemoving intermediate container d62b971844fa\nStep 6 : RUN apt-get install npm -y\n ---> Running in 5cbe9e32be9a\nReading package lists...\nBuilding dependency tree...\nReading state information...\nE: Unable to locate package npm\nThe command '/bin/sh -c apt-get install npm -y' returned a non-zero code: 100\n```\n\nBuilding with a `Dockerfile` without any DNS settings works, of course, since it's getting the packages from Debian directly and skipping the local mirror:\n\n```\n[...]\nStep 4 : RUN npm --version\n ---> Running in b47a8a8c7693\n1.4.21\n```\n\nThe final thing I tried was changing the `/etc/apt/sources.list` file _without_ changing the DNS settings, in case `docker build` took settings from the host machine, but this fails as well with the same error.\n\nI'm not sure where to start digging around to figure out why it works in `docker run` but not `docker build`. Feel free to send me any documentation worth reading, but this seems like an actual bug or missing feature somewhere.\n"},{"labels":[null,"enhancement1"],"text":"I would like to initiate a discussion about a different approach of solving the same issue that many people want to fix with those Mixins or Include proposals.\n\nHere is a list of those referred discussions:\nhttps://github.com/docker/docker/issues/24445\nhttps://github.com/docker/docker/issues/3378\nhttps://github.com/docker/docker/issues/735\n\nMy two cents. The approach of using nested builds would be actually pretty simple. Inside the Dockerfile there would be a new command for running another build (from now, I'll refer to it as the **subbuild**), and that subbuild would be as isolated as any other build, it will be in fact a build running externally in its own build context. Additionally, you declare which paths of the subbuild will be copied into the **parent build**.\n\n_Example:_\n\n```\nFROM nginx:1.11.1\nBUILDCOPY [\"Dockerfile.node-artifact\"]:/dist /usr/share/nginx/html\n# This syntax is just to illustrate the example, I'm not proposing any syntax here\n```\n\n_In the example you can see that we are taking nginx:1.11.1 as base image, and in the second line we are: 1. building the Dockerfile named \"Dockerfile.node-artifact\" located in the host to create a subbuild image, and 2. copying the folder \"/dist\" of the subbuild image to the location \"/usr/share/nginx/html\" of the resulting parent intermediate image_\n\nAlso, because of the existing --build-arg feature, there is a very nice opportunity to:\n1. Change the subbuild Dockerfile depending on an ARG/ENV variable.\n   \n   _Example_: \n   \n   ```\n   ARG SUB_BUILD=Dockerfile.production\n   BUILDCOPY [\"${SUB_BUILD}\"]:/subbuild/path /parent/path\n   ```\n2. Influence the subbuild from the parent build by passing arguments, so the subbuild can be generic if that makes sense for a given use case.\n   \n   _Example_: \n   \n   ```\n   BUILDCOPY --build-args NAME=${NAME} [\"Dockerfile.base\"]:/subbuild/path /parent/path\n   ```\n\nAnother extra could be using already existing images instead of Dockerfiles. \n\n_Example_: \n\n```\nFROMCOPY [\"nginx:1.11.1\"]:/subbuild/path /parent/path\n```\n\nThe main advantage of this approach vs include-like solutions is that the builds are still isolated. In one hand, the subbuilds keep their own cache, and can be built and tested without the parent build. And in the other, the input/output bounds are explicit and clear from the parent build perspective, and wouldn't produce conflicts between images.\n\nOther things that needs discussion and I didn't mention yet about this proposal:\n- subbuild context\n- syntax\n- recursion\n- subbuild intermediate image references\n\nI think implementation of this feature wouldn't be that complicated, as this would translate into another docker build, docker cp in the host system, followed by a COPY within the Dockerfile.\n"},{"labels":[null,"enhancement1",null],"text":"Docker's current IPv6 behavior currently still has the following outstanding problems:\n- it lacks -p/--publish and EXPOSE functionality similar to IPv4 (with non-EXPOSE'd ports being hidden, and -p/--publish only using the userland proxy which changes the source IP address to the userland proxy container which makes it near-useless for not all but some protocols because of security reasons - e.g. HTTP can deal with this with proxy headers, but other TCP-based protocols can't). This is a huge pitfall for many users of smaller servers who don't set up manual iptable rules but rely on docker to work as they have grown to expect it out of the box\n- it always requires setting up an entire IPv6 prefix which _should_ always be available in theory, but in certain home networks it requires changing the router configuration to be able to trivially use a larger IPv6 prefix range from a single machine, and multiple lan / wlan interfaces on a laptop can make this task more complicated as well\n\nDue to those and other reasons, it would be very nice if IPv6 NAT was added as han _option_ (**not** a full replacement for the current IPv6 default behavior) which could be enabled by the user if desired, to perfectly copy the current IPv4 NAT behavior and resolve all of the things listed above and provide an easy to use standard behavior that \"just works\".\n\nPlease note I agree NAT for IPv6 is often a terrible idea in a larger datacenter where you write custom routing and address assignments anyway - which is why NAT should be an option, not a replacement for the current behavior -, but for small servers and laptops where people just want a working default solution, it would be a big improvement.\n\nThere is a temporary implementation available from @robbertkl as a privileged container that modifies the iptables rules on the host with automated container discovery using dockergen: https://github.com/robbertkl/docker-ipv6nat . However, it would be neat if this functionality was integrated into docker itself like IPv4 NAT.\n\n(I created this ticket out of a discussion in ticket #13481 where you can read up more details on the previous discussion)\n"},{"labels":["enhancement1"],"text":"We recently implemented a feature to generate the man-pages (for a limited set of commands currently) from code (see https://github.com/docker/docker/pull/23825), and are testing how it works.\n\nOne thing that would help is to have a \"preview\" option to verify the generated man-pages; thinking of a container that serves the man-pages, so that they can be viewed in a web-browser.\n\n/cc @vdemeester @dnephin @SvenDowideit :smile:\n"},{"labels":[null,"enhancement1"],"text":"For simple tools that want to start a docker container with a service manager (systemd ahem.), it's useful to have it monitor  a pid file (like `runc run --pid-file`) especially now that we have `--live-restore`\n\nThe proposal is to add `--pid-file` support to `docker run` and `docker start` that will probably just proxied all the way to runc.\n"},{"labels":[null,null,"enhancement1"],"text":"I would like to suggest an added capability when launching a container that allows you to specify a volume to be mounted, which would be encrypted using a randomly generated key that would only be used one time.  This would allow a container to have a private location to store secrets, or other data that required similar handling, but that could not ever be recovered from the underlying hosts hard drives.\n\nThis, along with a centralized solution for storing and retrieving secrets securely, would greatly assist with defining some best practices around the handling of secrets and may have use cases beyond that.  See the following discussion: https://github.com/docker/docker/issues/13490\n"},{"labels":[null,"enhancement1"],"text":"**Output of `docker version`:**\n\n```\nDocker version 1.12.0, build 8eab29e\n```\n\n**Steps to reproduce the issue:**\n1. Create a service that will obviously have errors, like:\n   `docker service create --name ting-tang-walla-walla-bing-bang --replicas 1 busybox command-does-not-exist`\n2. Try and list the tasks of that service:\n   `docker service ps ting-tang-walla-walla-bing-bang`\n3. The error message might be truncated, so try and use the `--no-trunc` option from regular `docker ps`\n   `docker service ps --no-trunc ting-tang-walla-walla-bing-bang`\n\n**Describe the results you received:**\n\n```\nunknown flag: --no-trunc\nSee 'docker service ps --help'.\n```\n\n**Describe the results you expected:**\n\nStandard `docker ps` information with complete messages. \n"},{"labels":[null,"enhancement1"],"text":"The `service create` and `service update` commands do not support all options that `docker run` / `docker create` supports. Some options are not implemented yet, whereas other options may either _not_ be implemented (because they don't make sense in the context of a _service_, or are not portable / cross platform).\r\n\r\nWe should add more options for services, _however_ instead of blindly copying every option, we should make sure the options are implemented properly, which may require using different names for the options and/or different kind of values.\r\n\r\nI tried to create an overview of all options on `docker run`, and to match them with the `docker service create` options we currently have; I may have missed some, or made the wrong \"translation\", so input is welcome here\r\n\r\n| Status                | Issue                                          | `docker run`                 | `docker service`                                                                       | Notes                                                                                                                                                                |\r\n|:----------------------|:-----------------------------------------------|:-----------------------------|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------|\r\n| :white_check_mark:    | #27902                                         | `--add-host`                 |                                                                                        |                                                                                                                                                                      |\r\n| :question:            | #27552 (for `exec`)                            | `-a, --attach`               | n/a                                                                                    | ~~does not apply to _services_, as there are multiple containers backing it~~ There may be usecases for this, but design/implementation needs discussion             |\r\n|                       |                                                | `--blkio-weight`             |                                                                                        |                                                                                                                                                                      |\r\n|                       |                                                | `--blkio-weight-device`      |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    | #25885                                         | `--cap-add`                  |                                                                                        | ~docker/cli#2663~ docker/cli#2687 docker/cli#2709                                                                                                                    |\r\n| :white_check_mark:    | #25885                                         | `--cap-drop`                 |                                                                                        | ~docker/cli#2663~ docker/cli#2687 docker/cli#2709                                                                                                                    |\r\n|                       |                                                | `--cgroup-parent`            |                                                                                        |                                                                                                                                                                      |\r\n| :x:                   |                                                | `--cidfile`                  |                                                                                        | does not apply to _services_, as there are multiple containers backing it                                                                                            |\r\n| :question:            |                                                | `--cpu-percent`              |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    |                                                | `--cpu-period`               | `--limit-cpu`                                                                          | `--limit-cpu` sets a combination of \"cpu period\" and \"cpu quota\" see #27958 for the `docker run` implementation                                                      |\r\n| :white_check_mark:    |                                                | `--cpu-quota`                | `--limit-cpu`                                                                          | `--limit-cpu` sets a combination of \"cpu period\" and \"cpu quota\" see #27958 for the `docker run` implementation                                                      |\r\n| :question:            |                                                | `--cpu-rt-period`            |                                                                                        |                                                                                                                                                                      |\r\n| :question:            |                                                | `--cpu-rt-runtime`           |                                                                                        |                                                                                                                                                                      |\r\n| :question:            |                                                | `-c, --cpu-shares`           |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    |                                                | `--cpus`                     | `--limit-cpu`                                                                          | `--limit-cpu` sets a combination of \"cpu period\" and \"cpu quota\" see #27958 for the `docker run` implementation                                                      |\r\n| :question:            | #30477                                         | `--cpuset-cpus`              |                                                                                        |                                                                                                                                                                      |\r\n| :question:            |                                                | `--cpuset-mems`              |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    |                                                | `-d, --detach`               |                                                                                        | `-d` is the default                                                                                                                                                  |\r\n| :x:                   |                                                | `--detach-keys`              |                                                                                        | No interactive services, so not needed                                                                                                                               |\r\n| :question:            | #24865 / docker/swarmkit#1244                  | `--device`                   |                                                                                        |                                                                                                                                                                      |\r\n| :question:            |                                                | `--device-cgroup-rule`       |                                                                                        | devices are host specific, so may not make sense:question:                                                                                                           |\r\n| :question:            | #32602                                         | `--device-read-bps`          |                                                                                        | devices are host specific, so may not make sense:question:                                                                                                           |\r\n| :question:            | #32602                                         | `--device-read-iops`         |                                                                                        | devices are host specific, so may not make sense:question:                                                                                                           |\r\n| :question:            | #32602                                         | `--device-write-bps`         |                                                                                        | devices are host specific, so may not make sense:question:                                                                                                           |\r\n| :question:            | #32602                                         | `--device-write-iops`        |                                                                                        | devices are host specific, so may not make sense:question:                                                                                                           |\r\n|                       |                                                | `--disable-content-trust`    |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    | #24391                                         | `--dns`                      |                                                                                        | PR #27567                                                                                                                                                            |\r\n| :white_check_mark:    | #24391                                         | `--dns-option`               | `--dns-option`, `--dns-option-add`, `--dns-option-rm`                                  | PR #27567                                                                                                                                                            |\r\n| :white_check_mark:    | #24391                                         | `--dns-search`               | `--dns-search, `--dns-search-add, `--dns-search-rm`                                    | PR #27567                                                                                                                                                            |\r\n| :question:            | #29171                                         | `--entrypoint`               |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    |                                                | `-e, --env`                  | `-e, --env`                                                                            |                                                                                                                                                                      |\r\n| :white_check_mark:    | #24712 #31595                                  | `--env-file`                 |                                                                                        | PR #24844                                                                                                                                                            |\r\n|                       |                                                | `--expose`                   |                                                                                        |                                                                                                                                                                      |\r\n|                       |                                                | `--gpus`                     |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    | #25317                                         | `--group-add`                | `--group`                                                                              |                                                                                                                                                                      |\r\n| :white_check_mark:    | #27369                                         | `--health-cmd`               |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    | #27369                                         | `--health-interval duration` |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    | #27369                                         | `--health-retries`           |                                                                                        |                                                                                                                                                                      |\r\n| :question:            |                                                | `--health-start-period`      |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    | #27369                                         | `--health-timeout duration`  |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    | #24877                                         | `-h, --hostname`             |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    | #34529, docker/cli#51 #34639                   | `--init`                     | `--init`                                                                               | PR docker/swarmkit#2350, docker/swarmkit#2652, moby/moby#36895, moby/moby#37183,  docker/cli#1116, docker/cli#479, docker/cli#1129                                   |\r\n| :question:            | #32300                                         | `-i, --interactive`          |                                                                                        | ~~does not apply to _services_, as there are multiple containers backing it~~ There may be usecases for this, but design/implementation needs discussion             |\r\n| :question:            | #24170 / #29816                                | `--ip`                       |                                                                                        | ~~does not apply to _services_, as there are multiple containers backing it.~~ Update: possibly useful to set the VIP                                                |\r\n| :question:            | #24170 / #29816                                | `--ip6`                      |                                                                                        | ~~does not apply to _services_, as there are multiple containers backing it.~~ Update: possibly useful to set the VIP                                                |\r\n|                       |                                                | `--ipc`                      |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    | #31616, docker/cli#414                         | `--isolation`                |                                                                                        | PR #34424, docker/cli#426, docker/swarmkit#2342                                                                                                                      |\r\n| :x:                   |                                                | `--kernel-memory`            |                                                                                        | Feature is deprecated in the kernel; see #41254, #41252                                                                                                              |\r\n| :white_check_mark:    |                                                | `-l, --label`                | `--container-label`                                                                    |                                                                                                                                                                      |\r\n|                       |                                                | `--label-file`               |                                                                                        |                                                                                                                                                                      |\r\n| :x:                   |                                                | `--link`                     |                                                                                        | will be resolved through `--network-alias`:question:                                                                                                                 |\r\n| :x:                   |                                                | `--link-local-ip`            |                                                                                        | does not apply to _services_, as there are multiple containers backing it                                                                                            |\r\n| :white_check_mark:    |                                                | `--log-driver`               | `--log-driver`                                                                         |                                                                                                                                                                      |\r\n| :white_check_mark:    |                                                | `--log-opt`                  | `--log-opt`                                                                            |                                                                                                                                                                      |\r\n| :question:            | #31092                                         | `--mac-address`              |                                                                                        | does not apply to _services_, as there are multiple containers backing it                                                                                            |\r\n| :white_check_mark:    |                                                | `-m, --memory`               | `--limit-memory`                                                                       |                                                                                                                                                                      |\r\n| :white_check_mark:    |                                                | `--memory-reservation`       | `--reserve-memory`                                                                     |                                                                                                                                                                      |\r\n| :construction:        | #34654                                         | `--memory-swap`              |                                                                                        | PR: https://github.com/moby/moby/pull/37872                                                                                                                          |\r\n| :construction:        | #34654                                         | `--memory-swappiness`        |                                                                                        | PR: https://github.com/moby/moby/pull/37872                                                                                                                          |\r\n| :white_check_mark:    |                                                | `--mount`                    | `--mount`, `--mount-add`, `--mount-rm`                                                 |                                                                                                                                                                      |\r\n| :white_square_button: |                                                | `--name`                     |                                                                                        | NOTE: `--name` sets the _service_ name, not the container's name                                                                                                     |\r\n| :white_check_mark:    | #28573                                         | `--network`                  | `--network`                                                                            | `host` networking (see #25873) added through #32981.                                                                                                                 |\r\n| :white_check_mark:    | #28247                                         | --                           | `--network-add`/`--network-rm` are added in docker 17.05                               | docker/swarmkit#1029                                                                                                                                                 |\r\n|                       | #24787                                         | `--network-alias`            |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    |                                                | `--no-healthcheck`           |                                                                                        |                                                                                                                                                                      |\r\n|                       |                                                | `--oom-kill-disable`         |                                                                                        |                                                                                                                                                                      |\r\n| :white_square_button: | #34703                                         | `--oom-score-adj`            |                                                                                        | swarmkit PR: docker/swarmkit#2371                                                                                                                                    |\r\n|                       | docker/swarmkit#1605                           | `--pid`                      |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    | #28618                                         | `--pids-limit`               |                                                                                        | PR: #39882 swarmkit PR: docker/swarmkit#2415 (vendored: #35326)                                                                                                      |\r\n| :question:            |                                                | `--platform`                 |                                                                                        |                                                                                                                                                                      |\r\n|                       | #24862 / docker/swarmkit#1030                  | `--privileged`               |                                                                                        | docker/swarmkit#1722                                                                                                                                                 |\r\n| :white_check_mark:    |                                                | `-p, --publish`              | `-p, --publish`                                                                        | NOTE: does not support `<ip-address>` (#26696, #32299)                                                                                                               |\r\n| :x:                   |                                                | `-P, --publish-all`          |                                                                                        | when defining a service; explicitly define ports to publish                                                                                                          |\r\n| :white_check_mark:    | #30162                                         | `--read-only`                |                                                                                        | #29972                                                                                                                                                               |\r\n| :white_check_mark:    |                                                | `--restart`                  | `--restart-condition`, `--restart-delay`, `--restart-max-attempts`, `--restart-window` |                                                                                                                                                                      |\r\n| :x:                   |                                                | `--rm`                       |                                                                                        | SwarmKit keeps old tasks (containers) around, but removes them, based on `--task-history-limit`                                                                      |\r\n|                       |                                                | `--runtime`                  |                                                                                        |                                                                                                                                                                      |\r\n|                       | ~#25209~ -> #41371                             | `--security-opt`             | `--credential-spec` (#32339) is equivalent for `--security opt credentialspec=...`     | SELinux can be set through API (#32339)                                                                                                                              |\r\n| :question:            | #26714                                         | `--shm-size`                 | Possible through `--mount type=tmpfs,target=/dev/shm`                                  |                                                                                                                                                                      |\r\n|                       |                                                | `--sig-proxy`                |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    | #25696                                         | `--stop-signal`              |                                                                                        | PR #30754                                                                                                                                                            |\r\n| :white_check_mark:    |                                                | `--stop-timeout`             | `--stop-grace-period`                                                                  | New in 1.13 (see #22566)                                                                                                                                             |\r\n|                       | #28619                                         | `--storage-opt`              |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    | #25209, #31961, moby/libentitlement#35         | `--sysctl`                   |                                                                                        | PR #37701, docker/swarmkit#2729, docker/cli#1754                                                                                                                     |\r\n| :white_check_mark:    |                                                | `--tmpfs`                    | `--mount type=tmpfs`                                                                   |                                                                                                                                                                      |\r\n| :white_check_mark:    | #25644                                         | `-t, --tty`                  |                                                                                        | Implemented in SwarmKit docker/swarmkit#1370. Docker PR is #28076                                                                                                    |\r\n| :white_check_mark:    | #25209                                         | `--ulimit`                   |                                                                                        | PRs: docker/swarmkit#2967, #41284, ~docker/cli#2660~ docker/cli#2712                                                                                                 |\r\n| :white_check_mark:    | ~~#25304~~                                     | `-u, --user`                 | `-u, --user`                                                                           | ~~Does not support group / gid~~ (see [#25304 (comment)](https://github.com/docker/docker/issues/25304#issuecomment-236881192))                                      |\r\n| :question:            | #37560                                         | `--userns`                   |                                                                                        |                                                                                                                                                                      |\r\n|                       |                                                | `--uts`                      |                                                                                        |                                                                                                                                                                      |\r\n| :white_check_mark:    |                                                | `-v, --volume`               | `--mount`                                                                              | UX improvement needed (add `-v` flag?)                                                                                                                               |\r\n| :white_check_mark:    |                                                | `--volume-driver`            | `--mount`                                                                              | UX improvement needed (add `-v` flag?)                                                                                                                               |\r\n| :x:                   |                                                | `--volumes-from`             |                                                                                        | does not apply to _services_, as there are multiple containers backing it                                                                                            |\r\n| :white_check_mark:    |                                                | `-w, --workdir`              | `-w, --workdir`                                                                        |                                                                                                                                                                      |\r\n"},{"labels":[null,null,"enhancement1"],"text":"Hello,\n\nIt looks like there is no way to update volume labels or am I missing something?\nVolume API looks to support only supports listing, creating (with labels), inspect and remove a volume.\n\nCheers,\nMickaël\n"},{"labels":[null,null,"enhancement1"],"text":"Currently using `--publish` with Swarm Mode Services results in opening ports on the whole Swarm (`ingress` network if I understand correctly).\nNotably, it's not possible to publish a service on an individual IP only, i.e.:\n\n``` shell\n$ docker service create --name nginx --publish 192.168.99.101:80:80 nginx\ninvalid argument \"192.168.99.101:80:80\" for --publish: HostIP is not supported by a service.\n```\n\nIn any case, the above wouldn't be super useful because it would lock constrain the service to run only on the node which has the said IP.\n\nOn the other hand, currently there is no way to \"overload\" certain popular ports. If you publish port `80`, every single node in the Swarm will open that port, preventing you from running multiple services on the same port in the same Swarm.\n\nIt would be good if there was a way to partition the swarm into network sections, so that selected nodes, for example A, B, C, D belong to one network section (call it `partition-1`), and E, F, G, H into another one (`partition-2`). \n\nWhen publishing, we could select which network partition should be actually publishing the ports, i.e.:\n\n``` shell\n$ docker service create --name nginx --publish partition-1:80:80 nginx\n```\n\nThere should be an option to optionally constrain those services to run within the partition for increased performance (otherwise the swarm needs to proxy all the requests to the right node), but it's still much more manageable.\n\nThis would also help in the cases where the Node has multiple IPs assigned to it, as such node's IPs could be present in both partitions respectively.\n"},{"labels":[null,"enhancement1"],"text":"\"docker build\" does not support _\"--volumes-from\"_ directive/option (yet).\n\n\"docker build\" already supports \"--volume\" directive, and adding support from \"--volumes-from\" would make things more consistent with \"docker run\".\n\nThis would allow me to create something like this:\n\nData Dockerfile:\n\n```\nFROM scratch\nCOPY hugeinstaller.run /hugeinstaller.run\n```\n\nbuild, and start the data container.\n\n```\n$ docker create -v /datacontainer --name installer foo/bar /bin/true\n```\n\nAnd build the target image:\n\n```\n$ docker build --volumes-from installer -t foo/bartender:latest .\n```\n\nReal Dockerfile:\n\n```\nFROM ubuntu:latest\nMAINTAINER blah\n# run the installer in full silent autoinstall mode\nRUN /datacontainer/hugeinstaller.run --autoinstall --silent --full\nCMD foo-bar.sh\n```\n"},{"labels":[null,null,"enhancement1",null],"text":"Currently, when you create a service in global mode with a published port you get VIP load balancing by default. This is great when you want load balancing, but there are use cases where you don't. For example, lets say I want to run a container on every node that's aggregating stats (cadvisor, prometheus, etc) for the host. Then I have some external service aggregating all those stats together so you can get the big picture. I'd want to disable LB for this case and only route to the local container. I can't use `docker run` because if the services are in an overlay network w/ swarm mode you can't. \n"},{"labels":[null,"enhancement1",null],"text":"initial set of node labels during swarm join\n\nVersion Docker 1.12\n\nWhen getting a node to join a swarm it should be able to set some of its initial labels to advertise its hardware setup (network=public, storage=ssd ect...). Without this we may get containers provisioned on the node before we are able to set the label constraints, or if we have a node which is added to the swarm through auto scaling containers will not be provisioned to the node until the manager has set the node's config, and why should the  \n\nthere is an issue on swarmkit which may be related, is anyone looking at this what needs to be done to get this to work?\n\n[https://github.com/docker/swarmkit/issues/1160#issuecomment-235833933]\n\n**Output of `docker version`:**\n\n``````\nClient:\n Version:      1.12.0-rc5\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   a3f2063\n Built:        Tue Jul 26 13:21:59 2016\n OS/Arch:      linux/amd64\n Experimental: true\n\nServer:\n Version:      1.12.0-rc5\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   a3f2063\n Built:        Tue Jul 26 13:04:57 2016\n OS/Arch:      linux/amd64\n Experimental: true```\n\n\n**Output of `docker info`:**\n\n``````\n\nContainers: 2\n Running: 1\n Paused: 0\n Stopped: 1\nImages: 3\nServer Version: 1.12.0-rc5\nStorage Driver: aufs\n Root Dir: /mnt/sda1/var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 40\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: overlay null host bridge\nSwarm: active\n NodeID: b10jl9vt0ems2tmug5lln4bbp\n Is Manager: true\n Managers: 1\n Nodes: 4\n Node Address: 192.168.99.103\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 4.4.15-boot2docker\nOperating System: Boot2Docker 1.12.0-rc5 (TCL 7.2); HEAD : 9b97e11 - Tue Jul 26 22:20:20 UTC 2016\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 995.9 MiB\nName: master1\nID: BOGQ:N6C2:OJDY:IC7Z:JD3Y:AC3B:H25Y:YK4K:TCZP:MDUA:AHP5:3UDE\nDocker Root Dir: /mnt/sda1/var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 62\n Goroutines: 170\n System Time: 2016-07-29T12:08:04.249755937Z\n EventsListeners: 1\nRegistry: https://index.docker.io/v1/\nLabels:\n provider=virtualbox\nExperimental: true\nInsecure Registries:\n 127.0.0.0/8\n\n``````\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nVirtualBox\n\n\n**Steps to reproduce the issue:**\n```docker swarm join --token <worker token> <master address>:2377```\n\n**Describe the results you expected:**\ni would like to have flags so that i can `--label-add key=value` to tag the joining node\n``````\n"},{"labels":[null,"enhancement1"],"text":"Hi\n\nLooks like _docker service create_ doesn't have any kernel configuration options. eg: --security-opt, --sysctl, --ulimit... which are sometimes required.\nThis is stopping us on using swarm mode to deploy ELK 5 on our testing servers.\n\nCould you add at least a _--container-args_ option? eg: \n--container-args=\"--security-opt seccomp=unconfined --ulimit memlock=-1 --ulimit nofile=102400\"\n\nIf this can be done somehow, sorry for mistake. Please let me know how to do it.\n\nRegards.\n"},{"labels":[null,"enhancement1"],"text":"Docker should support dns options on `docker build` in the same way it supports them on `docker run`.\n\nThis feature has been requested a number of times in the past, but all issues have been closed with a references to an extremely impractical workaround (issue #5779). \n\nThis workaround requires to overwrite /etc/hosts from the Dockerfile at the beginning of the build and restoring it at the end. This especially means that there needs to be some proprietary configuration to store the ip address of the dns server  (in the Dockerfile,  a configuration file, someone else) .\n\nEven worse: If the author of the Dockerfile did not face the dns issue himself, he will not have added such a configuration option at all.\n\nA clean solution is to have `docker build` understand the dns configuration provided on the command line of `docker build` or the daemon in exactly the same way `docker run` does. This will provide a standardized solution that will work out of the box.\n"},{"labels":[null,null,"enhancement1"],"text":"This is Feature Request for Docker Swarm, where each node can specify port Ranges which can be open from the firewall, instead of default port options fro the node.\n"},{"labels":["enhancement1"],"text":"For example, I want filter my services or containers with label or name with regexp support\n\n`docker ls --filter label=labelName=*label*`\n`docker service ls --filter label=labelName=*label*`\n`docker ls --filter name=*name*`\n\nThanks\n"},{"labels":[null,null,"enhancement1",null,null],"text":"**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.0-rc4\n API version:  1.24\n Go version:   go1.6.2\n Git commit:   e4a0dbc\n Built:        \n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.0-rc4\n API version:  1.24\n Go version:   go1.6.2\n Git commit:   e4a0dbc\n Built:        \n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 1\n Running: 1\n Paused: 0\n Stopped: 0\nImages: 5\nServer Version: 1.12.0-rc4\nStorage Driver: devicemapper\n Pool Name: docker-253:1-176165418-pool\n Pool Blocksize: 65.54 kB\n Base Device Size: 10.74 GB\n Backing Filesystem: xfs\n Data file: /dev/loop0\n Metadata file: /dev/loop1\n Data Space Used: 987.5 MB\n Data Space Total: 107.4 GB\n Data Space Available: 40.39 GB\n Metadata Space Used: 1.88 MB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 2.146 GB\n Thin Pool Minimum Free Space: 10.74 GB\n Udev Sync Supported: true\n Deferred Removal Enabled: false\n Deferred Deletion Enabled: false\n Deferred Deleted Device Count: 0\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\n WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\n Library Version: 1.02.107-RHEL7 (2015-10-14)\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: overlay bridge null host\nSwarm: active\n NodeID: bhhed9vizh7mjdxve52lup55v\n IsManager: Yes\n Managers: 1\n Nodes: 3\n CACertHash: sha256:bbc6df07cdf4131d1cf5153b2edc95244c4f7966fa59cb7b6ea11515731d15d7\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 3.10.0-327.el7.x86_64\nOperating System: Red Hat Enterprise Linux Server 7.2 (Maipo)\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 5.671 GiB\nName: jf-a.localdomain\nID: FWPJ:7UNB:OLBN:23Y6:JFNN:ARRB:7MW6:RWIC:RG3P:I36U:RU47:3KE6\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 45\n Goroutines: 139\n System Time: 2016-07-21T15:19:10.688153846-04:00\n EventsListeners: 1\nRegistry: https://index.docker.io/v1/\nInsecure Registries:\n 127.0.0.0/8\n\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nVMs on OpenStack\n\n**Steps to reproduce the issue:**\n1. Spin up 3 node swarm mode cluster, including specifying the --listen-addr param\n2. Create 1 instance of tomcat: `docker service create --replicas 1 --publish 8080:8080 --name first_tomcat tomcat:8.0.36-jre8-alpine`\n3. Create 1 instance of tomcat (publish a different port): `docker service create --replicas 1 --publish 8081:8080 --name second_tomcat tomcat:8.0.36-jre8-alpine`\n4. `docker exec -it <container id of first_tomcat> sh`\n5. `wget -O - second_tomcat:8080` (using wget as alpine doesn't come with curl, sorry haha)\n\n**Describe the results you received:**\nTry using service VIP:\n\n```\n/usr/local/tomcat # wget -O - second_tomcat:8080\nConnecting to second_tomcat:8080 (10.255.0.7:8080)\n<timeout>\n```\n\nTry using fixed IP:\n\n```\n/usr/local/tomcat # wget -O - tasks.second_tomcat:8080\nConnecting to tasks.second_tomcat:8080 (10.255.0.8:8080)\n<timeout>\n```\n\n**Describe the results you expected:**\nShould be able to reach the other service's instance!\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\nAlways happens. Happens with both an in-house Tomcat container and the public Tomcat container.\n\nDocker inspect of the second_tomcat container:\n\n```\n# docker inspect 2f539696a2e0\n[\n    {\n        \"Id\": \"2f539696a2e0a228558cd1b25286d6637c88c6f124b3bd72d2f1b8737574b878\",\n        \"Created\": \"2016-07-21T19:15:30.727331171Z\",\n        \"Path\": \"catalina.sh\",\n        \"Args\": [\n            \"run\"\n        ],\n        \"State\": {\n            \"Status\": \"running\",\n            \"Running\": true,\n            \"Paused\": false,\n            \"Restarting\": false,\n            \"OOMKilled\": false,\n            \"Dead\": false,\n            \"Pid\": 17356,\n            \"ExitCode\": 0,\n            \"Error\": \"\",\n            \"StartedAt\": \"2016-07-21T19:15:31.380976163Z\",\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\n        },\n        \"Image\": \"sha256:b0ca4b09f74556a997ab35150b7995cd722a3d5aa98255404684341b8a5cd6a9\",\n        \"ResolvConfPath\": \"/var/lib/docker/containers/2f539696a2e0a228558cd1b25286d6637c88c6f124b3bd72d2f1b8737574b878/resolv.conf\",\n        \"HostnamePath\": \"/var/lib/docker/containers/2f539696a2e0a228558cd1b25286d6637c88c6f124b3bd72d2f1b8737574b878/hostname\",\n        \"HostsPath\": \"/var/lib/docker/containers/2f539696a2e0a228558cd1b25286d6637c88c6f124b3bd72d2f1b8737574b878/hosts\",\n        \"LogPath\": \"/var/lib/docker/containers/2f539696a2e0a228558cd1b25286d6637c88c6f124b3bd72d2f1b8737574b878/2f539696a2e0a228558cd1b25286d6637c88c6f124b3bd72d2f1b8737574b878-json.log\",\n        \"Name\": \"/second_tomcat.1.b5u1k8vbr9zs7rcouuscht0d6\",\n        \"RestartCount\": 0,\n        \"Driver\": \"devicemapper\",\n        \"MountLabel\": \"\",\n        \"ProcessLabel\": \"\",\n        \"AppArmorProfile\": \"\",\n        \"ExecIDs\": [\n            \"810f39444f184440e75361a2739178539a7d1c40410dbd1a2451dbad82660e4e\"\n        ],\n        \"HostConfig\": {\n            \"Binds\": null,\n            \"ContainerIDFile\": \"\",\n            \"LogConfig\": {\n                \"Type\": \"json-file\",\n                \"Config\": {}\n            },\n            \"NetworkMode\": \"default\",\n            \"PortBindings\": null,\n            \"RestartPolicy\": {\n                \"Name\": \"\",\n                \"MaximumRetryCount\": 0\n            },\n            \"AutoRemove\": false,\n            \"VolumeDriver\": \"\",\n            \"VolumesFrom\": null,\n            \"CapAdd\": null,\n            \"CapDrop\": null,\n            \"Dns\": null,\n            \"DnsOptions\": null,\n            \"DnsSearch\": null,\n            \"ExtraHosts\": null,\n            \"GroupAdd\": null,\n            \"IpcMode\": \"\",\n            \"Cgroup\": \"\",\n            \"Links\": null,\n            \"OomScoreAdj\": 0,\n            \"PidMode\": \"\",\n            \"Privileged\": false,\n            \"PublishAllPorts\": false,\n            \"ReadonlyRootfs\": false,\n            \"SecurityOpt\": null,\n            \"UTSMode\": \"\",\n            \"UsernsMode\": \"\",\n            \"ShmSize\": 67108864,\n            \"Runtime\": \"runc\",\n            \"ConsoleSize\": [\n                0,\n                0\n            ],\n            \"Isolation\": \"\",\n            \"CpuShares\": 0,\n            \"Memory\": 0,\n            \"CgroupParent\": \"\",\n            \"BlkioWeight\": 0,\n            \"BlkioWeightDevice\": null,\n            \"BlkioDeviceReadBps\": null,\n            \"BlkioDeviceWriteBps\": null,\n            \"BlkioDeviceReadIOps\": null,\n            \"BlkioDeviceWriteIOps\": null,\n            \"CpuPeriod\": 0,\n            \"CpuQuota\": 0,\n            \"CpusetCpus\": \"\",\n            \"CpusetMems\": \"\",\n            \"Devices\": null,\n            \"DiskQuota\": 0,\n            \"KernelMemory\": 0,\n            \"MemoryReservation\": 0,\n            \"MemorySwap\": 0,\n            \"MemorySwappiness\": -1,\n            \"OomKillDisable\": false,\n            \"PidsLimit\": 0,\n            \"Ulimits\": null,\n            \"CpuCount\": 0,\n            \"CpuPercent\": 0,\n            \"IOMaximumIOps\": 0,\n            \"IOMaximumBandwidth\": 0\n        },\n        \"GraphDriver\": {\n            \"Name\": \"devicemapper\",\n            \"Data\": {\n                \"DeviceId\": \"37\",\n                \"DeviceName\": \"docker-253:1-192941280-3a2635d8b7239a777fe02df652422d11fa7bb403fe040871b0997660e6fa284b\",\n                \"DeviceSize\": \"10737418240\"\n            }\n        },\n        \"Mounts\": [],\n        \"Config\": {\n            \"Hostname\": \"2f539696a2e0\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"ExposedPorts\": {\n                \"8080/tcp\": {}\n            },\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/usr/local/tomcat/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8-openjdk/jre/bin:/usr/lib/jvm/java-1.8-openjdk/bin\",\n                \"LANG=C.UTF-8\",\n                \"JAVA_HOME=/usr/lib/jvm/java-1.8-openjdk/jre\",\n                \"JAVA_VERSION=8u92\",\n                \"JAVA_ALPINE_VERSION=8.92.14-r1\",\n                \"CATALINA_HOME=/usr/local/tomcat\",\n                \"TOMCAT_NATIVE_LIBDIR=/usr/local/tomcat/native-jni-lib\",\n                \"LD_LIBRARY_PATH=/usr/local/tomcat/native-jni-lib\",\n                \"TOMCAT_MAJOR=8\",\n                \"TOMCAT_VERSION=8.0.36\",\n                \"TOMCAT_TGZ_URL=https://www.apache.org/dist/tomcat/tomcat-8/v8.0.36/bin/apache-tomcat-8.0.36.tar.gz\"\n            ],\n            \"Cmd\": [\n                \"catalina.sh\",\n                \"run\"\n            ],\n            \"Image\": \"tomcat:8.0.36-jre8-alpine\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"/usr/local/tomcat\",\n            \"Entrypoint\": null,\n            \"OnBuild\": null,\n            \"Labels\": {\n                \"com.docker.swarm.node.id\": \"70vxyod2eboxcxd8a0wo0ds7n\",\n                \"com.docker.swarm.service.id\": \"5d01jg29r59l7d9rs7o0y2u25\",\n                \"com.docker.swarm.service.name\": \"second_tomcat\",\n                \"com.docker.swarm.task\": \"\",\n                \"com.docker.swarm.task.id\": \"b5u1k8vbr9zs7rcouuscht0d6\",\n                \"com.docker.swarm.task.name\": \"second_tomcat.1\"\n            }\n        },\n        \"NetworkSettings\": {\n            \"Bridge\": \"\",\n            \"SandboxID\": \"6e37aebafcb111ca2e1fae9e377e0ff9f60857be123c45851e69719aee285d80\",\n            \"HairpinMode\": false,\n            \"LinkLocalIPv6Address\": \"\",\n            \"LinkLocalIPv6PrefixLen\": 0,\n            \"Ports\": {\n                \"8080/tcp\": null\n            },\n            \"SandboxKey\": \"/var/run/docker/netns/6e37aebafcb1\",\n            \"SecondaryIPAddresses\": null,\n            \"SecondaryIPv6Addresses\": null,\n            \"EndpointID\": \"\",\n            \"Gateway\": \"\",\n            \"GlobalIPv6Address\": \"\",\n            \"GlobalIPv6PrefixLen\": 0,\n            \"IPAddress\": \"\",\n            \"IPPrefixLen\": 0,\n            \"IPv6Gateway\": \"\",\n            \"MacAddress\": \"\",\n            \"Networks\": {\n                \"ingress\": {\n                    \"IPAMConfig\": {\n                        \"IPv4Address\": \"10.255.0.8\"\n                    },\n                    \"Links\": null,\n                    \"Aliases\": [\n                        \"2f539696a2e0\"\n                    ],\n                    \"NetworkID\": \"dzsgwnyjip8yqi4893da420md\",\n                    \"EndpointID\": \"0146171a88c911c30d8fc54abf87e51672cb9f4c4ed4146ff2b701d3751c0913\",\n                    \"Gateway\": \"\",\n                    \"IPAddress\": \"10.255.0.8\",\n                    \"IPPrefixLen\": 16,\n                    \"IPv6Gateway\": \"\",\n                    \"GlobalIPv6Address\": \"\",\n                    \"GlobalIPv6PrefixLen\": 0,\n                    \"MacAddress\": \"02:42:0a:ff:00:08\"\n                }\n            }\n        }\n    }\n]\n```\n\nDocker service inspect output:\n\n```\n# docker service inspect second_tomcat\n[\n    {\n        \"ID\": \"5d01jg29r59l7d9rs7o0y2u25\",\n        \"Version\": {\n            \"Index\": 1043\n        },\n        \"CreatedAt\": \"2016-07-21T19:13:14.020746042Z\",\n        \"UpdatedAt\": \"2016-07-21T19:13:14.029082747Z\",\n        \"Spec\": {\n            \"Name\": \"second_tomcat\",\n            \"TaskTemplate\": {\n                \"ContainerSpec\": {\n                    \"Image\": \"tomcat:8.0.36-jre8-alpine\"\n                },\n                \"Resources\": {\n                    \"Limits\": {},\n                    \"Reservations\": {}\n                },\n                \"RestartPolicy\": {\n                    \"Condition\": \"any\",\n                    \"MaxAttempts\": 0\n                },\n                \"Placement\": {}\n            },\n            \"Mode\": {\n                \"Replicated\": {\n                    \"Replicas\": 1\n                }\n            },\n            \"UpdateConfig\": {},\n            \"EndpointSpec\": {\n                \"Mode\": \"vip\",\n                \"Ports\": [\n                    {\n                        \"Protocol\": \"tcp\",\n                        \"TargetPort\": 8080,\n                        \"PublishedPort\": 8081\n                    }\n                ]\n            }\n        },\n        \"Endpoint\": {\n            \"Spec\": {\n                \"Mode\": \"vip\",\n                \"Ports\": [\n                    {\n                        \"Protocol\": \"tcp\",\n                        \"TargetPort\": 8080,\n                        \"PublishedPort\": 8081\n                    }\n                ]\n            },\n            \"Ports\": [\n                {\n                    \"Protocol\": \"tcp\",\n                    \"TargetPort\": 8080,\n                    \"PublishedPort\": 8081\n                }\n            ],\n            \"VirtualIPs\": [\n                {\n                    \"NetworkID\": \"dzsgwnyjip8yqi4893da420md\",\n                    \"Addr\": \"10.255.0.7/16\"\n                }\n            ]\n        }\n    }\n]\n```\n\nAnything else you need or want me to try, let me know!\n"},{"labels":[null,"enhancement1"],"text":"It would be great if docker supported a command to temporarily stop a service (i.e. shutdown all containers belonging to it) and start it over again later (i.e. restore the desired number of instances). For `replicated` services, this functionality can be mimicked by using `docker scale [servicename]=0` and `docker scale [servicename]=[desired number of instances]`. However, this option doesn't exist for services in `global` mode. With regard to them, I couldn't think of any practical workaround avoiding the deletion and re-creation of a service, which means you have to re-do all of the service configuration.\n"},{"labels":[null,"enhancement1"],"text":"Currently, it's not possible to add devices with `docker service create`, there is no equivalent for `docker run --device=/dev/foo`.\n\nI'm an author of [nvidia-docker](https://github.com/nvidia/nvidia-docker) with @3XX0 and we need to add devices files (the GPUs) and volumes to the starting containers in order to enable GPU apps as services.\nSee the discussion here: https://github.com/docker/docker/issues/23917#issuecomment-233670078 (summarized below).\n\nWe figured out how to add a volume provided by a volume plugin:\n\n```\n$ docker service create --mount type=volume,source=nvidia_driver_367.35,target=/usr/local/nvidia,volume-driver=nvidia-docker [...]\n```\n\nBut there is no solution for devices, @cpuguy83 and @justincormack suggested using `--mount type=bind`. But it doesn't seem to work, it's probably like doing a mknod but without the proper device cgroup whitelisting.\n\n```\n$ docker service create --mount type=bind,source=/dev/nvidiactl,target=/dev/nvidiactl ubuntu:14.04 sh -c 'echo foo > /dev/nvidiactl'\n$ docker logs stupefied_kilby.1.2445ld28x6ooo0rjns26ezsfg\nsh: 1: cannot create /dev/nvidiactl: Operation not permitted\n```\n\nIt's probably equivalent to this:\n\n```\n$ docker run -ti ubuntu:14.04                      \nroot@76d4bb08b07c:/# mknod -m 666 /dev/nvidiactl c 195 255\nroot@76d4bb08b07c:/# echo foo > /dev/nvidiactl\nbash: /dev/nvidiactl: Operation not permitted\n```\n\nWhereas the following works (invalid arg is normal, but no permission error):\n\n```\n$ docker run -ti --device /dev/nvidiactl ubuntu:14.04\nroot@ea53a1b96226:/# echo foo > /dev/nvidiactl\nbash: echo: write error: Invalid argument\n```\n"},{"labels":[null,"enhancement1",null],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.0-rc4\n API version:  1.24\n Go version:   go1.6.2\n Git commit:   e4a0dbc\n Built:        Wed Jul 13 03:39:43 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.0-rc4\n API version:  1.24\n Go version:   go1.6.2\n Git commit:   e4a0dbc\n Built:        Wed Jul 13 03:39:43 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 1\n Running: 1\n Paused: 0\n Stopped: 0\nImages: 54\nServer Version: 1.12.0-rc4\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 71\n Dirperm1 Supported: false\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge null host overlay\nSwarm: active\n NodeID: 33ops9juo9ea1twbfq2dyt89y\n IsManager: Yes\n Managers: 2\n Nodes: 5\n CACertHash: sha256:cef0da32ea05dd1038a5b8ae1a3a6956b6a5efa2d2fcad535a696dd568220197\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: apparmor seccomp\nKernel Version: 3.13.0-86-generic\nOperating System: Ubuntu 14.04.4 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 12\nTotal Memory: 94.42 GiB\nName: irvm-ggallag\nID: WA3H:N54J:H7F3:CQV6:74ZX:IWIZ:U6XG:2VCB:45LP:LDD5:FHB6:7CWZ\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nUbuntu 14.04 VM under KVM running Docker enginer 1.12 RC4\n\n**Steps to reproduce the issue:**\n1. docker service create <name>\n2. inside docker image NFS mount\n\n**Describe the results you received:**\nI can run \"docker run --privileged\" to allow an NFS mount from within my container, however there is no way to pass this --privileged flag to \"docker service\" and if I do not pass the --privileged flag, the contain would error internally on the mount like:\n\n```\nmount: permission denied\n```\n\n**Describe the results you expected:**\nI should be able to have my container mount an NFS server from within it. I do not want to do this externally or via a docker volume, for example, I am trying to drive a huge number of parallel containers running NFS mounts and I/O individually.\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n"},{"labels":["enhancement1"],"text":"This is somewhat of a follow-up to #10839. To ease cleanup I propose the following:\n- add label `generated-name` to containers that have auto-generated names\n- add label `build` to containers that are leftovers from broken `docker build` calls (which use auto-generated names as well)\n\nThis way one can easily remove build leftovers via\n\n`docker rm $(docker ps -qaf label=build)`\n\nand all containers with auto-generated names via\n\n`docker rm $(docker ps -qaf label=generated-name)`\n\nTo make sure these labels won't clash with existing ones, a prefix like `docker-` could also be used.\n\nI think this would make cleaning up easier and enable automated cleanups (e.g. via cron) without interfering with how people use docker right now.\n"},{"labels":[null,null,"enhancement1"],"text":"Building Oracle Database (non-XE) images is easy now that we have the _--shm-size_ option in **docker-build**(1).   But still we have to build, run (with the -h option) & commit.  The Enterprise Manager creates its repository based on the hostname, so...\n\nThe _-h_ (_--hostname_) option would be a welcome addition to Docker\n\nI'm sure that many other projects would benefit from this as well.\n\nRegards,\nR.\n\nPD: Setting the hostname and updating /etc/hosts didn't work.  \n"},{"labels":[null,null,null,"enhancement1"],"text":"This is a follow up to https://github.com/docker/docker/issues/23710. It's currently not possible to obtain logs for a service through `docker logs`. Docker 1.12 allows you to set a logging-driver for a service so that you can aggregate logs, but ideally we'd have support for viewing those logs (or the latest N entries) through the `docker logs` command (or equivalent, e.g. `docker service logs`).\n\nFrom the original issue:\n\nIn 1.12, I can create services, but there's no easy way to get the logs for either the service as a whole (all tasks), or an individual task;\n\n```\ndocker service create --name web -p 80:80 nginx:alpine\n\ndocker service ls\nID            NAME  REPLICAS  IMAGE         COMMAND\nbioa2ndrqp3h  web   1/1       nginx:alpine\n\ndocker logs bioa2ndrqp3h\nError: No such container: bioa2ndrqp3h\n\ndocker service ps web\nID                         NAME   SERVICE  IMAGE         LAST STATE         DESIRED STATE  NODE\n18t5r6j642rhncm76vbc2s8h5  web.1  web      nginx:alpine  Running 7 minutes  Running        moby\n\ndocker logs 18t5r6j642rhncm76vbc2s8h5\nError: No such container: 18t5r6j642rhncm76vbc2s8h5\n```\n\nBest I could come up with is;\n\n```\ndocker logs $(docker inspect --format \"{{.Status.ContainerStatus.ContainerID}}\" 18t5r6j642rhncm76vbc2s8h5)\n```\n\nBut this only gives me logs for a single task/container\n"},{"labels":[null,"enhancement1"],"text":"Hi,\nI'm a Kubernetes maintainer, and works mostly on multiarch and deployment things.\nI'm writing a proposal for multi-arch Kubernetes here: https://github.com/kubernetes/kubernetes/pull/26863, and part of the conclusion there is that manifest lists are a perfect fit for Kubernetes.\nNow we're trying to dramatically remove cluster deployment complexity, and as a part of that, I think it would be great to try to remove the image naming hack. (Now we're using `registry/binary-arch:version` as the image naming convention, which does its job, but a manifest list would be better)\n\nI know @estesp has a tool for this, and if nothing else comes out of this, we probably have to use that.\nBut it would be great if we could get a `manifest` command to docker.\n\nAnyway, I do not imagine this as a \"easy to fix\"-thing; rather something that's quite a big change, so therefore we probably need an assignee from the Docker team, and Docker have to stand behind the things proposed, otherwise we shouldn't do this right now.\n\nIn fact, the tasks that are yet to be done are well-documented here already: https://integratedcode.us/2016/04/22/a-step-towards-multi-platform-docker-images/ (thanks!)\n\nExample how the manifest command could work (very rough, appreciating discussion):\n\n``` console\n$ docker manifests\nID          NAME                     IMAGE                  (IMAGE ID)     OS         ARCH       VARIANT\n{sha}       luxas/debian:jessie      debian:jessie          {debian sha}   linux      amd64      N/A\n                                     armhf/debian:jessie    {sha }         linux      arm        GOARM=7\n                                     aarch64/debian:jessie  {sha }         linux      arm64      N/A\n$ docker manifests append luxas/debian:jessie ppc64le/debian:jessie --os linux --arch ppc64le\n$ docker manifests\nID          NAME                     IMAGE                  (IMAGE ID)     OS         ARCH       VARIANT\n{sha}       luxas/debian:jessie      debian:jessie          {debian sha}   linux      amd64      N/A\n                                     armhf/debian:jessie    {sha }         linux      arm        GOARM=7\n                                     aarch64/debian:jessie  {sha }         linux      arm64      N/A\n                                     ppc64le/debian:jessie  {sha }         linux      ppc64le    N/A\n$ docker images\nREPOSITORY             TAG           IMAGE ID            CREATED             SIZE\n?luxas/debian           jessie        {sha}\n...\n$ docker push luxas/debian:jessie\n```\n\nOpen questions:\n- Should we show the manifest list at all in `docker images`?\n- Should the manifest list be considered an \"image\" or just a placeholder for an image?\n- Should it be possible to `docker build --os x --arch y --variant z -t {manifest name} .`?\n- How should we display that a manifest list was downloaded, but not all images for all platform in it?\n- Can we add content addressability to a manifest list?\n- etc....\n\nRelated: https://github.com/docker/docker/issues/22816 https://github.com/docker/docker/issues/22106 https://github.com/docker/distribution/issues/200\n\n@tianon @estesp @jfrazelle @cpuguy83 @calavera @icecrime @jstarks @mikedanese  @thockin @vishh @david-mcmahon @stevvooe @paralin \n_Note: These are my personal ideas and might not represent the rest of the Kubernetes team_\n"},{"labels":["enhancement1"],"text":"Docker allows users to create their own authorization plugin. And 3c157713b31f542a4180e31da4cae7d677330a6f introduces TLS user information so that the plugins can know the name of the user who send a request.\n\nIf there is a way to get the owner of a docker object(service, container, ...), ownership-based access control is possible. For example, an authorization plugin can deny a request to delete a service owned by another. It will be useful for a docker cluster used by a lot of users.\n\nThere may be two possible solutions:\n1. Docker daemon includes ownership information, basd on TLS user information, in metadata of docker objects and has APIs to provide the information.\n2. Docker daemon allows authorization plugins to add ownership information into docker objects.\n"},{"labels":[null,null,"enhancement1",null],"text":"**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.0-rc4\n API version:  1.24\n Go version:   go1.6.2\n Git commit:   e4a0dbc\n Built:        \n OS/Arch:      linux/amd64\nServer:\n Version:      1.12.0-rc4\n API version:  1.24\n Go version:   go1.6.2\n Git commit:   e4a0dbc\n Built:        \n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 1\n Running: 0\n Paused: 0\n Stopped: 1\nImages: 2\nServer Version: 1.12.0-rc4\nStorage Driver: devicemapper\n Pool Name: docker-8:1-561338-pool\n Pool Blocksize: 65.54 kB\n Base Device Size: 10.74 GB\n Backing Filesystem: xfs\n Data file: /dev/loop0\n Metadata file: /dev/loop1\n Data Space Used: 89.33 MB\n Data Space Total: 107.4 GB\n Data Space Available: 9.149 GB\n Metadata Space Used: 692.2 kB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 2.147 GB\n Thin Pool Minimum Free Space: 10.74 GB\n Udev Sync Supported: true\nCgroup Driver: cgroupfs\n Deferred Removal Enabled: false\n Deferred Deletion Enabled: false\n Deferred Deleted Device Count: 0\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\n WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\n Library Version: 1.02.107-RHEL7 (2016-06-09)\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: null bridge host overlay\nSwarm: active\n NodeID: cwi3c7ufsp9isz655oc8kpxpg\n IsManager: Yes\n Managers: 1\n Nodes: 2\n CACertHash: sha256:39e643b316da5d78c3fe32c506895fda2db36667efbf1614e8cb667b9db757f6\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 3.10.0-327.22.2.el7.x86_64\nOperating System: CentOS Linux 7 (Core)\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 3.614 GiB\nName: managed-instance-4\nID: KH5Y:JLVS:YGQL:AACO:WXQJ:EQZ2:FY5M:OMHV:WWWG:VOBB:NCUZ:RXPM\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nWARNING: bridge-nf-call-ip6tables is disabled\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nVM on Google cloud\n\n**Steps to reproduce the issue:**\n1. Documentation (https://docs.docker.com/engine/reference/commandline/service_create/) says \n\n> Swarm administrators add node.labels for operational purposes by using the docker node update command.\n1. Checking how to add label to a node \n\n```\n$ sudo docker node update --help\nUsage:  docker node update [OPTIONS] NODE\nUpdate a node\nOptions:\n      --availability string   Availability of the node (active/pause/drain)\n      --help                  Print usage\n      --membership string     Membership of the node (accepted/rejected)\n      --role string           Role of the node (worker/manager)\n```\n\n**Describe the results you received:**\nThere is no option available to add/update label to a node\n\n**Describe the results you expected:**\nHave parameter to add label to node.labels context of the node using _docker node update_ command\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\nPermanent\n"},{"labels":[null,"enhancement1",null],"text":"On the update of a service, if no changes are made to the task template or the endpoint, no update will be carried out. The service is considered converged at this point no action will be taken by the orchestrator.\n\nThere are several cases where one may want to force a re-scheduling of all tasks for a service. A few examples are the following:\n1. A user has a change in their cluster that is unobservable to the orchestrator and would like to reschedule all tasks.\n2. One or more new nodes has been added and the user wants to rebalance the tasks among them in a controlled fashion.\n\nThis issue was partially described in https://github.com/docker/docker/issues/24066#issuecomment-231449232.\n\nThe proposal is to add a flag to `docker service update` that would force the service to do a rolling update, without changes:\n\n```\ndocker service update --reschedule <myservice>\n```\n\nAnother option would be to make this a top-level command:\n\n```\ndocker service reschedule <myservice>\n```\n\nThis approach is similar to how `docker service scale` is implemented.\n\nThe implementation would perturb a meaningless value on one the target task spec such that the orchestrator detects a change starts a rolling update.\n\nTasks required to close this out:\n- [ ] Add a field on TaskSpec or ServiceSpec that is perturbed when an update is required (swarmkit).\n- [ ] Add field to engine-api\n- [ ] Add flag or top-level command to carry out this rescheduling\n\ncc @aluzzardi @aaronlehmann @tiborvass \n"},{"labels":[null,"enhancement1"],"text":"I'm noticing that when I'm using `docker service update --image` it will stop and remove all the containers, pull the image and then restart the containers.\n\nDepending on the size of the application, then can cause the application to be unresponsive for some time. If there is a problem pulling the image (typo), then your application will go down while you're identifying the problem.\n\nIs there a reason not to pull the image first, then apply updates?\n"},{"labels":[null,"enhancement1"],"text":"Feature Request to support multiple inheritance in Docker.\n\nI have a scenario where I have several projects, each with their own docker image and I have larger images containing more than one project but which require re-building each one again as I can't inherit from multiple images, even though the end state contents are the same as the single project images. This is very wasteful of space, bandwidth and builds.\n\nIt's a massively useful feature to be able to inherit from multiple existing docker images using `FROM` - it would save build time, bandwidth and storage to be able to re-use the image layers.\n\nSince docker images are just filesystem layers conflict resolution could be as simple as default last write wins (ie. last FROM line wins) with configurability for first write wins or specific image layer wins or possibly an optional number priority suffix to the `FROM` line to maintain backwards compatability.\n"},{"labels":[null,"enhancement1"],"text":"When I use Docker networking, I usually miss one feature I would call `transparent bridge`. I've been enlightened by [Virtualbox bridged networking](https://www.virtualbox.org/manual/ch06.html#network_bridged), so the basic idea is excerpted as follows,\n\n_With potential `transparent bridge`, Docker would use a `networking plugin` on your host system that filters data from your physical network adapter._ \n\n_This allows Docker to intercept data from the physical network and inject data into it, effectively creating a new network interface in software. When a container is using such a new software interface, it looks to the host system as though the container were physically connected to the interface using a network cable: the host can send data to the container through that interface and receive data from it._ \n\n_This means that you can set up routing or bridging between the container and the rest of your network._\n\nFor this to work, Docker needs a `networking plugin`  on the host system. Is this kind of plugin already existing or beneficial to make?\n"},{"labels":[null,"enhancement1"],"text":"Currently it's only supported on run\n\n(working on a patch)\n"},{"labels":[null,"enhancement1"],"text":"It would be nice to be able to override the health check of a task. A use case would be when a task keeps flapping WRT its health check. It would be useful to put the task as unhealthy instead of letting if flap while you debug the task. \n"},{"labels":[null,null,"enhancement1"],"text":"There are some things that I want to run on docker, but are not fully engineered for dynamic infrastructure. Ceph is an example. Unfortunately, its monitor nodes require a static ip address, otherwise, it will break if restarted. See here for background: https://github.com/ceph/ceph-docker/issues/190\n\n`docker run` has an `--ip` and `--ip6` flag to set a static IP for the container. It would be nice if this is something we can do when creating a swarm service to take advantage of rolling updates and restart on failures.\n\nFor example, when we create a service, we could pass in a `--static-ip` and `--static-ip6` option. Docker would assign a static ip for each task for the life of the service. That is, as long as the service exists, those ip addresses would be reserved and mapped to each task. If the task scales up and down, then more ip addresses are reserved or relinquished. The ip address is then passed into each task as an environment variable such as `DOCKER_SWARM_TASK_IP` and `DOCKER_SWARM_TASK_IP6`\n"},{"labels":[null,"enhancement1"],"text":"We are using docker to run microservices. These microservices are all HTTP services. According to the documentation, it is possible to perform a health check by running a command inside the container.\n\nHowever, it would be much simpler (if the process in each container is a HTTP server) to perform the check over HTTP. The operator would define an endpoint, say `/health` and port `443, 80, etc`, and docker would periodically make an HTTP request to the endpoint running on the port to determine if the container is healthy or not.\n"},{"labels":[null,"enhancement1"],"text":"It would be nice to be able to automatically re-balance tasks when a new node joins.\n\nFor example, we have 4 nodes, running a redis service with 5 tasks. When a new node joins, swarm should automatically re-balance the tasks so that all nodes will be running 1 redis task.\n"},{"labels":[null,null,"enhancement1",null,null],"text":"There are a few conditions where we should not route traffic to a task:\n1. If the task is in a `STARTING` state.\n2. If the task has a desired state of `SHUTDOWN` and is still `RUNNING`.\n\nThis will ensure that tasks will not receive traffic before they are ready and after they may be shutting down.\n\nLet me know further details are needed.\n\n@mrjana @aluzzardi\n"},{"labels":[null,"enhancement1",null],"text":"The engine should probably have flags such as `--swarm-join`. This would help automation.\n\n/cc @icecrime @tonistiigi @tiborvass @stevvooe \n"},{"labels":[null,"enhancement1"],"text":"```\n  accept      Accept a node in the swarm\n  reject      Reject a node from the swarm\n  promote     Promote a node as manager in the swarm\n  demote      Demote a node as manager in the swarm\n```\n\nEither we do everything through `node update` or we have commands for everything, but right now it's incosistent\n\n/cc @dnephin @aanand @stevvooe \n"},{"labels":[null,"enhancement1",null],"text":"SwarmKit is able to keep a list of different managers, connect to a random one and re-connect to a different one if it fails.\n\nThis is usually a good approach when bootstrapping a cluster (you don't want to be tied to a single manager).\n\nThis reinforces the statement that join should not be synchronous - the agent might fail connecting to a manager, but it's not a problem since it'll retry and eventually manage to connect to one. /cc @stevvooe @tonistiigi @icecrime\n"},{"labels":[null,"enhancement1"],"text":"When loading a plugin (`docker plugin load`, `docker plugin install`), there needs to be a way to pass arguments/options to the plugin.\n\nFor example: if there was a btrfs volume driver, the driver would need to know what device(s) to use.\nNote this is more than just adding the device(s) to the running container since device names will be different from host to host.\n\nping @tiborvass @anusha-ragunathan \n"},{"labels":[null,"enhancement1"],"text":"I wan't to add Avro-flume log driver to push all data on log centralizer\n"},{"labels":["enhancement1"],"text":"Looks like most of the argument checking is done in the runconfig/opts/parse client-side. This means that each 'client' (Docker, Compose, etc...) has to do it's own checks. Specifically hit this with restart policies, right now compose accepts an invalid policy and doesn't error out (as the engine doesn't check validity). \n\nThis seems (?) like a fairly straightforward change (refactoring the checks for each piece incrementally perhaps, starting with restart-policy). \n\nGuessing they should / would move to daemon/container?\n\nI'd like to work on this as a first contribution, any pointers / someone willing to mentor me through it?\n"},{"labels":[null,"enhancement1"],"text":"It is possible to configure network drivers at network create time:\n\n```\ndocker network create -d overlay -o \"com.docker.network.mtu=9001\" foobar\n```\n\nIn some cases, you need to change an option for _all_ networks created on a system. Right now, the way to accomplish that will be to change _every_ compose file, Makefile, bash script, or other deploy mechanism to accommodate this host-wide requirement. This is not a scalable solution.\n\nThere should be a ui/ux that would allow a user to set defaults system-wide: \"all overlay networks will need to have this value for that option\".\n\nImplementing an alternate driver that has the needed behavior is not a viable workaround. It requires the same amount of effort to change _all_ compose/make/bash/deploy scripts to use the alternate driver, so this won't address the issue.\n"},{"labels":[null,"enhancement1"],"text":"**Output of `docker version`:** docker 1.12-rc2\n\nThis is more of a feature request than a bug report. Node labels are very important when it comes to setting constraints for where you want services to run in the cluster but currently there's no way of doing it. I have found that I am able to change the name of a node and modify labels using the API - this is just a matter of adding support to the Docker CLI.\n"},{"labels":["enhancement1",null],"text":"It would be great to be able to set the /etc/localtime link on the docker VM via the UI\n\nPS C:> date\nFri Jun 24 09:57:56 AUSEST 2016\nPS C:> docker run --rm -it centos date\nThu Jun 23 23:57:57 UTC 2016\nPS C:>\n"},{"labels":["enhancement1",null],"text":"It would be great if there was the log output of the docker daemon right in the GUI for mac.\n"},{"labels":["enhancement1",null],"text":"Error during installation:\n\n```\n#  curl -fsSL https://get.docker.com/ | sh\nError: you are not using a 64bit platform.\nDocker currently only supports 64bit platforms.\n```\n\nEnvironment:\n\n```\n[root@dyn423 ~]# uname -a\nLinux dyn423 4.7.0-rc3-next-20160617 #4 SMP Thu Jun 23 10:47:19 AEST 2016 ppc64le ppc64le ppc64le GNU/Linux\n[root@dyn423 ~]# more /etc/redhat-release\nCentOS Linux release 7.2.1511 (AltArch)\n```\n\nCentos with custom kernel in a KVM VM\n\nSo it is a 64bit VM so the bug is the script isn't generating the right error message.\n\nFeature request - can Centos packages for ppc64le be added to https://yum.dockerproject.org/repo/main/centos/7/Packages/\n\nThank you.\n"},{"labels":[null,null,"enhancement1"],"text":"## Description\n\nToday Swarm mode allows users to specify a group of homogenous containers which are meant to be kept running with the `docker service` CLI.  This abstraction, while powerful, may not be the right fit for containers which are _intended_ to eventually terminate or only run periodically.\n\nConsider, for instance:\n- An admin who wishes to allow users to submit long-running compiler jobs on a Swarm cluster\n- A website which needs to process all user uploaded images into thumbnails of various sizes\n- An operator who wishes to periodically run `docker rmi $(docker images --filter dangling=true -q)` on each machine\n## Problem\n\nThough some use cases could potentially be implemented by a service which pulls jobs off a distributed queue, there are some issues with this approach:\n1. It adds an operational burden of creating, administrating, and ensuring the health of such a queue.  For many, this will kick up the barrier to entry of performing such tasks.\n2. It does not necessarily ensure that failed jobs can be re-run the correct number of times or with the correct parameters.  Like above, this burden has now been offloaded to the user instead of being natively orchestrated.\n3. It does not allow for easy customization of job parallelism settings.\n\nAnything which is intended to be run periodically (such as the image garbage collection example above), could potentially cause a thundering herd problem if the scheduling is not handled by Swarm.  Imagine, for instance, that a user creates a `docker service` to periodically run a command inside the containers using normal old `cron`.  If these all wake up and attempt to execute at the same time during a time when production traffic is surging, it may be an issue for more critical production web services.  While it may help to mitigate the problem if the users do proper capacity planning and use flags such as `--reserve-cpu`, separating these concerns early on, especially when a temporal element is involved, seems to prudent.  (thanks to @jpetazzo who originally pointed out these concerns to me, and probably will have good insight as well)\n\nThis issue requests and outlines a proposed CLI, to get the ball rolling on discussion, track the issue, and gather information about potential use cases.\n## Proposal\n\nA new top-level command, `docker jobs`, could be introduced.  It would allow to specify that a container should be run X times, or every Y interval of time.  It could also be used to check up on these jobs.\n\nExamples.\n\nRun batch job once:\n\n``` console\n$ docker jobs create \\\n    -e S3_INPUT_BUCKET \\\n    -e S3_OUTPUT_BUCKET \\\n    -e IMAGE_NAME \\\n    nathanleclaire/convert\n3x3bq2ibh1qe\n\n$ docker jobs wait 3x3bq2ibh1qe; docker jobs ls\nID              NEXT PREV           FINISHED   FAILURES IMAGE\n3x3bq2ibh1qe    -    3 seconds ago  1/1        0/1      nathanleclaire/convert \n```\n\nRun batch job 16 times:\n\n```\n$ docker jobs create \\\n    --runs 16 \\\n    --parallel 3 \\\n    nathanleclaire/failer\nbku4f1s1ncm0\n\n$ docker jobs ls\nID            NEXT  PREV           FINISHED   FAILURES IMAGE\n3x3bq2ibh1qe  -     2 minutes ago  1/1        0/1      nathanleclaire/convert \nbku4f1s1ncm0  Now   Now            4/16       6/10     nathanleclaire/failer\n```\n\nRun a task every hour:\n\n```\n$ docker jobs create \\\n    --every 1hr \\\n    nathanleclaire/hourlytask\nddh7pqvgbd8l\n\n$ # One hour later...\n\n$ docker jobs ls\nID            NEXT PREV         FINISHED  FAILURES IMAGE\n3x3bq2ibh1qe  -    1 hour ago   1/1       0/1      nathanleclaire/convert \nbku4f1s1ncm0  -    1 hour ago   16/16     6/20     nathanleclaire/failer \nddh7pqvgbd8l  1hr  1 minute ago 1/1       0/1      nathanleclaire/hourlytask\n```\n\nInteractively re-run a job:\n\n``` console\n$ docker jobs restart 3x3bq2ibh1qe\nRunning job 3x3bq2ibh1qe again from the beginning\n```\n\nAlternatively, `service` model could be expanded to accommodate this?  But it seems they would be easier to manage (for users) as separate things.\n\nPlease let me know what you think (when you get a chance -- please focus on 1.12 first and foremost ;) ) @aluzzardi @vieux @stevvooe @abronan and others.\n## Cute animal\n\nSince I'm requesting feature, least I can do is provide cute animal picture.\n\n![image](https://cloud.githubusercontent.com/assets/1476820/16287318/23914b16-3897-11e6-99e8-5b92f84ec517.png)\n"},{"labels":[null,"enhancement1"],"text":"For 1.13 and later, I would like to suggest supporting \"indexed jobs\" in `docker service create`.\n\nThere has been a similar proposal in the Kubernetes community (seems not implemented yet)\nhttps://github.com/kubernetes/kubernetes/blob/v1.3.0-beta.2/docs/design/indexed-job.md\n\nExample:\n\n``` bash\n$ echo \"apple banana cherry\" > fruits.txt\n$ echo \"green yellow red\" > colors.txt\n\n$ kubectl run say-fruit --image=busybox \\\n     --per-completion-env=FRUIT=\"$(cat fruits.txt)\" \\\n     --per-completion-env=COLOR=\"$(cat colors.txt)\" \\\n     -- \\\n     sh -c 'echo \"Have a nice $COLOR $FRUIT\" && sleep 5'\n(pod0) Have a nice green apple\n(pod1) Have a nice yellow banana\n(pod2) Have a nice red cherry\n```\n\nThe corresponding command line for `docker service` will be like this:\n\n``` bash\n$ docker service create --replicas 3 --name say-fruit \\\n     --per-completion-env=FRUIT=\"$(cat fruits.txt)\" \\\n     --per-completion-env=COLOR=\"$(cat colors.txt)\" \\\n     --restart-condition=on_failure\n     busybox\n     'echo \"Have a nice $COLOR $FRUIT\" && sleep 5' \n```\n\n(I feel the name of `--per-completion-env` is too long and unclear.. I prefer something else like `--scatter`)\n\nI would like to hear thoughts from maintainers about having this feature.\nI think we need `docker service exec` and perhaps `docker service cp` as well for taking advantage of the feature.\n\nI believe the feature can be also used for effectively executing `test-integration-cli` in parallel across multiple Docker machines.\nSo there will be significant benefit for developers :smiley:\n"},{"labels":[null,null,"enhancement1",null,null],"text":"In 1.12, I can create services, but there's no easy way to get the logs for either the service as a whole (all tasks), or an individual task;\n\n```\ndocker service create --name web -p 80:80 nginx:alpine\n\ndocker service ls\nID            NAME  REPLICAS  IMAGE         COMMAND\nbioa2ndrqp3h  web   1/1       nginx:alpine\n\ndocker logs bioa2ndrqp3h\nError: No such container: bioa2ndrqp3h\n\ndocker service tasks web\nID                         NAME   SERVICE  IMAGE         LAST STATE         DESIRED STATE  NODE\n18t5r6j642rhncm76vbc2s8h5  web.1  web      nginx:alpine  Running 7 minutes  Running        moby\n\ndocker logs 18t5r6j642rhncm76vbc2s8h5\nError: No such container: 18t5r6j642rhncm76vbc2s8h5\n```\n\nBest I could come up with is;\n\n```\ndocker logs $(docker inspect --format \"{{.Status.ContainerStatus.ContainerID}}\" 18t5r6j642rhncm76vbc2s8h5)\n```\n\nBut this only gives me logs for a single task/container\n"},{"labels":[null,"enhancement1"],"text":"I would like to be able to \"unset\" an ENTRYPOINT at the command line when creating a container.\n\nUsing this Dockerfile and entrypoint.sh as an example:\n\nDockerfile:\n\n```\nFROM alpine\nADD entrypoint.sh /entrypoint.sh\nRUN chmod 755 /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\nCMD echo foobar\n```\n\nentrypoint:\n\n```\n#!/bin/sh\necho \"I am an entrypoint\"\nexec \"$@\"\n```\n\nIf I wanted to unset the entrypoint, I would like to be able to run this: `docker run --entrypoint='' --rm -it image echo foo`\n\nI would expect this to produce the following output:\n\n```\nfoo\n```\n\nbut it does the following instead:\n\n```\nI am an entrypoint\nfoo\n```\n\nProviding some way for `docker run` or `docker create` to force the ENTRYPOINT to be null without having to create another image would be great.\n"},{"labels":[null,null,"enhancement1"],"text":"Say I have an image FOOBASE in a private registry (hostname:port PRIVREGHOSTNAME:5000)\n\nI want to use this image in a Dockerfile for a new image FOONEW. Currently, it is done as follows:\n# Dockerfile\n\nFROM PRIVREGHOSTNAME:5000/FOOBASE\n...\n\nThat is, the hostname of the private registry is in the Dockerfile.\n\nNow say, I want to change the registries. I have to change the Dockerfile of FOOBASE, rebuild it, and rebuild the image FOONEW that depends on FOOBASE. Obviously, changing the Dockerfile is not desirable. The registry hostname should not be in the Dockerfile\n\nInstead, a better solution is to define a new parameter --registry-url for docker build. E.g.,\n\ndocker build -t . --registry-url PRIVREGHOSTNAME:5000\n\nSo by default, the image referenced in Dockerfile is searched first in the registry-url and then in Dockerhub.\n"},{"labels":["enhancement1"],"text":"Hi, All, \n\nIt is really Great for docker to support daemon events (https://github.com/docker/docker/pull/22590) in future's docker 1.12.0. Thank to @yongtang \n\nCurrently docker supports **reload** daemon events which is really important, while in my own opinion, we can enhance this to support more, such as \"stop\" and \"start\".\n\nHere is my reason:\nFor one user, he may use event policy to monitor all changes on the docker engine. For example, when service docker restart, user can only get container events such as kill or start, while he has no sense of the scene that docker has restarted. If we can support stop and start daemon events, user can have more info about what indeed happened in the scene.\n\nThis is indeed important for Swarm which uses events to get all information of Docker Engine.\n"},{"labels":["enhancement1"],"text":"Fedora 24 is not on the `https://yum.dockerproject.org/repo/main/fedora/` repos. Subsequently trying to install docker-engine with yum is failing. Fedora 24 is currently in public beta and will be going for GA later this month.\n"},{"labels":[null,null,"enhancement1"],"text":"I want to add storage limit to overlayfs. Here's my plan:\n1. Use lvm to create a storage-limited directory(just create a device&mount directory).\n2. Use this directory as the upper layer.\n\nApparently this needs to do some changes to the code. I wonder whether this plan is feasible or not.This is not a proposal, just for individual use.\nAny advice? Thank you!\n\n<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\n**Output of `docker version`:**\n\n```\n(paste your output here)\n```\n\n**Output of `docker info`:**\n\n```\n(paste your output here)\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n\n**Steps to reproduce the issue:**\n1.\n2.\n3.\n\n**Describe the results you received:**\n\n**Describe the results you expected:**\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n"},{"labels":["enhancement1"],"text":"Currently, there is no event for the \"untag\" event.\n\nAn example.  Given the following commands:\n\n```\n$ docker pull busybox:latest\n$ docker tag busybox:latest fish:bloop\n$ docker rmi fish:bloop\n```\n\nThen I get the following events:\n1. image pull\n2. image tag\n3. _Nothing..._ I expected an image untag event.\n\nThe use-case for this is for [docker-gc](https://github.com/docwhat/docker-gc) which could pro-actively remove records for any tags that are deleted.  It's an optimization, since it'll do the right thing on its own eventually.\n"},{"labels":[null,"enhancement1"],"text":"I have a setup where I have a common HTTP reverse proxy container for multiple other container. I would like that now that I can put other containers in their own networks, that one common HTTP reverse proxy container would be automatically available in each of that networks. Currently it seems that this is not possible. That there is no way to say that one Docker network should inherit for example everything from another (in which reverse proxy container is).\n"},{"labels":[null,"enhancement1",null],"text":"This is a proposal to add simple service discovery for containers on the single-host bridge network (which I think is NAT on Windows server) by rewriting the `c:\\Windows\\System32\\drivers\\etc\\hosts` file. This is also how [service discovery works on Linux on the bridge network](https://docs.docker.com/engine/userguide/networking/default_network/configure-dns/).\n\nThe IPAddress of started containers is a available and I've checked that they're routable from other containers:\n\n```\ndocker inspect --format '{{ json .NetworkSettings.Networks.nat.IPAddress }}' 8dccf7d0a4f0441820f6cb7991390b1b0b3ac8c660c1db514e6435e404dcca5e\n\"172.27.160.54\"\n```\n\nStefan has some old hackery, but I think that was necessary because `inspect` was not working on older Windows releases (eg. TP4): https://github.com/docker/docker-birthday-3/pull/165/files#diff-32b1d4f4a16110f0207b0c356fb94f48R2\n\ncc @JMesser81 @taylorb-microsoft @StefanScherer @mavenugo \n"},{"labels":[null,"enhancement1"],"text":"There is a strong need in provisioning/building docker instances to have some sort of mechanism of copying temporary data during the build process to a docker instance.  Later that temporary data can be (and often is) removed.  In some cases, this could be the installation of a local repository without adding credentials to the docker image.  In other instances, (e.g. gentoo), this might be several gigabytes worth of binary packages, only some of which are needed but many are used in other repositories.  The mountable volume mechanism seems kludgy at best considering that in many cases, I'll likely already have much of what I want on my local host.  A share with a temporary mount point only available during build would be far more preferable.\n\nNote: I'm new to docker, but I feel like this issue has not been properly addressed.  \n"},{"labels":["enhancement1",null],"text":"This is just a feature request.\n\nSo, would be nice to have docker beta as part of chocolatey:\nhttps://chocolatey.org/packages?q=docker\n"},{"labels":[null,"enhancement1"],"text":"A significant use case of docker logs with Java applications is the stack trace, which is by nature multiline. Right now each line becomes a new event, and while this can theoretically be pieced together by later stream processing, it is a much more complex problem to solve there than at the source of the serialized output of `stdout` or `stderrr` (which is where the docker log driver is located).\n"},{"labels":["enhancement1"],"text":"Currently Docker supports stats and events as endpoints on the API, which is great for a lot of things, however for people who want to collect these in a reliable way for safe keeping (or forwarding to something like prometheus) it's prone to losing information... and in order to monitor all container's stats, client has to listen the events api (or poll /containers/json) to determine if it should start collecting stats for a new container.\n\nThe use-case I'd like to support is a reliable way to forward events and stats to other external services without the races inherent in the above design, potential for duplicates, etc.\n"},{"labels":["enhancement1"],"text":"## Problem statement\n\nOne of the issues we hear about the most is how to reclaim disk space used by docker. I suggest to start small by adding a way to **understand and visualize** how disk space is used, before building additional support for managing this data.\n## Possible solution\n\nOne simple idea would by to provide the equivalent of a `docker df`, which would show you:\n- Space used by images (tagged versus untagged?)\n- Space used by containers (running versus stopped?)\n- Space used by volumes (referenced versus dangling?)\n\nA natural evolution of that would be to offer one-liners to reclaim space consumed by unused images, dangling volumes, or stopped containers.\n\nPing @docker/core-maintainers: I'm interested to hear what you think!\n"},{"labels":[null,"enhancement1"],"text":"Quoting [https://github.com/docker/docker/blob/master/docs/security/seccomp.md](https://github.com/docker/docker/blob/master/docs/security/seccomp.md):\n\n> Note: seccomp profiles require seccomp 2.2.1 and are only available starting with Debian 9 \"Stretch\", Ubuntu 15.10 \"Wily\", Fedora 22, CentOS 7 and Oracle Linux 7. To use this feature on Ubuntu 14.04, Debian Wheezy, or Debian Jessie, you must download the latest static Docker Linux binary. This feature is currently not available on other distributions.\n\nIn Ubuntu 14.04 I'm able to install the Ubuntu 16.04 kernel (currently 4.4.8) by installing the [linux-generic-lts-xenial](http://packages.ubuntu.com/trusty/linux-generic-lts-xenial) package _and_ the current version of [seccomp](http://packages.ubuntu.com/trusty-backports/seccomp) (2.2.3) by installing from the [trusty-backports](http://packages.ubuntu.com/trusty-backports/allpackages) repository.  All the SECCOMP kernel configuration variables are enabled in /boot/config-$(uname -r).\n"},{"labels":[null,"enhancement1"],"text":"Hi! First of all - thanks for great tool guys!\n\nI base on https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#build-cache\n\nIs it possible to have `--no-cache=true` for a single instruction in Dockerfile? \n\nThanks \n"},{"labels":[null,"enhancement1"],"text":"From distribution: https://github.com/docker/docker/blob/master/distribution/pull_v2.go#L619\n\nIt would be great to get this wired up in engine so that images get OS variant metadata when built and if this were used when choosing an image from a manifest list.\n\nThis has applicability for Windows, where we need to disambiguate between images for Nanoserver and Windowsservercore.\n\ncc @glennc @tianon @aaronlehmann @jstarks \n"},{"labels":[null,"enhancement1"],"text":"It would be very helpful to have config parameters or settings within the daemon.json config file to control some of the default parameters when a new bridge network is created. At the moment, the default appears to be to create a /16 network on the next available class B range in 172.n.0.0/16. I realise this is configurable when manually creating bridge networks on the cli and it is configurable within the docker-compose file if using docker-compose but I don't think the docker-compose file is the right place to be putting such default config - it's more for when you wish to define an exception.\n\nIt would be very helpful to have control over the IP ranges used - such as using /24 rather than /16 and moving the range from 172.x.0.0 to something else.\n"},{"labels":[null,"enhancement1"],"text":"When using a network plugin like Calico which can support ECMP/Anycast style configurations, it's a big limitation at the moment not being able to configure via the docker run command multiple IP addresses / interfaces to be added to a container (i.e. multiple libnetwork calls).\n\nIs this likely to be supported in the future? The use case would be adding a regular network interface for connectivity, and a separate possibly shared IP at the same time - ideally handled through a launch.\n\nThe use case would be when launching haproxy/load-balancer type containers which should be anycasted across a Swarm / other orchestrator network.\n"},{"labels":[null,"enhancement1"],"text":"The libnetwork API has an optional \"options\" map which can be passed into the CreateEndpoint call.\n\nDocker CLI does not have a way to pass any custom options to CreateEndpoint\n\nSo... I propose:\n\n``` sh\n% docker run --net-opt=[]\n```\n\nwhich is inline with:\n\n``` sh\n% docker run --help | grep -i \"\\-opt\"\n  --dns-opt=[]                    Set DNS options\n  --log-opt=[]                    Log driver options\n  --security-opt=[]               Security Options\n```\n\nOne use case is to support multi-tenant calls to kuryr through docker in openstack environments where users should provide an openstack token.\n\nAlternative use cases include passing runtime information to the runtime for operational purposes such as additional resource provisioning, QOS, firewall params.\n"},{"labels":[null,"enhancement1"],"text":"In the vein of #18480, any of the log drivers could accept a regular expression string as an argument to configure the driver to parse the log body (ie stdout) so that additional fields could be set for whatever log driver is bing used.  For Google Stackdriver Logging (aka Google Cloud Logging), the [driver](https://github.com/docker/docker/blob/master/daemon/logger/gcplogs/gcplogging.go) sets a `data` field which contains the full text of the log entry as a single string.  I suggest any of the following improvements:\n1. Use the stream to establish a Severity level:  `stdout` means `info`, and `stderr` means `error`.  Even just 2 log levels would be a big help.\n2. A regex string `--log-opt` could be used to set more [structured data here](https://developers.google.com/resources/api-libraries/documentation/logging/v2beta1/python/latest/logging_v2beta1.entries.html#write) which would end up in the `structPayload` part of the [log message in Google Stackdriver Logging, making it filterable](https://cloud.google.com/logging/docs/quickstart-sdk).\n3. Similarly, a regexp string could break a log message into existing fields, like `severity`\n\nBoth regex ideas could be implemented using [named groups](https://github.com/google/re2/wiki/Syntax).\n"},{"labels":[null,"enhancement1"],"text":"As per https://github.com/docker/docker/pull/22554 for seccomp, maybe we should do the same for the apparmor default profile? The main issues that have come up are `ptrace` and `mount` are always disabled by the default profile. Unfortunately that means that we would need multiple profiles, which is a bit messy.\n"},{"labels":[null,"enhancement1"],"text":"I'm unable to pip3 install a package from our local git repository.  I'd like an option to pass on the build command line that enables the dns and dns-search flags for the one-time purpose of the build.\n\nIt looks like I'm going to end up working around this by hacking the /etc/resolv.conf file for the host where the Dockerfile is built for the purpose of the contents getting copied into the container at build-time.\n\nThis has a few draw backs. First, it seems silly to touch the host to get the desired contents into the container. Second, it results in permanent nameservers that won't make sense after the container is deployed in another environment.\n\nBut it gets me to the point where I can provide a single script that only mangles a temporary file on the host that can be run to build the Dockerfile.\n\nThis does not appear to be the case with the other solutions I've seen suggested when this request has been submitted before.\n\nThe use-case is that I'd like any developer to be able to build the Dockerfile in a corporate data-center without changing their docker server settings. The Dockerfile will be deployed in a different environment, where its default nameserver implementation should work fine at run-time.\n"},{"labels":[null,"enhancement1"],"text":"Two part related feature request for `docker stats`:\n1. It would be fantastic if you were able to stream the output from `docker stats` to a CSV or similar simple output file for processing.\n2. Stats aggregation would be very helpful, i.e. add the ability to get the average for each stat as long as the command (or daemon perhaps?) was run.\n"},{"labels":["enhancement1"],"text":"It would be very useful to support a `volume ls` filter to only show named volumes. I can hack around this by checking for volume names that aren't exactly 65 characters, but that's sketchy.\n\nMy main intent for this is to create scripts to automatically back up any named volumes, since they always contain data I want to keep around and non-named data volumes generally don't (in our case).\n"},{"labels":[null,"enhancement1"],"text":"Right now every Dockerfile has to be distributed with a run script or a readme that documents whether it needs to be `run` with `-it`, `-i` or neither.\n\nThere should be a way to store this in the image by an instruction in the Dockerfile.\n\nThere was already an issue #1870 for this, but it was closed in favor of #4955 and then in turn #4882. I think this was by accident because the MR is not actually related to this feature, it just removes a buggy, incomplete implementation of a similar feature.\n"},{"labels":[null,"enhancement1"],"text":"Allow sharing pid namespaces between container like we do for ipc, network, etc today.\nThis allows one to create debug containers that can attach to a running container and ptrace them.\nThe syntax will be similar to `--ipc`, i.e. `--pid:container:<container_id>`\n"},{"labels":[null,"enhancement1",null],"text":"**Output of `docker version`:**\n\n```\nClient:\n Version:      1.11.0\n API version:  1.23\n Go version:   go1.5.4\n Git commit:   4dc5990\n Built:        Wed Apr 13 19:36:04 2016\n OS/Arch:      darwin/amd64\n\nServer:\n Version:      1.11.0\n API version:  1.23\n Go version:   go1.5.4\n Git commit:   a5315b8\n Built:        Tue Apr 26 15:23:39 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 35\n Running: 9\n Paused: 0\n Stopped: 26\nImages: 998\nServer Version: 1.11.0\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 801\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins: \n Volume: local\n Network: host bridge null\nKernel Version: 4.4.6-moby\nOperating System: Alpine Linux v3.3\nOSType: linux\nArchitecture: x86_64\nCPUs: 4\nTotal Memory: 3.859 GiB\nName: docker\nID: F64G:UTRO:Y4S4:V43M:7OMT:SCYT:NBPL:O77A:CLSL:JZWZ:7RMU:UP5E\nDocker Root Dir: /var/lib/docker\nDebug mode (client): false\nDebug mode (server): true\n File Descriptors: 61\n Goroutines: 110\n System Time: 2016-04-30T04:36:45.951786883Z\n EventsListeners: 2\nRegistry: https://index.docker.io/v1/\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n\nDocker for Mac, but want this ability in all scenarios, including docker swarm.\n\n**Steps to reproduce the issue:**\nI want my docker client (laptop) to be able to provide direct access to docker containers by their ip address. This is very useful when you're running/debugging a program locally that needs to talk to a cluster of servers running within docker. I can't access by published ports, my local program needs direct access each container by ip.\n\nThis works (at least used to) on my macbook with a docker-machine virtualbox server:\n\n```\nsudo route add -net 172.17.0.0/16 192.168.65.2 # 192.168.X.Y is the docker VM ip\n```\n\nbut it does not work (I can't ping a container's ip) on Docker for Mac, or with Swarm, or a remote docker server (e.g. digitalocean). \n\n**Describe the results you received:**\n\n```\nmacbook$ ping 172.18.0.8\nPING 172.18.0.8 (172.18.0.8): 56 data bytes\nping: sendto: Network is unreachable\n```\n\n**Describe the results you expected:**\n\nI expect to be able to `curl http://172.18.0.8` as if my macbook is in the container's network.\n\nSorry I don't know enough about networking to know if or how this is possible. Maybe it's possible now with a unix script or by using the `weave` network plugin.\n\nThis seems useful enough to be a \"batteries included\" feature. \n\nThe feature I'm imagining is `docker network route myproject_backend` will give the machine running the docker client an ip address on the docker network, e.g. created by a `docker-compose` project that names its network `backend`.\n"},{"labels":[null,"enhancement1"],"text":"_Before getting into the details, I want to point out that I suspect a feature like what I'm about to suggest could solve many permission related snags when using Docker._\r\n\r\n_**So as to prevent this suggestion from getting mired in philosophical objections, this ticket is not advocating permanently storing credentials inside a container.  I don't do that and I don't suggest anyone else do it either!**_\r\n\r\n---\r\n#### tldr;\r\n\r\nThe current method of UID spoofing _inside of a container_ via `--user` has limitations that end up mandating entire ecosystems of one-offs and very problematic workarounds to massage container state into the right frame of mind.  This is worsened by the fact that the need to perform these workarounds is influenced by what platform you are calling Docker from.\r\n\r\nThe feature I'd like to request is to have a flag added to `docker run` (and equivalents) that tells Docker to map all filesystem operations that a container performs to a specific user host-side.  This is very different to `--user` which simply sets the user of the process that runs inside the container.\r\n\r\nIf you're interested in an extended explanation, read on!\r\n\r\n---\r\n#### Rationale\r\n\r\nWe've all heard about people having issues with temporarily binding SSH credentials and similar `$UID` & `$USER` related difficulties.\r\nSomething that I've noticed when using Docker on OSX and Windows however is that I'm actually freed from the challenges of ensuring my ephemeral development containers (my primary use case) aren't writing files to the filesystem as root!  This is owed to the fact that they both rely on a variety of bridges back to the native platform's filesystem.  \r\n\r\nOne non-exclusive example is when trying to forward either an SSH socket or the SSH directory on a Linux host, because SSH insists on proper home directories, I've been unable to get secure connections working without having to make my container aware of the environment it is running in!  The best I've been able to dream up is bind mounting `/etc/passwd`, `/etc/groups` and `/home` into the container.  Which is exactly as horrible as it sounds, but also my only choice given that I don't know what uids will be right for the host filesystem.  This clearly runs against Dockers attitudes towards portability.\r\n\r\nAnother more common example would be when files are created by the container, if the bound filesystem is ext or otherwise linux-compatible, the UID of files is set to root (or whoever I run my container as).  On Linux, in order to not blast my filesystem with files having UID/GID 0, I have to tell my containers to run their processes as my current user via `--user`.\r\nThis might work in trivial scenarios, but again ends up falling apart in situations where the user system inside of the container is different to the host or the binaries being run enforce `/home` directory requirements like SSH above.  No host is likely to ever have a reliable way to influence the user system inside the container.  This is especially true when using images produced by third parties.\r\n\r\nSo, I currently am stuck avoiding the complexity by staying as root.  Everything works because all containers have a root user fully configured.  But now without any better option, all Docker containers that want to avoid being polluted with hacks are forced to write files as UID 0!\r\n\r\nAs I mentioned above, switch over to Windows or OSX and this problem goes away because they don't share the same users as the container and the filesystems aren't compatible.  Instead, vboxsf, samba and other drivers are actually emulating the feature I'm requesting here!\r\n\r\nSo, this clearly identifies the fact that while it's nice to control who the container runs its process as, it would actually be more desirable to optionally map the uid (and gid?) flag for any changes to the filesystem from the container, host-side. All the while, still allowing the container to function as whoever it needs to be internally.\r\nThis jives with the philosophy that Docker containers should be portable and require zero awareness of the environment that is running them.  Indeed if you examine the nature of this suggestion, it's conceptually parallel to binding ports and bind mounting filesystems.\r\n\r\nWe need a way to bind users as well.\r\n"},{"labels":[null,"enhancement1"],"text":"Currently the only way to expose a file from the build source content that is sent to the docker daemon during the build is to use COPY or ADD command in the Dockerfile to copy the file into the image. When the file is only used as a source for the compiler that is run in a subsequent RUN command this inevitably results in the image bloat as the file is stuck committed to the image as intermediate layer even if the RUN command removes it.\n\nTo avoid such bloat it would be nice if the RUN command could access the build content directly without the need to use intermediate COPY or ADD. For example, the content could be exposed over HTTP over some unix socket so the RUN command could use curl or similar to access the files either individually or as a tar file. Alternatively the content could be mounted as a directory. \n\nWith such access the compiler after it generated the binaries could just remove any temporaries leaving the image with the minimum number of files. In addition the RUN command could perform arbitrary tree refactoring/filtering without keeping the original source tree as a useless layer.\n"},{"labels":[null,"enhancement1"],"text":"Docker now has the ability to set seccomp profiles on a container (https://github.com/docker/docker/pull/17989), \nhowever, writing a custom profile can be tedious, and leads to a lot \nof repetition if a profile only differs slightly from the default profile.\n\nTo reduce this repetition, I suggest we allow users to write \"partial\" profiles,\nand combine them to make up for a full profile (by providing multiple\n `--security-opt seccomp=..` flags).\n\nIn addition, we can provide some easy to use partials for people to pick from,\nand include those in the default install.\n\nBasically;\n\ndocker-disable-chown-chmod.json:\n\n``` JSON\n{\n    \"defaultAction\": \"SCMP_ACT_ALLOW\",\n    \"architectures\": [\n        \"SCMP_ARCH_X86_64\",\n        \"SCMP_ARCH_X86\",\n        \"SCMP_ARCH_X32\"\n    ],\n    \"syscalls\": [\n        {\n            \"name\": \"chmod\",\n            \"action\": \"SCMP_ACT_ERRNO\",\n            \"args\": []\n        },\n        {\n            \"name\": \"chown\",\n            \"action\": \"SCMP_ACT_ERRNO\",\n            \"args\": []\n        },\n        {\n            \"name\": \"chown32\",\n            \"action\": \"SCMP_ACT_ERRNO\",\n            \"args\": []\n        }\n    ]\n}\n```\n\nRun an image, using the \"docker-default\" profile, and apply a second (partial)\nprofile that disables \"chmod\", \"chown\";\n\n```\ndocker run -d \\\n  --security-opt seccomp=docker-default.json \\\n  --security-opt seccomp=disable-chown-chmod.json \\\n  myimage\n```\n### Questions\n- Should the default profile always be applied, unless `--security-opt seccomp=unconfined` is used?\n- How to handle the `\"defaultAction\"`? i.e., if `\"defaultAction\"` is specified in a partial profile, does it change the \"overall\" default, or is it only used as a default for seccomp-calls inside that file (if no action is specified)?\n- Should we simply apply/merge profiles in the order they were specified, or should they be applied as \"`deny` trumps `allow`\", thus once denied, a profile cannot \"allow\"?\n- Should \"partial\" templates have a slightly different format, to indicate it's a \"partial\"?\n### Implementation\n\nThis depends a bit on the questions above; for a \"clean\" API, I suggest to perform the merging _client side_, so the client\ncombines/merges the profiles and sends them as a full profile to the daemon.\n\n_However_, this may be difficult if the default profile should always be applied, because the default profile\nis not available to the client. So, either the user should install the `docker-default` profile on the client,\nor merging the default profile should be done on the daemon (which will be backwards-incompatible with the current behavior).\n"},{"labels":[null,"enhancement1",null],"text":"Hi there,\n\nDo you guys think it could be a good idea to have in the API an endpoint which allows to get the stats of all the running containers at once?\n\nAt the moment I am using this collectd plugin (https://github.com/lebauce/docker-collectd-plugin) to get the stats out of my server. \n\nThe problem I am seeing is that when I am running 100+ container the server starts loading because I am making too many requests too often to the docker daemon (aka one call per container to the /stats endpoint every 20s).\n\nIf we could get all the stats at once that would fix this problem, for now I have implemented some rate limit on that plugin but not sure if it is the best as I do  not have a real time info anymore.\n\nCheers.\n"},{"labels":[null,"enhancement1"],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\n**Output of `docker version`:**\n\n```\n(paste your output here)\n```\n\n**Output of `docker info`:**\n\n```\n(paste your output here)\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n\n**Steps to reproduce the issue:**\n1.\n2.\n3.\n\n**Describe the results you received:**\n\n**Describe the results you expected:**\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n"},{"labels":[null,"enhancement1"],"text":"Right now, the following appears to be the current situation given docker's current state of implementation:\n\n**When using IPv4:**\n_(EDIT: Apparently an INCORRECT statement: Only ports EXPOSE'd are reachable by any other containers. This means random small programs inadvertently opening ports in a container aren't much of a problem. - Correction: EXPOSE only adds metadata, and ports are reachable anyway if you really want to even when not exposed)_\nOnly ports -p/--publish'd are reachable by the outside world. This means any containers having unsecured plain text and possibly password-lacking backends that is not EXPOSE'd is safely protected and cannot be reached directly in the outside world.\n\n**When enabling IPv6 support:**\nSuddenly, any sort of `[::0]` listen on any container is immediately reachable from everywhere in the world. (correct me if this is wrong. I hadn't had the chance to test this myself because of IPv6 configuration problems by my hosting provider, so I've had to rely on information provided by other docker users and developers. If I'm putting a factually incorrect statement out with this I'm sorry and I'll be happy to immediately retract this ticket)\n\n---\n\nThis behavior difference is **absolutely insane**. It **needs** to be changed. You are asking for users to get into trouble.\n\nTo make a more constructive remark: one solution would be to introduce an explicit `docker run` switch to make a container with _opt-in_ behavior globally reachable, and in absence of the switches docker should default to writing ip table rules to drop all incoming connections to any containers' global IPv6 addresses for non-published ports.\n"},{"labels":[null,null,null,null,"enhancement1"],"text":"Under certain circumstances, subsequent COPY instructions re-adds all files in every layer instead of only the files that have changed.\n\n**ISSUE RENAMED**: The original suggestion was to add a SYNC instruction, but the COPY instruction should already cover the use cases for which the SYNC instructions was intended. The original post was as follows:\n\nSuggestion:\nThe SYNC instruction compares <src> and <dest> and performs the necessary changes for <dest> to become identical to <src>. \n\nThis would alleviate a lot of pain present when building images that contain application source code that changes incrementally and often, since the whole source code needs to be COPIED (`COPY . /app`) to the image again and again, leading to longer deploy cycles and tons of unnecessary data pushed to and stored in the registries. \n\nIn extreme cases, the source code directory can reach up to gigabytes in size, and in order to publish a some smaller changes in a few hundred files, all gigabytes needs to be pushed and subsequently pulled by those who wish to get access to the changes, or the servers that are to serve the new source code.\n\nA SYNC command would instead allow for new image layers to include only the files that actually changed in comparison to the existing image contents, which probably won't amount to more than a few hundred kilobytes in most cases.\n\n(Optionally, an ability to access the build context from the RUN instruction would make this particular instruction unnecessary by allowing for instance rsync to compare build context against image contents)\n\nThe only currently feasible workaround today seems to be [to use rsync to analyze the differences between two images and then use the changelog output to craft a tar-file containing the relevant changes](http://stackoverflow.com/a/36561768/682317).\n"},{"labels":[null,"enhancement1"],"text":"There are many times when the Docker version on a client can differ to that on a server.\n\nThis is typically because that end users are faster to update their clients whereas hosted Docker services or servers in production are slower to upgrade.\n\nThis issue can be much worse when attempting to RC test the new version of Docker as you need to switch Docker Client versions or set `DOCKER_API_VERSION`. The latter can have consequences e.g newer client features and flags failing against older engine versions.\n## Today\n\nAttempting to connect to older server:\n\n```\n$ docker ps\nError response from daemon: client is newer than server (client API version: 1.23, server API version: 1.22)\n```\n\nAttempting to create an IPv6 network\n\n```\n$ DOCKER_API_VERSION=1.22 docker network create --ipv6 foo\n2d6a8ce8e8303d27fbfdc19cb1d2a73328d5d278a1dea173d31ec4b9d586e8ca\n$ DOCKER_API_VERSION=1.22 docker network inspect foo\n[\n    {\n        \"Name\": \"foo\",\n        \"Id\": \"2d6a8ce8e8303d27fbfdc19cb1d2a73328d5d278a1dea173d31ec4b9d586e8ca\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": {},\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.18.0.0/16\",\n                    \"Gateway\": \"172.18.0.1/16\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Containers\": {},\n        \"Options\": {},\n        \"Labels\": null\n    }\n]\n```\n\nThe command succeeds, but IPv6 wasn't enabled as this is only available in a newer API version!\n## Tomorrow\n\nRegardless of version, commands should work:\n\n```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```\n\nHandling of unsupported operations\n\n```\n$ docker network create --ipv6 foo\nError: `--ipv6` is not available on your server\nClient Version: 1.23 \nServer Version: 1.22\n```\n## Suggestion\n\nIf the Docker Client were able to maintain some form of session state, it could detect and preserve the Docker API version for a session. It should also be possible to prevent newer client features from being used on servers that do not support them to avoid inconsistencies like the example noted above\n"},{"labels":[null,"enhancement1"],"text":"Similar to the dns option for docker run, add an option to docker build.\n"},{"labels":[null,"enhancement1"],"text":"### Situation\n\nThe registration / deregistration is incomplete. The plugin starts the  registration process by with creating the unix socket  `/run/docker/plugins/<drivername>.sock`. Docker engine acknowledges the registration with the [handshake API](https://docs.docker.com/engine/extend/plugin_api/#handshake-api) calling `/Plugin.Activate`.  Then the plugin knows registration has completed.\nThis activation is done automatically when the plugin is [first needed](https://docs.docker.com/engine/extend/plugin_api/#plugin-activation).\n### Drawbacks\n1. It might take some time to initialise the plugin. This will increase startup time for the container first using this driver.\n2. It is not possible to update the plugin even if it's currently not in use [w/o stopping docker daemon](https://docs.docker.com/engine/extend/plugin_api/#plugin-lifecycle).\n3. It is not possible to remove a plugin. If you do so (stop the plugin, remove the plugin socket) many `docker volume` commands will [timout](https://docs.docker.com/engine/extend/plugin_api/#plugin-retries). The only way is to restart docker daemon.\n### Proposal\n1. Add a command `docker plugin-start <drivername>`. This activates the plugin without actually using it. This is optional, on-demand activation is still possible.\n2. Add a `/Plugin.Deactivate` API call indicating to the plugin it is not longer in use.\n3. Add a command `docker plugin-stop <drivername>`. This deactivates the plugin and forgets about the driver (as if not beeing activated). Of course, this would fail if still volumes managed by this driver exist.\n4. Add the option `docker volume rm -d <driver>`. This  would remove all volumes managed by this driver which are not in use by a container.\n\nAs further lifecycle elements I thouhgt about `/Plugin.Pause` and `/Plugin.Unpause` together with corresponding `docker plugin-<...>` commands. A paused plugin cannot be used to manage (create, rm, ls, mount) volumes, however existing volumes can be further used. This would allow to take the plugin in maintainance mode (if there is something in backend) or allow for the update of a plugin (the plugin would have to preserve state of it's volumes; the updated plugin would accept activation with `/Plugin.Unpause`).\nHowever, it is not clear whether there is a real use case for it.\n### Advantages\n\nThe lifecycle of a plugin gets more complete, plugin usage becomes more flexible and more deserves the term _cycle_.\n### Notes\n1. The proposal was first targeted on volume driver plugins it could be useful for other nplugin types as well.\n2. While this proposal provides a way to take a plugin out of use by docker engine, it is still the responsibility of an external entity (i.e. admin) to possibly stop and unload the plugin.\n"},{"labels":[null,null,"enhancement1",null,null],"text":"**Output of `docker version`:**\n\n```\nClient:\n Version:      1.10.1\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   9e83765\n Built:        Thu Feb 11 19:27:08 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.10.1\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   9e83765\n Built:        Thu Feb 11 19:27:08 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 14\n Running: 9\n Paused: 0\n Stopped: 5\nImages: 7\nServer Version: 1.10.1\nStorage Driver: aufs\n Root Dir: /data/docker/aufs\n Backing Filesystem: xfs\n Dirs: 63\n Dirperm1 Supported: false\nExecution Driver: native-0.2\nLogging Driver: json-file\nPlugins: \n Volume: local\n Network: null host bridge\nKernel Version: 3.13.0-74-generic\nOperating System: Ubuntu 14.04.3 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 16\nTotal Memory: 120.1 GiB\nName: my-machine-hostname\nID: PN5G:4THA:2ZTC:OF6B:BNMQ:MSB7:JCUK:TBAW:3ZRP:ARMT:ABWV:YFYU\nWARNING: No swap limit support\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nHappens on physical hosts and AWS/GCP\n\n**Steps to reproduce the issue:**\n1. Set `--bip=...` when starting a Docker daemon (e.g. to avoid conflicts with other things on your company's LAN). I set mine to `192.168.1.0/16`.\n2. Create a Docker bridge network (e.g. `docker network create ...`).\n3. Run `docker network inspect` against the new network and look at the subnet.\n\n**Describe the results you received:**\nThe subnet on my machine is, for example, `172.19.0.0/16`\n\n**Describe the results you expected:**\nSeeing as how I specified a default bridge IP for the daemon, I'd have expected that to be propagated to the bridge driver.\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\nAn obvious workaround is to use `--subnet` when creating the bridge networks. The problem, though, is that this can lead to race conditions if networks are started in multiple threads. That is, I end up writing a lot of exception handling for cases of overlapping subnet ranges, but it seems more straightforward for Docker to handle this (as it already does on the default subnet it seems to hardcode).\n"},{"labels":["enhancement1"],"text":"This is a follow-up issue to the closed PR #18958 that was only for containers.\n\nNow that networks, volumes, images and containers all support labels, we should probably rethink the idea (or not) of having a common way to update them, instead of only being able to update containers label and no others.\n\n/cc @icecrime @tiborvass @calavera @dnephin @ehazlett \n\nPasting @tiborvass comment from https://github.com/docker/docker/issues/20356#issuecomment-194467286 below.\n\n---\n\nBefore going into technical details, I would like to find a good UX. I could come up with these 3 solutions, none of which I really like, but I prefer the 3rd one among these:\n1. For each new object type (images, networks, volumes), add a new `update` subcommand to match the `docker update` that is for containers. Questions: is there anything else we would want to update on images networks and volumes? If so what? If we can't agree on those other things to update, then the problem with this solution is we would add a new subcommand just for labels. Moreover, for historical reasons `images` is not a subcommand the way `network` and `volume` are. So if we were to still go with this solution we should also choose between: 1.a) introduce `docker images update` subcommand OR 1.b) overload `docker update` the way we (unfortunately) overload `docker inspect` and accept: `docker update --label foo=bar $imageID`.\n2. Overload `docker update` to non-container objects as well. Problem: how to deal with `docker update --memory 2G $networkID` ? obviously it should error out not recognizing the memory flag for updating network objects, but we would have the same problem docker machine is having with all the cloud-provider-specific flags and it's not ideal for the help output. Also, if we are overloading `docker update` for non-container objects, why not `docker create` and more ? Since this solution is a generalization of 1.b, 1.b also suffers of this problem. Both solutions 1 and 2 would result in accepting #18958.\n3. Introduce `docker label` command that can act on any object. Problem: we already have `docker update` that updates the container settings (cgroups only for now) and this would be another toplevel command that would also update container settings, albeit only one setting: the labels. As much as I dislike having two commands do similar things, a counter-argument is to say: labels are special because they are meta. Obviously this would mean, the UX in #18958 would be rejected.\n\nIf you can think of another solution that would have none of these UX issues, I would be glad to read about it.\n"},{"labels":[null,"enhancement1"],"text":"When `docker load`ing a `tar` file with image(s), the [(current v1.22) API](https://docs.docker.com/engine/reference/api/docker_remote_api_v1.22/#load-a-tarball-with-a-set-of-images-and-tags-into-docker) should return the list of images it loaded.\n"},{"labels":[null,"enhancement1"],"text":"Wouldn't it be nice if there existed an API access shim that one could use inside of a container without bind-mounting the socket and ensuring the client binary or libraries are compatible with the daemon you it happens to be talking to?\n\nAnd, of course, it'd be critical to implement flexible access-control for what API calls are allowed.\n\nOne way to consider would be something as simple as:\n\n``` shell\ndocker run --with-api-access-shim run <any_image> sh -c \\\n  \"echo ps | socat - UNIX-CONNECT:/.docker.sock\"\ndocker run --with-api-access-shim run <any_image> sh -c \\\n  \"echo exec foo ls -la | socat - UNIX-CONNECT:/.docker.sock\"\n```\n\nFor those who need more, a shim shared library could be provided, with bindings for different languages.\n"},{"labels":[null,"enhancement1"],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.10.3\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   20f81dd\n Built:        Thu Mar 10 15:54:52 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.10.3\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   20f81dd\n Built:        Thu Mar 10 15:54:52 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nroot@ubuntu:~# docker info\nContainers: 6\n Running: 4\n Paused: 0\n Stopped: 2\nImages: 8\nServer Version: 1.10.3\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 63\n Dirperm1 Supported: false\nExecution Driver: native-0.2\nLogging Driver: json-file\nPlugins:\n Volume: local\n Network: bridge null host\nKernel Version: 3.13.0-46-generic\nOperating System: Ubuntu 14.04.1 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 1.955 GiB\nName: ubuntu\nID: 2HT4:3WC7:54TS:REFK:TREC:FK5A:JVX7:7JVW:JAYV:KGQL:JURW:6HXV\nWARNING: No swap limit support\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n\n```\nroot@ubuntu:~# uname -a\nLinux ubuntu 3.13.0-46-generic #76-Ubuntu SMP Thu Feb 26 18:52:13 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n```\n\n**Describe the results you received:**\nError deleting container: Error response from daemon: Unable to remove filesystem for 36bd1b1d3660775ab7de8c8ac0f11870ec55f7ad4f6bc82488c07988c3681130: remove /var/lib/docker/containers/36bd1b1d3660775ab7de8c8ac0f11870ec55f7ad4f6bc82488c07988c3681130/shm: device or resource busy\n\nissue same as \n#21111\n#17902\n\nI hope we can support an option in `docker run`, and this option is named by `--no-shm`.\nIf we use this option, we can start a container without mounting shm. \n\nThis way can help docker users have more rights to customize their container environment which I think is very reasonable.\n\nAnd maybe shm is a kind of example, something else can be added to this consideration, too.\n"},{"labels":[null,"enhancement1",null],"text":"**Output of `docker version`:**\n\n```\nClient:\n Version:      1.10.3\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   20f81dd\n Built:        Thu Mar 10 15:54:52 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.10.3\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   20f81dd\n Built:        Thu Mar 10 15:54:52 2016\n OS/Arch:      linux/amd64\n\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 98\n Running: 2\n Paused: 0\n Stopped: 96\nImages: 985\nServer Version: 1.10.3\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 972\n Dirperm1 Supported: true\nExecution Driver: native-0.2\nLogging Driver: json-file\nPlugins: \n Volume: local\n Network: bridge null host\nKernel Version: 3.19.0-56-generic\nOperating System: elementary OS Freya\nOSType: linux\nArchitecture: x86_64\nCPUs: 4\nTotal Memory: 5.717 GiB\nName: johnsa1\nID: DUQB:LKFJ:C5Q2:OGBE:Z4QI:R4UZ:ZHN3:YJ2K:45HQ:UFFV:RCNJ:PJWW\nUsername: pdxjohnny\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\n```\n\nphysical host\n\n**Steps to reproduce the issue:**\n1. ADD https://authenication.protected.domain/url\n\n**Describe the results you received:**\nError 403\n\n**Describe the results you expected:**\nIt would be great if I could ADD resources that need an Authorization header. I looked at the builder but wanted to get input on if this would be valuable before I start on a patch. Right now the work around is to have a shell script download them with curl/wget and --header then COPY in.\n\nIt seems to me like the best place to have this would be taken from the env variables of the client. This is what I envisioned.\n\n``` dockerfile\nFROM xyz\n\nADD https://authenication.protected.domain/url\n```\n\n``` bash\nexport AUTH_AUTHENICATION_PROCECTED_DOMAIN=\"Basic d3U6dGFuZw==\"\n```\n\nIn the builder we have\n\n``` go\n// builder/dockerfile/internals.go\n    resp, err := httputils.Download(srcURL)\n    if err != nil {\n        return\n    }\n```\n\nIn httputils we have\n\n``` go\n// Download requests a given URL and returns an io.Reader.                       \nfunc Download(url string) (resp *http.Response, err error) {\n    if resp, err = http.Get(url); err != nil {\n        return nil, err\n    }\n    if resp.StatusCode >= 400 {\n        return nil, fmt.Errorf(\"Got HTTP status code >= 400: %s\", resp.Status)\n    }\n    return resp, nil\n}\n```\n\nIf we were to add a check in Download or another function that creates a request with an Authorization header if there is an AUTH_that_domain_name env var present, then it would be all fixed. Should be a pretty simple fix if this is handled client side although I have the suspicion that this is daemon side. In which case we'd have to pass the env vars over.\n"},{"labels":[null,"enhancement1"],"text":"I am planning to replace the unionfs of docker with a distributed file system(DFS) which also provide incremental snapshot feature. Is that worthwhile? \nBecause in my understanding, if docker could run on a DFS, then when live migration, the majority of data need not to be transferred between hosts. We can just read the data from DFS. \n"},{"labels":["enhancement1",null],"text":"I work in the field of bioinformatics and like many of my fellow practitioners I have a workload which requires data to be piped from one process to another, sometimes through several steps. While it would be wonderful if the bioinformatics community at large could do something smarter like shared memory, sadly the extant (and very large) toolset does not facilitate this. I would like to, as per standard docker practice, wrap single commands in single docker containers and pipe the data between these. I do not want to have to put all commands into one large container as beyond violating the underlying philosophy these containers become big-balls-of-mud exceptionally quickly. Further, I do not see putting the commands in a particular pipeline into a particular container as a viable option either; this is because many different pipelines share common commands, and so the setup of these commands would be duplicated across those containers. While it is possible to do some sort of manual Dockerfile generation, stitching together other Dockerfiles with individual components, this would be a brittle approach and, to be honest, a problem much better solved by a configuration manager. I have considered a \"hack\", whereby I mount a common volume and set up named pipes in place of the actual pipes. However, there is a _lot_ of programmatic overhead in doing this - not least writing a shell parser in order to work out where the pipes are, and the setup/cleanup of those fifos before/after the scripts run.\n\nI have included some timings below, which were performed on a completely unloaded server. 2.5 seconds vs 6 minutes.\n\n```\n$ ls -lh\n-rw-rw-r-- 1 dunk dunk 3.0G Mar 19 22:17 Homo_sapiens.GRCh37.75.dna.primary_assembly.fa\n```\n\n```\ntime cat Homo_sapiens.GRCh37.75.dna.primary_assembly.fa > test1\nreal = 0m2.544s, user = 0m0.004s, sys = 0m2.537s\n```\n\n```\ntime cat Homo_sapiens.GRCh37.75.dna.primary_assembly.fa | docker run -i --rm ubuntu:trusty bash -c 'cat' > test2\nreal = 5m51.383s, user = 0m1.783s, sys = 0m16.789s\n```\n"},{"labels":[null,"enhancement1"],"text":"On some Dockerfiles you can have multiple exposed ports.  For example the selenium-debug one has a port exposed for VNC, RemoteWebDriver.  Others expose more...\nIt would be good if when we are exposing ports we assign names to them.\n\n**Dockerfile**\nEXPOSE 5900 VNC\nEXPOSE 4444 REMOTE_WEBDRIVER\nEXPOSE 5555 SOME_OTHER_PORT\n\n**command line**\ndocker port container-name\nVNC: 5900/tcp -> 0.0.0.0:32771\nREMOTE_WEBDRIVER: 4444/tcp -> 0.0.0.0:32773\nSOME_OTHER_PORT: 5555/tcp -> 0.0.0.0:32772\n"},{"labels":[null,"enhancement1"],"text":"This was the most wanted use case in #3285. Something like:\n\n```\ndocker update -p 80:8080 --rm-port 1234 containername\ndocker update -P anothercontainer # expose all unexposed ports\n```\n\nwould do.\n\nSee the original issue for the rationale.\n"},{"labels":["enhancement1"],"text":"I would love to see a more verbose output when stopping and deleting containers.\nCurrently only the name of the container is printed out, but not what operation was\nexecuted.\n\ne.g. \n\n``` shell\n$ docker stop gogs && docker rm gogs\ngogs\ngogs\n```\n\nWould then look something like this:\n\n``` shell\n$ docker stop gogs && docker rm gogs\nStopping container: 'gogs'\nRemoving container: 'gogs'\n```\n"},{"labels":[null,"enhancement1"],"text":"## Test Aware Container\n\nToday, Docker container lifecycle is naively focused on:\n\n> Build -> Ship -> Run\n\n Build the image, ship it to some registry, and run it anywhere. IMHO there's a critical missing step - **Test**. \n\nI would like to propose to make Docker engine **\"test friendly\"**. There should be a standard way not only to build, deploy and run Docker container, but also to test it.\n\nThe more complete sequence should be, like this:\n\n> Build -> **Test** -> Ship -> Run\n\nPeople are doing this today, by embedding testing tools, tests, required packages and scripts into application container. This pollutes the application image with unneeded binaries and test related artefacts. Of cause, it is possible to keep testing outside of container, on you CI server, for example, thus getting to the situation \"these tests pass in CI, I have no idea why they fail on your environment...\"\n\nHere, I would like to propose and extension to Dockerfile syntax to make it \"test-friendly\" and also add a new Docker command. \n\n```\nONTEST [INSTRUCTION]\n```\n\nThe new `ONTEST` instruction is essentially similar to existing [`UNBUILD`](https://docs.docker.com/engine/reference/builder/#onbuild) instruction. The `ONTEST` instruction adds a trigger instruction to the image to be executed at a later time when the image is tested. Any build instruction can be registered as a trigger.\n\nThe `ONTEST` instruction should be recognized by a new `docker test` command.\n\n```\ndocker test [OPTIONS] IMAGE [COMMAND] [ARG...]\n```\n\nThe `docker test` command syntax should be similar to `docker run` command, with one significant difference: a new \"testable\" image could be automatically generated and even tagged with `<image name>:<image tag>-test` tag (\"test\" postfix added to the original image tag). This \"testable\" image will be generated `FROM` the application image, executing all build instructions, defined after `ONTEST` command and executing `ONTEST CMD` (or `ONTEST ENTRYPOINT`).\nThe `docker test` command should return a non-zero code if any tests fail. The test results should be written into an automatically generated `VOLUME` that points to `/var/tests/results` folder.\n\nThe proposed approach allows not only to separate between application and tests, it also creates a special image that contains both application and tests (including testing tool). This \"testing\" image allows to achieve test portability. Test portability allows you to run same testing tools, same tests, same environment on any environment: CI, developer, staging, production, etc. \n\nPlease take a look at sample `Dockerfile` below - it includes the new proposed `ONTEST` instruction.\n\n``` dockerfile\nFROM \"<base image>\":\"<version>\"\n\nWORKDIR \"<path>\"\n\n# install packages required to run app\nRUN apt-get update && apt-get install -y \\\n    \"<app runtime> and <dependencies>\" \\  # add app runtime and required packages\n    && rm -rf /var/lib/apt/lists/*\n\n# install packages required to run tests   \nONTEST RUN apt-get update && apt-get install -y \\\n           \"<test tools> and <dependencies>\"    \\     # add testing tools and required packages\n           && rm -rf /var/lib/apt/lists/*\n\n# copy app files\nCOPY app app\nCOPY run.sh run.sh\n\n# copy test scripts\nONTEST COPY tests tests\n\n# copy \"main\" test command\nONTEST COPY test.sh test.sh\n\n# auto-generated volume for test results\n# ONTEST VOLUME \"/var/tests/results\"\n\n# ... EXPOSE, RUN, ADD ... for app and test environment\n\n# main app command\nCMD [run.sh, \"<app arguments>\"]\n\n# main test command\nONTEST CMD [/test.sh, \"<test arguments>\"]\n```\n\n![test_aware](https://cloud.githubusercontent.com/assets/1898375/13842749/97bf11f4-ec38-11e5-9265-34924ed617b0.png)\n\n**Note**: The original idea and other container testing approaches are described in this [blog post](https://medium.com/@alexeiled/testing-strategies-for-docker-containers-f633e261e75a#.y097ymjzr)\n"},{"labels":[null,"enhancement1"],"text":"The ask here is for a native log driver for Kafka 0.8+.  Kafka is a high volume open source distributed messaging framework, which is very popular in the logging community.  It would be widely useful to be able to stream logs from a particular container to a given kafka topic.  This would be analogous to the logspout-kafka adapter for logspout.  For the many environments with kafka in place, this would be a more robust and easier method of docker log collection to e.g. elk stack than the rsyslog/logspout/etc approaches.\n\nThe features of this log driver should be:\n\n1) Configurable with kafka topic, broker list and kafka producer properties.\n2) Fail gracefully if kafka is unavailable\n\n*_Note: *_ A potential stumbling block with this item may be in terms of the number of statically compiled external dependencies on docker itself - it would be nicer if there were some way to develop log drivers as plugins.  In this case, it is likely that one of the established go kafka clients would need to be added as a dependency.\n"},{"labels":[null,"enhancement1",null],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\nRight now, docker volume only support dangling filter, it would be good to support name and driver filter (network has kinds of support, like name and id etc)\n\nWhat will do like below:\n\n```\nroot@bf86488287ac:/go/src/github.com/docker/docker# docker volume ls -f driver=local\nDRIVER              VOLUME NAME\nlocal               testd\nlocal               wokrd\nroot@bf86488287ac:/go/src/github.com/docker/docker# \nroot@bf86488287ac:/go/src/github.com/docker/docker# docker volume ls -f name=testd\nDRIVER              VOLUME NAME\nlocal               testd\n\n```\n"},{"labels":[null,"enhancement1"],"text":"Hi,\n\nPlease consider implementing a recursive RUN command for Dockerfile -- i.e. running the command on its own cache output. This would vastly accelerate any package management command.\n\nFor example:\n\n```\nRRUN cache_tag apt-get -qqy install pkg1 pkg2\n```\n\nWhen this changes, the new `RRUN cache_tag` line should run on the old output. (when the ancestor layers are the same).\n\n```\nRRUN cache_tag apt-get -qqy install pkg1 pkg2 pkg3\n```\n\nAllowing volumes at build-time (https://github.com/docker/docker/issues/14080) would probably be just as good. The non-sequential caching proposal (https://github.com/docker/docker/issues/17260) also may be asking the same question.\n"},{"labels":["enhancement1"],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\nThis is a Feature Request.\n\nAttempting to automate docker containers through jenkins (docker in docker, and docker beside docker, etc) is proving to be difficult. Most of the automation solutions are centered around the fact that docker socket is mounted from the host and then the other container can manage the docker daemon on the docker host, such as building container images, starting or stopping containers, etc. \n\nThe problem is that this socket has read permissions for the group docker, which may have different group ID on different systems. E.g. I have Fedora 23 as my desktop, and it has the docker group id as \"978\" (strange!) . Inside a jenkins container (I installed docker inside it), the group id for docker group is 999. On a production (ubuntu) server the group id of docker group is 1001. It is quite crazy to try to make docker a group member of all possible / conceivable group ids, when setting up some automation, e.g. using jenkins. I thought of using 999 as a standard group ID for docker across the board, but then I saw Fedora 23 (on my work computer), is using 999 for some group named \"input\" . \n\nI though of what group ID to make standard across all distros, and I though of \"dkr\" which translates to \"357\" when pressed on a phone key-pad. A quick check revealed that this group is unused on all three distros . May be Docker (you guys) can use this group id and make it de-facto standard on all distros?\nThanks!\n"},{"labels":["enhancement1"],"text":"## Problem statement\n\nThis is a followup to #21142 to discuss the idea of adding a way to specify a default \"alive probe\" for all containers started from a particular image.\n## Proposal\n\nIntroduce a new Dockerfile instruction to specify the probe. The (yet to be used) support for instructions options could be leveraged to model the design proposed for the CLI.\n- An image would optionally have a single probe definition, and any new occurrence of that instruction (either in the same Dockerfile, or in a child image) would override the previous one.\n- The default probe could be overridden through the command-line.\n## Open questions:\n- Ping @tianon: what's your opinion on that in the context of official images?\n\n_Ping @crosbymichael @tonistiigi @mgoelzer @aluzzardi @ehazlett_\n"},{"labels":["enhancement1"],"text":"## Problem statement\n\nDocker currently doesn't provide any built-in way to determine if a container is \"alive\" in the sense that the service it provides is up and running. This is useful in many scenarios, for example:\n- Sequencing dependent containers (e.g., docker/compose#374)\n- Taking informed load-balancing decisions\n- Restarting the container on an application-specific criteria\n\nThis issue cover the support for \"alive probes\" at the Engine level.\n## Proposal\n\nEvery container would support **one** **optional** container-specific probe to determine whether a service is alive. This would translate in `docker` UX through several new command line options for the `run` sub-command (naming to be discussed):\n\n| Option | Default value | Description |\n| --- | --- | --- |\n| `--probe` | `\"\"` | URI of an HTTP endpoint or in-container script to probe for service liveliness |\n| `--probe-interval` | `60` | Interval in seconds between probes |\n| `--probe-retry` | `1` | Number of successive probe failures before considering the container failing |\n| `--probe-timeout` | TBD | Number of seconds for the probe run before failure is assumed |\n| `--probe-grace` | TBD | Number of seconds since container start time before the probe is active |\n\n---\n\nA container is considered alive when the probe returns `0` for a `file://` based probe, or a status code in the 200-399 range for an `http[s]://` based probe.\n\nExamples:\n\n```\ndocker run -d \\\n           --probe=\"file:///some/script.sh\" \\\n           --probe-interval=120 myimage\n```\n## Open questions\n\n**Implementation**\n- Should it be a new value for the container state, or should it be a new field in the container definition?\n- Should probe failure be reported in the `events` API?\n\n**Restart policies**\n- Should we back restart policies on the probe result?\n- Can we implicitly alter the behavior of `--restart=always` and `--restart=on-failure` when a `--probe` is specified? That is roughly equivalent to assuming that the restart policy was always backed by a default probe which behavior is to look for the container process being alive.\n## References\n- Credits to @dongluochen for the initial design\n\n_Ping @crosbymichael @tonistiigi @mgoelzer @aluzzardi @ehazlett_\n"},{"labels":[null,null,"enhancement1"],"text":"Output of `docker version`:\n\n```\nClient:\n Version:      1.10.2\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   c3959b1\n Built:        Mon Feb 22 22:37:33 2016\n OS/Arch:      darwin/amd64\n\nServer:\n Version:      1.10.2\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   c3959b1\n Built:        Mon Feb 22 22:37:33 2016\n OS/Arch:      linux/amd64\n```\n\nOutput of `docker info`:\n\n```\nContainers: 1\n Running: 0\n Paused: 0\n Stopped: 1\nImages: 3\nServer Version: 1.10.2\nStorage Driver: aufs\n Root Dir: /mnt/sda1/var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 10\n Dirperm1 Supported: true\nExecution Driver: native-0.2\nLogging Driver: json-file\nPlugins: \n Volume: local\n Network: bridge null host\nKernel Version: 4.1.18-boot2docker\nOperating System: Boot2Docker 1.10.2 (TCL 6.4.1); master : 611be10 - Mon Feb 22 22:47:06 UTC 2016\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 1.956 GiB\nName: default\nID: IJCF:IXCO:C32D:S3BP:3WRN:VFRE:DKKL:ZOPL:ZJAZ:ORJG:GJSV:K2WC\nDebug mode (server): true\n File Descriptors: 10\n Goroutines: 22\n System Time: 2016-03-01T15:34:54.098983226Z\n EventsListeners: 0\n Init SHA1: \n Init Path: /usr/local/bin/docker\n Docker Root Dir: /mnt/sda1/var/lib/docker\nUsername: msimons\nRegistry: https://index.docker.io/v1/\nLabels:\n provider=virtualbox\n```\n\nCurrently there doesn't seem to be a way to change the seccomp profile used during docker builds.  It would be nice if you could specify  `--security-opt seccomp:<profile path>` when building.\n\nScenario:\n\n```\nFROM microsoft\\dotnet\n\nWORKDIR /myapp\n# TODO: obtain your dotnet app - pull from SCC or use COPY\n\n# restore the dependencies\nRUN dotnet restore\n\n# build and run the app on entry\nENTRYPOINT [\"dotnet\", \"run\"]\n```\n\nBuilding the above Dockerfile will fail because the dotnet CLI tooling relies on the LTTng library which makes a blacklisted syscall.  (see #20818)\n\nThis feature request was mentioned in #20634 as a possible solution.\n"},{"labels":["enhancement1",null],"text":"Output of `docker version`:\n\n```\nClient:\n Version:      1.10.2\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   c3959b1\n Built:        Mon Feb 22 21:22:53 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.10.2\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   c3959b1\n Built:        Mon Feb 22 21:22:53 2016\n OS/Arch:      linux/amd64\n```\n\nOutput of `docker info`:\n\n```\nContainers: 15\n Running: 0\n Paused: 0\n Stopped: 15\nImages: 68\nServer Version: 1.10.2\nStorage Driver: devicemapper\n Pool Name: docker-8:19-7864325-pool\n Pool Blocksize: 65.54 kB\n Base Device Size: 10.74 GB\n Backing Filesystem: ext4\n Data file: /dev/loop0\n Metadata file: /dev/loop1\n Data Space Used: 7.486 GB\n Data Space Total: 107.4 GB\n Data Space Available: 91.31 GB\n Metadata Space Used: 9.703 MB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 2.138 GB\n Udev Sync Supported: true\n Deferred Removal Enabled: false\n Deferred Deletion Enabled: false\n Deferred Deleted Device Count: 0\n Data loop file: /media/me/4bfe4369-4691-4a63-8cc6-a0e418ae9c50/docker/data/devicemapper/devicemapper/data\n WARNING: Usage of loopback devices is strongly discouraged for production use. Either use `--storage-opt dm.thinpooldev` or use `--storage-opt dm.no_warn_on_loop_devices=true` to suppress this warning.\n Metadata loop file: /media/me/4bfe4369-4691-4a63-8cc6-a0e418ae9c50/docker/data/devicemapper/devicemapper/metadata\n Library Version: 1.02.90 (2014-09-01)\nExecution Driver: native-0.2\nLogging Driver: json-file\nPlugins:\n Volume: local\n Network: bridge null host\nKernel Version: 4.3.0-0.bpo.1-amd64\nOperating System: Debian GNU/Linux 8 (jessie)\nOSType: linux\nArchitecture: x86_64\nCPUs: 4\nTotal Memory: 11.61 GiB\nName: dk-dev-01\nID: S2DM:6WDI:WYCQ:OMIB:IKFC:5DPV:WVKR:REOD:YKR4:V3OU:HMOF:FUQI\nUsername: codemedic\nRegistry: https://index.docker.io/v1/\nWARNING: No memory limit support\nWARNING: No swap limit support\nWARNING: No oom kill disable support\n```\n\nDocker is running on a physical box running Debian Jessie (with backports)\n\n```\nLinux my-dev-01 4.3.0-0.bpo.1-amd64 #1 SMP Debian 4.3.3-7~bpo8+1 (2016-01-19) x86_64 GNU/Linux\n```\n\nWhile working with images; I found it annoying that I dont have access to my favourite tools. So as a way to get around them, I have been static linking the tool and then pass in the files and (trying to) add a symlink to the executable / wrapper script in `/bin`. For example, I compiled and setup [static-vim](https://github.com/ericpruitt/static-vim) as below.\n\n```\n~/apps/vim-static$ ls -l\ntotal 4956\n-rw-r--r--  1 me mygroup    1062 Mar  8 14:30 Makefile\ndrwxr-xr-x 18 me mygroup    4096 Mar  8 14:30 runtime\n-rwxr-xr-x  1 me mygroup 4210752 Mar  8 14:30 vim\n-rwxr-xr-x  1 me mygroup     129 Mar  9 11:43 vim.sh  <----- the wrapper script\n-rwxr-xr-x  1 me mygroup  840672 Mar  8 14:30 xxd\n```\n\nThis folder is then meant to be passed in as a volume as below.\n\n```\ndocker run -it --rm -v $(pwd):/vim-static:ro busybox\n```\n\nWhen I run the command `/vim-static/vim.sh`, it works as expected.\n\nBeing lazy, I wanted to make the command as natural as possible; i.e make it invokable with just `vim`. So I added a \"dead\" symlink to the above folder like below.\n\n```\n~/apps/vim-static$ ln -s /vim-static/vim.sh vim.symlink\n~/apps/vim-static$ ls -l vim.symlink\nlrwxrwxrwx  1 me mygroup      18 Mar  9 11:55 vim.symlink -> /vim-static/vim.sh\n```\n\nNow I tried to make it work, expecting to `--volume` mount the symlink as `/bin/vim`; but unexpectedly it failed, with obscure error.\n\n```\n~/apps/vim-static$ docker run -it --rm -v $(pwd):/vim-static:ro -v $(pwd)/vim.symlink:/bin/vim:ro busybox\ndocker: Error response from daemon: mkdir /home/me/apps/vim-static/vim.symlink: file exists.\n```\n\nSo I had a gut feeling that `docker` is doing something fishy with the \"dead\" symlink. So I made the symlink \"real\"; and the error went away; even though it did not work according to my expectation.\n\n```\n~/apps/vim-static$ sudo ln -s $(pwd) /vim-static\n~/apps/vim-static$ docker run -it --rm -v $(pwd):/vim-static:ro -v $(pwd)/vim.symlink:/bin/vim:ro busybox\n/ # ls -l /bin/vim\n-rwxr-xr-x    1 1000     1000           129 Mar  9 11:43 /bin/vim\n```\n\nAs you can see; `/bin/vim` is not a symlink; its been dereferenced.\n\nIs there a way to get around this ?\nIt would be nice to be able to add symlinks to an image, be it dead or alive.\n"},{"labels":[null,"enhancement1"],"text":"Hi!\nAm very excited and pleased about new docker networking features, as introduced in version 1.10.x. However something is missing:\n\nFor every new container I create, if no `--net` flag was specified, the container's network defaults to the default `docker0` bridge network. Or legacy docker0 / no network / whatever.\n\nWhat would be more helpful than that is if user can specify their own custom docker network to be the default option when `--net` was not specified. To save having to put the same `--net my_custom_docker_network` flags in each time over for every container.\n\nNow I did have a go today on docker v1.10.2 / ubuntu wily 15.10. But no luck :(\n\na) There has always been a legacy startup flag `docker daemon --bridge br-my-docker-bridge`. To specify some other bridge interface instead of `docker0`. However that doesnt seem to be aware of the new docker networks or set any kind of default internal `--net` flag inside the go code. What happens is that docker will startup allright, but then omiting the `--net` flag from docker run results in some error:\n\n``` sh\ndocker: Error response from daemon: User specified IP address is supported on user defined networks only\n```\n\nb) Alternatively there is also some bridge driver flag named `com.docker.network.bridge.default_bridge`. Which is visible on the docker0 `bridge` network. You can see it by running the `docker network inspect bridge`. However creating a new network with that flag doesnt work either. And results in some other error message. So I am stumped as things currently stand.\n\nIf theres not a way to do it yet, then I guess this would be a new feature request!\n"},{"labels":[null,"enhancement1"],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.9.1\n API version:  1.21\n Go version:   go1.4.3\n Git commit:   9894698\n Built:        \n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.9.1\n API version:  1.21\n Go version:   go1.4.3\n Git commit:   9894698\n Built:        \n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 10\nImages: 289\nServer Version: 1.9.1\nStorage Driver: btrfs\n Build Version: Btrfs v4.2.2\n Library Version: 101\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 4.3.6-coreos\nOperating System: CoreOS 899.9.0\nCPUs: 4\nTotal Memory: 7.655 GiB\nName: media-srv-1.lungo.co.uk\nID: ANSS:L2RV:YMWL:ZS3X:VYHV:GGQC:HGXQ:YUZS:LGFT:4OXN:QC4P:63G4\n```\n\n**Provide additional environment details (AWS, VirtualBox, physical, etc.):**\nPhysical\n\n**List the steps to reproduce the issue:**\n1. Use `brctl` or any other means to create a bridge to an existing network, in my case the bridge is called `br-services`\n2. Run `docker network create -o com.docker.network.bridge.name=br-services --subnet 172.16.32.0/20 --ip-range 172.16.46.0/23 --gateway 172.16.32.1 services` to create a bridge network using the existing bridge device.\n\n**Describe the results you received:**\n\nNetwork is created and `172.16.32.1` is allocated to the bridge device on the host.\n\n**Describe the results you expected:**\n\nThere should be an option to not allocate the default gateway address to the host machine. In my case I have created the bridge to a vlan network which has a router running on `172.16.32.1` which I would like to be used as the default gateway.\n\n**Provide additional info you think is important:**\n\nEnabling such option should automatically set `com.docker.network.bridge.enable_ip_masquerade` to `false`.\n\nLet me know if you need any more details.\n"},{"labels":[null,"enhancement1",null],"text":"I can't find anything which relates to this, but it would be useful if `.dockerignore` files could exist in any directory, defining paths relative to themselves (like `.gitignore`).\n\nExample:\n\n**Working dir**\n\n```\n.\n├── add-dir\n│   ├── add-me\n│   ├── .dockerignore\n│   └── ignore-me\n├── add-me\n├── Dockerfile\n├── .dockerignore\n└── ignore-me\n```\n\n**Dockerfile**\n\n``` dockerfile\nFROM alpine\nWORKDIR /workdir\nCOPY . .\nCMD ls -R\n```\n\n**both .dockerignore files**\n\n```\nignore-me\n```\n\n**Commands**\n\n``` shell\n$ docker build -t dockerignore-test .\n$ docker run dockerignore-test\n.:\nDockerfile\nadd-dir\nadd-me\n\n./add-dir:\nadd-me\nignore-me # this file would not be inside the container\n```\n"},{"labels":[null,"enhancement1"],"text":"Container image build and deployment is considered a separate process from container runtime in many enterprise environments.\nIt would be super-cool (liberating and sensible) if docker image build was independent of docker-engine, docker file-system storage and didn't require any special privileges on a build host. \n\nIt can be solved in a form of a command line tool responsible for the following:\n- building new image layers from provided content,\n- building new image manifest referencing parent layers and new layers from the step above,\n- uploading new layers/blobs to the registry repository,\n- mounting or uploading parent layers to the registry repository,\n- uploading the new manifest to the registry\n\nSometimes an example is worth a thousand words:\n\n`imagebuild [PARENT]... [LAYER]... --name=IMAGE`\n`PARENT: <IMAGE>`\n`LAYER: <uri>`\n`IMAGE: <registry>/<reponame>[:<tag>]`\n\nI can arrange for contribution of the code if there's any interest? Proof of concept of the above described tool already exists.\n\nImage build split out mentioned: https://github.com/docker/docker/issues/18596 \nPartial overlap of requirements: https://github.com/docker/docker/issues/18585, https://github.com/docker/docker/issues/17982, https://github.com/docker/docker/issues/7115\n"},{"labels":[null,"enhancement1"],"text":"Hi everyone, \n\nNot sure if someone raised this issue yet..\nI would like to be able to use an existing bridge (VLAN) with the default gateway IP configured on an external router. By doing this, all the containers would have it's own IP address directly accessible from the external network without the NAT and port address translation stuff.\n\nContainer ---> Docker network ---> Bridge --> Physical switch ---> Physical Router (Gateway IP). \n\nWhen I create the network with the flag --gateway, this configure the IP address of the gateway on the Bridge interface. This cause a conflict IP address with my physical Router that is on the same VLAN. \n\nThat would be useful to be able to tell Docker to not configure the IP address on the bridge but still manage the IPAM. It could be a key word like \"--external-gateway=true\" or \"--external-gateway=192.168.1.1\" instead of \" --gateway=192.168.1.1\". \n\nEven if the gateway address is external, I still want to use the IPAM provided by Docker to assign IP addresses to my containers with the flag --ip. Moreover, I still want Docker to assign a dynamic IP with the right parameters (default route or dns for instance) to my containers. \n\nThe workaround I'm using right now:\n\nI'm running one script before the Docker daemon start to shutdown the VLAN interface to avoid the Gateway IP conflict. Then, I'm running a script after the Docker daemon is started to remove the IP address from the bridge and bring up the external interface. \n\nThe scripts : \n\ncat /lib/systemd/system/docker.service\n[Service]\nType=notify\nExecStartPre=/opt/IT/docker-startpre.sh\nExecStartPost=/opt/IT/docker-startpost.sh\nExecStart=/usr/bin/docker daemon -H fd://\n\ncat /opt/IT/docker-startpre.sh\n# !/bin/bash\n\nlogger \"DOCKER PRE SCRIPT\"\nlogger \"Disabling interfaces\"\nifdown ens224.555\n\ncat /opt/IT/docker-startpost.sh\n# !/bin/bash\n\nlogger \"DOCKER POST SCRIPT\"\nlogger \"Remove IP address from the bridge\"\nip address delete 10.199.45.1/24 dev br-vlan555\nlogger \"Bring up interfaces\"\nifup ens224.555\n\nThe setup:\n\nExternal VLAN interface (TAG)\n[root@mouimet177 ~]# ifconfig ens224.555\nens224.555: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n        inet6 fe80::250:56ff:fe84:a755  prefixlen 64  scopeid 0x20<link>\n\nBridge interface attached with the VLAN interface\n[root@mouimet177 ~]# ifconfig br-vlan555\nbr-vlan555: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n        inet6 fe80::250:56ff:fe84:a755  prefixlen 64  scopeid 0x20<link>\n\nBridge\n[root@mouimet177 ~]# brctl show\nbridge name bridge id       STP enabled interfaces\nbr-vlan555      8000.00505684a755   no      ens224.555\n\n[root@mouimet177 ~]# docker network inspect net555\n[\n    {\n        \"Name\": \"net555\",\n        \"Id\": \"064395b1573a5ee08c61411dabe750b0b06b0d619fcb5a91c9fbaadde968e039\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"10.199.45.0/24\",\n                    \"Gateway\": \"10.199.45.1\"\n                }\n            ]\n        },\n        \"Containers\": {\n            \"66cc3e0e67343b6c1acc6efcfb1a8d28b9bf8344671fe83247d77aa15e43e15a\": {\n                \"Name\": \"web1\",\n                \"EndpointID\": \"075535a4ae0c0a65406ab108a3a1e2c7663c80e746e448b538ec57c6e9bba77a\",\n                \"MacAddress\": \"02:42:0a:c7:2d:0b\",\n                \"IPv4Address\": \"10.199.45.11/24\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {\n            \"com.docker.network.bridge.name\": \"br-vlan555\"\n        }\n    }\n]\n\nIn this particular case, \"Gateway\": \"10.199.45.1\" is an external router. \n\nThank you in advance for your support, \n"},{"labels":[null,"enhancement1"],"text":"Please add an option to rate-limiting container startup. (when deamon startup / container fail+restart)\n\nReason:\n- On rotational disk, starting lots of container at the same time actually slow down the process.\n- Some container have external dependency. We want it retry/restart automagically when external service fail, but not that fast.\n\nSimilar Issue: #1073 #18936 \n"},{"labels":[null,"enhancement1",null],"text":"In the non-Windows command line, filtering, modifying, and sorting objects can be challenging. It would be great if the `docker` command line were able to emit JSON, instead of pre-formatted, tabular data. Then, utilities such as `jq` could be used to perform filtering, modification, and sorting on the JSON objects that are emitted by the `docker` command. This also works really well in scenarios where you want to perform batch operations with `docker` (eg. kill multiple containers), and want to use `xargs` to assist with that. By emitting JSON, instead of pre-formatted text, you reduce the boilerplate code that's necessary to parse values from output, and improve the confidence that developers have from the output.\n\nThe Microsoft Azure Cross-Platform (xPlat) CLI tool is a great example of this. For example, I can list Resource Groups  in a human-readable, tabular format, or I can use the standardized `--json` switch parameter to change the output to machine-readable JSON text. :heart:\n\nhttps://github.com/Azure/azure-xplat-cli\n\nIf this capability already exists, please let me know how I can access it. :smile: \n\nCheers,\nTrevor Sullivan\nMicrosoft MVP: PowerShell\n"},{"labels":[null,"enhancement1"],"text":"Add instruction TAG with same behavion as build -t option\n\nThis allows you to keep the information on the name of the image file instead of building the script and making it easier to to find a file of this image.\n"},{"labels":[null,"enhancement1"],"text":"Or is there?  I couldn't find this in the docs.\n"},{"labels":[null,"enhancement1"],"text":"I'm sorry if this issue has been discussed some place.\n\nI am using Docker-1.8. Shortly after I can understand it (mostly) there is a new release Docker-1.9. My team is studying to upgrade from 1.8 to 1.9 and we suddenly heard there is also Docker 1.10. Omg, it's very fast!\n\nCould you please have a LTS version, with some features/security patches are ported from the upstream? It's very useful because we would have control of our environments, not the hackers!\n\nThanks a ton for your great work!\n\nUpdate: Fix typoe (which -> with)\n"},{"labels":[null,null,"enhancement1"],"text":"As suggested, I transfer the issue from https://github.com/docker/compose/issues/2657\n\nWhen I test with a compose file like this one\n\n```\nversion: '2'\nservices:\n  #\n  # Application services\n  #\n  app:\n    image: hello-world\n    depends_on:\n      - logstash\n    links:\n      - logstash\n    logging:\n      driver: syslog\n      options:\n        syslog-address: \"tcp://logstash:5514\"\n  #\n  # ELK services\n  #\n  elasticsearch:\n    image: 'XXX/elasticsearch:latest'\n    ports:\n      - '9200:9200'\n    volumes:\n      - elasticsearch-data:/usr/share/elasticsearch/data\n  kibana:\n    image: 'XXX/kibana:latest'\n    links:\n      - elasticsearch\n    depends_on:\n      - elasticsearch\n    ports:\n      - '5601:5601'\n  logstash:\n    image: 'XXX/logstash:latest'\n    command: 'logstash -f /etc/logstash/conf.d/logstash.conf'\n    links:\n      - elasticsearch\n    depends_on:\n      - elasticsearch\n    ports:\n      - '5000:5000'\n      - '5514:5514'\nvolumes:\n  elasticsearch-data:\n    driver: local\n```\n\nI have the ‘ERROR: Failed to initialize logging driver: dial tcp: lookup logstash on 8.8.8.8:53: no such host’ error.\n\nSeems that in the logging option of the app, the logstash service cannot be referenced in syslog-address: \"tcp://logstash:5514\".\nAny idea of a workaround ?\n"},{"labels":[null,"enhancement1"],"text":"I have a Docker image for which I expect the API to change in the future - I expect it to require a different set of environment variables, entrypoint arguments, or mount paths in the future. I could theoretically make a README and require that users read it, but I think it would make my image easier to use if the container could easily enumerate all of these parameters (environment variables, entrypoint arguments, and mount paths) that were given to it, and validate not only that all required parameters are provided, but that no extraneous parameters were provided either.\n\nValidating entrypoint arguments shouldn't be too difficult; I just mention it for completeness as something I want to validate. I imagine that my entrypoint script would just iterate over all the arguments, and make sure that every provided argument is recognized and valid.\n\nValidating environment variables is doable but doesn't seem easy to me. For any environment variables that (the current version of) my entrypoint script currently recognizes, I could have it iterate over those environment variables and make sure that their values are valid. However, I also want it to assert that no unexpected environment variables were given any values: partially to protect against spelling mistakes, partially to more easily validate that if the set of allowed vars changes then there is a clear error, and partially to make it clear which vars are intended to be changed and which are supposed to be internal implementation details.\n\nTheoretically I could write a script that implements the validation that no unexpected environment variables are set by saving the environment at build time and checking for unexpected keys at runtime that aren't found in the build-time environment, but it seems to me that it makes more sense to make something built-in to Docker so that all users can benefit. I think it'd be really nice if my Dockerfile could whitelist environment variables and specify that only those variables are allowed to be changed by anyone that runs the container.\n\nThe validation of mounted file paths I'm not sure how to do, because theoretically even the script doing the validation could be overwritten by a file mount. This feels to me like a less-important problem, though still a similar type in terms of user experience.\n\nOne could make the argument that docker-compose somewhat addresses this problem of wanting to document the required parameters of a container using a working system. Docker-compose does help somewhat but unfortunately, it both lists inputs and gives their values, whereas I'm looking for an intermediate entity that only lists which inputs are valid, so that if I add or remove an input, then I only have to update one place to ensure that it is validated correctly. Since docker-compose files provide values too, we may have multiple docker-compose files per docker image. The compose files also don't automatically come with a docker image - compose files reference docker images rather than the other way around.\n\nAny thoughts?\n"},{"labels":[null,"enhancement1"],"text":"Please document and add option for listing only named or unnamed volumes:\n\n**How**\nI suggest to make something possible like this: `docker volume ls -qf dangling=true,named=false`\nIf there already is such a filter, please document it on docker volume ls --help\n\n**Why?**\nThe reasoning is simple: I have a script to clean up all the old stuff I don't need, and for volumes the most common case is that if they are dangling _and unnamed_, I probably didn't create them to be kept. However, if they are _named_ I am most certainly interested in keeping them even if they aren't currently used by a container - obviously they were important enough that I bothered to give them a name.\n\nTherefore, the common suggestion which involves deleting everything output by `docker volume ls -qf dangling=true` isn't helping here because that command also lists all named volumes, and a script (unlike a human) cannot easily tell the unnamed from the named ones in the resulting output of that command.\n"},{"labels":["enhancement1",null],"text":"I would like to propose having labels for non-container resources.  To start with, these would include:\n- Networks\n- Volumes\n- Images (post build)?\n\nFor networks and volumes the same syntax as containers could be used.  For example:\n\n`docker volume create -d local --label env=staging --label backup=1 redis-data`\n\n`docker network create -d overlay --label env=staging --label app=test-app-v0 backend`\n\nFor images, it would be best to have something like https://github.com/docker/docker/pull/18958 to add labels.  However, update support / image labels might be out of scope for this proposal.\n"},{"labels":[null,"enhancement1",null],"text":"Hi,\n\nThere are some services which have their own formatting, such as Loggly, and I wonder if it would be possible to allow the users to format the whole log line not just the tag.\nThank you.\n"},{"labels":[null,"enhancement1"],"text":"Hello !\n\n**TL;DR:**\nWhen I create a docker network with a bridge that is already configured, Docker overrides the addressing I've set up. Could it be possible that when I create a docker network with an already configured bridge, if I do not supply any additionnal parameters, it configures the IPAM with the in place bridge configuration ?\n\n**More details**\n\nWhat I have :\n\n``` bash\nroot@machine # ip address show br0\n27: br0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 0c:c4:7a:07:6a:7f brd ff:ff:ff:ff:ff:ff\n    inet 172.18.4.1/24 brd 172.18.4.255 scope global br0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::ec4:7aff:fe07:6a7f/64 scope link \n       valid_lft forever preferred_lft forever\n```\n\nThen I create my network\n\n``` bash\nroot@machine # docker network create --driver bridge -o com.docker.network.bridge.name=br0 mynetwork\nc228be0db663a5add797899688d9feb8c2699fc4b0a309edd0476a88526e576b\nroot@machine # docker network inspect c228be0db663a5add797899688d9feb8c2699fc4b0a309edd0476a88526e576b\n[\n    {\n        \"Name\": \"mynetwork\",\n        \"Id\": \"c228be0db663a5add797899688d9feb8c2699fc4b0a309edd0476a88526e576b\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": {},\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.19.0.0/16\",\n                    \"Gateway\": \"172.19.0.1/16\"\n                }\n            ]\n        },\n        \"Containers\": {},\n        \"Options\": {\n            \"com.docker.network.bridge.name\": \"br0\"\n        }\n    }\n]\nroot@machine # ip address show br0\n27: br0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 0c:c4:7a:07:6a:7f brd ff:ff:ff:ff:ff:ff\n    inet 172.19.0.1/16 scope global br0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::ec4:7aff:fe07:6a7f/64 scope link \n       valid_lft forever preferred_lft forever\n```\n\nThe bridge addressing went from `172.18.4.0/24` to `172.19.0.0/16`. Could it be relevant to use already in place configuration ?\n\nAlso, side remark, could the bridge not be deleted if it has not been created with Docker networks ?\n\nThanks a lot !\n"},{"labels":["enhancement1"],"text":"[NoNewPrivileges](https://www.kernel.org/doc/Documentation/prctl/no_new_privs.txt) support was added to the OCI spec and is in the process of being added to [runc](https://github.com/opencontainers/runc/pull/557). The purpose of this issue is to discuss options for integrating this into docker. There are two options:\n1. Add a flag to enable this setting optionally.\n2. Enable this setting by default for all containers.\n\nAny thoughts?\n\n@crosbymichael @LK4D4 @rhatdan \n"},{"labels":["enhancement1"],"text":"### Add (experimental) support for checkpoint/restore to Docker\n\nSupport for C/R has been implemented before (#13602 and #13442), but due to the upcoming move to integrate [`containerd`](https://containerd.tools) in Docker 1.11, the existing checkpoint/restore PR unfortunately had to be closed (see https://github.com/docker/docker/pull/13602#issuecomment-181614492).\n\nThis issue is to track progress on the re-implementation of the checkpoint/restore feature after containerd has been integrated.\n"},{"labels":[null,"enhancement1"],"text":"Hi,\n\nCurrently the existing implementation of user namespaces works as all-or-nothing feature. \nThat is, once user namespaces are set in the global daemon level, there is no way to use any of the advance container privileged operations (e.g., pid=host, net==host or --privileged or mounting host folder w/o changing permissions).\n\nThere are several scenarios where customers would like use user namespaces in the global level but still allow running a subset of privileged containers for various administrative purposes (e.g., maintenance or security).\n\nWe purpose to extend the existing user namespace mechanism by adding a skip flag when creating or attaching to existing containers.\nSpecifically:\n\n``` bash\n$ docker run --skip-userns ...\n$ docker exec --skip-userns ...\n```\n\nTechnically, when --skip-userns option is used, the Docker daemon will ignore converting the user id and will respect the actual --user parameter (or root if no user is specified).\nAlso, when the --skip-userns option is specified, the existing hard-coded daemon limitation (privileged and host/pid namespace) will be removed.\n\nI would be happy to own the design and implementation once we agree on the approach.\n"},{"labels":[null,"enhancement1"],"text":"Is there an example showing how to specify a seccomp bpf filter program to a container? eg. http://man7.org/linux/man-pages/man2/seccomp.2.html\n"},{"labels":[null,"enhancement1",null],"text":"A driver similar to the existing ones, that will forward the logs to a redis server and append them to a specified list. The list holds string representations of jsons. For more information about redis see: http://redis.io.\n\nSpecific log options will be:\n- redis-address (required)\n- redis-database (only necessary, if you have multiple databases on your redis server)\n- redis-password (only necessary, if you have secured your redis server with a password)\n- redis-key (the key to the list of logs, default key 'docker-logger')\n\nWith this new driver one can store logs in a highly available key-value store and process the logs further from there. For example one can easily use logstash to transport the logs to elastic and get a nice dashboard for your logs with the help of kibana.\n\nThe logs will contain:\n- message (the log message)\n- container (the container name)\n- hostname\n- tag (the container tag)\n- attrs (a struct in which all defined envs and labels get stored)\n"},{"labels":["enhancement1"],"text":"From https://github.com/docker/swarm/issues/1692\nI want docker-compose pull command to support pull with constraint on swarm cluster.\nSwarm contributor said it's limitation of Docker Engine so I create issue here.\n\nFor example\n\n``` bash\n$ docker info\nContainers: 29\nImages: 56\nRole: primary\nStrategy: spread\nFilters: health, port, dependency, affinity, constraint\nNodes: 3\n pre-01: pre-01:2376\n  └ Status: Healthy\n  └ Containers: 4\n  └ Reserved CPUs: 0 / 2\n  └ Reserved Memory: 0 B / 2.044 GiB\n  └ Labels: preproduction=true, executiondriver=native-0.2, kernelversion=3.19.0-47-generic, operatingsystem=Ubuntu 14.04.3 LTS, provider=generic, storagedriver=overlay\n prd-01: prd-01:2376\n  └ Status: Healthy\n  └ Containers: 5\n  └ Reserved CPUs: 0 / 4\n  └ Reserved Memory: 0 B / 8.179 GiB\n  └ Labels: production=true, executiondriver=native-0.2, kernelversion=3.19.0-47-generic, operatingsystem=Ubuntu 14.04.3 LTS, provider=generic, storagedriver=overlay\n prd-02: prd-02:2376\n  └ Status: Healthy\n  └ Containers: 4\n  └ Reserved CPUs: 0 / 4\n  └ Reserved Memory: 0 B / 8.179 GiB\n  └ Labels: production=true, executiondriver=native-0.2, kernelversion=3.19.0-47-generic, operatingsystem=Ubuntu 14.04.3 LTS, provider=generic, storagedriver=overlay\n\n$ cat docker-compose.yml\nnginx:\n    image: nginx\n    environment:\n        - \"constraint:label==production=true\"\n```\n\nIt should pull to host by only specific constraint label\n\n``` bash\n$ docker-compose pull\nPulling nginx (nginx:latest)...\nprd-01: Pulling nginx:latest... : downloaded\nprd-02: Pulling nginx:latest... : downloaded\n```\n"},{"labels":[null,"enhancement1"],"text":"It would be extremely useful to be able to edit the logging driver/options for a container without having to recreate it.\n\nThe expected behavior:\n1. Run container `foo` with no log driver\n2. Hit a daemon API endpoint with `{\n\"driver\": \"syslog\",\n\"syslog-address\": \"udp://syslog.server:12345\",\n\"tag\": \"{{.Name}}/{{.ID}}\",\n}`\n3. New logs in `foo` will now go to my syslog server.\n"},{"labels":["enhancement1"],"text":"I am concerned with some of my containers taking more than 10s to shutdown gracefully but I also understand that you don’t want breaking changes to the stop command and have thus refused #6446.\n\nI take care to always use `docker stop -t 100` or `docker kill -s TERM` but shutting down my machine will just stop these containers with the default timeout.\n\nI think it should be possible to configure the default grace period when starting the container (e.g. `docker run --stop-timeout=100`).\n\nGoing further I think this should also be configurable at the image level (i.e. the `Dockerfile`). Also, there should be an argument to the `docker daemon` to configure the default value (for containers that don’t have an explicit value configured).\n"},{"labels":[null,"enhancement1"],"text":"As pylint for Python, we could improve docker build by integrating hadolint from Lukas martinelli\nhttps://github.com/lukasmartinelli/hadolint\nand his online checker\nhttp://hadolint.lukasmartinelli.ch/\n\ndocker build would still build an image, but the author could see the mistakes and immediately correct his Dockerfile\n"},{"labels":[null,"enhancement1"],"text":"Feature Request for `docker build` to add a switch to validate a Dockerfile's syntax is correct.\n\nSince `docker build` has to parse the Dockerfile anyway, exposing a no op syntax test switch shouldn't be a big deal?\n"},{"labels":[null,"enhancement1"],"text":"Now the container's embedded DNS just support alias which not contains `'.'`. If it support multi-level domain alias. Service discovery between containers will be more easy than before.\n### There is a simple service discovery scenario below:\n\nUse nginx to proxy http request to web service, two simple webservice config:\n`docker run -d --net user-define-network --net-alias  wordpress wordpress`\n`docker run -d --net user-define-network --net-alias django django`\n\nThe nginx config below:\n`docker run -d --net user-define-network --net-alias routing --net-alias django.routing --net-alias wordpress.routing nginx`\nOR\n`docker run -d --net user-define-network --net-alias routing --net-alias *.routing nginx`\n\nAnd access the web service through the routing service in other containers:\n`curl -vL wordpress.routing`\n"},{"labels":["enhancement1"],"text":"I would like to be able to share files between containers without using volumes (because that will copy files from the image(s) to host's file system )\n\nLet's say you have a Foobar application, and a docker image for it is provide by a vendor. Inside that image, the application is located in `/opt/FoobarApp/`.\n\nThe application requires extensive configuration ( and other static data ) in order to operate. Application expects that config to be located in `/opt/FoobarApp/config`\n\nWe want to distribute the app and the config as separate images to the hosting servers\n\nThe classic way of solving this would then be to put the config in it's own image/container ( with `VOLUME [ /opt/FoobarApp/config ]` in it's Dockerfile and use `--volumes-from=configcontainer` when starting the application container )\n\nThis approach has the drawback that it will copy the content of `/opt/FoobarApp/config` in the config image to the host's filesystem which is both waste of disk space, IO and time.\n\nThe only solution I see for the  time being is to build the config image on top of the application image ( `FROM FoobarApp` in Dockerfile ). This has the drawback that everytime vendor comes with a new version of FoobarApp image, I have to rebuild my config image from scratch. I will  not be able to use the vendor's image as-is on the production servers. Neither is this approach desirable...\n\nSo my proposal is to be able to mount a directory from container A into container B, something like this:\n\n```\ndocker run --mount=configcontainer:/opt/FoobarApp/config:/opt/FoobarApp/config:ro FoobarApp\n```\n\nI don't know whatever it is technically possible to do a bind mount of union file systems like this, or if it can be solved by \"injecting\" the layer of the config container into FoobarApp container somehow?\n\nMaybe this is a edge-case which I am the only one facing?\n"},{"labels":[null,"enhancement1"],"text":"Is it possible to make restart policy works in such way:\n- if restart caused by OOM, Segfault or other erros in container, it will be restarted\n- if restart caused by reboot, it not restarted, with is helps in case need to run something before any containers starts, but after docker start?\n"},{"labels":[null,"enhancement1"],"text":"In the current docker, we can only use the system global config of /proc/sys/kernel/core_pattern to specify the output place of core dump in container. There are several problems in this way:\n1. It maybe affect the original core dump behaviour config for host.\n2. It can't specify the output place of core dump independently for every container.\n3. The specified output place of core dump is in the container, but there are no tools (gdb e.g.) to investigate the core dump file in the container, and the core dump file may be too large. So it's better\n   to store the core dump out of the container.\n\nOne proposal of implementation steps:\n1. Introduce \"--coredump-helper\" option in docker daemon, this option enables and specifies core dump helper program.\n   E.g.:\n    `docker daemon --coredump-helper=/usr/libexec/docker-coredump-helper`\n   \n    docker-coredump-helper is an executable program built from a project of docker-coredump-helper (it doesn't exsist now, also need to be implemented).\n   \n   Process steps:\n    1) Require root \n    2) Check /usr/libexec/docker-coredump-helper\n    3) Copy /proc/sys/kernel/core_pattern to /var/run/docker/core_patterns/host\n    4) Set /proc/sys/kernel/core_pattern with \"|/usr/libexec/docker-coredump-helper %c %d %e %E %g %h %i %I %p %P %s %t %u\"\n   \n    If the value of \"--coredump-helper\" isn't specified, docker daemon need not do the above steps.\n2. Introduce \"--core-pattern\" option in docker run/create, this option specifies the container core pattern.\n   E.g.:\n    a) `docker run --core-pattern=\"#/var/core.%s.%p.%e\" [IMAGE] [CMD]`  // path with '#' means it would be in host mnt namespace\n    b) `docker run --core-pattern=\"/var/core.%s.%p.%e\" [IMAGE] [CMD]`   // path is in the container mnt namespace\n    c) `docker run [IMAGE] [CMD]`  // if \"--core-pattern\" not specified, default as: \"#/var/run/docker/core_files/[ContainerID]/core.%p\"\n   \n   Process steps:\n     1) Check \"--core-pattern\" option\n     2) Save core-pattern value to /var/run/docker/core_patterns/[ContainerID]\n   \n   ```\n   Tree of core_patterns:\n    /var/run/docker/core_patterns/\n    ├── host\n    ├── [ContainerID1]\n    ├── [ContainerID2]\n    ├── ...\n    └── [ContainerIDn]\n   ```\n   \n     3) When core dump is triggered, the docker-coredump-helper is executed. This program can judge the core dump source process and dispatch the core dump file according the config of /var/run/docker/core_patterns/*.\n"},{"labels":[null,"enhancement1"],"text":"### Proposal\n\n`docker run` currently has a `--user` option for setting the UID and GID for the command to be run inside the container.  I'm proposing adding a `--umask` option that also sets the umask of the containerized process.\n### Implementation\n\nI think this should be straightforward to add.  The actual setting of the umask would happen in [`libcontainer.setupUser()`](https://github.com/opencontainers/runc/blob/749928a0a144c249eb4b39c05e43131b1cc6601a/libcontainer/init_linux.go#L163-L213), right after the [call to `system.Setuid()`](https://github.com/opencontainers/runc/blob/749928a0a144c249eb4b39c05e43131b1cc6601a/libcontainer/init_linux.go#L203).\n\nThis would also require adding a `Umask` system call wrapper in `libcontainer.system` (e.g. in  [`syscall_linux_64.go`](https://github.com/opencontainers/runc/blob/749928a0a144c249eb4b39c05e43131b1cc6601a/libcontainer/system/syscall_linux_64.go))\n### `libcontainer` Spec\n\nInterestingly, the [libcontainer SPEC](https://github.com/docker/docker/blob/0b5a28590f278881309b77f42be06a1da4212e1e/vendor/src/github.com/opencontainers/runc/libcontainer/SPEC.md) says:\n\n> The umask is set back to 0022 after the filesystem setup has been completed.\n\nand indeed, [it is](https://github.com/docker/docker/blob/0b5a28590f278881309b77f42be06a1da4212e1e/vendor/src/github.com/opencontainers/runc/libcontainer/rootfs_linux.go#L81), in [`libpackage.setupRootfs()`](https://github.com/docker/docker/blob/0b5a28590f278881309b77f42be06a1da4212e1e/vendor/src/github.com/opencontainers/runc/libcontainer/rootfs_linux.go#L27-L83).\n\nHowever, the spec never indicates that `umask` was ever changed beforehand.  Since it was not changed outside of the filesystem setup, it seems like an odd place for the umask to be _set_ to `0022` for the execution of the container. It seems like setting this where I suggested, in [`libcontainer.setupUser()`](https://github.com/opencontainers/runc/blob/749928a0a144c249eb4b39c05e43131b1cc6601a/libcontainer/init_linux.go#L163-L213) makes more sense.\n### Use case\n\n_I apologize for the verbosity here, but this is a somewhat complex problem._\n\nI have a utility called [Scuba](https://github.com/JonathonReinhart/scuba) which aids in using Docker to containerize build environments (e.g. with cross-compilers, etc). Scuba wraps up the invocation of `docker run` to make running one-off command lines, like `make`, easier to do inside a docker container. The goal is that users forget they're using Docker at all.\n\nThe most challenging problem has been setting up the call to `docker run` such that artifacts created during the build look like they were created directly on the host. This means:\n##### Owner UID/GID\n\nThis was the hardest part.  Doing nothing, you end up with files in the build directory owned by `root` which is unacceptable. Using `--user` to set the UID/GID of the containerized process works okay, except you end up with a user who has no entry in `/etc/passwd` which causes lots of problems. See JonathonReinhart/scuba#11.\n\nThere are various issues open (see docker/docker#7198 and docker/docker#18825). My latest solution for Scuba (JonathonReinhart/scuba#24) is to inject `/etc/passwd`, `/etc/group` and `/etc/shadow` files into the container which include entries for the invoking user.\n##### Permissions\n\nThe `umask` of the containerized process is different than that of the host (JonathonReinhart/scuba#14). This causes files to have different permissions when created using Scuba than if created natively.\n\nInitially, to solve the uid/gid problem, I had created a script called `.scubainit` which created the user/group and set the umask before using `su` to run the user command. (JonathonReinhart/scuba#13), but this caused problems because the user command was no longer PID 1 (JonathonReinhart/scuba#22).\n\nSo in fixing that (JonathonReinhart/scuba#24), I lost the ability to set the `umask` in the container.\n"},{"labels":[null,"enhancement1"],"text":"When specifying a fluentd-tag value, you can use the following markup tags:\n\n{{.ID}}: short container id (12 characters)\n{{.FullID}}: full container id\n{{.Name}}: container name\n\nIt would be ideal to apply this to the awslogs-stream property.\n"},{"labels":[null,"enhancement1"],"text":"As discussed in #15790, it is too easy to run Docker without content trust enabled.\n\nhttps://github.com/docker/docker/issues/15790#issuecomment-134296126 suggests that this may eventually be fixed simply by flipping the default value of the `--disable-content-trust` option. If that never happens (or until it happens?), it would be useful to support a system-wide configuration option enabling content trust (forcibly, or just unless the user explicitly disables this?).\n\n(No, this is not a new discovery, but AFAICS there is no issue open for tracking this at the moment, and this wording is more specific to what needs to happen than #15790.)\n"},{"labels":["enhancement1"],"text":"I have a PHP application that uses at least 3 locations in its file system to store various temporary files and configs. The paths are hardcoded and cannot be changed. As I would like to access those files from another container, currently the only options I have is to either create 3 volumes for each location or turn the locations into symlinks into directories and files under the common volume and expose only that. \n\nThe first alternative in unattractive due to complexity and the need to be aware about a particular PHP application directory layout in the second container to look for mount points of volumes. The second is better but it still requires that symbolic links is maintained during software upgrade to make sure that those are not overwritten with files and directories from the software distribution.\n\nWhat would be ideal if Docker had an option to specify an explicit bind mount between 2 directories in the container in addition to volume mounts. Then I could run the app like:\n\n```\ndocker run ... -v /volume \\\n    --bind-mount=/volume/cache1:/opt/php-app/cache \\\n    --bind-mount=/volume/cache2:/opt/php-app/some-subcomponent/cache \\\n    --bind-mount=/volume/config-file.json:/opt/php-app/config-file.json ...\n```\n\nHere the bind-mounts are created after `/volume` is created and initialized with content and permissions from the image.\n"},{"labels":["enhancement1"],"text":"Talking in IRC about the new networking that came with 1.9, and how it looks like it's going to replace the `--link` functionality, I thought it would be nice if there were some way to decouple the property of `--link` to pass environment variables to the linked container. Just as an example, `--env-from` was mentioned as a possibility. So what `--env-from` would do is just like it sounds, it would have available the environment variables of the specified container.\n"},{"labels":[null,"enhancement1"],"text":"FEATURE REQUEST\n\nAs Dockerfile build instructions are still changing, I miss the possibility to tag somehow the minimal supported version to clearly state minumum requirements on the building system. - as we know it from Maven. (Yes after all the system would spit buckets of ambigous errors, so I thought a more declarative style might be a benefit)\n\nSomething like\n\nLANGUAGE_VERSION=1.9 \n\nwould me in my opinion really helpful!\n\nthanks. :)\n"},{"labels":["enhancement1"],"text":"Today, the resource constraints values is defined on each `docker run` command. Thinking about cloud-scale Docker deployment, could be nice to define this defaults on Docker daemon. I think that `/etc/default/docker` / `DOCKER_OPTS` could be a nice place to configure it. This way, each container would start with right resources.\n"},{"labels":[null,"enhancement1"],"text":"Feature request for the addition of a `--state` (`-s`) option in the `docker wait` command allowing any valid state to be waited on including:\n- `created`\n- `running`\n- `paused`\n- `stopped` (**default** in order to maintain backwards compatibility) \n- `deleted`\n\nThis would be really useful in scripting environments - the current method that I am using is the following one liner (based off a suggestion from #7228):\n\n```\nuntil [ \"$(docker inspect -f {{.State.Running}} ${DOCKER_CONTAINER})\" == \"true\" ]; do sleep 0.1; done\n```\n\nWhich under my suggestion, would be equivalent to:\n\n```\ndocker wait --state running ${DOCKER_CONTAINER}\n```\n\nThis should be fairly easy to implement given the existing events system. One thing that would be required would be the ability for the container specified to `docker wait` to not exist yet - possibly restricted/defaulted based on which state is chosen or with another flag that would whether to ignore if the container doesn't exist yet: the `created` state, for example, could _never_ be waited upon if the container had to exist.\n# References\n- https://docs.docker.com/engine/reference/commandline/wait/\n- https://github.com/docker/docker/issues/7228#issuecomment-50108257\n"},{"labels":[null,"enhancement1"],"text":"Can we add a simple switch to output only official images for `docker search`, as there is for `--automated=true|false` ?\n"},{"labels":[null,"enhancement1"],"text":"I just found out that a machine had a disk full and was swapping badly because of too many (bad) containers.\n\nDocker was unresponsive and I had to shut it down.\n\nNow I want to bring it back up to clean it up, but without restarting all containers, otherwise I am back to square one.\n\nI think that this deserves an option for docker daemon.\nCheers\n"},{"labels":[null,"enhancement1"],"text":"Provide a way to pass options to the network driver when creating a new user defined network. This would allow (for example) the setting of the `com.docker.network.bridge.name` allow friendly bridge names on the host and allow the re-use of an existing bridge on the system (providing the bridge driver handled this correctly).\n\nExample use:\n\n```\ndocker network create --option=\"com.docker.network.bridge.name=br0\" br0\n```\n# Environment\n\nEnvironment information, although not sure its needed for a feature request:\n## `docker version`\n\n```\nClient:\n Version:      1.9.1\n API version:  1.21\n Go version:   go1.4.3\n Git commit:   4419fdb-dirty\n Built:        Thu Nov 26 02:50:10 UTC 2015\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.9.1\n API version:  1.21\n Go version:   go1.4.3\n Git commit:   4419fdb-dirty\n Built:        Thu Nov 26 02:50:10 UTC 2015\n OS/Arch:      linux/amd64\n```\n## `docker info`\n\n```\nContainers: 2\nImages: 38\nServer Version: 1.9.1\nStorage Driver: btrfs\n Build Version: Btrfs v4.0.1\n Library Version: 101\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 4.3.0-coreos\nOperating System: CoreOS 877.1.0\nCPUs: 4\nTotal Memory: 7.655 GiB\nName: corefs\nID: ANSS:L2RV:YMWL:ZS3X:VYHV:GGQC:HGXQ:YUZS:LGFT:4OXN:QC4P:63G4\n```\n## `uname -a`\n\n```\nLinux corefs 4.3.0-coreos #2 SMP Thu Nov 26 02:49:19 UTC 2015 x86_64 Intel(R) Core(TM) i3-2100 CPU @ 3.10GHz GenuineIntel GNU/Linux\n```\n"},{"labels":[null,"enhancement1"],"text":"Some containers do not need access to outside networks, and exist only to provide services to other containers via the overlay network. These containers should not have the docker_gwbridge interface and there should be an option to prevent them from getting it. Perhaps with a flag to `docker run` or a property that can be added to the overlay network that indicates that machines started with this network do not need the docker_gwbridge.\n"},{"labels":["enhancement1"],"text":"Currently docker sets the ownership and permissions for -v volumes based on those in the image mount point. This is inflexible when the image is used to run several containers with different user ids based on the --user option. In such case one typically wants for the volume to be owned by user:group passed to --user, not the owner of the mount point in the image. In fact unless the mount point has world-writable permissions, that volume would not be useful in the container.\n\nI can workaround this by providing, for example, a setuid command in the container to change ownership of the volume on startup before running the main application or by using a host volume with explicitly set permissions, but this brings rather considerable complexity.\n\nSo it would be ideal if docker provided an option to set the volume ownership based on -u user:group option. Note that this should not affect the host volumes. As currently, they should keep their host ownership. \n"},{"labels":["enhancement1"],"text":"It should be possible for image authors to lock some files/folders in a way that these can't be modified by inherited images. For example:\n\n```\nFROM somelinux\nRUN mkdir /mysoftware\nWORKDIR /mysoftware\nCOPY mysoftware.tar.gz ./\nCOPY script.sh ./\nENTRYPOINT sh /mysoftware/script.sh\nCMD [\"bash\"]\n```\n\nIn the sample above, a user can extend this image and modify the contents of /mysoftware.\n\n```\nFROM imageabove\nRUN do-whatever-with-mysoftware-contents-like-overriding-scripts.sh\nENTRYPOINT by-pass-entrypoint-of-image-above.sh\nCMD [\"bash\"]\n```\n\nOne may say this feature request is solved by [Docker Content Trust](https://blog.docker.com/2015/08/content-trust-docker-1-8/), but in reality not only the source of the image must be guaranteed, but also the image content must be secured that if someone wants to extend it, must do it so in a way that some files/folders (chosen by the image author) can't be modified.\n\nThe image author must be allowed to define what can be modified and what can't.\n"},{"labels":[null,"enhancement1"],"text":"Proposed in #7727 by @virtuald\n\n> docker run has a --cidfile argument. It's a bit strange that docker build doesn't have a similar thing,\n> [...]  a --cidfile option is something that would really only be useful for scripting\n\nProviding this argument can be a nice alternative to the docker-build's quiet mode (read more [here](https://github.com/docker/docker/pull/17428)).\n\nThoughts?\n\nThanks.\n"},{"labels":["enhancement1"],"text":"Unless I missed something, it is not possible to create an image and lock down ENTRYPOINT. With the following command, a user can bypass any instruction that may be required by the image to be performed in order to properly execute:\n\n```\n    $ docker run --entrypoint some-other-existing-script.sh someimage\n```\n\nIt must be possible for an image author to define and lock an entrypoint to make sure a given image is executed by only calling that entrypoint definition and whatever operations that call allows.\n\nIt is also important to ensure that the ENTRYPOINT of an image is always called, either when creating a new container out of this image, or of an extended image. To verify this use case, I created this [Test Scenario](https://gist.github.com/brunoborges/b9e9dd4e3e3ccaa3137b). Run the script **test.sh**. \n\nHere's the output:\n\n```\nbruno@lenovo:~/test-lock-entrypoint$ sh test.sh \nUntagged: entrypoint:latest\nUntagged: overridden:latest\nDeleted: ccbeaae06c6d9236727cfa0293e26acaa912df691f7dfa9014b31d02148de66f\nDeleted: 590ab72673b0e94d815d0370b53c358dce108b9c26366d32ddc70640a0909e6f\nDeleted: 1427d84e7af9c86d173e852157389c7bf650d9b4e1b953e8516a15f6111cd933\nDeleted: 05efde4dcccc7f9c8fdb167467424543453dbd2ade7575b316ceb3678a821932\nDeleted: 2887b23be62d48d530b077e9530af2795938d9d4c528f633a3b2242d82ac3c37\nDeleted: 1e625e037281571d3fd2ea08d677a56d05ae2adcbe8c566d9379281254a8d4e6\nDeleted: 1c0d1742c5c7cd50be97309f3808b356bdfa1047d90a340833fe3818d8532d11\nDeleted: 02c6919bae7562f30ab264c9ad68583d13d507f93b41fdc750f3f2336a38c032\nSending build context to Docker daemon 6.656 kB\nStep 1 : FROM alpine\n ---> d6ead20d5571\nStep 2 : COPY verify.sh ./\n ---> 883e7c374571\nRemoving intermediate container 83006da82f39\nStep 3 : RUN chmod +x verify.sh\n ---> Running in 3ba6771d6b7b\n ---> b3fb87b1bd67\nRemoving intermediate container 3ba6771d6b7b\nStep 4 : ENTRYPOINT ./verify.sh\n ---> Running in 3aaa08502726\n ---> e7aeda0d084d\nRemoving intermediate container 3aaa08502726\nStep 5 : CMD sh\n ---> Running in 5acf82a75b52\n ---> 652c119975c7\nRemoving intermediate container 5acf82a75b52\nSuccessfully built 652c119975c7\nSending build context to Docker daemon 6.656 kB\nStep 1 : FROM entrypoint\n ---> 652c119975c7\nStep 2 : COPY overridden.sh ./\n ---> a8ece7704616\nRemoving intermediate container a6e3bc1666d5\nStep 3 : RUN chmod +x overridden.sh\n ---> Running in 2506fca8cb7d\n ---> d29bff80ded7\nRemoving intermediate container 2506fca8cb7d\nStep 4 : ENTRYPOINT ./overridden.sh\n ---> Running in 2bb4f06c0d6e\n ---> 014226640ad5\nRemoving intermediate container 2bb4f06c0d6e\nStep 5 : CMD sh\n ---> Running in bddb37662abb\n ---> 6e7eb9b1d4c6\nRemoving intermediate container bddb37662abb\nSuccessfully built 6e7eb9b1d4c6\n\n\nDocker run 'entrypoint' without VAR=ok must fail:\n[entrypoint/verify.sh] Verifying variable VAR...\n[entrypoint/verify.sh] Missing $VAR=ok\nTest passed\n\n\nDocker run 'entrypoint' with VAR=ok must pass:\n[entrypoint/verify.sh] Verifying variable VAR...\n[entrypoint/verify.sh] variable exists and it is 'ok'\nTest passed\n\n\nDocker run 'overridden' without VAR=ok must fail:\n[overridden/overridden.sh] Overridden entrypoint. Should be OK to display this, but not without '-e VAR=ok'\nTest failed.\n\n\nDocker run 'overridden' with VAR=ok must pass:\n[overridden/overridden.sh] Overridden entrypoint. Should be OK to display this, but not without '-e VAR=ok'\nTest passed\nbruno@lenovo:~/test-lock-entrypoint$ \n```\n"},{"labels":[null,"enhancement1"],"text":"It would be really useful if `docker stats` included a totals flag like `-t` so that it can add up the stats at the bottom like so:\n\n```\nCONTAINER           CPU %               MEM USAGE / LIMIT    MEM %               NET I/O               BLOCK I/O\ndn1                 0.25%               243.5 MB / 6.24 GB   3.90%               576.4 kB / 1.313 MB   11.5 MB / 589.8 kB\ndn2                 0.27%               246.3 MB / 6.24 GB   3.95%               565.7 kB / 1.327 MB   10.24 MB / 585.7 kB\n...\n=======================================================================================================================\n1175.5kB            1%                  ....\n```\n"},{"labels":[null,"enhancement1"],"text":" Cloud Logging in the Google cloud service(https://cloud.google.com/logging/docs/) is looks similar with AWS Cloud Watch(already supprorted for Docker log).\n It's worth for implement to log driver? if yes, i want to try it.\n"},{"labels":[null,"enhancement1"],"text":"Similar to [`docker export`](https://docs.docker.com/engine/reference/commandline/export/), add support for `docker volume export VOLUMENAME`, which is what [this answer does](http://stackoverflow.com/a/23778599). Similarly, it would be useful to be able to import content into a volume. \n"},{"labels":[null,"enhancement1"],"text":"It should be possible to run two containers that run under the exact same cgroup. One easy and natural way to support that is similar to how namespaces are supported by allowing one to use the cgroups of another container.\n\n@LK4D4 @crosbymichael WDYT?\n"},{"labels":["enhancement1"],"text":"I've been working with @jainvipin on the following proposal for the last week or so...\n## Introduction\n\nIn an enterprise environment, strict control over how the infrastructure is running is required is required to ensure compliance with either business policies or with regulations such as those imposed by HIPAA, SOX or PCI etc…\n\nWhile the work on AuthN #13697 and AuthZ #14674 will allow us to provide Role-Based Access Control (RBAC) this deals with **who** can execute commands and **what** can they execute. Policy is complementary to this as it is focussed on **how** the container is being run.\n- A policy consists of a selector and a one or more constraints\n- A selector is a means of identifying containers that are to be targeted by this policy \n- Constraints are a set of conditions that are to be enforced by the policy \n\nNeither of these concepts are native to the Docker Engine and it is expected that a Policy Engine is responsible for implementing them.\n\nThe following would be examples of policy:\n- “Containers that have the label ‘environment=production’ may only be deployed on networks named ‘prod-*’”\n- “Containers that have the label ‘environment=production’ are to receive preferred access to memory, cpu and network”\n- “Containers that are using the ‘postgres’ image are to be started with at least 2GB RAM\n\nA Policy Engine is responsible for determining:\n- What policies are defined\n- Which policies should be applied to a given container\n- In cases where more than one policy applies, it should resolve any conflicts It should determine:\n  - If a container can be run as intended\n  - If a container will not be permitted to be run\n  - If a policy changes, which containers are affected\n\nIn order for these decisions to be made, I'm proposing that we add a Policy Extension Point to the Docker Engine.\n## Requirements\n\nIn order for this decision to be made, the policy engine _should_ be passed the `ContainerConfig` and `HostConfig`. It may use any of the passed parameters like 'labels', 'network', 'exposed ports', 'domain name', 'ENV' to specify a language to implement a selector. \nIn Future, we may also pass information about the Authenticated User and Roles applied once AuthN and AuthZ work has concluded\n\nThe policy engine should also have the opportunity to make changes to the configuration.\nAs there are cases where the Docker Engine is not responsible for enforcing the result of policy, for example, when the implementation is handled by a Docker Plugin, it is recommended that the result of the policy engine’s computations should be written to the `HostConfig` in such a way that it can be interpreted by the necessary drivers.\n\nThe fundamental reason for being to override behaviour is that constraints are specified by someone who has override authority above whoever is doing 'docker run'.\n\nIn general, there are multiple roles in the infra:\n- Developer\n- SRE or Ops person in a DevOps team\n- Infra Owner (this person is responsible for infra to be up and running either bare-metal servers or VMs, physical/virtual network, physical/virtual storage)\n\nThe override rules are specified by the SRE/Ops person who is responsible for security, and well maintained application infrastructure. The infrastructure operator who wants to offer a set of profiles may also create policies to be consumed by the application SRE/Ops person to use to run those applications.\n\nIt’s the responsibility of the user to choose a compatible Policy Engine, Network Driver, IP Addressing Driver, Volume Driver combination. The communication between these components is expected to take place out-of-band and is outside the scope of this proposal.\n## Proposed Implementation\n### Changes to HostConfig\n\nA new field should be added to HostConfig in support of this change\n\n``` go\ntype HostConfig struct\n    Policy map[string]string\n```\n### Changes to existing plugin protocols\n\nThe Policy field from HostConfig should also be passed as part of the Plugin protocol for Networks, IPAM and Volumes.\n### Policy Extension Point\n\nThe implementation centers around a new Policy Engine extension point.\nThe suggested protocol is as follows:\n#### PolicyEngine.CreateContainer\n\nThis method should be called before the container is started\n\n**Request:**\n\n```\n{\n    “ContainerConfig”: {},\n    “HostConfig”: {}\n}\n```\n\nThe request contains the `ContainerConfig` and `HostConfig` so the policy engine can derive which policies should apply in this instance. In future, this could be expanded to include additional context, for example, the authenticated user that is making the request\n\n**Response:**\n\n```\n{\n    “Err”: “”,\n    “Action”: “permit|deny”,\n    “Policy”: {\n        “com.example.network_policy” : “aa12345b”\n    },\n    “Reason”: “”\n}\n```\n\nThe response contains facility to return a human readable error message in the case of a problem in the container configuration.\n\nThe `Action` field determines whether a container can be run:\n- `permit` - run the container without modification\n- `deny` -  do not run the container\n\nIn every case other than `permit`, the `Reason` field should be completed to provide feedback to the user.\n\nIn all cases other than `deny`, the Map `HostConfig.Policy` should contain results from the policy engine that contains information about any policies that were applied.\nThe keys in the `Policy` field should conform to the same reverse DNS notation used by labels.\n## UX Considerations\n### Select a policy engine\n\n```\n$ docker daemon \\\n    # Policy Engine Plugin to use\n    --policy-engine=mypolicyengine\n```\n### Run in-policy container\n\n```\n$ docker run -itd --net=prod-foo -l environment=production nginx\n959a66911d2ea582b1f9b07cabc6010ca1eb8114d10201041887b783939b241\n```\n### Run out-of-policy container\n\n```\n$ docker run -itd --net=stage-foo -l environment=production nginx\n[Error] Policy Violation: Containers labelled production must be deployed on a network called prod-*\n```\n### Swarm Considerations\n\nThis proposal should work seamlessly with Swarm today, provided that all Engines within a swarm are configured with identical `--policy-engine` flags\n\nIt is clearly desirable for Swarm to be able to apply policy before the request is dispatched to a Docker Engine. Our recommendation would be to extend Swarm and to allow it to do this using the same plugin protocol… This way, policy could be enforced at both the cluster and the host level\n"},{"labels":[null,"enhancement1"],"text":"I know you are [reluctant](https://github.com/docker/docker/blob/master/ROADMAP.md#22-dockerfile-syntax) to accept changes to Dockerfile syntax, but this is important and needed feature.\n\nThis is simpler alternative for [nested](https://github.com/docker/docker/issues/7115) [builds](https://github.com/docker/docker/issues/7149), and extension of \"[multiple FROM](https://github.com/docker/docker/issues/13026)\" already present in Docker.\n\nThe issues Docker currently has:\n1. Builds can only be created by \"inheriting\" other builds, it's impossible to \"compose\" from other builds\n   - this makes it impossible to create multi-stage build, like:\n     - compiling static website in first step of build with Webpack / Jekyll (complex environment), and then packaging result in simple static server (simple environment), like nginx\n     - compiling in dev environment (like go-lang program), and serving in plain busybox image (currently the recommended solution seems to be custom script that extract artifact from build with `cat` https://github.com/docker-library/hello-world/blob/master/update.sh)\n     - people are even desperately [hacking](https://github.com/docker/docker/issues/13026#issuecomment-99580691) multi-FROM syntax to achieve it\n2. Build sizes & layer numbers go through the roof, because final builds have usually include all development dependencies, as well as layers used for building\n   - it's not possible to \"[drop](1996)\" or \"[flatten](https://github.com/docker/docker/issues/332)\" or \"[remove cache](https://github.com/docker/docker/pull/10682)\" of not needed layers\n   - it's also impossible to depend (FROM) on another image without using all the layers it used, even if we only want small portion of it (again, inheritance vs composition)\n3. It is not possible to build multiple images within one Dockerfile with shared layers\n   - one could argue the solution is to split into multiple Dockerfiles, but then it's really hard to [enforce dependencies between images and share layers between them](https://github.com/NVIDIA/nvidia-docker/issues/18#issuecomment-163791061)\n4. The current form \"multiple FROM\" feature is mostly useless, but it could be useful\n   - \"non-final images that are produced are not easily found w/o some ugly parsing of the build output. It would then also leads to people wanting to do things like add TAG commands to tag each image so they're easily used/found. \"\n   - The solution of tagging images [was](https://github.com/docker/docker/pull/886) [proposed](https://github.com/docker/docker/issues/5603) [multiple](https://github.com/docker/docker/issues/4817) [times](https://github.com/docker/docker/issues/13026) (and me) and rejected without good reason\n     - @shykes is concerned about \"polluting global space of tags\", but they can be local to Dockerfile. no need to automatically publish them unless told explicitly with `-t` flag\n     - @tiborvass closed #5603 because of the same reason. Again \"ubuntu\" can be a local-only tag and only exposed if explicitly told with `-t ubuntu:ubuntu` or similar syntax\n     - Also from time to time someone mentions \"no real use cases\" or \"it's not defined problem\", or \"we don't accept new keyword in Dockerfile\" and closes issue..\n\nThose are all _real_ issues with docker build _with_ use cases, and **they could be all solved by three simple modifications to Dockerfile that don't sacrifice repeatability of builds**:\n1. Allow to TAG each layer in Dockerfile (only _locally_)\n2. Allow to COPY from any tagged layer within Dockerfile\n3. Allow to explicitly export local tags with `-t` flag of docker build\n\n---\n\nTo give an example:\n\n```\nFROM dockerfile/nodejs\nADD ./ /app\nWORKDIR /app\nRUN npm install\nRUN npm run build\nTAG development\n\nFROM dockerfile/nginx\nCOPY development:/app /var/www/html\nTAG runtime # not really necessary\n```\n\nThen you can export both runtime and development image with:\n\n```\ndocker build \\\n  -t sheerun/app_development:dev_host_tag:development \\\n  -t sheerun/app_development:run_host_tag\n```\n\nIt would create `sheerun/app_development:dev_host_tag` host image from `development` local tag, and `sheerun/app_development:run_host_tag` host image from `runtime` local tag (by default we export last FROM from Dockerfile, so this change is backward compatible). `development` must be explicitly exported, otherwise it's ignored.\n\nPlease don't close this without a good reason, and without addressing 4 issues I mentioned 💙\n"},{"labels":[null,"enhancement1"],"text":"Other issues (in particular [the proposed `INCLUDE` instruction](https://github.com/docker/docker/pull/12749) #12749) have suggested ways to avoid repetition and I too have an idea.\n\nIf the `FROM` instruction was extended to support a path, base images can be built on demand.\n\nWhereas the `INCLUDE` instruction would effectively rebase layers onto a parent they weren't designed to work with, `FROM <path>` is merely a convenience in not having to first build a regular base image and then build the dependent image.\n\nThis will extend the utility of the `FROM` instruction but not add another top-level instruction to the syntax.\n\nThe path should probably start with `./` to signify that it's a path and not a reference to an image.\n"},{"labels":[null,"enhancement1"],"text":"As it is now, when starting docker, then docker containers, there is no easy way to send logs to a logging daemon running within a container. The container must be started before logs can be sent to it, of course, but any logs from that container, or others that must be started first, can't be sent to the container yet. \n\nIt's a sort of chicken and egg problem, really.\n\nOne solution would be to have docker store all logs using one log-driver at start up, then, with a command (such as `docker daemon --log-driver syslog --log-opt container=<ID/Name> --log-opt forward-logs=true`). All logs stored as the current logging type, and all new logs, would be forwarded to the new logger, in the container.\n\nSo, I'm proposing two new --log-opt options:\n- `forward-logs=<bool>`, which will forward any accumulated logs to the new log driver.\n- `container=<ID/Name>`, which will send all logs to the given container, possibly via a link with the container set up beforehand, or an exposed port of the container.\n\nThis would allow a logging daemon to be easily set up completely within docker with much fewer problems.\n"},{"labels":[null,"enhancement1"],"text":"how does a command like \n\n```\nRUN_P [\"cd app1/; npm install\", \"cd app2; npm install\"]\n```\n\nBoth of these commands would run on separate threads asynchronously.\n\nsound to you all? I think it would be cool to be able to run commands and parallelize build time.\n"},{"labels":[null,"enhancement1"],"text":"I couldn't find this reported elsewhere, so... sorry if it's a duplicate.\n\nI'm looking at the `master` branch.  This is not a bug hence I can't provide any bug report data.\n\nIn the message priority, currently the Docker syslog driver can only do severity `ERR` or `INFO`, depending on whether the message was in `STDERR` or `STDOUT`.\n\nA \"soft\" way to add severity would be to follow the spec in the `--prio-prefix` logger option, which is in newer implementations. See http://man7.org/linux/man-pages/man1/logger.1.html but basically you prefix with `<3>` to indicate numeric severity 3.\n\nHowever it's done, it would be really helpful to have at least 4 severity levels.\n"},{"labels":[null,null,"enhancement1"],"text":"I have a container which needs to be restarted when the machine reboots.  If the process crashes or exits with an error I would like it to stay down so it can be diagnosed.\n\nExisting restart policies:\n- `on-failure` will not achieve either requirement.\n- `always` will achieve the first requirement but not the second.\n- `unless-stopped` is slightly better than `always`, but not quite what I want.\n\nI apologise if this is a misguided request; I searched around quite a lot but didn't find an open discussion of the topic.  I found [this](https://www.pivotfreight.com/devblog/on-docker-restart-policies) which asserts that `on-failure` will restart containers on a daemon restart, but this does not match my experiments or my reading of the code.  On a hard reset of the machine, the exit code will be recorded as zero so no restart.\n\nIn #7586 it is [asserted](https://github.com/docker/docker/issues/7586#issuecomment-57061376) that `always` has a max-retry count, and a max-retry count of 1 would be good enough for my purposes, but again this does not match what I find in the code.\n"},{"labels":[null,"enhancement1"],"text":"Applications which have multiple output streams to files in many cases cannot be easily or at all reconfigured to log to stdout/stderr.\n\nDocker should be able to expose additional FDs to containers which could be collected as logging streams, tagged with a name and FD number. This would let multiple log streams from dockerized applications point their log streams to predictable fifo's to receive them into the docker daemon for aggregation.\n\nA portable implementation would require a Dockerfile syntax change - adding a LOG directive would be most appropriate i.e.\n\n```\nLOG /log/main\nLOG /log/app\nLOG /log/scheduler\nLOG /log/auth\n```\n\nThis would create fifo's at the given paths, that would be collected as structured logs with that name, or an optional given name with a second directive\n\n```\nLOG /log/cron scheduler\n```\n\nTo allow users to harmonize the configuration, equivalent `docker run` directives would also be appropriate:\n`docker run --log /log/cron:scheduler` would override the tagging on the log-stream, to allow log processing to deal with foreign containers.\n\nThis would greatly ease migrating applications to run on the docker platform, and handling logging from containers where multiple apps are needed in the same namespace.\n"},{"labels":[null,"enhancement1"],"text":"Currently, it's not possible to have a cross-architecture dockerfile. This can be evidenced by the docker CI itself where the main image built downloads busybox (linux/amd64) in the dockerfile. Subsequently hoops are jumped through during test-integration-cli which ends up in .ensure-frozen-images to download/update/tag the linux/arm images to ARM specific ones.\n\nThis architecture problem obviously also applies to Windows as well, but is not a Windows specific proposal. I suggest using golang build-style directive-type syntax as a preface to a command. So a dockerfile might contain something like\n\n```\nARCH !windows: COPY linux.sh /runthis.sh\nARCH windows: COPY windows.cmd /runthis.cmd\nARCH !windows: ENTRYPOINT /runthis.sh\nARCH windows: ENTRYPOINT /runthis.cmd\n```\n\nor some more complex examples using the golang type and/or syntax\n\n```\nARCH linux,freebsd 386: <somecommand>\n```\n"},{"labels":[null,null,"enhancement1"],"text":"Hi,\n\nI am testing docker with btrfs, but find 'docker rmi' is very aggressive and doesn't follow\n## btrfs' feature under the following working flow:\n\nimages:\n## Base_tag -> my_tag1 -> my_tag2\n\nIt's impossible to only remove the image of  my_tag1 (rmi will only untag it).\n\nI assume this is some thing related to aufs/overlay, as for such layered fs, it's impossible to remove any middle layer.\n\nBut for btrfs, there is no such limitation, any subvolume/snapshot can exist by its own.\nSo I hope to such behavior can be added when using btrfs.\n\nThis is quite useful is my_tag1 is only a temporary image and differs from both base_tag and my_tags2.\n\nThanks,\n"},{"labels":["enhancement1"],"text":"Would be great to have a similar one to [docker ps --format](https://docs.docker.com/engine/reference/commandline/ps/#formatting) option.\n"},{"labels":[null,"enhancement1"],"text":"My Dockerfile runs several temporary scripts in the container, and thus looks like:\n\n```\nCOPY script1 /tmp/\nRUN /tmp/script1\n\nCOPY script2 /tmp/\nRUN /tmp/script2\n\nCOPY script3 /tmp/\nRUN /tmp/script3\n\nCOPY script4 /tmp/\nRUN /tmp/script4\n\nCOPY script5 /tmp/\nRUN /tmp/script5\n\nRUN rm /tmp/script*\n```\n\nI would like a single instruction which copies a script into the container, executes it, then removes it in a single layer. This would have a number of advantages:\n1. Cleaner dockerfiles\n2. Cleaner `docker history`\n3. Faster builds, as committing layers is expensive on some platforms\n"},{"labels":[null,"enhancement1"],"text":"Currently the default bridge is named always `bridge`. I would be great if there would be a way to name it (while keeping everything else as it is). This would allow better self-documentation.\n"},{"labels":[null,"enhancement1"],"text":"I see the need for a UNVOLUME syntax in Dockerfile for overwriting Volumes from used  Images.\n"},{"labels":[null,null,"enhancement1"],"text":"I see the need for a docker run --volumes-no option that enables me to run a normal production image and start a container instance of it so that it saves the content on docker commit even in the directorys that are normal defined as volume by the source image.\n"},{"labels":["enhancement1"],"text":"### Description of problem:\n\nMany tools used for local development include restart scripts which watch the file system and restart the app when a change occurs. Virtual machines do not propagate filesystem events from mounted volumes, preventing containers with mounted volumes running in virtual machines from seeing these events (read, boot2docker via docker-machine).\n\nMany of us use Docker as a tool for rapidly iterating on code, using [restart scripts](https://github.com/wblankenship/ground-control/blob/master/dockerfiles/graph.sh) watching mounted volumes. But this workflow breaks down when sharing our tooling with team members who run OSx or Windows (since they are relying on virtual machines).\n\nWhile this is a shortcoming with Virtualbox and similar virtualization tools, the maintainers have stated they will not be implementing fs events on mounted volumes. For more background, refer to the `Additional Info` section below.\n\nSince it appears that the virtual machines will not be supporting this anytime soon, would it be worth solving this at the docker level?\n### Proposed Solution\n\nA popular current solution is to rsync the contents of the volume on loop to the virtualmachine and mount the volume from there, avoiding the shared volume all together. This is essentially polling and is expensive.\n\nI propose that the docker tooling take on this responsibility. When running `docker run` with a mounted volume on a remote docker daemon do the following:\n1. sync the contents of the volume to the remote daemon\n2. Mount the remote directory to the container.\n3. Establish a recursive `inotify`/`kqueue`/`ReadDirectoryChangesW` on the local directory.\n4. When a change to the directory is detected, re-run sync.\n\nThis avoids the need of shared volumes for the virtual machine, allowing for the use of filesystem watches within containers\n### Downsides to the proposed solution\n1. Requires a daemon like process to establish and maintain `inotify` events during the lifetime of the remote container.\n2. Potentially expensive operation when syncing large files.\n3. May fill remove VM's hard disk with synced files.\n4. Need to implement a form of garbage collection for remote directories when the container is removed.\n### docker version`\n\n```\nDocker version 1.9.0 build 76d6bc9\n```\n### docker-machine -v\n\n```\ndocker-machine version 0.5.0 (04cfa58)\n```\n### docker info\n\n```\nContainers: 0\nImages: 4457\nServer Version: 1.9.0\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 4712\n Dirperm1 Supported: true\nExecution Driver: native-0.2\naLogging Driver: json-file\nKernel Version: 3.16.0-4-amd64\nOperating System: Debian GNU/Linux 8 (jessie)\nCPUs: 8\nTotal Memory: 31.37 GiB\nName: BigBlu\nID: ABYD:SSMM:VFCM:VX2N:ZJA3:IDFP:6UA2:67XD:5J5J:IDSX:UXYB:63PW\nUsername: wblankenship\nRegistry: https://index.docker.io/v1/\n```\n### uname -a\n\n```\nLinux BigBlu 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1+deb8u6 (2015-11-09) x86_64 GNU/Linux\n```\n### Environment details (AWS, VirtualBox, physical, etc.):\n\nvboxmanage --version\n\n```\n4.3.18_Debianr96516\n```\n### How reproducible:\n\n100%\n### Steps to Reproduce:\n1. `docker machine create -d virtualbox inotify`\n2. `eval \"$(docker machine env inotify)\"`\n3. `touch test`\n4. docker run -v ${PWD}/test:/usr/src/test -it debian:jessie bash -c \"apt-get update && apt-get install -y inotify-tools && inotifywait /usr/src/test\"\n5. Finally, open another terminal and `touch test`\n### Actual Results:\n\nScript doesn't exit\n### Expected Results:\n\n`inotifywait` should have returned `test OPEN` and the script exited.\n### Additional Info:\n\nClosed tickets on the Virtualbox bug tracker referring to this short coming:\n- https://www.virtualbox.org/ticket/10660\n- https://www.virtualbox.org/ticket/14234\n\nThis may be a duplicate issue, I searched around across the docker, swarm, and machine repos and found nothing. Here are similar issues that I found:\n- https://www.virtualbox.org/ticket/14234\n- https://github.com/docker/docker/issues/4023\n- https://github.com/docker/docker/issues/1044\n- https://github.com/docker/docker/issues/4213\n- https://github.com/docker/machine/issues/179\n\nThis seems to be the state of the art solution currently, from @brikis98 et. al.:\n\nhttps://github.com/brikis98/docker-osx-dev\n"},{"labels":[null,"enhancement1"],"text":"I often write Dockerfile like this:\n\n``` dockerfile\nFROM ubuntu:trusty\nMAINTAINER Kurnia D Win <kurnia.d.win@gmail.com>\n\nADD _build /_build/\nRUN /_build/run && rm -rf /_build\n```\n\nbecause sometime 'RUN' is not enough\n\nwith this approach, 'ADD' and 'RUN' will create 2 layer, because each statement will commit an image\ni think, it will be great if these statement can be combine into something,\n\nlike this:\n\n``` dockerfile\nFROM ubuntu:trusty\nMAINTAINER Kurnia D Win <kurnia.d.win@gmail.com>\n\nADD_AND_RUN _build /_build/ /_build/run && rm -rf /_build\n```\n\nor this:\n\n``` dockerfile\nFROM ubuntu:trusty\nMAINTAINER Kurnia D Win <kurnia.d.win@gmail.com>\n\nADD_AND_RUN my_package.tar /target/ /target/setup.sh\nENTRYPOINT [\"/target/run.sh\"]\n```\n"},{"labels":[null,null,"enhancement1"],"text":"Instead of leaving a dangling image around or potentially purging common layers to early, it would be nice if there was an option to cleanly replace an image when pulling or building and tagging so there will not be a dangling image left behind.\n"},{"labels":[null,"enhancement1"],"text":"Most images have some sort of a final cleanup process or other commands that run on every image.\n\nI think it as a supplement of the ONBUILD command it would be great if we had the AFTERBUILD as well so we could run it on every image the uses the FROM inheritance\n\nfor example \n\nAFTERBUILD RUN apt-get purge -y --auto-remove && rm -rf /var/lib/apt/lists/\\* && apt-get clean\n\nthese would be for commands that would be run only after the final command in the dockerfile\n\nplease share your ideas and suggest improvements.\n"},{"labels":[null,"enhancement1"],"text":"(This is a **feature request**)\n\nPerhaps this was discussed somewhere, but I couldn't find something with a quick search.\n\nI was under the impression that the new volume subsystem in 1.9 would replace the need for data-only containers. There are a couple of issues with the data-only containers approach. Basically the need is for a persistent named set of data volumes which are initialised with the contents of respective dirs in the base image. However data-only containers add noise to both the list of containers and the list of images, and prevent me from assigning meaningful names to data volumes.\n\nConsider these two scenarios:\n1. I need a persistent volume for storing uploaded images or I want to deploy my app code/jars somewhere but still be able to re-tweak the container using it.\n2. I need a persistent volume for my mysql's `/var/lib/mysql`.\n\nUnless I missed something, in the first case I can:\n1. Create a named data volume.\n2. Mount it when creating a container using the notation <volume name>:<path inside container>\n\nIn this case:\n- `docker ps` will not show any redundant never-to-be-run containers.\n- `docker volume ls` will be easy to interpret - I gave a name to this particular volume when I created it.\n- `docker inspect` will easily show me that a container is using my named volume.\n- `docker images` will remain clean and not retain unnamed images after retagging.\n\nHowever now consider scenario no. 2, all that has really changed is that when I create this volume, I want it initialised from `/var/lib/mysql` of a particular image. However I am forced to do the following:\n1. Create a \"data only container\".\n2. Use the `--volumes-from` option when creating the app container.\n\nIn this case:\n- `docker ps` shows x2 number of containers, and unless I choose a good naming scheme could end up very messy.\n- `docker volume ls` shows a bunch of random ids that tell me nothing about what's inside those volumes or where I've mounted them.\n- `docker inspect` again shows the random ids which are less helpful.\n- `docker images` will show unnamed images, since if I make changes to the base image, I can't easily re-create the data-only container to use the new image with the existing volumes. This container sticks around pointing to the old image.\n\n---\n\nThere are a couple of ways I can think of that can help here:\n1. Simplest option IMO - provide a `volume rename` verb. This isn't the ideal solution, but I can easily script the rest. However even if using a different solution, I think this operation should be available regardless.\n2. Add some option to initialise a new volume from a declared volume of some image. The problem here is that an image can declare several volumes and they have no names, so will have to be referred to by path. Not as convenient as the `--volumes-from` switch.\n3. Add or tweak existing options to `run` or `create` to give names to certain volumes when they are auto-created and initialised just as they are now. This will replace the need for `--volumes-from` and data-only containers altogether. For example:\n   \n   ```\n   docker run -d --name us.mysql -v us.data:/var/lib/mysql mysql \n   docker run -d --name eu.mysql -v eu.data:/var/lib/mysql mysql \n   ```\n   \n   Will result in the creation of two data volumes named `us.data` and `eu.data` _and_ their init from the contents of `/var/lib/mysql` of the `mysql` image _if_ they don't exist, or mount the existing ones if they already exist.\n4. Another option I thought of is declaring \"data volume sets\" or \"groups\". Which basically completely replace data-only containers. In this case the commands might look something like:\n   \n   ```\n   docker volume create-set --name us.mysql.data mysql\n   docker run -d --name us.mysql --volumes-from us.mysql.data mysql\n   ```\n   \n   In this case the `--volumes-from` refers to the set of data volumes which are collectively named rather than individually named. The only problem here is that it creates possible clashes between volume names and container names, perhaps a solution to that is a new `--volume-set` option instead.\n"},{"labels":[null,"enhancement1",null],"text":"ping @vishh\n"},{"labels":[null,"enhancement1"],"text":"Problem to solve:\nWhen I'm running/destroying bunch of container docker create a lot of volumes. From time to time I'd like to clean up those, but there is no way to distinguish between autocreated volumes and those that I created via `docker volume create`.  \n\nProposed solution:\nAllow add labels to volumes. Then I can label my volumes (or if those autocreated ones have its own label I can use those).\n\nI know this solution is very generic, but I'm just proposing what was done for containers.\n"},{"labels":[null,"enhancement1"],"text":"I'd like to be able to demux logs to multiple log drivers. The use case for this is to log to disk with `json-log` driver so that `docker logs` (live streaming) works but also be able to send the logs to archive via e.g. `fluentd` log driver for longer term viewing/searching/filtering/etc.\n"},{"labels":[null,"enhancement1"],"text":"Currently the docker client always performs a streaming upload (POST, PATCH, PUT). This works well for larger images (presumably where the time it takes to read an image and compress it is long enough for parallel network work to be useful), but for smaller images this is inefficient. See `graph/push_v2.go:pushV2Image` for the area of interest.\n\nIn my prototype (https://github.com/ngorskig/docker/tree/prototype/monolithicuploads), I found that performing a monolithic upload was faster for images up to 16MB. For images <=1MB, monolithic uploads are 2-3 times faster. Monolithic uploads also have lower variance (an order of magnitude in my tests).\n\nThe linked prototype contains two commits. One commit changes `pushV2Image` to perform a branch on image size; if the image is \"small\" (<=16MB), the compressed image is loaded into memory and we call `BlobIngester.Put` instead of starting a streaming upload.\n\nThe other commit changes `BlobIngester.Put` to perform a monolithic POST. For prototyping purposes, it is in the vendored docker/distribution code; getting the client to do this will need a prerequisite change in docker/distribution to be merged into `vendor/`. This is the first blocking issue.\n\nAdditionally, the distribution registry doesn't currently support monolithic uploads. Support will need to be added for the client to work with the registry, but luckily this seems pretty trivial. I have a prototype of this as well (https://github.com/ngorskig/distribution/tree/prototype/monolithicuploads). This is the second blocking issue.\n\nI will file two issues in docker/distribution for these two blockers and link to them here. Let me know what you think.\n"},{"labels":["enhancement1",null],"text":"The demand for Docker on ARM devices is growing as ARM-based IoT devices and ARM-based servers are getting more and more traction. \n\nIn order to properly support Docker on ARM the existing way of building, testing and packaging Docker needs to work on ARM devices. \n\nThat means it should be possible to clone the docker/docker repository on an ARM device and then to execute the appropriate make commands to test, build and package Docker.\n"},{"labels":[null,"enhancement1"],"text":"We now using Docker build to compile our project and then delivery the result executable binary to small docker container, we need new feature, mount volume when run docker build\n## Build binary\n\n```\ndocker build -f BuildDockerfile  -v $(pwd):/out   --rm -t mybuild  (and other arguments)\n```\n\nAfter build finish, will generate two file , one executable binary, and one configure file\n## Make production light weight container image\n\nexample production Dockerfile\n\n```\nFROM scratch\n\nADD mybinary /mybinary\nADD main.conf /main.conf\n\nEXPOSE 80\n\nENTRYPOINT [\"/mybinary\", \"-c\", \"/main.conf\"]\nCMD []\n```\n\nand we can run docker build again to generate a light weight container images container the must have files to run an instance with docker\n"},{"labels":[null,"enhancement1"],"text":"It would be nice if `docker run --rm` accepted an option to keep containers that exit with a non-zero status.\n"},{"labels":[null,"enhancement1"],"text":"This proposal builds upon the [persistent configuration file proposal](https://github.com/docker/docker/issues/17559) and exposes an API and UI for runtime editing of the daemon configuration.  Where possible, configuration changes will be performed immediately on the running daemon and yield a 204 response.  When not possible, the configuration changes will be recorded in the configuration file and take effect on a subsequent restart of the daemon, and yield a 202 response, with a response body message indicating a restart of the daemon is required for the changes to take effect.  Multiple changes may be applied in one operation, or individual changes can be performed one at a time.\n\nAn initial implementation is envisioned to only support KV store and label configuration.  Additional settings may be added over time.  Initial configuration of the KV store would be supported without requiring a restart, however changing the KV store would require restarting the daemon.  Updating daemon labels would be supported without requiring a restart.\n\nIf the configuration change requires a restart, subsequent queries for the configuration will return the persistent configuration and a status of “restart required”  Future enhancements could offer a query parameter to distinguish active from persisted state.\n### Display current configuration\n\n`GET /config`\n\nDisplay current configuration\n\n**Example request**:\n\n```\nGET /config HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    “status”: “active”,\n    “configuration”: {\n        \"cluster-advertise\": “192.168.1.2:2376”,\n        \"cluster-store\": “etcd://192.168.1.2:2379”,\n        \"cluster-store-opt\": {\n            “kv.cacertfile”: “/etc/docker/ssl/ca.pem”,\n            “kv.certfile”: “/etc/docker/ssl/cert.pem”,\n            “kv.keyfile”: “/etc/docker/ssl/key.pem”\n        },\n        “label”: {\n            “storage”: “ssd”,\n            “rack”: “42”\n        }\n    }\n}\n```\n### Change configuration\n\n`POST /config`\n\nChange configuration\n\n**Example request**:\n\n```\nPOST /config HTTP/1.1\n\n{\n    \"cluster-advertise\": “192.168.1.2:2376”,\n    \"cluster-store\": “etcd://192.168.1.2:2379”,\n    \"cluster-store-opt\": {\n        “kv.cacertfile”: “/etc/docker/ssl/ca.pem”,\n        “kv.certfile”: “/etc/docker/ssl/cert.pem”,\n        “kv.keyfile”: “/etc/docker/ssl/key.pem”\n    },\n    “label”: {\n        “storage”: “ssd”,\n        “rack”: “42”\n    }\n}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 204 OK\n```\n\nIf the caller attempts to change a setting (or a map key) that was passed to the daemon as a CLI argument, the request will be rejected with a 403 response code, and a payload indicating the setting was specified via a command line argument.  To change the setting, the daemon flags must be changed manually.\n\nTo remove a setting from the configuration file, the key is passed with an empty string value in the POST.\n\nThe following examples illustrate the UI for configuration management:\n\n``` bash\n% docker config set \\\ncluster-store=etcd://1.2.3.4:5678 \\\ncluster-advertise=tcp://1.2.3.4:2376 \\\ncluster-store-opt=kv.cacertpath=/etc/docker/ssl/ca.pem \\\ncluster-store-opt=kv.certpath=/etc/docker/ssl/cert.pem \\\ncluster-store-opt=kv.keypath=/etc/docker/ssl/key.pem \\\nlabel=storage=ssd\n\n% docker config ls\ncluster-store=etcd://1.2.3.4:5678\ncluster-advertise=tcp://1.2.3.4:2376\ncluster-store-opt=kv.cacertpath=/etc/docker/ssl/ca.pem\ncluster-store-opt=kv.certpath=/etc/docker/ssl/cert.pem\ncluster-store-opt=kv.keypath=/etc/docker/ssl/key.pem\nlabel=storage=ssd\n```\n"},{"labels":["enhancement1",null],"text":"This proposal describes a new configuration file format for the docker daemon.  The objective is to provide a consistent mechanism for configuration of the daemon across different operating systems and distributions.  The file should be both human and machine friendly, supporting direct editing by users, as well as by the daemon based on API calls.  The file will be encode in [TOML](https://github.com/toml-lang/toml).\n\nThe configuration will live in a well known location of `/var/lib/docker/engine.cfg` with a sym-link from `/etc/docker/engine.cfg`\n\nWhen the daemon starts, the configuration file will be loaded first.  Then any CLI flags will be processed, and where the CLI flags overlap with the configuration file, the values provided by the CLI flags will override the configuration file.  For map style settings (e.g., labels) the final value will be the union of settings from the configuration file and CLI arguments, and in the case of overlapping keys, the CLI values will override the values in the configuration file.  No mechanism will be provided to “unset” a value via the CLI flags that was set in the configuration file.\n\nThe following example illustrates a hypothetical configuration file:\n\n``` TOML\n# Example docker daemon configuration\ncluster-advertise=\"192.168.1.2:2376\"\ncluster-store=\"etcd://192.168.1.2:2379\"\n\n[cluster-store-opt]\n\"kv.cacertfile\"=\"/etc/docker/ssl/ca.pem\"\n\"kv.certfile\"=\"/etc/docker/ssl/cert.pem\"\n\"kv.keyfile\"=\"/etc/docker/ssl/key.pem\"\n\n[label]\nstorage=\"ssd\"\nrack=\"42\"\n```\n"},{"labels":[null,null,"enhancement1"],"text":"Currently, privileged isn't supported when user namespaces are on. However, there are many use cases where one would want to run privileged containers side by side with user namespaced containers.\n\nHow about we disable (don't apply) user namespace mappings when --privileged is passed, so we can have privilege mean the same whether user namespaces are in use or not?\n"},{"labels":["enhancement1"],"text":"Should we add this option back since a lot of people want it? \n\nhttps://github.com/docker/docker/pull/5001#issuecomment-151058939\n"},{"labels":[null,"enhancement1"],"text":"In @boucher 's branch, Docker supports container checkpoint (C) and restore (R). One important use case of the C/R is to migrate a container from one host to another. Currently a p.haul driver type is being developed and reviewed by @xemul for migrating Docker container [1].\n\nHowever, the biggest challenge is that after transferring the criu images along with other necessary container files, how to load the container state by the docker daemon on the destination side? In the current patch [1], the docker haul driver has to restart the docker daemon on the destination side so that the docker daemon can read the container rootfs under /var/lib/docker/ and load\nthe container state.\n\nFor example, a container with ID 765 is migrated from host A to host B. The docker haul driver pulls the rootfs of contianer 765 from host A to B; the roots /var/lib/docker/aufs/mnt/765.... is created on host B. However, docker daemon cannot recognize the checkpointed container 765 until being restarted. Then \"docker ps\" shows container 765 in the \"checkpointed\" state. All other running container have to be restarted due to restarting the daemon.\n\nTherefore, we propose adding to the C/R branch the API that supports loading container state to docker daemon without restarting the docker daemon. The usage of this API is \n\n```\ndocker load-state [directory name of checkpointed container's rootfs]\n```\n\nSince both rootfs and configuration files have already been created on the destination node; the implementation of this API essentially notify the docker daemon about the new container with its checkpointed state.\n\nWith this API, the migration process would become more smooth and less interruptive to the destination node.\n\nDifferences from \"docker create\" and \"docker load\".\n- \"docker create\" creates a new filesystem layer and a new container ID. This container\n  is different than the migrated container.\n- \"docker load\" loads the container images and does not deal of the state the \n  migrated container. Nor does \"docker load\" informs the docker daemon of the\n  container ID.\n\n[1] https://lists.openvz.org/pipermail/criu/2015-October/023047.html\n"},{"labels":[null,"enhancement1"],"text":"libnetwork allows driver specific options for ipam drivers but there does not seem to be a way to pass them in via the cli or api.\n\ndocker version: `Docker version 1.9.0-dev, build 4816626`\ndocker into:\n\n```\nContainers: 13\nImages: 9\nServer Version: 1.9.0-dev\nStorage Driver: devicemapper\n Pool Name: docker-253:0-33662528-pool\n Pool Blocksize: 65.54 kB\n Base Device Size: 107.4 GB\n Backing Filesystem: xfs\n Data file: /dev/loop0\n Metadata file: /dev/loop1\n Data Space Used: 2.595 GB\n Data Space Total: 107.4 GB\n Data Space Available: 10.87 GB\n Metadata Space Used: 2.691 MB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 2.145 GB\n Udev Sync Supported: false\n Deferred Removal Enabled: false\n Deferred Deletion Enabled: false\n Deferred Deleted Device Count: 0\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\n Library Version: 1.02.82-git (2013-10-04)\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 3.10.0-229.14.1.el7.x86_64\nOperating System: CentOS Linux 7 (Core)\nCPUs: 2\nTotal Memory: 1.797 GiB\nName: ccompute1.rmb938.com\nID: IAQS:6E74:7NGG:5JOG:JXFM:26VD:IAQV:FZNU:E23J:QUAA:NI4O:DI3S\n```\n\nuname -a `Linux ccompute1.rmb938.com 3.10.0-229.14.1.el7.x86_64 #1 SMP Tue Sep 15 15:05:51 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux`\n\nThe current cli and api has the ability to pass in driver specific options for network drivers but not ipam drivers.\n"},{"labels":["enhancement1"],"text":"With PR #15348 `--restart=unless-stopped` was added as an option. This is a great alternative for `--restart=always`, but it doesn't cover cases where you want `--restart=on-failure=...` as well as keeping the container stopped upon reboot when it was manually stopped earlier. Is there any option to do this already?\n\nSide note: this is related to dokku where on-failure is now used by default. Each time an application is published by dokku a new container is created and the old container is stopped. However, upon rebooting the system, all old containers will start again too. Usually this means an old version of an application is listening on the published TCP port instead of the latest version. See https://github.com/progrium/dokku/issues/1505.\n"},{"labels":[null,"enhancement1"],"text":"Hi! I do know that current Dockerfile format is freezed and you do not accept any new proposals, but I want it to be as a \"TODO\" for the future when this \"dockerfile freeze\" will end.\n# Problem\n\nCurrently docker cache (which is filesystem-based cache) works in the way of sequential caching - if one step changed then all steps will be invalidated. It's good until you have some 100% non-conflict filesystem changes. Few examples:\n1. You're using some Java application server. To install it, you need: some base image like ubuntu, java and server itself (like Tomcat). And your WAR file as well. And all of them could be installed in parallel because they don't share filesystem (Java goes to /opt/java/, Tomcat goes to /opt/tomcat/, etc).\n2. You're deploying your app as a Go statically linked binary and you use \"alpine\" as a base image. Then you update base image (because of some critical security fix). Currently Docker will re-apply your Go app on top of alpine, but you don't need it since they have no conflicts in file system.\n# Proposal\n\nIntroduce _CACHE__ commands in Dockerfile. Example:\n### Before\n\n``` Dockerfile\n# base image update will clear the cache\nFROM ubuntu\n\n# Java update (which is frequent) will do the same\nRUN <install Java> \n\nRUN <download and install Tomcat>\n\nADD app.war /path/to/ROOT.war\n```\n\nSo, all of them are separate folders in file system.\n### After\n\n``` Dockerfile\n# will force re-cache only on dependent layers\nCACHE_LAYER ubuntu_installed\nFROM ubuntu\n\n# Will be changed only in case of change in base image\nCACHE_FROM ubuntu_installed\nRUN apt-get update && apt-get install -y htop && rm -rf /var/lib/apt/lists/*\n\n# tools for other steps\nCACHE_FROM ubuntu_installed\nCACHE_LAYER base_tools\nRUN apt-get update && apt-get install -y unzip && rm -rf /var/lib/apt/lists/*\n\n# We still need ubuntu because of wget/tar, but we don't want this step to be re-executed after it was cached once (because each version of wget/tar would be fine)\nCACHE_FROM scratch\nCACHE_AFTER ubuntu_installed\nCACHE_LAYER java\nRUN <download and untar Java>\n\nCACHE_FROM base_tools\nCACHE_LAYER tomcat\nRUN <download and unzip Tomcat>\n\n# means that we want to use \"scratch\" meta-label which is \"empty filesystem\"\nCACHE_FROM scratch\nCACHE_LAYER app\nADD app.war /path/to/ROOT.war\nENV JAVA_OPTS=\"-Xmx1024m\"\n\nCACHE_FROM java, tomcat, app\nRUN <deploy app>\n```\n\nIf Java step was changed, then it will execute only following steps and will use `base_tools` layer as a base\n\n``` Dockerfile\nRUN <download and unpack Java>\nRUN <deploy app>\n```\n\nIf app was changed, then it will execute only following steps:\n\n``` Dockerfile\nADD app.war /path/to/ROOT.war\nRUN <deploy app>\n```\n\nIf base image was changed, it will trigger cache invalidation for the following steps (because of base tools dependency):\n\n``` Dockerfile\nRUN apt-get update && apt-get install -y htop && rm -rf /var/lib/apt/lists/*\nRUN apt-get update && apt-get install -y wget curl unzip && rm -rf /var/lib/apt/lists/*\nRUN <download and unzip Tomcat>\nRUN <deploy app>\n```\n"},{"labels":[null,"enhancement1"],"text":"There are numerous open issues in regards to docker and DNS handling within containers: (#17190 #16619 #15978 #14627 #15819 and likely many others which I was fuzzy on)\n\nOne solution I think which would solve all these issues would be if docker acted as a DNS server. It would answer lookup requests for linked containers, and when the request isn't for a linked container, it would forward it upstream (to the host's name servers).\n\nThe `/etc/hosts` file inside the container would then be static, containing only the container itself.\nWe could also not touch `/etc/hosts` at all, and leave the container's entry to DNS. This would allow image builds to manipulate the file and persist the changes.\n\nFor performance, it would probably be good if docker cached the upstream DNS records. Records come back with a TTL, so docker should cache the record until this TTL expires.\n"},{"labels":[null,null,null,"enhancement1"],"text":"Right now, through the help of some fantastic folks like @albers and @sdurrheimer, autocompletion scripts for bash, zsh, and fish are part of this repo.\n\nIt would be really awesome if we could auto-generate these completion scripts from source rather than continuously having to play catch up.\n"},{"labels":["enhancement1",null],"text":"I wonder why there is no log driver for simple text based log files. I don't want JSON format. I want the same format as what I get from `docker logs` in a plaintext file.\n\nSo unless I've missed something I suggest a simple `file` log driver. It would also be nice if this driver offered a way to rotate the logs with logrotate, without messing things up (e.g. still logging to the old file after rotation).\n"},{"labels":[null,"enhancement1"],"text":"Hi,\n\nIn my Dockerfile, I would like to do something like:\n\nENV MY_USER=`whoami`\n\nUSER root\nCOPY ...\nRUN chmod ... && chown ...\n\nUSER $MY_USER\n\nI.e. storing the current user,\nthen using root user,\nthen copying files and setting expected permissions,\nthen finally restore current user\n\nAny ideas?\n"},{"labels":["enhancement1"],"text":"Now that Docker is integrating with an increasing number of third parties, it would be great if Docker could make it easier for others to run integration tests against pre-release versions of Docker. This would allow them to spot issues and regressions before they made it into releases.\n\nThe docker:dind image (https://hub.docker.com/_/docker/) is a great tool for testing but it only has tags for official releases. It should be easy to set up an automated build to update a `dind-master` on each checkin, but that's something that only Docker can easily do.\n\nping @jfrazelle @tianon @jpetazzo \n"},{"labels":["enhancement1"],"text":"Please add support for mac os x Containers.\n\nMac os x contains an already built-in Application Container system that can easily be utilized and built in Docker.  Settings are stored in .plist files inside these containers, and the application containers can contain Mac Frameworks, Commandline libraries, etc.  It is called the App Sandbox and meant for security, but exploitable for Docker.\n\nSee https://developer.apple.com/library/mac/documentation/Security/Conceptual/AppSandboxDesignGuide/AppSandboxInDepth/AppSandboxInDepth.html\n\nThe Mac Desktop can basically launch GUI Apps in this format but Docker can utilize this format without any low-level code in the OS.  The code to use the containers can be made generic as part of the container code that docker can manage.  Access to some shared code in the OS is optionally available to settings in the plist files.\n"},{"labels":[null,null,"enhancement1",null],"text":"Description of problem:\n\nDocker uses AppArmor on Ubuntu, which is great, but without an include line as mentioned above the user is forced to modify the file _every time she upgrades docker._  This is both very frustrating and already solved by local include files in more typical dists.  See `/etc/apparmor.d/local/README` on ubuntu for details.\n\n`docker version`: \n\n``` Client:\n Version:      1.8.2\n API version:  1.20\n Go version:   go1.4.2\n Git commit:   0a8c2e3\n Built:        Thu Sep 10 19:21:21 UTC 2015\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.8.2\n API version:  1.20\n Go version:   go1.4.2\n Git commit:   0a8c2e3\n Built:        Thu Sep 10 19:21:21 UTC 2015\n OS/Arch:      linux/amd64\n```\n\n`docker info`: \n\n```\nContainers: 5\nImages: 154\nStorage Driver: overlay\n Backing Filesystem: extfs\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 3.19.0-30-generic\nOperating System: Ubuntu 15.04\nCPUs: 4\nTotal Memory: 15.37 GiB\nName: zfp\nID: WWAV:EBRG:XQKU:LLZ7:JUMU:MDR4:BPJ4:C7T3:XKVQ:WFUT:3DAP:6ZHU\nWARNING: No swap limit support\n```\n\n`uname -a`:\n`Linux zfp 3.19.0-30-generic #33-Ubuntu SMP Mon Sep 21 20:58:04 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux`\n\nEnvironment details: n/a\n\nHow reproducable: 100%\n\nSteps to reproduce:\n1. Install docker\n2. Be frustrated when ptrace doesn't work in a container (eg rsyslog, monit, and strace all break) and you have to add a ptrace line to /etc/apparmor.d/docker and restart the apparmor \"service\"\n\nActual Results: annoyance and frustrating ;)\n\nExpected Results: the ability to tweak this once and have it Just Work\n"},{"labels":["enhancement1",null],"text":"Modify contrib/mkimage-arch.sh and add .conf files to support generation of base images on Arch Linux ARM machines.\n"},{"labels":[null,"enhancement1"],"text":"docker version: 1.8.2 client / 1.8.2 server\ndocker info: local virtualbox (boot2docker) created by docker-machine\nuname -a: OSX\n\nList the steps to reproduce the issue:\n1. I expect to be able to set a disk volume memory limit, e.g. -v '/var/elasticsearch-data(5gb)' to create a volume that can only use 5gb of disk space.\n\nDescribe the results you received:\n\nIt's not possible to set a limit/quota on a docker volume. There are suggested workarounds (see below), but require special docker server configuration and manual host directory management.\n\nDescribe the results you expected:\n\nIt would work similarly to setting the `--cpu-quota` or `--memory`.\n\nProvide additional info you think is important:\n\nIt seems like it would be useful and consistent to allow docker volumes to be limited in size, similar to the way cpu can be limited when running a container.\n\nI see the suggestions here for limiting the disk space available to a container:\nhttps://groups.google.com/d/msg/docker-user/jo1Muu7FXg4/5pIAz7nPHo8J\n\n> - use the devicemapper driver; then the limit will be the size of the filesystem (10 GB by default; which is maybe a lot compared to what you really want!)\n\nI use docker-machine/boot2docker, devicemapper is not enabled by default afaik, is it safe? I have very bad memories of docker on on CentOS / devicemapper.\n\n> - use USER (or another mechanism) to make sure that the process in the container runs a specific UID, then bind-mount a volume of a limited size from the host, and make it so that's the only place where this UID can write\n\nDoes that imply docker's \"automatic\" volumes can't be used, a fixed directory on the docker host must be used instead? I currently rely on docker managing the volumes.\n"},{"labels":[null,null,"enhancement1"],"text":"# Problem\n\nIn private cluster, we run docker registry in none https way, and may multiple registries. But docker must set daemon parameter `--insecure-registry`. This is not easy to use. For example, some team builds a docker cluster, they must modify docker daemon parameter and restart hole cluster in order to use a new none https registry.\n# Proposal Request\n\nWe prefer to add docker cli to support this. like this:\n\n```\ndocker registry add --insecure-registry registry.xxx.xxx.com\n```\n"},{"labels":[null,"enhancement1"],"text":"This is a quick write up of a design to enable programmatic bootstrapping of a Docker cluster.  I will quickly submit a PR if we can decide this is generally the right direction.  The motivation of this is to allow bootstrapping a cluster in a more user friendly fashion and additionally allow scripts and other programs to setup a cluster without having to deal with distro specific configuration files to handle daemon flags.\n## API\n\n`POST /join`\n\n``` json\n{\n  \"DiscoveryBackend\": \"...\",\n  \"DiscoveryAddress\": \"...\",\n  \"KvStore\": \"...\",\n}\n```\n## CLI\n\n`docker join --backend ... --address .. --kv-store ...`\n## Design\n\nIf no options are passed as daemon flags for `--discovery-backend`, `--discovery-address`, and `--kv-store` then the `/join` API can be used.  If any of those daemon flags are set the `/join` API should fail.  When POST-ing to `/join` the three arguments will be passed to `pkg/discovery` to initialize the discovery backend.  Additionally these parameters will be saved in `/var/lib/docker/discovery/config.json` to be used if the daemon restarts.\n\nOn start of the daemon discovery info will be read first from the CLI and then if not available will look in `/var/lib/docker/cluster/config.json`\n"},{"labels":[null,"enhancement1"],"text":"Hi Guys,\n  I am making images that needs some information from the user at build time to make their stuff, for example we want to provide an NVM image that abstract the user of how their version is installed\n\nFor example want our users to have the next kind of images\n\n```\nFROM magic_nvm\nENV NODE_VERSION=4.0.0\n```\n\nSo when the image is build we can read that version and make something or install a default\n\nThe idea is to have a **magic_nvm** image like the next\n\n```\n FROM ubuntu\n RUN install_nvm\n ENV NODE_VERSION=2.0.0\n AFTERBUILD update_nvm_version\n```\n\nI imagine this feature like the ONBUILD but instead of add the lines to begining of the next Dockerfile add to the end.\n\nThat feature make sense for you?\n"},{"labels":[null,"enhancement1"],"text":"i have ran into problems in the past attempting to mount a file that is accidentally not present.\n\ndocker will assume that i want to mount a folder and it will create a folder (named after the missing file) creating confusion.\n\nfor example:\n\n```\ndocker run -v /path/to/file.txt:/file.txt me/my-image\n```\n\nwill create a folder named `file.txt` at `/path/to` if `/path/to/file.txt` doesn't exist.\n\ni burned a few hours on this once with a docker-ized node app that was complaining that i was trying to open a folder as a file.\n\ni thought it might be smoother if i could signal to docker that i am going after a file v a folder, and then if i specify `:ro` have it throw an error if it doesn't exist, or if it is `:rw` (default) have it just create the file.\n\nso something like a `-f` option to indicate mounting a file, like so:\n\n```\ndocker run -f /path/to/file.txt:/file.txt me/my-image\n```\n"},{"labels":[null,"enhancement1"],"text":"I'm currently using Logstash to ship the logs from my Docker containers to a central location. At the end I'm feeding the logs into Elasticsearch and using Kibana to visualise the data. In the middle I've got a RabbitMQ cluster routing the log messages which means that the system can deal with spikes in log traffic and the temporary loss of the Elasticsearch servers without losing any data.\nAt the moment I have the containers writing their logs to a mounted volume and Logstash sends this to the RabbitMQ cluster to be routed. Ideally I'd have the containers sending their logs directly to the RabbitMQ cluster so I could do away with the volume mount and having to run Logstash or Logstash-forwarder on each server. There's already an issue open to create a Logstash log-driver (#14949) but I don't feel that system would be as scalable or resilient as using a messaging broker to handle the traffic. Another benefit is that, because the log messages aren't altered before being sent, any system(s) can consume them from the broker.\nI've already put together a proof of concept that is built on the fluentd log-driver. My plan is to develop it further and bring in SSL support but I first wanted to make sure that there was some interest and a willingness to incorporate this feature into Docker.\n"},{"labels":[null,"enhancement1"],"text":"As the Docker website continues to expand, it is going to become increasing difficult to find the information required by navigation alone. \n\neg. To find more information about a particular command, such as docker info, I used the following search string in Google: `docker info site:docs.docker.com`\n\nSee: https://www.python.org/\n"},{"labels":["enhancement1"],"text":"I liked the idea of having flexible and expressive JSON messages squeezed in a single line and create a stream of them one per line for logging. I liked the idea of breaking single root node restriction posed by JSON which makes parsing JSON a nightmare for long files. I also like the idea of adding some additional lookup key information in front of each entry such as timestamp or something else (for example fluentd adds one or more keys in front of each JSON block). This helps using text processing tools like grep, awk, sort, and cut etc. on such files while keeping the data portion as flexible as possible.\n\nI am a Web archive researcher. We have huge file based indexes. For quite some time we were using a predefined column based format called CDX which is very specific to Web archiving needs and it does not have a lot of options for extension. Some time ago an archiving tool developer realized that we can combine CDX and JSON to bring more flexibility in it, we called it CDX-JSON or CDXJ. Later I found that similar format is already being used in Docker and Fluentd logs and is quite useful.\n\nHowever, calling them JSON as a whole is technically incorrect and no JSON parser will parse such files. To me, they are stream of objects instead and deserve their own media type, which allow general tooling around this format. For this purpose we are trying to formalize this format and if enough interest is shown, we are planning to write an RFC for this. I have published [a blog post about the ORS](http://ws-dl.blogspot.com/2015/09/2015-09-10-cdxj-object-resource-stream.html) and would like to hear some thoughts from the Docker community on it. First half of the blog post might not be very relevant to some people as it is specifically talking about an archiving related issue, but the portion where couple of railroad diagrams are added is more general description of the Object Resource Stream (ORS) serialization.\n\n![Object Resource Stream (ORS)](http://4.bp.blogspot.com/-7NUHdUWRC6E/VenV6AlqGnI/AAAAAAAAe0I/JtslxAIHrkQ/s1600/ors-railroad.png)\n"},{"labels":[null,"enhancement1"],"text":"Hello Docker community!\n\nI work on [Splunk's](http://www.splunk.com) core platform. We've been seeing interest from customers in pushing logs captured from containers to Splunk. Once in Splunk, customers would like to utilize Splunk's query, reporting, dash-boarding and alerting to monitor their Docker containers.\n\nWe're interested in contributing a log driver for Splunk. The driver will push events to the new [HTTP Event Collector](http://dev.splunk.com/view/event-collector/SP-CAAAE6M) in Splunk 6.3. With the driver in place, a Docker user will be able to select the driver and specify their  Splunk host information after which events will start to flow. The new driver would be compatible with [Splunk Enterprise](http://www.splunk.com/en_us/products/splunk-enterprise.html), [Splunk Light](http://www.splunk.com/en_us/products/splunk-light.html), and [Splunk Cloud](http://www.splunk.com/en_us/products/splunk-cloud.html).\n\nWe'd love your feedback on this proposed work. We're keen on pushing this forward!\n# High level design\n- A new `splunk` log-driver will be introduced in the docker command line\n- A `\\daemon\\loggers\\splunk` folder will be added containing the driver. The driver will push events to the HTTP Event Collector. This approach will remove the need to have any additional Splunk-specific components deployed within Docker such as [this](http://blogs.splunk.com/2015/08/24/collecting-docker-logs-and-stats-with-splunk/) post \n- The driver will have automatic retry logic to handle cases where the endpoint is unavailable.\n# Command line options\n\nThe `--log-opt NAME=VALUE` flag to specify these additional Splunk\nlogging driver options.\n- `splunk-token` required, Splunk [HTTP Event Collector token](http://dev.splunk.com/view/event-collector/SP-CAAAE7C)\n- `splunk-url` required, path to your Splunk instance including scheme and HTTP Event Collector port.  `https://your_splunk_instance:8088`.\n- `splunk-source` optional, event source\n- `splunk-sourcetype` optional, event source type\n- `splunk-host` optional, host where the event originated\n- `splunk-index` optional, index where the event will be stored\n- `splunk-capath` optional, path to root certificate.\n- `splunk-caname` optional, name which should be used for validating server\n    certificate, by default the hostname of the `splunk-url` will be used\n- `splunk-insecureskipverify` optional, ignore server certificate validation\n\nExample of the logging option specified for the Splunk Enterprise instance\ninstalled locally on the same box where is Docker daemon is running, HTTPS schema\nis used with specified path to the root certificate and Common Name which\nshould be used for verification (`SplunkServerDefaultCert` is a name, which is\nused for autogenerated by Splunk certificates)\n\n`docker run --log-driver=splunk \\\n    --log-opt splunk-token=6F68ABCB-0168-4D88-B186-677BF575ECA6 \\\n    --log-opt splunk-url=https://localhost:8088 \\\n    --log-opt splunk-capath=/opt/splunk/etc/auth/cacert.pem \\\n    --log-opt splunk-caname=SplunkServerDefaultCert`\n"},{"labels":["enhancement1"],"text":"Currently the output of `docker info` does not have infomation about volume drivers.\nWhen create a volume using `--driver=`, I want to know what driver are supported. But No where to get.\n\nMaybe we can add volume driver list in docker info, such as:\n\n```\n[root@Host80 ~]# docker info\nContainers: 1\nImages: 64\nStorage Driver: devicemapper\n...\nVolume Drivers: local,a,b,c\n...\nName: Host80\n```\n\nAlso, this way we can also use it in Swarm. Swarm can get supported volume driver list on every machines. Then `create volume` in swarm can use it(see related issue on the Swarm side docker/swarm#1189).\n"},{"labels":[null,null,"enhancement1",null],"text":"Hi guys,\n\nWe use swarm as orchestration tools. And we add labels on docker daemon to let swarm to do some specific scheduling, such as schedule redis container to the docker daemon which has `ssd` label.\n\nBut in some case, we need to change the docker daemon labels, such as we want add MapR-ability label to some daemon in the midnight, so the map reduce container could be scheduled to this daemon. \n\nThere's no API to change the daemon labels on the fly, and we can't afford to restart daemon. So we want add API to do that, such as:\n\n```\nUsage: docker labels [COMMAND] [OPTION]\n\nManage docker daemon labels:\n\nlist - show the docker daemon labels.\nadd - add new labels to docker daemon, docker labels add --label=[k=v]\nremove - remove any exist labels from docker daemon, docker labels remove --label=[k]\n```\n"},{"labels":["enhancement1"],"text":"Permit [`docker cp`](https://docs.docker.com/reference/commandline/cp/) to copy from an image but fail when copying to an image. \n\n[Perhaps a [`docker cp`] command that targets an image doesn't need to fail.](https://github.com/docker/docker/issues/16079#issuecomment-139710931)\n\n[Motivation.](https://github.com/docker/docker/issues/16079#issuecomment-139722773)\n\nAlthough declined, feature was implemented as bash git hub project [`dkrcp`](http://tinyurl.com/z964b4p)\n"},{"labels":[null,"enhancement1"],"text":"Relates to https://github.com/docker/docker/issues/16068 and https://github.com/docker/docker/issues/16069\n\nCurrently, `docker run -v namedvolume:/foo` will automatically create a named volume if the volume doesn't exist (**or cannot be found**). \n\nThis feature was a requirement, because the volume API (i.e. `docker volume create`) was not yet implemented. Now that `docker volume create` is implemented, I think we should deprecate this feature.\n\nDeprecating this feature has a number of advantages;\n- It prevents unexpected behavior, and potential data-loss as described in https://github.com/docker/docker/issues/16068\n- We can error-out if a volume was created (using `docker volume create`), but could not be found. This situation can occur if, for example, the volume-plugin is not running or the volume was removed \n\n**Migration path**\n\nFor users wanting to use named volumes, the following alternatives should be mentioned:\n\n_Creating the volume in-line_:\n\n``` bash\ndocker run -d -v $(docker volume create --d=foo --name=volumename):/data myimage\n```\n\n_Creating the volume in advance_:\n\n``` bash\ndocker volume create -d=foo --name=volumename\ndocker run -d -v volumename:/data myimage\n```\n\n**Deprecation proposal**\n\n_Docker 1.9_, _Docker 1.10_:\nOutput a warning if a volume with the given name does not exist (or could not be found), e.g.;\n\n```\nInfo: created volume <volumename>\nWarning: auto-creating named volumes is deprecated and will be removed in docker 1.11\n```\n\n_Docker 1.11_:\nError-out if the volume could not be found\n\n```\nError: failed to start container, because volume <volumename> could not be found.\n```\n"},{"labels":["enhancement1"],"text":"The `--volume-driver` option was never a good fit; this option can only be specified _once_, which isn't really useful if a container uses multiple volumes (and different drivers need to be used for each one)\n\nNow that `volume create` command has been merged, I think we should deprecate `--volume-driver` in favor of `volume create`, i.e.:\n\n**old approach**\n\n``` bash\ndocker run -d -v volumename:/data --volume-driver=flocker myimage\n```\n\n**new approach**\n\n``` bash\ndocker run -d \\\n    -v $(docker volume create --driver=flocker --name=volumename):/data \\\n    -v $(docker volume create --driver=foobar --name=something):/somewhere \\\n    myimage\n```\n\nI'd like to have this decided on for the 1.9 release, to give a clear migration path for users that currently rely on this feature.\n"},{"labels":[null,"enhancement1",null],"text":"I want to chattr custom /bin/uname and /bin/arch files and make sure they don't get replaced later by setting the immutable flag on them during the build step.  Currently, I think this can only be done by using \"docker run --privileged\" which would be hacky to do as part of my image build process.\n\ndocker version\nClient version: 1.6.2\nClient API version: 1.18\nGo version (client): go1.4.2\nGit commit (client): ba1f6c3/1.6.2\nOS/Arch (client): linux/amd64\nServer version: 1.6.2\nServer API version: 1.18\nGo version (server): go1.4.2\nGit commit (server): ba1f6c3/1.6.2\nOS/Arch (server): linux/amd64\n\ndocker info\nContainers: 39\nImages: 106\nStorage Driver: devicemapper\n Pool Name: docker-253:1-1175046-pool\n Pool Blocksize: 65.54 kB\n Backing Filesystem: extfs\n Data file: /dev/loop1\n Metadata file: /dev/loop2\n Data Space Used: 7.254 GB\n Data Space Total: 107.4 GB\n Data Space Available: 14.51 GB\n Metadata Space Used: 10.8 MB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 2.137 GB\n Udev Sync Supported: true\n Data loop file: /media/docker/lib/docker/devicemapper/devicemapper/data\n Metadata loop file: /media/docker/lib/docker/devicemapper/devicemapper/metadata\n Library Version: 1.02.93-RHEL7 (2015-01-28)\nExecution Driver: native-0.2\nKernel Version: 3.10.0-123.el7.x86_64\nOperating System: CentOS Linux 7 (Core)\nCPUs: 1\nTotal Memory: 1.798 GiB\nName: builder\nID: UDUT:CUP5:KCRC:QMZ7:MPH3:CM3D:5JE7:ASC3:T2QC:ILQZ:65HF:GZX6\n\nuname -a\nLinux builder 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 12:09:22 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\n\ncat Dockerfile\nFROM busybox\nRUN chattr +i /bin/uname\n\ndocker build -t asdf ./\nSending build context to Docker daemon 2.048 kB\nSending build context to Docker daemon\nStep 0 : FROM busybox\n ---> 8c2e06607696\nStep 1 : RUN chattr +i /bin/uname\n ---> Running in 45d5d3a99246\nchattr: setting flags on /bin/uname: Operation not permitted\n ---> c39f3e7f364c\nRemoving intermediate container 45d5d3a99246\nSuccessfully built c39f3e7f364c\n\ndocker run --rm asdf lsattr /bin/uname\n------------- /bin/uname\n"},{"labels":[null,"enhancement1"],"text":"At the university of oslo, we have a slurm cluster, Abel (https://www.notur.no/hardware/abel) with 630 nodes. We want to enable docker on these nodes.\nThe details of the issue can be found here, since this represents exactly what we need:\nhttp://stackoverflow.com/questions/27794567/how-to-share-images-between-multiple-docker-hosts?answertab=active#tab-top\n\nDocker swarm is not an option (at least for Abel) since we already have slurm as a queuing system. We just need a way to share the image repository to avoid waste of disk space. \n\nAlso setting up a private registry (discussed here: https://github.com/docker/docker/issues/9935) is not what we are looking for, since it won't prevent image replication (630 times in our case).\n\nI understand that each host needs to have its own containers repo, but for images, I don't see any logical reason not to be able to share them between any number of hosts\n"},{"labels":[null,"enhancement1",null],"text":"This issue will be used as a synchronization point between **docker/distribution** PRs and core Docker changes required to implement the multi-architecture support between registry schema changes (to enable tagging with information like `arch` and `os`) and smart parsing of fat manifests on the daemon side during pull.\n\nAlso, a design for fat manifest creation (which may need to be different than basic `docker push`) will be discussed here, and, as needed, PRs will be created to implement these capabilities.\n\n**References**\n- https://github.com/docker/distribution/pull/62 - _proposal for new generic content schema for registry v2; rework by @stevvooe TBD_\n- https://github.com/docker/distribution/issues/200 - _specific issue to collect multi-arch requirements/discussion on_ **docker/distribution** _repo_\n\n**Completed work**\n- https://github.com/docker/distribution/pull/912 - _moves current schema to specific versioned package to make schema2 implementation work possible_\n"},{"labels":[null,"enhancement1"],"text":"Problems:\n- we want our containers to only receive traffic from known ip sets\n- we have services that should only reach out to external services and not internal services\n\nCurrently doing these things outside of docker is very clunky. It would be nice if daemon and containers can accept `--allow-ipset` and `--reject-ipset` and insert proper rules into iptables instead of end user having to manage these rules outside of docker.\n"},{"labels":[null,"enhancement1"],"text":"Currently, plugins (external and internal) talk to docker via a known common interface, for example \n[libnetwork remote design](https://github.com/docker/libnetwork/blob/master/docs/remote.md) and [libnetwork api design](https://github.com/docker/libnetwork/blob/master/docs/design.md#api)\n\nOne of the problem i want to solve is https://github.com/docker/docker/issues/6574, specifically https://github.com/docker/docker/issues/6574#issuecomment-134198175. To summarize the problem:\n- Our systems run iptables-restore frequently agains a set of known rules\n- Whenever iptables-restore is run, docker loses its own iptables rules\n- We want the iptables rules to be recreated without restarting lots of containers on the host\n\nTo do this, there are two ways i can think of:\n1. Expose an additional API to the common interface that drivers can choose to implement ( i.e. bridge driver can use it to reset iptables, rewrite proc values etc.. ). However this may not be approriate for all drivers. So we probably want to decide how a drivers can update / refresh their own settings.\n2. Allow some sort of extension mechanisms for plugins / drivers so they can register their own hooks, and that is exposed to Docker user. For example, bridge may want to register an API to refresh iptables settings, proc values etc. while remote plugins can also register their own extension.\n\nSome diccussion on IRC: https://botbot.me/freenode/docker-network/2015-08-25/?msg=48091484&page=1\n\ncc @mavenugo @mrjana \n"},{"labels":[null,null,"enhancement1"],"text":"I need to COPY a _part_ of a context directory to the container (the other part is subject to another COPY).  Unfortunately, the current possibilities for this are suboptimal:\n1. COPY and prune.  I could remove the unwanted material after an unlimited COPY.  The problem is that the unwanted material may have changed, so the cache is invalidated.\n2. COPY every file in a COPY instruction of it own.  This adds a _lot_ of unnecessary layers to the image.\n3. Writing a wrapper around the \"docker build\" call that prepares the context in some way so that the Dockerfile can comfortably copy the wanted material.  Cumbersome and difficult to maintain.\n"},{"labels":["enhancement1",null,null],"text":"Howdy,\n\nI'm unable to identify a mechanism from either https://docs.docker.com/docker/userguide/labels-custom-metadata/ or the `docker` cli `help` which would let me add, change, and/or delete labels on _running_ Docker containers. Would like to use this metadata on multi-tenant hosts, as opposed to doing it out-of-band (e.g. a key/value store which also references running container IDs).\n\nIf this feature does exist, could someone point me toward it? If not, requesting it.\n\nThanks!\n"},{"labels":[null,null,"enhancement1"],"text":"It may be interesting, for monitoring purpose, to report the storage utilization of the graph drivers.\n\nThe information is already reported by some graph drivers, e.g.\n\nhttps://github.com/docker/docker/blob/a15b676/daemon/graphdriver/devmapper/driver.go#L73-L86\n\nbut it's in an unstructured fashion.\n\nI think each driver should report, total and used (or free, or both used/free) space in such a way that could be reliably monitored by external applications.\n"},{"labels":[null,"enhancement1"],"text":"Problem: \nWe have users who are running docker (Production environment), using lvm devmapper on top of loopback devices. Since this storage option is not production ready, we recommend them to switch to a different storage. Being a production environment they don't want to loose their containers when migrating from one storage to another.\n\nCurrently they can achieve this using docker export/import. However there are some shortcomings with these commands.\nLet's take an example.\n\n1) I have my source docker (devmapper running on loopback devices) running with 1 image {fedora}.\n2) I start a container on this image: docker run --name fedora_container -it fedora /bin/bash\n3) I make some changes in the container: echo foobar > /home/foobar.txt\n4) I stop the container and I want to migrate my images {fedora} and containers {fedora_container} to the new storage.\n\nCurrent setup:\n\n1) docker save fedora > fedora.tar\n2) docker export -o fedora_container.tar fedora_container [This will tar up the container rootfs including the changes {foobar.txt} we made]\n3) Switch to the new storage.\n4) docker load < fedora.tar\n5) cat fedora_container.tar | docker import - fedora_new:latest\n6) Now I can start a container which will have my changes {foobar.txt} on this new image, fedora_new:latest\n\nShortcomings:\n\n1) The container on the new storage is based on image {fedora_new} eventhough the original container was based on image {fedora}.\n2) The image fedora_new is a completely standalone image which is not based on any image.\n    docker inspect -f {{.Parent}} fedora_new = \"\"\n3) Any metadata associated with the containers e.g Mount points is lost, since the container is recreated at the new storage.\n4) Docker import results into an image (not container). This means if a user is trying to migrate X images and Y Containers.\n   At the new storage he will see X+Y Images, and then he has to start all the containers again on these new images.\n5) Since docker export tars up the entire rootfs, the tarball is heavy (In our example, we just created one file {foobar.txt} and the tar ball is ~200MB)\n\nProposal:\n\n1) docker export with a new flag (--diff-metadata) will create a tarball with only the diff and metadata of the container. {~ 17 Kb in size}\n2) docker import will:\n        a) createRootfs on the new storage.\n        b) Apply diff.\n        c) setup metadata.\n        d) load the container in the daemon.\n\nAfter the import the user will see exact replica of `docker images` and `docker ps -a` in the new storage.\n\nI have a working version of this proposal on my github branch.\nhttps://github.com/shishir-a412ed/docker/tree/migrate_containers\n\nHow to:\nEXPORT: docker export -o fedora_container.tar --diff-metadata fedora_container\nIMPORT: cat fedora_container.tar|docker import --diff-metadata -\n\nI am opening this proposal so that we can discuss how to best approach this problem. \nYou can also take a look at my implementation, and if you like that approach I can open a PR and we can take it there.\n"},{"labels":[null,"enhancement1"],"text":"(Extracted from #11336)\n\nAt the moment you can configure the docker daemon with e.g. `--fixed-cidr=10.10.0.192/28`, and the allocator will allocate addresses within that range - in this case starting 10.10.0.193, probably ending 10.10.0.206 or 10.10.0.205 (untested)\n\nIt would be helpful to be able to specify arbitrary start and end IPs instead, e.g. `--fixed-range=10.10.0.100,10.10.0.199`\n\nThis would allow more flexibility in avoiding fixed infrastructure (e.g. routers, switches), other VMs, existing DHCP ranges etc.\n"},{"labels":[null,"enhancement1"],"text":"Since we now have apt.dockerproject.org and yum.dockerproject.org I propose we migrate old static binaries to dl.dockerproject.org and start pushing new ones there. Then we can turn on indexing like we have on apt. and yum. and things could be more easily found and also then we can stop pushing things to s3 once and for all and have one infrastructure for hosting things.\n\nping @docker/core-maintainers\n"},{"labels":["enhancement1"],"text":"## 1.   Introduction\n\nThis proposal is inspired by the https://github.com/docker/docker/issues/14049 “Global Image/Layer Namespace”. It stands by itself as well, as a separate proposal to improve docker’s graphdriver architecture, in particular – support of the union filesystems.\n\nUnion filesystems form a class of their own which is layered directly on top of actual (true) local filesystems such as, for instance, ext4 or zfs. There are variations and differences of course from the original Unionfs circa 2004. For the docker those differences are irrelevant - what's relevant is whether this or that or the other union filesystem is upstreamed in the Linux kernel or included in a given distribution, and which one performs better under given circumstances. None of these arguments has an impact on the corresponding docker drivers.\n\nSo. Arguments for this Proposal include at least one architectural reason (i.e., layering) and one good-coding reason (functional overlap between the overlayfs and aufs graphdrivers). \n\nMore practically though, the idea to support global image namespace required minor changes in the union filesystems drivers, to support immutable remote images and layers. And the idea to change them in turn prompted this development and this proposal..\n## 2.   Union-Federated Driver\n\nThere’s now (a proposed) one and only one docker driver, called union-federated, that can be configured to run both aufs or overlayfs. In the future the driver can be extended to support other kernel-based union-type filesystems that support multiple read-only layers.\n\nThe driver unifies the unification drivers, and that is the `#1`. In addition, union-federated provides support for an optional global image namespace: you can either run it locally on a single host over aufs or overlayfs. Or, you can use it exactly as described in the https://github.com/docker/docker/issues/14049. And that’s `#2`.\n## 3.   The Patch\n- Local separation of (local) images and layers: https://github.com/Ramzec/docker/commit/0579f2ff3ecb08426f645ad6d75edecf28c96bf8\n- Union-federated graphdriver: https://github.com/Ramzec/docker/commit/9aecfbc12a524ba9818e1080a56b74f0a9279b1c\n## 4.   Getting started\n\n`# docker –graph=/var/lib/docker_uf –storage-opt=uf.subdriver=overlay`\n\nThis will run docker with union-federated driver; “subdriver” option tells it which of the union filesystems to use. Both aufs and overlayfs are supported, with one comment: \n\n• As far as the current patch (`#3` above), overlayfs option requires Linux kernel 3.19 or later. This is due to the fact that older overlayfs does not support so called “multi-layer” – see e.g. http://www.phoronix.com/scan.php?page=news_item&px=MTg1Nzg.  \n\nIn addition to the mandatory uf.subdriver=aufs|overlay option, other (optional) –storage-opt options include:\n\n• uf.localonly = (true | false) \n• uf.nfsroot = (local pathname)\n\nFor instance, to use the local-only option with aufs and to work with only the local images, run union-federated as follows:\n\n`# docker –graph=/var/lib/docker_uf –storage-opt=uf.localonly=true –storage-opt=uf.subdriver=aufs`\n\n(Note: suggested to make aufs default..)\n## 5.   Directory Layout\n\nThe driver imposes/supports the following directory layout to store and access images and layers, local and remote:\n\n```\n  ├──host1\n  │       ├── layers // Metadata of layers\n  │       │   ├── 1\n  │       │   ├── 2\n  │       │   └── 3\n  │       ├── diff  // Content of the layer\n  │       │   ├── 1  // Contains layers that need to be mounted for the id\n  │       │   ├── 2\n  │       │   └── 3\n  │       └── mnt    // Mount points for the rw layers to be mounted\n  │              ├── 1\n  │              ├── 2\n  │              └── 3\n  ├──host1\n  │       ├── layers // Metadata of layers\n  │       │   ├── 1\n  │       │   ├── 2\n  │       │   └── 3\n  │       ├── diff  // Content of the layer\n  │       │   ├── 1  // Contains layers that need to be mounted for the id\n  │       │   ├── 2\n  │       │   └── 3\n  │       └── mnt    // Mount points for the rw layers to be mounted\n  │              ├── 1\n  │              ├── 2\n  │              └── 3\n  .\n  .\n  .\n  ├──hostN\n\nrootLocalContainerLayers - The directory that contains\n                                                     only container-layers\n  ./\n  ├── layers // Metadata of layers\n  │   ├── 1\n  │   ├── 2\n  │   └── 3\n  ├── diff  // Content of the layer\n │   ├── 1  // Contains layers that need to be mounted for the id\n  │   ├── 2\n  │   └── 3\n  └── mnt    // Mount points for the rw layers to be mounted\n      ├── 1\n      ├── 2\n      └── 3\n\nrootLocalImageLayers - The directory that contains\n                                             only image-layers\n  ./\n  ├── layers // Metadata of layers\n  │   ├── 1\n  │   ├── 2\n  │   └── 3\n  ├── diff  // Content of the layer\n  │   ├── 1  // Contains layers that need to be mounted for the id\n  │   ├── 2\n  │   └── 3\n  └── mnt    // Mount points for the rw layers to be mounted\n      ├── 1\n      ├── 2\n      └── 3\n```\n\nAs usual, the ‘diff’ directories store actual layer content; locally the driver stores images under \n\n• root/image-layers/(diff|layers|mnt|work) \n\nand containers under: \n\n• root/container-layers/(diff|layers|mnt|work) \n\nClean separation of local images vs. local containers is intended to support global image namespace: access to the immutable images (and images only) from other dockerized hosts..\n## 6.   Initialization and checks\n\nAt init time the driver looks in the /proc for requested aufs/overlayfs option. Assuming the corresponding union filesystem is included, for the overlayfs there’s another check: kernel version >= 3.19\n## 7.   What’s Next\n\nApart from couple minor points mentioned above (aufs default, pre-3.19 overlayfs), union-federated driver seems to be done and is immediately usable. The other roadmap is described at the bottom of “Global Namespace” https://github.com/docker/docker/issues/14049\n"},{"labels":[null,"enhancement1"],"text":"The 'docker stats' output should have one column about disk I/O, as that is very often a performance bottle-neck.\n\nThe stats JSON contains data about disk I/O, so all that needs to be extended a little bit is api/client/stats.go\n\n```\necho -e \"GET /containers/{ID}/stats HTTP/1.0\\r\\n\" | sudo nc -U /var/run/docker.sock\n[...]\n    \"blkio_stats\": {\n        \"io_service_bytes_recursive\": [\n            {\n                \"major\": 253,\n                \"minor\": 16,\n                \"op\": \"Read\",\n                \"value\": 323584\n            },\n            {\n                \"major\": 253,\n                \"minor\": 16,\n                \"op\": \"Write\",\n                \"value\": 0\n            },\n            {\n                \"major\": 253,\n                \"minor\": 16,\n                \"op\": \"Sync\",\n                \"value\": 0\n            },\n            {\n                \"major\": 253,\n                \"minor\": 16,\n                \"op\": \"Async\",\n                \"value\": 323584\n            },\n            {\n                \"major\": 253,\n                \"minor\": 16,\n                \"op\": \"Total\",\n                \"value\": 323584\n            },\n            {\n                \"major\": 253,\n                \"minor\": 0,\n                \"op\": \"Read\",\n                \"value\": 12054528\n            },\n            {\n                \"major\": 253,\n                \"minor\": 0,\n                \"op\": \"Write\",\n                \"value\": 204800\n            },\n            {\n                \"major\": 253,\n                \"minor\": 0,\n                \"op\": \"Sync\",\n                \"value\": 81920\n            },\n            {\n                \"major\": 253,\n                \"minor\": 0,\n                \"op\": \"Async\",\n                \"value\": 12177408\n            },\n            {\n                \"major\": 253,\n                \"minor\": 0,\n                \"op\": \"Total\",\n                \"value\": 12259328\n            }\n        ],\n        \"io_serviced_recursive\": [\n            {\n                \"major\": 253,\n                \"minor\": 16,\n                \"op\": \"Read\",\n                \"value\": 79\n            },\n            {\n                \"major\": 253,\n                \"minor\": 16,\n                \"op\": \"Write\",\n                \"value\": 0\n            },\n            {\n                \"major\": 253,\n                \"minor\": 16,\n                \"op\": \"Sync\",\n                \"value\": 0\n            },\n            {\n                \"major\": 253,\n                \"minor\": 16,\n                \"op\": \"Async\",\n                \"value\": 79\n            },\n            {\n                \"major\": 253,\n                \"minor\": 16,\n                \"op\": \"Total\",\n                \"value\": 79\n            },\n            {\n                \"major\": 253,\n                \"minor\": 0,\n                \"op\": \"Read\",\n                \"value\": 739\n            },\n            {\n                \"major\": 253,\n                \"minor\": 0,\n                \"op\": \"Write\",\n                \"value\": 49\n            },\n            {\n                \"major\": 253,\n                \"minor\": 0,\n                \"op\": \"Sync\",\n                \"value\": 19\n            },\n            {\n                \"major\": 253,\n                \"minor\": 0,\n                \"op\": \"Async\",\n                \"value\": 769\n            },\n            {\n                \"major\": 253,\n                \"minor\": 0,\n                \"op\": \"Total\",\n                \"value\": 788\n            }\n        ],\n        \"io_queue_recursive\": [],\n        \"io_service_time_recursive\": [],\n        \"io_wait_time_recursive\": [],\n        \"io_merged_recursive\": [],\n        \"io_time_recursive\": [],\n        \"sectors_recursive\": []\n    }\n}\n```\n"},{"labels":[null,null,"enhancement1",null],"text":"Hi,\n\nA plan to support logstash as a log-driver ?\n"},{"labels":[null,"enhancement1"],"text":"Currently (i.e. _docker v1.7.1_), if you try to start a container that uses the same host port as a already running container you get a error:  :no_entry: **failed: port is already allocated**. Example:\n\n```\n$docker run -p 8080:80 -d nginx\nc851f7955d48efae7455649922285c185560e39d44de0b618dd31e3437cb4623\n\n$docker run -p 8080:80 -d nginx\nb6e8728e24c56bcdf24a69fae0c95de7d31f1c4fa6216b257dab10f175699a93\nError response from daemon: Cannot start container b6e8728e24c56bcdf24a69fae0c95de7d31f1c4fa6216b257dab10f175699a93: Bind for 0.0.0.0:8080 failed: port is already allocated\n```\n\nIt would be very useful if there was a option to \"force\" the start of the 2nd container. Maybe the original container could be stopped or paused or be given another free port (like if `-P` was used).\n\nWhy would this be useful? _i.e. why can't you just stop the other container first?_\nIf you are running a webapp in a container and getting many requests per second and want to do a upgrade because you have a new version of the containerised webapp, then stopping the container and starting the new version will result in a little downtime.\nThis \"port stealing\" feature would allow zero-downtime deployments without needing a proxy (or whatever you want to call it, e.g. load-balancer , or [ambassador](http://docs.docker.com/articles/ambassador_pattern_linking/) , or [jumper](https://coreos.com/blog/jumpers-and-the-software-defined-localhost/), etc. hipache, [nginx-proxy](https://github.com/jwilder/nginx-proxy), etc).\n"},{"labels":["enhancement1"],"text":"I have a git repo that has two dockerfiles in it. When I attempt to use the alternate dockerfile with -t I get the following error \n\n> The Dockerfile (Dockerfile-dev) must be within the build context (/var/folders/22/qk_79ksd4bs7_z_c4p1vkq980000gn/T/docker-build-git278991242)\n\nBUG REPORT INFORMATION\n\nUse the commands below to provide key information from your environment:\n\ndocker version:\n\n```\nClient version: 1.7.0\nClient API version: 1.19\nGo version (client): go1.4.2\nGit commit (client): 0baf609\nOS/Arch (client): darwin/amd64\nServer version: 1.7.0\nServer API version: 1.19\nGo version (server): go1.4.2\nGit commit (server): 0baf609\nOS/Arch (server): linux/amd64\n```\n\ndocker info:\n\n```\nContainers: 22\nImages: 373\nStorage Driver: aufs\n Root Dir: /mnt/sda1/var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 421\n Dirperm1 Supported: true\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 4.0.5-boot2docker\nOperating System: Boot2Docker 1.7.0 (TCL 6.3); master : 7960f90 - Thu Jun 18 18:31:45 UTC 2015\nCPUs: 2\nTotal Memory: 1.956 GiB\nName: boot2docker\nID: BK3M:3XOE:MICO:QBSK:2ZEP:5UVW:JNJF:GDK6:B4NW:523M:Y2IG:74BE\nDebug mode (server): true\nFile Descriptors: 21\nGoroutines: 34\nSystem Time: 2015-07-22T00:16:33.802979003Z    \nEventsListeners: 1\nInit SHA1:\nInit Path: /usr/local/bin/docker\nDocker Root Dir: /mnt/sda1/var/lib/docker\n```\n\nuname -a:\n\n```\nDarwin mini 13.4.0 Darwin Kernel Version 13.4.0: Wed Mar 18 16:20:14 PDT 2015; root:xnu-2422.115.14~1/RELEASE_X86_64 x86_64 i386 Macmini3,1 Darwin\n```\n\nProvide additional environment details (AWS, VirtualBox, physical, etc.):\n\nRunning using boot2docker on an OSX machine.\n\nList the steps to reproduce the issue:\n1.run the following on the command line\n\n```\ndocker build -t shaarli -f Dockerfile-dev https://github.com/elrac/docker-shaarli.git\n```\n\nDescribe the results you received:\n\nI get the following error\n\n> The Dockerfile (Dockerfile-dev) must be within the build context (/var/folders/22/qk_79ksd4bs7_z_c4p1vkq980000gn/T/docker-build-git278991242)\n\nDescribe the results you expected:\n\nI expect it to build using the file Dockerfile-dev in the git repo as the dockerfile.\n\nProvide additional info you think is important:\n"},{"labels":["enhancement1"],"text":"As per comments here #14691 and current comments on code here https://github.com/docker/docker/blob/master/daemon/start.go#L30 we could actually deprecate passing hostConfig to container start for newer api (> 1.21)\n\nclose #10695\n\nping @icecrime @cpuguy83 @LK4D4 \n"},{"labels":[null,"enhancement1"],"text":"@NathanMcCauley, @thaJeztah, @nalind, \n\nAs discussed in #13697 I'd like to start a separate discussion on the _access control_ model in Docker daemon. \n\nBelow is the list of core principles that I think the model should be build upon:\n1. The model should be pluggable, so that anyone can add / develop a plug-in that suits her needs. \n2. It should be possible to add a plug-in to a deployed Docker daemon without the need to rebuild the daemon. Daemon restart may be required. \n3. Each plug-in should receive the information about the user and the command she is trying to perform on each request.\n4. Each plug-in should be able to influence the Docker daemon processing of the command (e.g. block the process of the current operation).\n\nBelow is the high level list of the information that each plug-in should receive and the set of possible output values for each plug-in. \n\nPlug-in inputs:\n1. The notification type: 1) The command is about to be processed 2) The response is about to be returned\n2. The principal name of the user who it performing the command\n3. The command that the user is doing, including all the arguments \n4. The full user request, and in case of 2nd type notification the response that is about to be sent to the user\n\nPlug-in outputs:\n1. Whether the access is approved or denied\n2. The message that describes why the access was approved / denied\n3. Whether the next plug-in should be called or not for this specific notification (default is yes)\n\nImprovements that I think should be done at the later stage, after the initial framework is implemented. \n1. Passing the authentication method that was used to authenticate the user to the plug-ins. I think this should be done only if we find a good reason for this, and by default we should keep authentication and authorization mechanisms as de-coupled as possible. \n2. Add more integration points (notifications): e.g., the authentication was just completed, the \"access denied\" is about to be sent, etc. \n3. Add ability for the plug-in to register on specific notifications. For the sake of simplicity I wouldn't do subscription model for plug-ins right away. I think this should be treated as optimization and added only when / if needed. \n\nYour comments are, of course, very welcome. In case the above makes sense, I'd be glad to share a more in depth design of each component and continue the discussion at a lower level. \n"},{"labels":[null,"enhancement1"],"text":"Propagation modes control how mount and umount events propagate between mount namespaces.  In order to enable use-cases like a container that performs mounts that are visible to other containers (such as containerized Kubernetes/OpenShift), it should be possible to control the mount propagation mode of a container.  There is already an issue for this in runc, and a corresponding PR:\n\nhttps://github.com/opencontainers/specs/issues/56\nhttps://github.com/opencontainers/runc/pull/77\n\n...but those don't cover docker until runc is integrated.  This issue is to make this functionality available in docker before runc is integrated.\n\n@mrunalp @rhatdan @crosbymichael\n"},{"labels":[null,"enhancement1"],"text":"I frequently see a lot of requests to extend the docker daemon with lots of little minutiae. I know there are lots of things I would like to see added, but I also don't want to see docker bogged down with feature bloat (and I think it's already got a lot).\nI think a lot of these requests could be satisfied if users were able to run a custom command before the container is started.\n\nTherefore I propose a single hook that executes immediately after the container is created. Once the container is created, but before the `exec` of the init process, the user specified command would be run. After it exits, if exit code is 0, continue normally, otherwise kill the container.\n\nThis custom command is either a path to an executable, or a script (determination is made by doing a simple file existence/executable check). If a script, the docker daemon writes the contents to a file, sets the executable bit, and executes it. This will allow users to pass in shell scripts via the remote API.\nThe hook will execute within the context of the host. If the hook wishes to execute something within the context of the container, it can use `docker exec` to do so.\nA single environment variable will be provided (aside from those inherited from the docker daemon): `CONTAINER_ID`.\n\nThe hook would be stored in the container configuration, so any time the container is started, the hook would be run. If the user wishes to modify the hook, a new container must be created.\n\nThis should be very simple to implement, and I'd be willing to code it up.\n"},{"labels":[null,"enhancement1",null],"text":"There are technologies, like Software Collections [1] or environment modules, where the whole trick to make some binary (installed into alternate directory /opt) available is to change environment variables. For the changing environment variables there is a dedicated tool, since the variables are not static (we want only to adjust the PATH, LD_LIBRARY_PATH, etc). When delivering such packages as Docker images, calling a command to set environment variables is quite challenging.\n\nIt lead to a small study how we can do it:\n\nCommands that users run in the container:\n- bash\n- showenv.sh (simulates any bash script)\n- showenv.py (simulates any non-bash binary)\n\nCases where we check if env. variables are changed:\nc#1) echo \"\" | docker run $IMG showenv.py\nc#2) docker run -ti $IMG showenv.sh\nc#3) docker run $IMG bash -c 'something'\nc#4) docker run -ti $IMG bash\nc#5) docker run -ti $IMG bash -l\n\nc#6) docker exec -ti $CONT showenv.py\nc#7) docker exec -ti $CONT showenv.sh\nc#8) docker exec $CONT bash -c 'echo $MYVAR'\nc#9) docker exec -ti $CONT bash -l\nc#10) docker exec -ti $CONT bash\n\nPossible implementations of changing environment:\nA) change in the entrypoint\nB) set BASH_ENV variable\nC) define HOME and place a script into ~/.bashrc\nD) place a script into /etc/profile.d\n\nResult matrix for cases above and particular solution variants:\n\n| Solution | c#1 | c#2 | c#3 | c#4 | c#5 | c#6 | c#7 | c#8 | c#9 | c#10 |\n| --- | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n| A | YES | YES | YES | YES | YES | NO | NO | NO | NO | NO |\n| B | NO | YES | YES | NO | NO | NO | YES | YES | NO | NO |\n| C | NO | NO | NO | NO | YES | NO | NO | NO | NO | YES |\n| D | NO | NO | NO | YES | NO | NO | NO | NO | YES | NO |\n| B+C+D | NO | YES | YES | YES | YES | NO | YES | YES | YES | YES |\n| A+B+C+D | YES | YES | YES | YES | YES | NO | YES | YES | YES | YES |\n\nWe see that in order to address the most of the use cases, we need to implement all the ways mentioned above together.\n\nWe also see that there is no way (at least nothing I'm aware of) to change environment for running a non-bash binary as `docker exec` -- such a process will always have default environment.\n\nThis is an RFE request for implementing this in the docker properly. I've also send it to discussions while back, but without any feedback [2].\n\n[1] http://softwarecollections.org/\n[2] https://groups.google.com/forum/#!searchin/docker-user/environment/docker-user/l-LUQQdZqKw/yQRy7ffadFgJ\n"},{"labels":["enhancement1"],"text":"This suggestion will probably have the lowest priority that any suggestion has ever had, however I think it could possibly be a very low cost, high value feature.\n\nI would like the ability to lock containers so that they cannot be removed without unlocking them. I would use this for data volume containers to stop them being accidentally removed. For example:\n\n```\n$ docker run -d -v data --name example-data busybox /bin/bash\n72fc9b9fed363fb925badc7e2cc6fb3efc29323b9cb2e2b5b088262fb7fd3442\n\n$ docker lock 72fc9b9fed36\n72fc9b9fed36 is now locked\n\n$ docker rm -f 72fc9b9fed36\nError response from daemon: container locked: 72fc9b9fed36\nError: failed to remove containers: [72fc9b9fed36]\n\n$ docker unlock 72fc9b9fed36\n72fc9b9fed36 is now unlocked\n\n$ docker rm -f 72fc9b9fed36\n72fc9b9fed363fb925badc7e2cc6fb3efc29323b9cb2e2b5b088262fb7fd3442\n```\n\nI know this seems like a really small and nonsensical feature but it really would make the docker ps -a | grep etc | xargs docker rm \"cleanup\" routines easier. I might be tempted to get my feet wet and work on this myself but I wondered what people's thoughts were on this first?\n"},{"labels":[null,"enhancement1"],"text":"At the DockerCon BOF on UX, one area was discussed which I've just struggled with again: image management on each machine.  As such, I'm filing a meta-issue for the general image UX.  While we can deal with the problems with image management piecemeal, I thought it would be useful to see them all in one place so that the UX engineers can develop a \"big picture\".\n\nOverall, I think now that systems exist in the wild which have been running Docker for over a year, we're starting to see some significant issues with image garbage collection, selection, pulls, and similar issues.  The original images feature wasn't designed to deal with any of this -- IIRC, rmi wasn't added until 1.0 -- because it wasn't a problem, but it is now.  I feel that we should rethink the UX around image management on each machine.\n\nGetting rid of unused images:\n- feature request #9054\n- bug  #8926\n\nPulling Problems:\n- bug #13331\n- bug #13309\n- UX request: #6928 \n\nPushing Problems:\n- bug #12489\n- bug #7336\n\nUI enhancements:\n- feature request #14482\n- feature request #13509\n"},{"labels":[null,"enhancement1"],"text":"As a Docker user, I'd like to be able to inspect remote images without downloading binary blobs.\n## Possible interface\n\nAdd `-r` flag to `docker inspect` denoting _inspect remote registry_. Unless given, keep the old behavior.\n### Example:\n\n```\ndocker inspect -r docker.io/fedora\n[\n{\n    \"Id\": \"ded7cd95e059788f2586a51c275a4f151653779d6a7f4dad77c2bd34601d94e4\",\n    \"Parent\": \"48ecf305d2cf7046c1f5f8fcbcd4994403173441d4a7f125b1bb0ceead9de731\",\n    ...\n    \"Os\": \"linux\",\n    \"Size\": 186515831\n}]\n```\n"},{"labels":[null,"enhancement1"],"text":"As a Docker user, I'd like to be able to list tags and Image IDs of remote images without downloading binary blobs.\n## Possible interface\n\nAdd `-l` flag to `docker tag` denoting _list tags_. If given name matches local image, its tags will be listed. Otherwise a remote registry will be listed.\n### Example:\n\n```\ndocker tag -l docker.io/fedora\nREPOSITORY          TAG                 IMAGE ID\nfedora              20                  d7f0e75cf11fd6fe9fc3e676bb423f1504ea20da110d4d480f74cda8cb94f574\nfedora              21                  e26efd418c4841f7299832fe7689de3e820d91a16bb4cff5b72eb9b09d712753\nfedora              22                  ded7cd95e059788f2586a51c275a4f151653779d6a7f4dad77c2bd34601d94e4\nfedora              heisenbug           d7f0e75cf11fd6fe9fc3e676bb423f1504ea20da110d4d480f74cda8cb94f574\nfedora              latest              ded7cd95e059788f2586a51c275a4f151653779d6a7f4dad77c2bd34601d94e4\nfedora              rawhide             f62b946038358811c82b9ce50601674a7e1b52ee742557429f244415064251b6\n```\n"},{"labels":[null,null,"enhancement1"],"text":"Hi All,\n\nNow that volume plugins were added to Docker experimental channel, I would like to suggest adding server-side plugins for handling Docker commands. This will allow Docker users to achieve similar capabilities as supplied by [PowerStrip](https://github.com/ClusterHQ/powerstrip).\n\nThe server side command handler plugins are pre/post extension to Docker command handling, allowing to modify input commands before processed by Docker engine, and output data after processed by Docker Engine (but before sent to client). It will allow adding new capabilities into Docker engine, implemented as out-of-process plugins.\n\nI've implemented a similar mechanism for a project I am working on. I'll be glad to get your feedback on the design and contribute the code if it makes sense.\n## Plugin Mechanics\n\nThe Command Handler plugins are similar to [Volume Plugins](https://github.com/docker/docker/blob/master/experimental/plugin_api.md): \n- Processes running on same host as Docker Engine. \n- Registered by placing a file in `/usr/share/docker/plugins/srv/cmd`. \n- There can be two type of plugin files:\n  - `.sock` files are UNIX domain sockets.\n  - `.spec` files are text files containing a URL, such as unix:///other.sock.\n- The name of the file (excluding the extension) determines the plugin name.\n- Plugins should be started before Docker, and stopped after Docker.\n- The Plugin API is RPC-style JSON over HTTP. Requests flow from the Docker daemon to the plugin.\n## Plugin Initialization\n\nUnlike volume plugins, the command handler plugins should be initialized upon Docker Engine startup. \nDocker Engine sends a /Plugin.Activate request and receives list of commands and type of handling (input or output) that plugin is interested in.\n### /Plugin.Activate\n\n**Request:** empty body\n\n**Response:**\n\n```\n{\n    \"Implements\": [\n        {\"cmd\":\"run\", \"type\":\"input\"}, \n        {\"cmd\":\"stop\", \"type\":\"input\"}, \n        {\"cmd\":\"inspect\", \"type\":\"output\"}\n    ]\n}\n```\n## Plugin Invocation\n\nOnce a plugin is registered for set of commands, Docker Engine calls the plugin before processing the command (\"input\" command handler) and before sending the output to client (\"output\" command handler). \n\nPlugin invocation is done by calling /Plugin.Execute request, while passing type of processing (input or output) and the associated payload (complete input command or output message).\n### /Plugin.Execute\n\n**Request:** \n\n```\n{\n    \"Command\": \"run\",\n    \"Type\": \"input\" or \"output\"\n    \"Payload\": { COMMAND PAYLOAD }\n}\n```\n\n**Response:**\n\n```\n{\n    \"Action\": \"continue\" or \"stop\"\n    \"Payload\": {UPDATED PAYLOAD}\n    \"Return_Code\": 0\n    \"Error_Msg\": {OPTIONAL ERROR MESSAGE}        \n}\n```\n\nDocker Engine should block until plugin responds back (timeouts?). It will continue processing the command in case the plugin responded with \"continue\" response action. It should stop processing the command (and returning return code and message) in case the response action is \"stop\".\n\nIn case of \"continue\" action, Docker Engine should use the Payload returned from the plugin response instead of the original command payload.\nThis allows plugins to modify command input parameters or output data.\n\nNote that multiple command handler plugins can be registered for the same command. Docker Engine will continue calling plugins for the same command as long as it did not get \"stop\" action from one of the handlers. After processing all plugins registered for a command, Docker Engine should continue handling the modified command (as returned from the last plugin).\n## Example Plugins\n\nExample for capabilities we would be able to implement using command handler plugins:\n- Run additional commands before/after container is started/stopped. E.g. when containerA is started/stopped, automatically start/stop containerB.\n- Add missing parameters to command line. E.g. add security parameters (cap-add, cap-drop) even if not specified in command line.\n- Prevent running command with certain parameters.\n- Modify Docker output to include additional data. E.g. modifying 'docker inspect' output to include container start time (or anything else which is useful). Note that this one is tricky as the CLI might be expecting certain outputs.\n"},{"labels":["enhancement1"],"text":"Use case: I want to tag my images with metadata such as the SCM branch name or revision from which the image was built.\n\nCounter-argument: determinism is very important during the build process, which is why all state involved in a build (env, files, labels) comes from the Dockerfile or the build context.\n\nRebuttal to counter-argument: labels typically don't govern the behavior of containers; rather, they're intended as a way to add arbitrary metadata to containers (and to images, so that the metadata will appear in containers derived from that image). It seems reasonable that some metadata might be contextual to the image build.\n\nAlternatives considered:\n1) Adding a subset of .git to the container (or otherwise adding metadata in the form of files). This works, but it's hard to view the metadata from the host, or to make decisions based on the metadata without actually running a container from the image.\n2) Running the built image using `--label` and then re-committing the (hopefully) unchanged image. Also works, but boy, what a hack!\n\nI'd be happy to submit a PR for this feature, but wanted to see how it was received by the community.\n"},{"labels":[null,"enhancement1"],"text":"I would like to filter who can connect to the port I have bound to the host. But it seems that this is non-trivial to do with Docker. The reason is that it adds NAT rules into FORWARDing iptables table, so DROP inside INPUT does not match. One cannot DROP in PREROUTING, and after FORWARD table port is not the same as it is outside, but it is already mapped to the internal port. I would like to use external port so that if I change internals, outside firewall rules do not have to change.\n"},{"labels":[null,"enhancement1"],"text":"As suggested by @cpuguy83 in https://github.com/docker/docker/issues/3156\nhere is the use case for a flexible -v option at build time.\n\nWhen building a Docker image I need to install a database and an app. It's all wrapped up in two tarballs: 1 for the DB and 1 for the App that needs to be installed in it (schema, objects, static data, credentials etc.).  The whole solution is then run via a shell script that handles several shell variables and tune OS credentials and other things accordingly.\nWhen I explode the above tarball (or use the Dockerfile ADD directive) the whole thing bloats up to about 1.5GB(!). Not ideal as you can immagine.\n\nI would like to have this '-v /distrib/ready2installApp:/distrib' directive still possible (as it is today in the Dockerfile)\nbut\n## I would like to disassociate the declarative build process (infrastructure as code) from the container run-time deployable artifact. I do not want to have to deal with the dead weight of 1.5GB that I do not need.\n\nCould we have an --unmount-volume option that I can run at the end of the Dockerfile?\nor\nGiven how Volume works right now in a Dockerfile, maybe we need a new Dockerfile directive for a temporary volume that people use while installing? I think the Puppet example supplied by @fatherlinux was on a similar line...\nor\nWhatever you guys can think of.\nThe objective is avoiding to have to carry around all that dead weight that is useless for a deployed App or Service. However that dead weight is necessary @install-time. Not Everybody has a simple \"yum install\" from the official repositories. :)\n\nthank you very much\n"},{"labels":[null,"enhancement1"],"text":"## 1. Terms\n\nGlobal Namespace: often refers to the capability to aggregate remote filesystems via unified (file/directory) naming while at the same time supporting unmodified clients. Not to be confused with LXC pid etc. namespaces\n## 2. sha256\n\nDocker Registry V2 introduces content-addressable globally unique (*) digests for both image manifests and image layers. The default checksum is sha256. \n\nSide note: sha256 covers a space of more than 10 *\\* 77 unique random digests, which is about as much as the number of atoms in the observable universe. Apart from this unimaginable number sha256 has all the good crypto-qualities including collision resistance, avalanche effect for small changes, pre-image resistance and second pre-image resistance. \n\nThe same applies to sha512 and SHA-3 crypto-checksums, as well as, likely, Edon-R and Blake2 to name a few. \n\nThose are the distinct properties that allows us to say the following: two docker images that have the same sha256 digest are bitwise identical; the same holds for layers and manifests or, for that matter, any other sha256 content-addressable \"asset\". \n\nThis simple fact can be used not only to self-validate the images and index them locally via Graph’s in-memory index. This can be further used to support global container/image namespace and global deduplication. That is:\n\n_Global Namespace_\n_Global Deduplication_\n- for image layers. Hence, this Proposal.  \n## 3. Docker Cluster\n\nRest of this document describes only the initial implementation and the corresponding proof-of-concept patch:\n- https://github.com/alex-aizman/docker/commit/ef086c46caf46bb1fd812535e7509d21cc5769f1\n- https://github.com/alex-aizman/docker/commit/c86bb999adec44fb0ec68abd11fe0dbae6798be3\n\nThe setup is a number (N >= 2) of hosts or VMs, logically grouped in a cluster and visible to each other through, for instance, NFS. Every node in the cluster runs docker daemon. Each node performs a dual role: it is NFS server to all other nodes, with NFS share sitting directly on the node’s local rootfs. Simultaneously, each node is NFS client, as per the diagram below:\n\n![docker-namespace-federated](https://cloud.githubusercontent.com/assets/1682697/8260489/e680bba8-1678-11e5-9c3c-72394a9c205f.png)\n\nBlue arrows reflect actual NFS mounts. \n\nThere are no separate NAS servers: each node, on one hand, shares its docker (layers, images) metadata and, separately, driver-specific data. And vice versa, each node mounts all clustered shares locally, under respective hostnames as shown above.\n#### Note: hyper-convergence\n\nOften times this type of depicted clustered symmetry, combined with the lack of physically separate storage backend is referred to as storage/compute \"hyper-convergence\". But that's another big story outside this scope..\n#### Note: runtime mounting\n\nAs far as this initial implementation (link above) all the NFS shares are mounted statically and prior to the daemon’s startup. This can be changed to on-demand mount and more..\n\nBack to the diagram. There are two logical layers: Graph (image and container metadata) and Driver (image and container data). This patch  patches them both - the latter currently is done for aufs only. \n## 4. Benefits\n- An orchestrator can run container on an image-less node, without waiting for the image to get pulled\n- Scale-out: by adding a new node to the cluster, we incrementally add CPU, memory _and_ storage capacity for more docker images and containers that, in turn, can use the aggregated resource\n- Deduplication: any image or layer that exists in two or more instances can be, effectively, deduplicated. This may require pause/commit and restart of associated containers; this will require reference-counting (next)\n## 5. Comments\n\nIt's been noted in the forums and elsewhere that mixing images and containers in the Graph layer is probably not a good idea. From the clustered perspective it is easy to see that it is definitely not a good idea - makes sense to fork /var/lib/docker/graph/images and /var/lib/docker/graph/containers, or similar.\n## 6. What’s Next\n\nThe patch works as it is, with the capability to “see” and run remote images. There are multiple next steps, some self-evident others may be less. \n\nThe most obvious one is to un-HACK aufs and introduce a new multi-rooted (suggested name: namespace) driver that would be in-turn configurable to use the underlying OS aufs or overlayfs mount/unmount. \n\nThis is easy but this, as well as the other points below, requires positive feedback and consensus.\n\nOther immediate steps include:\n- graph.TagStore to tag all layers including remote\n- rootNFS setting via .conf for Graph\n- fix migrate.go accordingly\n\nOnce done, next steps could be:\n- on demand mounting and remounting via distributed daemon (likely etcd)\n- node add/delete runtime support - same \n- local cache invalidation upon new-image-pulled, image-deleted, etc. events (“cache” here implies Graph.idIndex, etc.)\n- image/layer reference counting, to correctly handle remote usage vs. ‘docker rmi’ for instance\n- and more\n\nAnd later: \n- shadow copying of read-only layers, to trade local space for performance\n- and vice versa, removal of duplicated layers (the “dedup”)\n- container inter-node migration\n- container HA failover\n- object storage as the alternative backend for docker images and layers (which are in fact immutable versioned objects, believe it or not). \n\nSome of these are definitely beyond just the docker daemon and would require API and orchestrator (cluster-level) awareness. But that’s, again, outside the scope of this proposal.\n## 7. Instead of Conclusion\n\nIn the end the one thing that makes it – all of the above - doable and feasible is the immutable nature of image layers and their unique and global naming via crypto-content-hashes.\n"},{"labels":["enhancement1",null],"text":"Hi all,\n\nI have an application that calls the sched_setscheduler system call [1]. This requires that I set the followings (assuming a CentOS 7 for example):\n\n$ C=$(docker run ...)\n$ echo 950000 > /sys/fs/cgroup/cpu/system.slice/cpu.rt_runtime_us\n$ echo 950000 > /sys/fs/cgroup/cpu/system.slice/docker-$C.scope/cpu.rt_runtime_us\n\nUnlike cpu.shares, this cannot be done directly via the docker CLI (as of 1.6.2) which is a bit of an issue because my process fails when it calls sched_setscheduler as I didn't get a chance to set the appropriate system values. We can't rely on the fact the parent cgroup has the appropriate value since the kernel clearly states that children cgroups will always get initialized to 0 anyhow [2]\n\n> By default all bandwidth is assigned to the root group and new groups get the period from /proc/sys/kernel/sched_rt_period_us and a run time of 0.\n\nBrowsing the code, I can see some other options have been added [3] but not the ones regarding real-time scheduling.\n\nHowever, looking at the libcontainer's code, I can see the underlying code is already there [4]. So, what's lacking is to expose them from the docker interface.\n\nIs there a chance this could be added to the CLI as well? This would make using docker and real-time scheduling much friendlier.\n\nThanks,\n\n[1] http://man7.org/linux/man-pages/man2/sched_setscheduler.2.html\n[2] http://lxr.free-electrons.com/source/Documentation/scheduler/sched-rt-group.txt?v=3.10\n[3] https://github.com/docker/docker/blob/master/runconfig/parse.go#L70\n[4] https://github.com/docker/docker/blob/04c6f09fbdf60c7765cc4cb78883faaa9d971fa5/vendor/src/github.com/docker/libcontainer/cgroups/fs/cpu.go#L54\n"},{"labels":["enhancement1"],"text":"As discussed a little bit on there https://github.com/docker/docker/issues/10409 (and more other issues and IRC I guess), we might want to have a package to handle validations that could be used in all builder, api and cli. I'm quoting @cpuguy83. An example is the following issue : https://github.com/docker/docker/issues/13821 where the check is done in `cli` and not at all in the API part of it ; leading to unwanted behavior.\n\n> What we could do is have an actual validator that gets checked before a container is even attempted to be created, and make sure that validation happens for each user input (builder, api, cli), and that it's all the same validator, since right now it's all over the place.\n\nI propose to have a `pkg/validation` that would centralize validations and ease maintaining this as it gets updated. Not sure the name `pkg/validation` is the right one but.. \n\nI'm thinking of a way to make it as easy and flexible as possible but do not have a final idea in the head. In the current state of thinking I had (which is still pretty small), I was thinking we could have some sort of struct where we could set the _context_ of the validation (cli, api, builder). The validation method would skip things or do differently depending on the _context_ ; something roughly like the following.\n\n``` go\n// Have a way to get/build it easily\ncliValidator := validators.ForCli()\nbuilderValidator := validators.ForAPI()\n\n// Same method on both\ncliValidator.ValidateTagName(…, …)\nbuilderValidator.ValidateTagName(…, …)\n\n// Method\nfunc (v *Validator) ValidateTageName(…) error {\n    // Do some common validation\n    if isCli(v) {\n        // Do some cli-only specific validation\n    }\n    if isAPI(v) {\n        // Do some API-only validation\n    }\n    // Do some more common validation\n}\n```\n\nIt's really rough and I'm still looking for information, opinion, critics :wink:. Any input is more than welcome :stuck_out_tongue_closed_eyes:.\n\nIf it feels we want to implement this, it should probably be cut in several PR (for the general way to do this, and for each type of validations : volumes, repository/tag name, …), but I'm probably way ahead of myself here :sweat_smile:.\n"},{"labels":["enhancement1"],"text":"Updating docker to 1.6.2 is giving a lot of trouble with containers and images, with bugs already reported elsewhere (I checked). As I installed docker through the get.docker.io script, it would be useful to be able to add a version of docker to the url, and use for example https://get.docker.io/1.6.0 to install a specific version.\nAlso, having a combined resource with all the possible ways to use get.docker.io would be very useful.\n"},{"labels":[null,"enhancement1"],"text":"storage-drivers allow for additional options (http://docs.docker.com/reference/commandline/cli/#daemon-storage-driver-option)\n\nI propose expected behavior of the storage driver preserving the provided options at runtime. If the options change in a way that is not migratable, then it can bail with a message to the user on how to mitigate the issue. (e.g. whether `/var/lib/docker` needs to be a clean slate, etc)\n"},{"labels":["enhancement1"],"text":"See discussion here: https://github.com/docker/docker/pull/13038#issuecomment-99721834\n\nFor those of us that need Registry 2.0 support on the Ubuntu 14.04 platform, the only game in town is dynamically-linked 1.7.0-dev binaries.  Building these binaries and packaging them up in .debs takes some work and it's especially painful for people who want to install them using an apt repo.  \n\nIdeal solution: nightly (or at least regular) builds of 1.7.0-dev (dynbinary) available as signed packages in the official get.docker.io repo.\n\nLess ideal solution: nightly builds of 1.7.0-dev (dynbinary) packaged as .debs and made available on master.dockerproject.org.   The problem with this solution is that this site isn't (to my knowledge...) set up as an apt repository.  Perhaps someone could go through the process of making it into one?\n\nThanks!\n"},{"labels":[null,"enhancement1"],"text":"There is an enormous amount of struct definitions and fields without documentation. Too examples of code docker datatypes, absolutely critical to the internals of docker, having passing documentation:\n\nhttps://github.com/docker/docker/blob/master/image/image.go#L24\nhttps://github.com/docker/docker/blob/master/runconfig/config.go#L109\n\nThe lack of documentation on these fields leads to major issues in a large codebase. The largest issue is that intent of the field is completely lost to history. This makes adding new features more complex and leads to situations where new fields are added in other, non-optimal locations. It also leads to misuse of said fields. Discovery within the codebase is heavily impacted. One example of this is `(image.Image).Container`. One looks at that and asks, why would an image know anything about a container? Don't we create images from containers? What is the intended relationship? Such detail can only be inferred by search existing uses. However, such uses don't provide any detail about correctness, making it much harder to spot inconsistencies and bugs.\n\nOne of the best features of Go is the automatic documentation. It not only allows one to understand available APIs, but also provides deep links into the source. There are few better tools to understand the architecture of an application. Looking at [image.Image](http://godoc.org/github.com/docker/docker/image#Image), the lack of documentation here is opting out of fully leveraging such a fantastic tool.\n\nI propose two things with this issue:\n- [ ] No new fields should be added in without complete documentation. Ideally, we'd adopt full go documentation standards.\n- [ ] Existing fields should be documented (this is a larger effort)\n\nThe effect of taking these actions will be two folds:\n1. New contributors will spend less time understanding the application and provide better contributions.\n2. Obvious architectural mistakes will become apparent and can be addressed and fixed.\n\nI understand this style of issue has a danger of not being actionable, but I hope this can open a conversation on code quality that is much needed.\n"},{"labels":["enhancement1"],"text":"Currently building an arbitrary Dockerile results in an image that does not have a repository nor a tag name.\nIt would be useful to have a syntax similar to FROM that is defining the base image, to actualy provide the _default_ repository and tag name, so that they do not have to be passed with -t flag to the docker build command.\nThis will make it easier to create new images from new versions of software, just by bumping some number that will be for both, installing the actual package and for the tag name of the docker build.\n\nExample:\n\n```\nFROM repository/base-linux:latest\nENV PKG_VERSION 2.3.4\nTO repository/jenkins:$PKG_VERSION\n...\n...\nRUN apt-get install jenkins=$PKG_VERSION\n\n```\n"},{"labels":["enhancement1"],"text":"For an already running instance (with it's port mapping configuration and data stored inside of it) I can't go back to doing a \"docker run --restart=always\", so it would be nice to have a way of setting the RestartPolicy on start also, not only on run. \n"},{"labels":[null,"enhancement1"],"text":"Hi,\n\nCurrent implementation of the syslog logging driver can only send STDOUT and STDERR of the process running under container to the host's syslog with info and err levels accordingly. However, lots of software support logging to syslog natively, that allows to specify different parameters (like severity, facility, tag, etc) for different messages. By using STDOUT/ERR logging only we loose these abilities.\n\nThe idea is to make the syslog logging driver to also try mounting host's /dev/log to container'd /dev/log, so the software supporting logging to syslog could do this. Probably it would also be nice to redefine the message line of the syslog messages to reflect the container id like it is done for STDOUT/ERR currently.\n\nYes, I know that I can do it myself by specifying \"-v /dev/log:/dev/log\", but I think that it makes a lot of sense to have it supported natively by the driver.\n\nThoughts?\n\nThank you. \n"},{"labels":[null,null,"enhancement1",null],"text":"I am trying to get autofs to work inside containers and it does not work. Specifically, I need indirect mapping to work, though I can't even get direct mapping to work. There are times that it works, but most of the times it just hangs. \n\ndocker version:\n\n```\nClient version: 1.6.2\nClient API version: 1.18\nGo version (client): go1.4.2\nGit commit (client): 7c8fca2\nOS/Arch (client): linux/amd64\nServer version: 1.6.2\nServer API version: 1.18\nGo version (server): go1.4.2\nGit commit (server): 7c8fca2\nOS/Arch (server): linux/amd64\n```\n\ndocker info:\n\n```\nContainers: 24\nImages: 23\nStorage Driver: aufs\n Root Dir: /work/docker/aufs\n Backing Filesystem: extfs\n Dirs: 73\n Dirperm1 Supported: false\nExecution Driver: native-0.2\nKernel Version: 3.13.0-24-generic\nOperating System: Ubuntu 14.04 LTS\nCPUs: 32\nTotal Memory: 125.8 GiB\nName: amberjack\nID: ZMR6:XRED:SO7Q:URGP:I3PK:4AAH:5R4J:VDQZ:RR5P:NO3Q:PVK4:DPZQ\nWARNING: No swap limit support\n```\n\nuname -a:\n\n```\nLinux amberjack 3.13.0-24-generic #46-Ubuntu SMP Thu Apr 10 19:11:08 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nEnvironmental details:\nThis host is Ubuntu 14.04 and the container is Scientific Linux 6.1.\n\nHow reproducible:\nIt's very consistent\n\nSteps to reproduce:\nI created an image with this Dockerfile\n\n```\nFROM sl6.1-64:rockmachine\nCOPY start-services.sh /start-services.sh\nRUN chmod a+x /start-services.sh\nENTRYPOINT [\"/start-services.sh\"]\n```\n\nThere's nothing special in sl6.1-64:rockmachine - it's just the base Scientific Linux 6.1 image with a bunch of packages already installed\n\nstart-services.sh looks like this\n\n```\n#!/bin/sh\n\n# Start services\nservice rpcidmapd start\nservice rpcbind start\nservice nfslock start\nservice autofs start\n\n# autofs does not work until killed and started again\nkill -9 `pgrep -f automount`\nservice autofs start\n\n/usr/sbin/sshd -D\n```\n\nIncidentally, I noticed that I need kill and restart autofs service for the rare times it does work.\n\nBuild image:\n\n```\nsudo docker build -t ksalman:test .\n```\n\nSince I am using indirect maps, there's not much in auto.master\n\n```\n[root@3029cb0205ae ~]# cat /etc/auto.master\n+auto.master\n/net -hosts -dev\n```\n\nThe container was started with in privileged mode:\n\n```\nsudo docker run --privileged --expose=22 -d -P -v /etc:/shared/etc:ro --name ksalman ksalman:test\n```\n\nActual results:\nI can browse the parent directories but nothing after that\n\n```\n[root@3029cb0205ae ~]# ls /net/lurker/datapool/\nenglab  repository  scratch  winpe\n[root@3029cb0205ae ~]# ls /net/lurker/datapool/repository\n^Cls: cannot access /net/lurker/datapool/repository: Interrupted system call\n```\n\nIt just hangs here and I ctrl-c out.\n\nExpected results:\n\n```\nksalman@latite ~ $ ls /net/lurker/datapool/repository\nadd-rpms  centos-archive-rsync.txt  epel            rhel                  sl                                 ubuntu\naptly     centos-rsync.txt          epel-rsync.txt  opencsw  rpmforge              sysdump-swash-20141217-080344.tgz  VNC-5.2.2-Windows.exe\ncentos    debian                    fedora          ovas     scientific-rsync.txt  treasury-isos                      windows\n```\n"},{"labels":[null,"enhancement1"],"text":"Hi Guys\n\nWe develop a container security suite oriented to the enterprises. As part of it we support Kerberos as an authentication mechanism between Docker CLI and the Docker daemon. We currently have it up and running but, honestly, think it should be part of the platform. Hence, we would like to contribute the code to Docker. \n\nToday we use Nalin's GSS Go wrapper (https://github.com/nalind/gss) over vanilla MIT Kerberos (https://github.com/krb5/krb5). \n\nI'd like to propose the following, simple, addition to the command line interface that enables generating Kerberos ticket (TGS) on the client (CLI), and require validating Kerberos ticket on the daemon.\n\nCLI example:\n\n```\ndocker -H docker-host:8080 --kerbverify --kerbspn=<service_name> ps\n```\n\nDocker daemon example:\n\n```\ndocker -d --kerbverify --kerbspn=<service_name / *> --kerbkeytab=/tmp/docker_keytab\n```\n\nThe SPN argument is mandatory. However, keytab argument is not. In case keytab is not specified, a default path is used. The default path is: `/etc/krb5.keytab` (as in [MIT krb5](http://web.mit.edu/KERBEROS/krb5-1.12/doc/mitK5defaults.html))\n\nAll variables can be configured using environmental variables:\n\n```\nDOCKER_KERB_VERIFY\nDOCKER_KERB_SPN\nDOCKER_KERB_KEYTAB\n```\n\nWe will support MIT krb5 and Microsoft Active Directory as KDC.\n\nI would be glad to hear you general thoughts and if make sense I'll send a more elaborated technical descriptions of what we'd like to do. \n"},{"labels":[null,"enhancement1"],"text":"It would be nice if there was a way to enable default arguments for `docker run`/`docker exec` when specifying `CMD`. For example, when using `bash` one does something like this normally.\n\n```\nCMD [ \"/bin/bash\" ]\n```\n\nTo enable default TTY one might do something like the following. This could naturally be expanded to include other arguments like various redirects and forwardings.\n\n```\nCMD [ \"-t\", \"/bin/bash\" ]\n```\n"},{"labels":[null,"enhancement1"],"text":"Right now docker's mixing of container and image commands leads to a bit of a usability problem. For example, `docker inspect` doesn't know if we're talking about containers or images, so we check for both. Which can lead to conflicts since images and containers can have the same name, and partial IDs.  Solutions like #13187 are ok but not really ideal.  We have similar issues with 'docker rm', but in that case we created 'docker rmi', which works, but feels hacky.\n\nIt would be better if there was a dedicated `docker image` command so that its clear we're talking about images and not containers.  It also allows us to add new image-specific commands (and container-specific commands at the top level) w/o the possibility of conflict.\n\nWhile, people may naturally then ask about a `docker container` command, and that is certainly possible, but I'm not suggesting we do that right now.  I think keeping all `docker verb` commands acting on containers makes sense because docker's main focus is on containers and the extra typing involved in writing `docker container verb` would IMO be a UX hit.\n\nI've create a branch for people to play with to see how it feels:  https://github.com/duglin/docker/tree/ImageCmd\nAll `docker image verb` commands just call the appropriate `docker verb` command so we can avoid duplication of logic/code.  However it would require more code than what I show in the branch in order to finish this work and to give a better UX, in particular:\n- make the help for each image related command show both the `docker verb` and `docker image verb` help text in both flows\n- will need to add code so that the error messages show the correct command based on what the user used.\n- will need to leverage #13187 for `docker image inspect` or some other code if #13187 doesn't go in\n\nI'm sure there's more, but this branch does seem to show that the basic idea does seem to work, but before I go much further I'd like to know if we'd like to head this direction.\n\nTo be clear, I'm not suggesting we remove the old `docker verb` commands at this time, we can't break people.  I think this path is consistent with the proposals I've heard to add other top-level `docker noun` commands (like `docker volume` and `docker network` ...).\n\nping @crosbymichael @shykes \n"},{"labels":[null,null,"enhancement1"],"text":"For day-to-day use and simple scripting purposes It'd be great if the --filter function was extended with filters on more columns.\n\nSpecifically:\n\n```\n$ docker ps --filter image=myname/test \n```\n\nWould be great so we can kill all our tests in one go like so:\n\n```\n$ docker ps --filter image=myname/test -q | xargs docker rm -f\n```\n\nThis is helpful specifically in this combination because I want only image id's to be returned, and |grep thus doesn't work.\n\nOther filters that would make sense to me:\n\n_For images_\n- Repository (with tag)\n- Age? (--filter agemore=4h --filter ageless=2w)\n\n_For containers_\n- Image (with tag)\n- Name (this is implemented by the way https://github.com/docker/docker/issues/10897, but undocumented)\n- Age? (as above)\n- Status (running, exited, etc)\n\n**references**\nI found this ticket: https://github.com/docker/docker/pull/11904 which describes something that sounds similar but (I think) is actually for the bash completion (only).\n"},{"labels":[null,"enhancement1"],"text":"`docker run` should have an `--pull` or `--update` option to pull down the most up to date version of an image tag, then run. The behavior should match `docker build`.\n\nSee #4238 \n"},{"labels":[null,"enhancement1"],"text":"For security reasons not all ports can be opened on firewall and there is no way to define port allocation range https://github.com/docker/docker/blob/73b7cee89c8cde951012c0b0004c5faa579a0028/daemon/networkdriver/portallocator/portallocator.go#L14-L16\n\nIf will be very suitable to allow specify this settings for daemon.\n\nP.S. Also as i see portallocator has no strategies and by default uses +1 port for any newly run container. As separate enhancement  this issue can be masked with strategy that will search always for lowest available port (though it will be slow i suppose).\n"},{"labels":[null,"enhancement1"],"text":"#12422 has only implemented `--log-opts`  for `docker --daemon`.\n\nHowever a logging driver  (`--log-driver`) and thus its options can be specified:\n- on daemon in `docker -d` (as default logging driver)\n- per container in `docker run`/`docker create` (to override daemon's default logging driver)\n\nCurrently logopts (stored at `LogConfig.Config`) is not passed to the logging drivers (via `logger.Context`). I was gonna do that but then I realized unless `--log-opt` is implemented for `docker run/create` as well, any container-specific `--log-driver` setting will lose `--log-opt` of the daemon. So this better be fixed first.\n\nAlso we have no integration tests verifying if the --log-opts are passed to the daemon correctly.\n\n@LK4D4 @wlan0 is any of you willing to pick this up?\n"},{"labels":["enhancement1"],"text":"## Background\n\nExternal tools, like [docker/compose](https://github.com/docker/compose) are currently using name convention to identify image they build as part as their workflow, in order to find them back and manage them. As proposed in this [issue #1403](https://github.com/docker/compose/issues/1403), it would be easier and more flexible to use _labels_ for this use cases.\n\nThe current way to add labels to images are to add `LABEL` lines in `Dockerfile`. This proposal aims to add a flag(s) to the `build` command to allow settings labels to images when building them **and** support this in the API as well (so external tools can work with it).\n\nAlthough the main use case that comes in mind for me is the use in external tools like compose, I think it's good to be abel to add image label as part of the building, that weren't defined in the `Dockerfile`.\n## Syntax\n\nTo be coherent, the syntax should act the same as when using the `run` command (`--label` and `--label-files`).\n\n``` bash\n$ docker build --label key=value -t myimage .\n$ docker build --label key1=value1 --label key2=value2 -t myimage .\n$ docker build --label-file=mybuildinglabels -t myimage .\n```\n\nLet me now how you feel about that. I can work on it if this is something the community finds it useful.\n"},{"labels":[null,null,"enhancement1"],"text":"As a frequent exporter of docker filesystems, I want an option to tell `docker cp` to copy symlinks as literals, so that my filesystem archives receive the symlinks that my containers contain.\n\nMaybe `docker cp -a`?\n"},{"labels":[null,"enhancement1"],"text":"Some people expressed desire to have capability to be able to quickly inspect an image. Right now one can save image content to tar file and untar it later and look into image. But taring the content creates additional copy which is slow.\n\nAnother method seems to be to create a container and then run a command inside that container. But that requires command to be present inside an image at the same time running container of image being inspected is not very clean. People argue that one does not want to run the container which is\nbeing inspected.\n\nSo it was suggested how about implementing a new set of commands say mount/umount which just mount image read only at a given directory. That will allow host to quickly inspect image without creating copy and then unmount the image.\n\nI am about to post a very basic patch to be able to do that. That can serve as place for discussion on this topic and how to go about solving this issue.\n"},{"labels":["enhancement1"],"text":"This has to do with https://github.com/docker/docker/issues/6906 in a way.\n\nSo I need to install a huge (~400MB) rpm. I used to do it with:\n  COPY product.rpm /tmp\n  RUN rpm -iv /tmp/product.rpm && rm /tmp/product.rpm\nAs a result you end up with an image twice the size.\n\nSince the rpm is already a sibling to the Dockerfile I started serving the whole build context over http befors the build starts and have a single RUN layer instead:\n\n  RUN rpm -iv http://build.context.local/product.rpm \n\nThe downside is you need to run at least \"python -m SimpleHTTPServer\" beforehand. Then run dnsmasq alongside with docker and to support parallel builds, pre-process the Dockerfile so that different build contexts are served on different base urls.\n\nHowever to support this approach the docker daemon could easily run a http server exposing the whole build context on / for every build started. The hostname build.context.local is available only during build and can be different.\n\nThis is an alternative to mounting a volume during build. It seems not so intrusive - it is just as portable as RUNning \"yum install\". On the other hand not every tool out there might be as http-friendly as rpm.\n"},{"labels":["enhancement1"],"text":"One thing that would be useful is prioritizing certain containers when it comes to the OOM killer. Right now, docker only supports a simple on/off. It would be much more useful to be able to score containers, so that if a process has to be killed, it favors certain containers.\nThis behavior is controlled by the `/proc/$pid/oom_score_adj` tunable. A score of `-1000` disables the OOM killer, and the max score of `1000` makes it a near certain target for the OOM killer.\n\nAdditionally, since `-1000` disables the OOM killer, this setting would deprecate the current `--oom-kill-disable` parameter.\n"},{"labels":[null,"enhancement1"],"text":"Some docker commands like `docker events` or `docker logs`(#12362) accept `--since` or `--until` parameters to filter events out. Although these methods accept either RFC3339 timestamps or Unix epoch times.\n\nI can hardly imagine anybody trying to take a look into their container's past hour's logs sitting down and writing a command like :\n\n```\ndocker logs \\\n    --since=(oh what's the time now)2015-05-09T13:00:00(I hope format is correct )\\\n    mycontainer \n```\n\nTherefore I propose `time.Duration` inputs in these commands such as:\n\n```\ndocker logs --since=1h mycontainer\ndocker events --since=1d --until=5m\n```\n\nThe way that's gonna work is instead of trying to parse specified string into RFC3339 first and then falling back to epoch, we will also try parsing with `time.ParseDuration` (see `pkg/timeutils/utils.go#GetTimestamp` I submitted at #12362).\n\nOnce the duration is parsed, it will be subtracted from `time.Now()` and will be converted to Unix epoch time (as the Remote API requires) and that value will be sent. Assumption here is time drift between client vs server is minimal<sup>1</sup>. Therefore when the durations are used the duration will be relative to client machine's time (different behavior than providing epoch or RFC3339 value).\n\n<sup>1</sup>: [citation needed] lol.  Computers connected to internet have pretty precise clocks these days, give or take 5-10 secs max, but I don't expect anybody to use durations for precise timing on the engine-side. This is more for “usability”.\n"},{"labels":[null,"enhancement1"],"text":"We have --tail but not --head for whatever reason.\n\nThis is useful for something like mongoDB where there is a lot of valuable startup info when the container starts, but a LOT of repeated, less useful logs after. If startup fails and I can't look at the logs for a while, I have to risk filling up my screen with every log since startup and then scroll alllllll the way to the beginning to find what I need....\n"},{"labels":["enhancement1"],"text":"Unlike docker runtime dir, which can be configured with ``--graph` option,\nthere doesn't seem to be any way to manually set execdriver's rootdir.\n\nSeems the hard-coded `/var/run/docker` path was introduced with this PR #11716\n\nIt'd be great if we could configure this with command line arg?\n"},{"labels":[null,"enhancement1",null],"text":"See https://github.com/docker/docker/issues/5603 for some history.\n\nAllowing for multiple images to be built from a single Dockerfile (via multiple FROM commands), while interesting, isn't fully supported. In particular, the non-final images that are produced are not easily found w/o some ugly parsing of the build output.  It would then also leads to people wanting to do things like add TAG commands to tag each image so they're easily used/found.\n\nI think there are two options:\n1) complete the \"multiple FROM\" support we have by making it easier for people to find/use those intermediate images.\n2) remove support for multiple FROMs until we figure out exactly how people are supposed to use it.\n\nThis issue is to get the conversation started and to propose option 2 - remove it.\n\nQuestion: are people using this feature today and if so what are the usecases for it?\n"},{"labels":[null,"enhancement1"],"text":"Let's say I want to rerun all of the steps in a docker build starting at Layer 6 (1265e16d0c28) and the last layer generated by the dockerfile is 1265e16d0c28\n\nToday if I want to delete Layer 6 and rerun `docker build` I have to do something like this:\n\n```\ndocker rmi $(docker history -q 89785ab09669 | grep -B 100 1265e16d0c28)\n```\n\nWhich basically takes all of the images 'after' 1265e16d0c28 (inclusive) and deletes them in reverse chronological order.\n\nIf docker could do that for me, with something like:\n\n```\ndocker rmi -r 1265e16d0c28\n```\n\nThat would be a lot easier to explain to people (especially new people) and it reduces the frequency with which someone has to use --no-cache\n"},{"labels":["enhancement1"],"text":"TAG command should allow to:\n- Create multiple images from a single Dockerfile\n- Create different tags for the same image\n\nSee more info in #5603 commented by @thaJeztah .\n\nI also think it is nessary to add a 'COMPRESS' command. It is more likely to the #332.\nIt can compress the layers between a parent layer and the last layer.\n\nFor example:\n\n```\nFROM centos:latest\nRUN yum install -y xxx\nMAINTAINER xxx\nEXPOSE 80\nCOMPRESS centos:latest\nEXPOSE 8080\n```\n\nNow, we can get the images like this.\n\n```\n└─ad18ff9f83df Virtual Size: 484.7 MB Tags: centos:latest\n    └─f45f88e50248 Virtual Size: 552.7 MB\n        └─3e5747d65960 Virtual Size: 552.8 MB\n            └─8c381ae7a086 Virtual Size: 552.8 MB\n    └─13d909f018b8 Virtual Size: 552.8 MB\n        └─5701e3b9b9ed Virtual Size: 552.8 MB\n```\n- ad18ff9f83df: FROM centos:latest\n- f45f88e50248: RUN yum install -y xxx\n- 3e5747d65960: MAINTAINER xxx\n- 8c381ae7a086: EXPOSE 80\n- 13d909f018b8: COMPRESS centos:latest\n- 5701e3b9b9ed: EXPOSE 8080\n\nAs we see, 'COMPRESS' command can compress the layers between 'ad18ff9f83df' and '8c381ae7a086', and build a new layer '13d909f018b8' which contains all the content of 'f45f88e50248', '3e5747d65960' and '8c381ae7a086'. Besides, the new layer '13d909f018b8' will be the child of 'centos:latest'. The command after it will be based on this image layer.\n\nBy this two command, we will have some interesting method to organise the Dockerfile.\n\nFor example:\n\n```\nFROM centos:latest\nRUN yum install -y xxx\nMAINTAINER xxx\nEXPOSE 80    \nCOMPRESS centos:latest\nTAG mytest:base\nEXPOSE 8080\nTAG mytest:v1\nRUN yum install -y xxx\nCOMPRESS mytest:base\nTAG mytest:v2\n```\n\nWe can get images below.\n\n```\n└─ad18ff9f83df Virtual Size: 484.7 MB Tags: centos:latest\n    └─f45f88e50248 Virtual Size: 552.7 MB\n        └─3e5747d65960 Virtual Size: 552.8 MB\n            └─8c381ae7a086 Virtual Size: 552.8 MB\n    └─13d909f018b8 Virtual Size: 552.8 MB Tags: mytest:base\n        └─5701e3b9b9ed Virtual Size: 552.9 MB Tags: mytest:v1\n            └─33c38b267b5e Virtual Size: 553.9 MB\n                └─e7da47ddcdf7 Virtual Size: 553.9 MB\n        └─98354e5ca627 Virtual Size: 553.9 MB Tags: mytest:v2\n```\n- ad18ff9f83df: FROM centos:latest\n- f45f88e50248: RUN yum install -y xxx\n- 3e5747d65960: MAINTAINER xxx\n- 8c381ae7a086: EXPOSE 80\n- 13d909f018b8: COMPRESS centos:latest\n- 13d909f018b8: TAG mytest:base\n- 5701e3b9b9ed: EXPOSE 8080  \n- 5701e3b9b9ed: TAG mytest:v1\n- 33c38b267b5e: RUN yum install -y xxx\n- e7da47ddcdf7：EXPOSE 8090\n- 98354e5ca627：COMPRESS mytest:base\n- 98354e5ca627：TAG mytest:v2\n\nAs we see, this is very useful for the iterative development.\n\nTAG command has been written and tested. I make a pull request, see #12983 . \n\nCOMPRESS command has also been written but not already tested. I will take the opinions of the community and then make another pull request.\n"},{"labels":[null,"enhancement1"],"text":"I'd like to propose/explore the idea of preventing containers from running as root with a daemon config. This would be useful for a workflow we envisage for an environment where we could run Docker on desktops in a secure enterprise environment.\n\nI've written a basic proof of concept which seems to work:\n\nhttps://github.com/ianmiell/docker/tree/disallow_root\n\nDoes anyone have any views on the feasibility of this, or where this might go - for example, do people want to exclude more users in a list, or by uid? Suggestions welcome.\n"},{"labels":[null,null,"enhancement1",null],"text":"As several people have mentioned (@thaJeztah, @duglin) in #9707, it would be great to be able specify the .dockerignore file using -i/--ignore in conjunction with named dockerfiles.  It is often difficult to use named dockerfiles because the build context becomes too large.\n"},{"labels":[null,"enhancement1"],"text":"## Problem description\n\nUsers and developers use different editors and tool to edit, backup and process code, documentation and assets. It is not feasible to catch them all in a project's `.dockerignore`. Instead, a user should be able to blacklist the file of their own toolchain.\n## Proposed solution\n\nAdd a global `.dockerignore`, e.g. at `$HOME/config/docker/dockerignore`, which gets combined with the project's `.dockerignore`.\n## References\n\nDocker is not the only tool with this problem. The problem is well known in the VCS world. GIT for example accepts the global `core.excludesfile` option which points to a global `.gitignore`. This file is combined with the projects `.gitignore` to blacklist more files.\n## Open questions\n- Where should the config file be located on Windows?\n- Should the global ignore file virtually appended or prepended to the local file? Does this make a huge different? If so, what are the implications and the pros and cons of the different strategies?\n"},{"labels":[null,"enhancement1"],"text":"Just used Maven image to turn Java source to image on build!  Maven will downloads all the dependencies into /root/.m2/repository directory.  But since the fs is not persistent, Maven will download all the dependencies on every single build.\n\nIt would be great if we can mount the repository directory when building - so that the dependencies don't need to be re-downloaded each time.\n"},{"labels":[null,"enhancement1"],"text":"By default, `docker` executes `iptables` command to alter system firewall rules.  This is fine as long as the host system hasn't any automatic way to handle firewall. \n\nIf the host system has their own firewall deployment (puppet, chef, automation tools,...), then Docker just creates a mess. Especially, if the firewall is reset/flushed after a period (this is true on my systems), all Docker rules are gone away. The only way to make running containers alive is to restart them. Sometimes, Docker can't start new container because some base rules are missing (e.g, when `nat/DOCKER` was removed.)\n\nNow I have to deploy an in-house tool to generate firewall (_NAT_) rules from containers's information; we also have to use `iptables=false`. Instead of reinventing the wheel, I suggest Docker to have an ability to export their `iptables` commands so `sysadmin` can use the rules any time they want.\n\nExample\n\n```\n$ docker run -p 1234:1234 --name foo bar\n$ docker inspect --format \"{{.iptables.}}\" foo \niptables -t nat bla bla\niptables -t nat bla bla ...\n```\n"},{"labels":[null,null,"enhancement1",null],"text":"I would like to be able to tell the difference if a container was created by a user or by a Docker build.  Would anybody be opposed to adding a label on the container indicating it is from a build?\n"},{"labels":[null,"enhancement1"],"text":"As discussed on IRC it would be great to be able to diff two images to see the changes on the file system level. The pragmatic approach would be to integrate it into the history command.\n\n```\ncpuguy83:   tcurdt: No, there's no built in way to get a diff between two layers.\ntcurdt: cpuguy83 thx. do you think that would be hard to implement?\ncpuguy83:   Would be cool if that was part of the history message\ncpuguy83:   tcurdt: I wouldn't think so since we already have to get the diff to create the layer.\nvdemeester: tcurdt: that would be easy yes as cpuguy83 it's already implemented for the diff command\nvdemeester: it's just you can't say diff layer1..layer2 in the cli\ntcurdt: cpuguy83 vdemeester : shall I open an issue for that and take it from there?\ntcurdt: feature request reallly\ncpuguy83:   tcurdt: Sure, sounds cool!\n```\n"},{"labels":["enhancement1"],"text":"In interactive mode (docker run -it) the key sequence Control-P Control-Q causes a detach.\n\nIt's good that you can do this, but this conflicts with the use of Control-P to move backwards in the bash command history.\n\nSo, when running bash inside docker interactive mode, two Control-P's are necessary to move backwards in the command history, instead of just one as in an ordinary invocation of bash.\n\nRather than reprogramming my brain, I'd like to be able to change this key binding to something else. It seems to be hard-coded now.\n"},{"labels":["enhancement1"],"text":"Currently the `/build` endpoint of the API [accept](https://github.com/docker/docker/blob/d12a2d8aeb483870ae064f70308e6ec7109f2a06/api/client/build.go#L287) an additional query parameter `?dockerfile=` that specify the path to the `Dockerfile` relative to the context.\n\nThis introduce the following limitations:\na. when using `-f`, the client will [fail](https://github.com/docker/docker/blob/d12a2d8aeb483870ae064f70308e6ec7109f2a06/api/client/build.go#L148) a build if the Dockerfile is not  contained in the context directory\nb. this prevent reusing the same Dockerfile between different context.\n\nWhat about passing the `Dockerfile` _content_ in the `POST /build` payload instead? We could optionally accept a multipart body with both the `Dockerfile` and the `context.tar` content.\n\nRelated #8418 #12072.\n"},{"labels":["enhancement1"],"text":"There doesn't seem to be a way to limit a containers disk usage, like there is with memory/cpu (https://docs.docker.com/reference/run/#runtime-constraints-on-cpu-and-memory). \n\nWas wanting to do this to test my container in \"out of disk space\" scenarios, though seems useful in general.\n\nI see the server options `--storage-opt` for changing basesize but that is a global setting https://docs.docker.com/reference/commandline/cli/ ?\n"},{"labels":[null,"enhancement1"],"text":"At the moment it is very hard to have an automated system to provision a server with multiple docker containers if containers come from different repositories. It would be awesome if one could have something like \"docker pull awesome/image --pull-key=SOME_KEY\". This way it would be easier to use tools like chef/puppet/ansible to deploy docker containers on remote servers. At the moment, if one wants to start a container he/she first needs to log in, then pull the image and start it. If the user is not logged in docker is not be able to pull the image. The problem is even more serious when an administrator wants to start multiple containers coming from different repositories. He/she then needs to log into one repository, pull the image, log into next one, pull the image, and so on... Not so cool provisioning process.\n"},{"labels":[null,"enhancement1"],"text":"It would be helpful in some instances when linking containers to be able to specify that the env variables in the source container, as well as the additional ones Docker injects (such as <alias>_PORT) should not be injected into the target.\n\nWe have hit situations where the <alias>_PORT Docker automatically injects into a target container conflicts with env variables our application is looking for in a different format.\n"},{"labels":[null,"enhancement1"],"text":"You will agree that creating a docker image is mostly about starting from another one and add to it. We can already ADD (and COPY, this is already quite confusing), which append new files to the image.\n\nWhat I think is a very common pattern is the following : we need to 'adjust' a previously existing configuration file. This would be done by appending a shunk of configuration at the end of this file. Typical stuff would be\n- add new configuration options (\"AAA=xxx\")\n- append to existing variables (\"PATH=\"/new/shiny/software/just/installed:${PATH}\")\n\nSome software allows configuration files to be splitted inside a \".d\" directory, and that would solves the problem here, but not all of them can be used this way.\n\nThe current solution I have is something along:\n\n```\nCOPY files/myconfig.append /tmp/\nRUN cat /tmp/myconfig.append >> /etc/portage/make.conf  && rm -f /tmp/myconfig.append\n```\n\nBut it's far from ideal. It's cumbersome, but my main concern with it is that it creates a useless commit with temporary data (delete just the step after).\n\nMy ideal new Dockefile command would hence be: \n\n```\nAPPEND localfile /path/to/config/file/in/image\n```\n"},{"labels":[null,"enhancement1"],"text":"A common use case when writing Dockerfiles is to `ADD` source code then followed by compiling and then packaging the application.  Currently the `ADD` results in its own layer, without the ability to remove it.  \nIdeally I would be able to do this in a Dockerfile\n\n```\nADD src /src && \\\n        cd /src && \\\n        make install && \\\n        rm -rf /src\n```\n\nThis way the `src` will never end up in a layer.\n"},{"labels":["enhancement1"],"text":"To prevent code inside a container from creating objects that could be potential security holes on the host.\n\nMight also show a warning if a container is started and the filesystem used to mount a volume does not have those options (\"the filesystem used allows device, executable and suid files to be created, please consider using a separate filesystem with nodev,noexec,nosuid options\").\n"},{"labels":[null,"enhancement1"],"text":"Images often require environment variable configuration to run (passwords, API keys, etc) and there is currently no way for image to require that environment variable to be set at runtime.\n\nI am proposing that we add a way for images to require an environment variable to be set at runtime, without setting a value. For example:\n\n`Dockerfile`\n\n```\nFROM python:2.7\nADD . /code\nWORKDIR /code\nRUN pip install -r requirements.txt\nENV MAILGUN_API_KEY\nCMD python app.py\n```\n\nThen when running this image:\n\n```\n$ docker build -t myapp .\n$ docker run myapp\nThis image requires the MAILGUN_API_KEY environment variable to be set. Please set it with the -e option.\n$ docker run -e MAILGUN_API_KEY=abc123 myapp\n...\n```\n\nThis is somewhat analogous to being able to set `EXPOSE` or `VOLUME` in an image without a specific port or volume on the host. This is also a variation on a number of previous ideas (#7247, #8272).\n## Background\n\nMore generally, I'm thinking of ways that images can better define their various inputs and outputs. Images can currently `EXPOSE` ports as output, specify `VOLUME` as an input/output, etc, but there is no way of requiring configuration as input.\n"},{"labels":["enhancement1"],"text":"Currently, when a client issues a `build/` request with a `remote=<http_url>` query parameter, docker assumes the remote resource to be a raw Dockerfile (when the `Content-type` of the resource is e.g. `application/gzip`, docker blindly tries to run it as if it were a text file, resulting in binary garbage in the build log.\n\nApparently the solution is simply a matter of detecting the actual content type of the resource in `builder/job.go` and only creating a tar archive when the type is `text/*`. When the detected mime type maps to binary content the bytes from the response should be simply passed forward.\n\nIf this sounds reasonable I can submit a PR with the fix.\n"},{"labels":[null,"enhancement1"],"text":"/cc @ibukanov @thockin @jfrazelle\n\nAuthor: @ibukanov (Most text is taken from his first issue on this)\nImplementation and further ideas: @ibukanov @thockin @runcom\n\nOriginal proposal/issue: #10462\nRelated: #11777, maybe others\n\nHere I'm trying to draw a big picture of what is needed and how can be achieved.\nOf course, your feedbacks on this is strongly required/needed\nSorry in advance if my english is not perfect. I'm not native and I'll try to be\nas clear as possible.\n\nNOTE: I might just refer to network namespace here but this applies to all\nnamespaces (even user?)\n## PROBLEM\n\nAs of today, for instance, linking two docker containers requires the first one\nto be up and running before starting another one with flags that share first\ncontainer's namespaces (eg. `--net:container:foo`, even `--link foo`).\nIn this scenario it'is not possible to restart or stop the first container. Also,\norder matters and in a chain of linked containers one has to be really careful\nwith startup order.\nThe fact that something has to be running to keep it open means that there's a\nrisk - if that running process dies for any reason, all of the things that joined\nthe (net) namespace are in jeopardy and have to be restarted.\n## SO?\n\nWouldn't be great if all of this just acts like Volumes? (They continue to exist\nand can be used even if the container that firstly created them is not running\nanymore)\nContainer FOO \"can\" just create the namespaces needed to be \"shared\". Container\nBAR or any other just use FOO's namespaces as needed. Better, as `--parent-group`\noption, a container can join whatever namespace available.\nWouldn't be nice if `--net=container:foo` would not require foo running? Rather,\ndocker can create foo's network namespace and allow other containers to join it.\nWhen foo later starts, it would just join that network namespace. Also, when foo\nexits and starts again, it should join the original namespace if other container\nstill use it rather than creating a new one again.\n\nWe just got `--parent-cgroup`, which side-steps the \"model everything as a\ncontainer\". If we had `--pid=ns:name`, `--net=ns:name`, `--ipc=ns:name` we could\ndo \"pinned\" namespace management outside of docker too.\n## QUESTIONS AND POSSIBLE SOLUTIONS (? NEED FEEDBACKS)\n\nGiven agreements on the above:\n## 1) UX\n\n@thockin gave some hints on how namespaces could be pinned from a container with\nthe help of a new flag called `--pin-namespaces`:\n\nThis would pin all container's namespaces:\n\n```\n$ docker run -d --pin-namespaces dockerfile/noop\n123456789abcdef\n\n\n# this would normally throw an error if the first container exited!!!\n$ docker run --net=container:123456789abcdef my/realjob\n8675309deadbeef\n```\n\nThis way, the first command will just pin its namespaces without\nrunning (I'll explain later what I mean by 'without running') and the\nsecond container will join first one's net namespace.\n\nAlso, what about specifying which namespace to save:\n\n```\n$ docker run -d --pin-namespaces=net dockerfile/noop\n123456789abcdef\n\n$ docker run --net=container:123456789abcdef my/realjob\n8675309deadbeef\n```\n\nSame as above, but only net namespace is pinned for later use.\n## 2) how are namespaces pinned from containers?\n\nPinning container's namespaces requires firstly to have a process running\nso it can `setns`ing the namespaces it need (or being `clone`ed or `unshare`ed).\nNow `/proc/{pid}/ns` is populated (initialized). How to pin them and let the\ncontainer exit without erasing its namespaces?\n- file descriptors to its namespaces should be held before exiting. This will\n  preserve them\n- bind mounting its namespaces somewhere\n\nFirst option is a little tricky to me and require holding a pointer indefinitively\n(how long? where? what if daemon is restarted?).\n\nSecond one just requires starting a container just to have its `/prod/{pid}/ns`\npopulated and mount bind each file somewhere like `/var/lib/docker/{container}/ns`.\nHowever I'm not sure if it's hacky to just run a container to have its namespaces.\nHow then? just running it with something like `# true` or `# sleep 100`?\n\n```\n# docker run -d busybox sleep 100\n  0c9c7bee7196845688d8de6a81f49201f3185263a3ca54dcea30b33ee844e83c\n# docker inspect --format=\"{{ .State.Pid }}\" 0c9c7bee7196\n  24024\n# mkdir -p /var/lib/docker/ns/0c9c7bee7196\n# touch /var/lib/docker/ns/0c9c7bee7196/net\n# mount --bind /proc/24283/ns/net /var/lib/docker/ns/0c9c7bee7196/net\n# docker stop 0c9c7bee7196845688d8de6a81f49201f3185263a3ca54dcea30b33ee844e83c\n# cd /var/lib/docker/ns/0c9c7bee7196 && nsenter --net=net\n# echo $$\n  24587\n# #inspecting stat of /proc/24587/ns/net show it's using ns from the first stopped container\n```\n\nEither case require working on libcontainer too (to my knowledge)\n## 3) how are pinned namespaces garbage collected? Do they?\n\nWhat if the first container is started?\n\nI think it will just look if he has his namespaces pinned somewhere and STILL used,\notherwise it will just create new ones and join them. (This may conflict)\n\nWhat if the first container is never started and the second one exits?\n\nPinned namespaces from the first container should be garbage collected (? how, do they?)\n## Final thoughts\n\nThis, as said before, would open the possibility to even go further and let\ncontainers use whatever namespace they want (`--net:ns:somensfromnotdockertoo`)\nas --parent-cgroup is doing now.\n## PLEASE COMMENT BACK AND WRITE DOWN YOUR FEEDBACK!\n\nThis is far from a perfect proposal but I'll try to make it better and keep working on it\n"},{"labels":["enhancement1"],"text":"docker build seems to create a internal container and 'runs' things in them.\nsadly there is no way to adjust the container settings to use a custom DNS so in my case it can't resolve an internal hostname to do a git pull from.\n\nany chance of adding this as an option to the 'build' side?\n\nThanks!\n"},{"labels":[null,"enhancement1"],"text":"I have some container whose prefix  are the same, so I want use docker inspect to get the data config once time. I think this feature are usefull.\nit like below steps\n1. `docker ps -a` \n\n```\nCONTAINER ID        IMAGE                 COMMAND                CREATED             STATUS              PORTS               NAMES\n5112fc65dff7        39225369ceae:latest   \"/usr/bin/supervisor   2 hours ago         Up 2 hours                              test12345           \n8d84cb3b47bb        39225369ceae:latest   \"/usr/bin/supervisor   2 hours ago         Up 2 hours                              test123456          \n87085a48a399        39225369ceae:latest   \"/usr/bin/supervisor   2 hours ago         Up 2 hours                              test3455            \n640f966a6490        39225369ceae:latest   \"/usr/bin/supervisor   2 hours ago         Up 2 hours                              mad_engelbart       \n19faa0f4f258        39225369ceae:latest   \"/usr/bin/supervisor   2 hours ago         Up 2 hours                              sharp_ritchie       \ne09e9e5174da        39225369ceae:latest   \"/usr/bin/supervisor   6 hours ago         Up 6 hours                              test123  \n```\n1. `docker inspect test`.\n   this will return all data that has prefix \"test\" of the  container name.now it return Error: No such image or container: test\n   []\n\nthanks\n"},{"labels":[null,"enhancement1"],"text":"   Now docker load cmd load a tar archive on STDIN, that's must copy the file to the node where the docker running. If the docker load can load file from a URL,then we can avoid copy the tar archive to the node where docker running.It't would look like:\n   docker load -i http://$ip/test.tar\n"},{"labels":[null,null,"enhancement1"],"text":"My idea is to implement a graphdriver based on qcow2 files.  Every layer of the image is a qcow2 file. \nUse qemu-nbd or other tools to make the qcow2 files looks like a block device, and then used them for the docker container.\n\nHow  do you think about this idea? Any advice is welcome.\n##### About qcow2:\n\nThe QCOW image format is one of the disk image formats supported by the QEMU processor emulator. It is a representation of a fixed size block device in a file.Benefits it offers over using raw dump representation include:\n- Smaller file size, even on filesystems which don't support holes (i.e. sparse files)\n- Copy-on-write support, where the image only represents changes made to an underlying disk image\n- Snapshot support, where the image can contain multiple snapshots of the images history\n- Optional zlib based compression\n- Optional AES encryption  \n\nhttps://people.gnome.org/~markmc/qcow-image-format.html\n"},{"labels":[null,"enhancement1"],"text":"If the `/etc/sysconfig/docker` file is managed by a configuration management system then it does not allow individual configuration of options. Ideally we could set some default options that a user can append their own options to.\n\n```\nOPTIONS=--selinux-enabled\n```\n\nand then another user can extend those options with configuration in the docker.d folder\n\n```\nOPTIONS=-H fd://127.0.0.1\n```\n\nThe options can then be combined at runtime without needing additional config management changes to the system. The daemon in the above example would be started with `docker -d --selinux-enabled -H fd://127.0.0.1`\n"},{"labels":[null,"enhancement1"],"text":"This is an extension of issue #11815.\n### Use Case\n\nAs an enterprise administrator, I'd like be able to tell Docker daemon to block particular registries. Public registries in particular. Since our company builds mostly images for private use, I'd like to prevent our developers from accidentally pushing images to public registries.\n### Requirements\n- Docker daemon recognizes a list of registries blocked for pushes.\n- Docker daemon can be given a list of registries blocked for pulls.\n- Docker daemon actually blocks access to specified registries.\n- User is informed that his operation failed due to registry being blocked.\n### Proposal\n\nUsing a configuration schema from #11815. Let's extend it with a new registry property:\n- `policy` with possible values:\n  1. one of following strings:  `allow`, `ask`, `block`\n  2. a map for fine-grained specicication with possible keyes: `push`, `pull`,\n     `search` having the same choices (`allow`, `ask`, `block`)\n\nDefault value would be `allow`.\n\n`ask` would make Docker client ask the user for a confirmation before an execution. I don't find it necessery to ask before a pull or search it should be recognized only by a push operation. In other cases it would map to `allow`. Affected commands (push) shall receive additional flag `-f --force` that would suppress the confirmation.\n#### Examples\n\nAsk for a confirmation when pushing to public registry, allow everything else:\n\n```\npublic:\n    policy:\n        push: ask\n\nredhat:\n    index: registry.access.stage.redhat.com\n```\n\nSame as above:\n\n```\npublic:\n    policy: ask\n\nredhat:\n    index: registry.access.stage.redhat.com\n```\n\nBlock public registry completely:\n\n```\npublic:\n    policy: block\n\nredhat:\n    index: registry.access.stage.redhat.com\n```\n\nBlock all not configured registries (but allow for searching):\n\n```\ndefaults:\n    policy:\n        push: block\n        pull: block\n\nprivate:\n    policy: allow\n    index: my.private.registry.ltd\n\nredhat:\n    policy: allow\n    index: registry.access.stage.redhat.com\n```\n\nTry to push to a public registry with `ask` policy:\n\n```\n$ docker push docker.io/user/app\nDo you really want to push to docker.io registry? [Y/n]: Y\n...\n```\n"},{"labels":[null,"enhancement1"],"text":"This is based on issue #11815. And together with _pull_ (issue #11816) and _push_ proposals (issue #11817) supersedes #8329.\n### Use case\n\nAs a regular Docker user working with several private and corporate registries, I'd like to be able to search them all at once. Docker daemon shall accept a list of registries that shall be queried during searches.\n\nSo when I issue:\n\n```\n$ docker search ubuntu\n```\n\nit'd show results gathered from all the registries given. I'd also expect Docker to print source index name for each repository found e.g.:\n\n```\nNAME                                DESCRIPTION                                     STAR\ndocker.io/ubuntu                    Official Ubuntu base image                      1227\ndocker.io/dockerfile/ubuntu         Trusted automated Ubuntu (http://www.ubunt...   40\ndocker.io/tutum/ubuntu              Ubuntu image with SSH access. For the root...   14\ndocker.io/tleyden5iwx/ubuntu-cuda   Ubuntu 14.04 with CUDA drivers pre-installed    1\ndocker.io/partlab/ubuntu            Simple Ubuntu docker images.                    0\nregistry.my.org/xavier/ubuntu                                                       2\nregistry.my.org/custom/ubuntu                                                       0\nregistry.my.org/my/ubuntu                                                           0\n```\n\nTo aid legibility, Docker should sort the results primarily by index name.\n### Requirements\n- Docker daemon accepts a list of registries used for searching.\n- Docker daemon queries all registries given.\n- Docker doesn't budge when one or more registries is inaccessible.\n- Output of search command shows source index name.\n- Results are sorted by index name.\n### Proposal\n\nThis is based on configuration schema outlined in #11815. It uses the same default list of registries as a pull proposal (issue #11816). Here they are being searched \n\n```\ngeneral:\n    # Default registries used during search.\n    default_registries:\n        - redhat\n        - public\n```\n\nIn this case the order doesn't matter. They could be searched in parallel.\n"},{"labels":[null,"enhancement1"],"text":"Partially superseds issue #8329. It will be superseded completely with upcoming, consequent proposals based on this.\n\nTogether with extensions it obsoletes PR #10411 but is still compatible with it.\n\nThis is a root proposal for issues concerning usage and configuration of additional registries. It outlines:\n\n```\n1. configuration file paths\n2. their format\n3. their structure\n```\n### Use case\n\nAs an enterprise user interacting with multiple private registries and having little or no interest in public one, I'd like to be able to pull from them without the need to specify them on command line. I'd like to specify which registries can be pushed to, which registries can be pulled from. I'd like to be able to prevent our developers from interacting with public registry or at least let Docker ask them for confirmation before a push.\n\nIn order to do this, Docker has to provide some way of configuration which would be extensible, legible and easy to edit.\n### Requirements\n- Allow user to configure a list of additional registries specifying at least:\n  1. index name\n  2. index aliases\n  3. associated mirrors\n### Proposal\n\nMake Docker daemon read registry configuration file e.g. `/etc/docker/registries.yml` in YAML format.\n#### Example\n\n```\n# Special object grouping auxiliary settings concerning registry configuration.\ngeneral:\n    # Optional directory relative to this config file with additional\n    # registries. Defaults to empty string.\n    load_dir: /etc/docker/registries.d\n\n# Special object with default properties for all registries (even those not\n# configured).\ndefaults:\n\npublic:\n    index: docker.io                # this is redundant\n    aliases: [index.docker.io]      # this as well\n    mirrors: [127.0.0.1:5000, docker-mirror.domain.ltd]\n\n# Each following entry represents additional registry if it has an `index`.\nredhat:\n    index: registry.access.redhat.com\n    mirrors: [127.0.0.1:5001]\n```\n\n`public` registry would stay hardcoded in docker for cases where registry configuration is absent or `public` entry is missing.\n\nThe same applies to default values. Right now there aren't any properties we could put there - they'll be specified in extension proposals.\n### Proposed Extensions\n- #11816 pull from default registries\n- #11817 push to default registry\n- #11818 search default registries\n- #11819 block registries\n- #11820 authentication support\n"},{"labels":[null,null,"enhancement1",null],"text":"If you have two containers both with a restart policy of always and they share a network stack, the docker daemon may not start them in the correct order.\n\nFor example, if you have two containers started like so\n\n```\ndocker run -d --name a --restart always some_image\ndocker run -d --name b --restart always --net=container:a some_other_image\n```\n\nThen you restart the docker daemon, it may try and restart container b before container a, causing container b to fail to start up and this message to show up in the daemon logs\n\n`[debug] daemon.go:384 Failed to start container id_of_b: cannot join network of a non running container: a`\n\nI would expect docker to boot up container a before container b, much like it boots up linked containers in the correct order.\n\nThis is similar to https://github.com/docker/docker/issues/10462\n\n```\ndocker version\nClient version: 1.3.2\nClient API version: 1.15\nGo version (client): go1.3.3\nGit commit (client): 39fa2fa/1.3.2\nOS/Arch (client): linux/amd64\nServer version: 1.3.2\nServer API version: 1.15\nGo version (server): go1.3.3\nGit commit (server): 39fa2fa/1.3.2\n```\n\n```\ndocker info\nContainers: 3\nImages: 233\nStorage Driver: devicemapper\n Pool Name: docker-253:2-131678-pool\n Pool Blocksize: 65.54 kB\n Data file: /var/lib/docker/devicemapper/devicemapper/data\n Metadata file: /var/lib/docker/devicemapper/devicemapper/metadata\n Data Space Used: 11.31 GB\n Data Space Total: 107.4 GB\n Metadata Space Used: 17.21 MB\n Metadata Space Total: 2.147 GB\n Library Version: 1.02.89-RHEL6 (2014-09-01)\nExecution Driver: native-0.2\nKernel Version: 2.6.32-504.3.3.el6.x86_64\nOperating System: <unknown>\nDebug mode (server): true\nDebug mode (client): false\nFds: 16\nGoroutines: 13\nEventsListeners: 0\nInit SHA1: da6572a9f895dbbc52c7a03d3d391e45e56dc021\nInit Path: /usr/libexec/docker/dockerinit\nUsername: appfoliodocker\nRegistry: [https://index.docker.io/v1/]\n```\n\nEnvironment details (AWS, VirtualBox, physical, etc.):\nCentOS 6.6 box\n\nHow reproducible:\nIt seems that a given configuration of containers will reliably fail. However it seems adding or removing other containers can change the load order and may produce different results.\n"},{"labels":["enhancement1"],"text":"There needs to be a cleaner way to get docker stats for all running containers from the cli. \n\n```\ndocker stats `docker ps | awk '{print $NF}' | grep -v NAMES`\n```\n\nis just way too much to type. \n\nsimplest is to just have `docker stats` provide stats for all running containers instead of usage\n"},{"labels":["enhancement1"],"text":"Is there any way we can bring Docker on AIX datacenter? \n\nOur team are trying to start a transformation inside a traditional \"big\" company, but it seems AIX is an obstacle we should pass!\n"},{"labels":[null,"enhancement1"],"text":"I posted [this as a comment](https://github.com/docker/swarm/issues/475#issuecomment-85102816) on a Swarm issue, but @bacongobbler suggested this should be posted as a feature request to the main docker app.  (Which I think would work better as a cluster-wide metric with Swarm)\n\nI think it would be nice to specify an app's utilization in MIPS. Lets say you benchmark an app, and at 5 concurrent requests it uses 500MB of memory, and utilizes 50% of a CPU running at 8500 BogoMIPS. \n\nIn production, if I wanted my application to scale up to 50 concurrent users, I'd fire off 5 containers, each with a limit of 1GB of memory and 8500 MIPS. (With some added wiggle room) I would know that my total application is worth 5GB of memory, and 42,500 MIPS. I could forecast and purchase hardware a lot easier that way too.\n\nIf you have a mixture of different speeds of machines, it's easier to distribute shares across a cluster. My app's requirement for 8500 MIPS might just be 1 CPU on one machine, or 2 CPUs on older hardware.\n\nSave the arguments over bogomips for later though.  Maybe this could be FLOPS, or Hz, or... whatever.  If you could override the host's total available bogomips setting on the docker daemon command line, then it doesn't matter what the unit is.  Web ops has control over the ceiling for each host, and dev ops has control over requesting CPU utilization with a properly weighted metric.  \n\nWhatever the chosen unit is, the main point is that it's a consistent value throughout the lifecycle of the container.  A static 0 - 1024 in cgroups isn't very helpful on the grand scheme of clustering.\n\nThanks for considering this!\n"},{"labels":[null,"enhancement1"],"text":"Setting `net.ipv4.ip_forward=1` is troublesome for various reasons, including security.\n\nWould it be possible to stop using that and instead use something like `iptables` rule below instead?\n\nExample for container started with `--port 0.0.0.0:29418:29418`\n\n```\niptables -I PREROUTING 1 -t nat -i eth0 -p tcp --dport 29418 -j REDIRECT --to-port 29418\n```\n\nThis should work, though translating `0.0.0.0` to `eth0` is a potential problem.  I'm not an `iptables` wizard, so there is certainly a better way to do this.\n"},{"labels":["enhancement1"],"text":"### Problem\n\nSometimes (or frequently) users have trouble talking to the docker engine with their docker client because they accidentally update their docker client and the server now has an older version. \n\nIt's too hard to get this right and that this usually happens by one off (e.g. client:1.18 server:1.17) and there is not major differences between these versions. This especially happens to the people who are compiling docker from `master` and talking to a Linux daemon in boot2docker or some cloud VM.\n\nThis happens to me a few times a day as I'm finding myself upgrading to latest docker client version and all my servers are old or CoreOS is totally lagging two versions behind or something.\n### Proposal\n\nI propose a power user environment variable `DOCKER_APIVERSION` used in the client to override the constant in [api/common.go](https://github.com/docker/docker/blob/068d343ddfad23ca2f7b407f5be03803029ac311/api/common.go#L18) when set:\n\n```\n$ ./docker info\nFATA[0000] Error: client and server don't have same version (client : 1.18, server: 1.17)\n\n$ DOCKER_APIVERSION=1.17 ./docker info\nContainers: 0\nImages: 0\n[...]\n```\n\nThis can easily help people to try out latest client binaries downloaded from [master.dockerproject.com](https://master.dockerproject.com) which has the latest bug fixes against their existing docker host deployed somewhere on the cloud.\n### Considerations\n\nSince this enables the way of undefined behavior and feature set mismatch between the client and the engine this must be thoroughly documented that it is dangerous area. (and probably should exist only in development environment documentation instead of user guide).\n\nUsers will be seeing feature mismatches most of the time in the cutting-edge new features added to cli but missing from the docker host.\n"},{"labels":[null,null,"enhancement1"],"text":"Kernel has several parameters controlling filtering of bridge traffic (viz [sysctl entries](https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/Documentation/networking/ip-sysctl.txt?id=v3.16#n1438)):\n- `net.bridge.bridge-nf-call-arptables` - pass bridged ARP traffic to arptables' FORWARD chain.\n- `net.bridge.bridge-nf-call-ip6tables` - pass bridged IPv4 traffic to iptables' chains.\n- `net.bridge.bridge-nf-call-iptables` - pass bridged IPv6 traffic to ip6tables' chains.\n- `net.bridge.bridge-nf-filter-vlan-tagged` - pass bridged vlan-tagged ARP/IP traffic to arptables/iptables.\n- `net.bridge.bridge-nf-filter-pppoe-tagged` - pass bridged pppoe-tagged IP/IPv6 traffic to {ip,ip6}tables.\n\nTheir default value on Fedora/RHEL systems is 0 (since [bug #512206](https://bugzilla.redhat.com/show_bug.cgi?id=512206)). Therefore, by default, iptables' rules do not affect bridge's traffic. Thus docker images are allowed to communicate regardless of `--icc=false` option by default on these distributions, which is quite surprising behavior.\n\nDocker should check these parameters and refuse to start if inter-container communication is prohibited with `--icc` option and bridge traffic is not filtered by iptables' chains. It should be mentioned in documentation as well.\n\nI assume that checking `net.bridge.bridge-nf-call-iptables != 0 && net.bridge.bridge-nf-call-ip6tables != 0` will be enough. I'm not sure about `net.bridge.bridge-nf-call-arptables` though.\n\nI'll post PR concerning this issue later today. \n"},{"labels":["enhancement1"],"text":"docker stats could be enhanced with a sort on any column, e.g. hit 2 -> sort by cpu usage, 5 -> sort by network I/O downloading, 6 -> sort by network I/O uoloading\n"},{"labels":[null,"enhancement1"],"text":"If, for example, I have two web app containers each exposing a volume like `/var/www/static`\n\n...I can't currently do `--volumes-from` these on my web server container as the paths conflict\n\nIt needs a way to alias the imported volumes, like how it's possible to specify `HOST_PATH:CONTAINER_PATH` when defining a volume\n\none might also want to import only a specific volume, rather than all volumes\n\nI realise syntax for this is going to be a bit tricky\n\nIt seems only solution currently for my particular case is to give each app container a unique volume path?\n"},{"labels":[null,"enhancement1"],"text":"In #1143, @altaurog proposed a `--link-host` option, which I'd like to resurrect for consideration, to link to host ports, with port number rewiring. At the moment, Ambassador provides this ability, but requires forwarding through a proxy (socat). Here's an example concept for --link-host:\n\n```\ndocker run --link-host hostip:hostport:aliasport[:hostport:aliasport:...]:alias\n# where hostip is any routable IP from the host, or 127.0.0.1 for localhost\n```\n\nThere is an alternative practice of using `--add-host=alias:$(hostip)`, but this practice doesn't address the port rewiring that could be required.\n\nThanks for your consideration.\n"},{"labels":[null,null,"enhancement1"],"text":"Hello,\n\nActually there is 2 way for creating images with Docker:\n1. The popular Dockerfile\n2. Creating a new container, make some changes and commit this container to an Image\n\nFor various reasons, I really don't like using a Dockerfile (this is not the topic of why, but i can comment more on this subject if you want) and I prefer to use the commit method.\n\nHowever, using this workflow for long term and in a team has serious downsides: the history of an image is too light (see #9785 for example) and there is no possibility to have a diff between 2 images (only 2 containers atm).\n\nHere are a bunch of propositions in order to improve this workflow:\n## Commit message should be mandatory\n\nOn a new commit you will have to set a message of what you have changed, which can be empty. Having it mandatory will force people to set something, and we can hope that it will be something useful. (Do you imagine the crap in a repository if git commit would not require a message ?)\n## Add a mandatory author information for a commit\n\nA commit message specifies the what and/or how of the change. For collaboration we also need the who (mainly to make the author wear a dunce cap when his commit is stupid), for that purpose a commit, in docker, should require a name and an email of the commit author.\n\nTo not have too much information to enter at each commit, the docker client can use environnements variables to set the default author name and email.\n## Add information to history output\n\nA line, in docker history, should return the message, author name and author email of commit. (see also #9785)\n## Diff between 2 images\n\nHistory is great but not enough, when reviewing a commit, we also need to see which and how these files have been changed. For a start this diff can exactly be like a diff between 2 containers but it can also be improved:\n### Binary (actual) diff\n\nA diff between 2 binary files is not possible, so only the information about the status of the file (added/modified/deleted) should be show (like the current diff on containers).\n### Textual diff\n\nWhen the file is a text (like a configuration), it would be nice if the diff could show more information than the binary one, which lines has changed, before and after content, ... like a git diff\n## Docker hub and repository integration\n\nWith all these information Docker Hub or any other image storage service would be able to show better insight of an image and gain trust from users using images created without Dockerfile. \n"},{"labels":["enhancement1"],"text":"## Proposal: Operational Intent - Drivers API\n\nAuthors: @jainvipin, @mapuri, @tgraf, and @dvorkinista\n\nThis document recommends the APIs used to implement the goal described in Operational Intent for Docker Deployment (#11187). The proposal is a work in progress and needs community input.\n### Definitions\n- Driver: An entity that exposes the infrastructure capability and takes the scheduler and/or operational intent distribution to realize the operational intent.\n- Capability: A smallest unit of a unique hardware or software feature that an application can leverage. The capabilities are exposed by the backend drivers e.g. network or storage extensions. Capabilities are also associated with the containers to allow the best match-making of needs vs. availability for a given operational intent \n\nThe driver API proposed is expected to run on each node that is candidate for resource scheduling.\n### Capability Structure\n\n```\ntype Caps struct {\n    Name        string\n    Type        interface{}\n    Value        interface{}\n    GroupId    string\n    OptionalData    string\n}\n```\n\nThe above structure can generically specify a variety of capabilities, that could then be acted upon by rest of the system generically. \n- Name is an string (arbitrary) that uniquely identifies a capability. It must be unique for a type of driver e.g. network driver, storage driver, or compute. It is also assumed that all drivers emit the capability uniformly. Please see the validation of the capabilities below.\n- Type and Value are defined to be an generic interface, because it can be one of the following:\n  1. Qualitative: These capabilities are supported or not supported by the drivers. i.e. the type is ‘bool’. For example a host can exhibit “vxlan-offload” capability, or “aes’ encryption capability, etc.\n  2. Quantitative: These capabilities represent a value associated with the capability i.e. the type is ‘int’. For example a driver can expose network bandwidth of 10Gbps. The normalization of the values is done by drivers and validating the capabilities against the set described for a given deployment. It is the scheduler that will allow the allocation of quantitative resources in the system.\n  3. Buckets: These capabilities represent a set of possible string values that a driver can support. For example a driver can expose a capability called “Priority” that can be “low”, “medium”, or “high”. Usually an application is assign one of the buckets.\n- GroupId: arbitrary association of resource grouping. This could help share some of the resources between hosts because they belong to a specific group. For example, two hosts can share an external storage and expose the capability accordingly. An empty GroupId is valid in case an anticipated grouping is not desired.\n- OptionalData: This is an optional metadata that driver wants to expose, not consumed by the scheduler, however can be used in variety of ways by scheduler, and other entities, that is outside the scope of this document.\n\nFor example, a network driver can advertise following capabilities:\n\n```\n        capsToPublish := []Caps{\n        {\n                Name: \"vxlan-offload\",\n                Type: \"qualitative\",\n                Value: \"supported\",\n                GroupId: \"\",\n                OptionalData: \"\",\n        },\n        {\n                Name: \"bandwidth\",\n                Type: \"quantitative\",\n                Value: \"10G\",\n                GroupId: \"\",\n                OptionalData: \"\",\n        },\n        {\n                Name: \"qos\",\n                Type: \"set\",\n                Value: []string{\"low\", \"medium\", \"high\"},\n                GroupId: \"\",\n                OptionalData: \"\",\n        },\n        {\n                Name: \"encryption\",\n                Type: \"qualitative\",\n                Value: \"not-supported\",\n                GroupId: \"\",\n                OptionalData: \"\",\n        }}\n```\n### Driver APIs\n\nThe proposed changes optionally extend the APIs defined in extensions drivers. For example [network drivers](#9983) can be extended as follows.\n\n```\n// The APIs below calls for extending driver APIs to allow for\n// capabilities to be discovered and propagated\n\ntype Driver interface {\n\n  //  Init - return various capabilities during driver initialization\n  Init() ([]Caps, error)\n\n  // Update - this event is triggered seldom, but possible if the host discovers a\n  // resource availability or .its unavailability after Init was called. It however requires\n  // a host level event to know that this API needs to be called to fetch newer capabilities\n  FetchCapabilities() ([]Caps, error)\n\n  // AddNetwork and DeleteNetwork to take in []Caps, which can be passed in \n  // either as ‘params’ in proposed APIs or a separate argument. This is TBD\n  // The driver is expected to allocate the resources for the network and its endpoints\n  // that were indiciated in the arguments by []Caps\n  AddNetwork(netid string, params []string) error\n  // OR\n  AddNetwork(netid string, params []string, []Caps) error\n\n}\n```\n\nSimilar applicable changes can be assumed for storage and compute resource advertisement.\n### Capability Validation\n\nFor quantitative and buckets type of capabilities the validation requires ensuring that the values fall within the allowed range (for a specific deployment). Ensuring consistency for drivers to publish the information is critical, but can be done with a consistent set of driver running within a cluster.\n\nA validation schema can be thought of as an array of ‘network’, ‘storage’ and ‘compute’ capabilities that are a superset of the allowed/permitted capabilities.\n### Scheduler use of capabilities\n\nDepending on the type of capability, scheduler could make respective decisions without knowing the details of the capability:\n- Qualitative Capability: it can help in yes/no type of decision for job placement, etc.\n- Quantitative Capability: it can help keeping track of the capabilities and deducting them against the available pool for a given GroupId. The allocated value is communicated back to the driver to consume for a specific container that was allocated the resource\n- Buckets Capability: Use it as a policy criteria to map the desired policy to available policies for a given capability\n\nA validation schema can be thought off as an array of ‘network’, ‘storage’ and ‘compute’ capabilities that are a super-set of the allowed/permitted capabilities.\n### Items to be discussed\n- Type of Capability: Qualitative, Quantitative, and Buckets may not represent all abstractions of capabilities and extending capabilities into arrays/sets and/or Maps may be needed for certain situations. Exploring specific use cases would help clarify how scheduler and rest of the system can consume them.\n- Flatten out the Capability information: Most generically, structured data is not as friendly as opaque data, therefore it calls for flattening the capabilities structure. However doing this may jeopardize the scheduler function without knowing and parsing the details of capabilities. Note that transporting capabilities, when marshalled, is not structured and the arguably APIs can use marshalled data instead of using Caps structure. However, the structure would need to be understood by the scheduler.\n- Explore storage-driver API functions as community discussions around that settles\n- Explore if there is a generic way to feed in compute capabilities, which doesn’t have a extension drivers but nonetheless require generic representation. And so, who does it and how swarm backends consume them in a flexible way. This needs to be discussed.\n"},{"labels":[null,"enhancement1"],"text":"When we pull down a select group of docker images and fire up a cluster of containers based on them, on our project we tend to refer to them by their name:tag. When we choose to dynamically clean up the pulled images, we have to do some bash magic to determine the Image ID of a given name:tag before calling `docker rmi`. But it would be really nice to be able to call `docker rmi --deep [name:tag]` and docker would remove not just the local tag, but all tag references and the image. Currently if you are removing using name:tag, you have to delete each tag, and the last one deletes the actual image (understandable default behavior).\n\nThis would be handy for CI/CD workflows that run integration/system tests on dynamically produced and tagged docker image clusters.\n\nJust food for thought...\n"},{"labels":[null,"enhancement1"],"text":"We have vast IPv6 subnets, why not utilize them and randomize IPv6 addresses in containers?\n\nCurrently a static MAC address is incremented and SLAAC is used.\nI'd expect `net.ipv6.conf.*.use_tempaddr=1` to have the desired effect on docker containers.\n\nRandomizing the MAC address could yield similar results and it wouldn't have to be a system wide option. It would also be platform independent. Thoughts?\n"},{"labels":["enhancement1"],"text":"[Maven build tool](http://maven.apache.org/) allows users to see the flattened version of the project definition file (i.e. `pom.xml`) via the command: `mvn help:effective-pom`. This can be really be helpful to visualize default and inherited configuration in a single place.\n\nThat said, I could not find any equivalent for Dockerfile.\n\nThis would be useful to check all layers etc in a single place as well, rather than checking every single pulled image.\n"},{"labels":[null,"enhancement1",null],"text":"Http/https proxy is set through environment at\ndocker daemon starts, and can't be modified. \n\nIn current statuation, i need set special proxy for pull/push \naction. so i hope i can set proxy server through cli pull/push\ncommand.\n"},{"labels":["enhancement1"],"text":"# Docker Storage API - Proposal\n## Motivation\n1. Data volumes cannot be described fully by docker/containers.\n2. Image storage for containers is not configurable.\n3. There is currently no infrastructure to support distributed\n     storage volumes from within docker.\n4. To manage life-cycle of data, image and metadata volumes\n5. Ability to manage shared volumes across a cluster of docker hosts\n## Volumes as a first class object in Docker.\n\n   We achieve these goals by promoting Volumes and Storage as\n   first class entities in docker on the same level as containers.\n\n   This would also allow containers to describe the volumes they must be\n   attached and store the container configuration in a way that permits the\n   creation of Highly available containers.\n\n   Another feature would be the ability to discover capabilities of various\n   storage backends and automatically provision and configure storage volume\n   based on QoS specs for containers.\n## Use Case Example.\n\n   Lets take the example of a common pattern for multi-tiered database application.\n\n   Such an application may consist of one or more database servers, a number of\n   application processes, HTTP servers, caching servers, load balancers, management, etc.\n\n   The deployment of such an application imposes different levels of performance,\n   availability and reliability requirements. It is useful to be able to specify\n   what each volume and each container type needs, and for the available storage\n   to be matched and provisioned automatically.\n\n   Thus user facing terms like 'high', 'low' can be translated by docker and/or volume \n   drivers to concrete metrics such as IOPS, etc.\n\n   Examples of such containers and their storage needs:\n\n```\n        :database-master:\n          build: .\n          ports:\n           - \"8000:8000\"\n          volumes:\n            name: DataVolume\n            size: 10G\n            type: block\n            path: /dev/sdb\n            qos-spec:\n                availability: high\n                performance: high\n\n        :database-replica:\n          build: .\n          ports:\n           - \"3307:3307\"\n          volumes:\n            name: DataVolumeReplica1\n            size: 10G\n            type: block\n            path: /dev/sdb\n            qos-spec:\n                availability: medium\n                performance: high\n\n        :caching-server:\n          build: .\n          ports:\n           - \"3307:3307\"\n          volumes:\n            name: VarnishCache\n            size: 2G\n            type: fs\n            path: /var/lib/varnish-cache\n            qos-spec:\n                availability: low\n                performance: medium\n\n        :application-worker:\n          build: .\n          ports:\n           - \"3307:3307\"\n          volumes:\n            name: ScratchVolume1\n            size: 1G\n            type: fs\n            path: /var/lib/app-scratch\n            qos-spec:\n                availability: low\n                performance: low\n\n        :load-balancer:\n          build: .\n          ports:\n           - \"3307:3307\"\n          volumes:\n            name: LoggingVolume1\n            size: 1G\n            type: fs\n            path: /var/lib/balancer\n            read-only: true\n            qos-spec:\n                availability: low\n                performance: low\n```\n## Design Overview.\n\nWe create a new type of driver interface called the 'storagedriver'.\n\nJust like the existing 'graphdriver' and 'execdriver' interfaces,\nthis will allow the existence of multiple implementations of\nthe interface - referred to below as 'drivers'.\n\nThe central new entity introduced by this interface is called the\n'Volume'. Each driver allows the creation, manipulation and destruction\nof these volumes.\n\n'Volumes' are meant to supersede and formalize the existing concept of\ndata volumes. They can also be used to represent individual layers in a\ncontainer. Thus any snapshot capability offered by the storage can be used\nto implement the layering of filesystems needs for docker to work.\n\nWe also create definitions of service levels and performance criteria\nthat can serve as input to the storage drivers to manage their resources.\n## Interface/API definitions.\n\n```\n    package driver\n\n    // Custom metadata for volumes.\n    type VolumeTags map[string]string\n\n    const (\n        METADATA = iota\n        DATA     = iota\n    )\n\n    type storageDriverInfo struct {\n        driver *Driver\n    }\n\n    // List of available storage drivers.\n    var storageDrivers []storageDriverInfo\n\n    func Register(d *Driver) (err error) {\n        return nil\n    }\n\n    type PolicySpec struct {\n        AvailabilityZone []string // List of availablity zones to replicate to.\n                                  // Also specifies no. of copies.\n        MinIOPS          uint64   // Min IOPS expected.\n        MaxIOPS          uint64   // Max IOPS allowed.\n        MinLatency       uint64   // Min Latency allowed.\n        MaxLatency       uint64   // Max Latency expected.\n        Burst            uint64   // Number to denote 'burstiness'\n        MinThroughput    uint64   // Guaranteed throughput\n        MaxThroughput    uint64   // Max throughput allowed.\n        MinResv          uint64   // Minimum bytes to reserve.\n        MaxResv          uint64   // Max bytes reserved. Same as capacity/size of the volume\n                                  // if Min = 0, implies thin provisioning\n                                  // if Min == Max, implies thick provisioning\n        Dedup            bool     // Dedup enabled.\n        Compression      string   // Compression type, if enabled.\n        Encryption       string   // Encryption type, if enabled.\n        RPO              uint64   // Recovery Point Objective - represented by date/time\n        RTO              uint64   // Recovery Time Objective - represented by date/time\n    }\n\n\n    // Interface to be implemented by each of the drivers.\n    type Driver interface {\n        // Create a Volume given a name, a policy specification, and optionally a\n        // source snapshot to fork from\n        // tags is a set of arbitrary key value pairs\n        // to associate meta-data.\n        Create(name string, snapshot VolumeID, spec VolumePolicySpec, tags VolumeTags) (v *VolumeID, err error)\n\n        // Get the runtime structure representing a volume.\n        Get(v *VolumeID) (vol *Volume, err error)\n\n        // Update the PolicySpec and/or other parameters\n        Update(v *VolumeID, spec VolumePolicySpec) (err error)\n\n        // Delete this volume\n        // lazy implies that delete will be\n        // kept pending until last mount is detached,\n        // instead of failing if its still in use.\n        Delete(v *VolumeID, bool lazy) (err error)\n\n        // Clone a volume from another volume or snapshot.\n        // This creates a writeable copy of this volume.\n        Clone(v *VolumeID, name string, spec VolumePolicySpec, tags VolumeTags) (v *VolumeID, err error)\n\n        // Create a read-only point in time snapshot.\n        Snapshot(v *VolumeID, name string, spec VolumePolicySpec, tags VolumeTags) (v *VolumeID, err error)\n\n        // Get a list of all child snapshots or clones created\n        // from this volume.\n        EnumerateSnapshots(v *VolumeID) (snapshots *[]VolumeID, err error)\n\n        // Stream snapshot diff between two volumes/snapshots to a third Volume.\n        SnapshotDiff(v *VolumeID, other *VolumeID, dest *VolumeID) (err error)\n\n        // Restore the contents of this volume from a previous snapshot.\n        Revert(v *VolumeID, to VolumeID) (err error)\n\n        // Ask the driver to setup the volume,\n        // and return the access point - eg. block device file, local directory, nfs mountpoint, iscsi volume etc.\n        Attach(v *VolumeID) (id string, t AccessType, err error)\n\n        // Ask the driver to destroy the access point\n        // and free resources.\n        Detach(v *VolumeID, lazy bool) (err error)\n\n        // General lookup of all volumes provided by this driver,\n        // search could be by ID, name, policy, snapshot-create-time, etc.\n        Search(query string) (result *[]VolumeID, err error)\n    }\n\n    // Representation of the run time status\n    // of the volume and any of its access points,\n    // associated containers, hosts, etc.\n    type Volume struct {\n        // Run time details.\n        Name  string\n        Spec  VolumePolicySpec\n        Stats string\n        // alarms, hosts attached, policy compliance status, etc.\n        Status string\n    }\n```\n## Future work.\n- Allow Delegation to out of process daemon to permit hot plugging of drivers.\n"},{"labels":["enhancement1",null],"text":"## Problem\n\nConverting Windows file attributes + ACLs to 9-bit UNIX permissions when\ndocker images are built from windows CLI. Concept of 'execute' doesn't\nmatch 1:1 on Windows vs POSIX.\n### Scenario\n1. User downloads Docker client for Windows (docker.exe)\n2. User develops an \"app\" to run on Linux containers.\n3. User runs the command `docker build .`\n4. Context tar is created on client-side and transferred to daemon (running on Linux) for building.\n### Problem steps:\n1. Let's say the \"app\" is actually something user cloned from a remote git repo.\n2. Repo had a `setup.sh` script (`chmod +x`ed)\n3. There's no `x` bit on windows, so during `git clone` we lost that bit\n4. Today all files packaged to context from windows get `-rw-rw-rw-` permissions.\n5. This can also happen if user unzips a file or downloads from ftp etc.\n6. If user tries to `RUN ./setup.sh` or `ENTRYPOINT ./setup.sh` it will fail\n   due to a vague \"permission denied\" error because script was not `chmod +x`ed.\n7. Result: a dockerfile that builds correctly fine from OS X/Linux cli has failed on windows.\n8. User will take a lot of time to figure `+x` bit was missing.\n9. User will go add `RUN chmod +x setup.sh` as a dockerfile instruction if they're\n   building from Windows.\n### Root cause 1: No 'x' (execute) bit on Windows\n- Windows does not have an execute permission as the 'x' bit in Unix\n- By default, almost all files are executable on windows. (different than unix)\n- You can execute a binary if the extension is exe/bat/cmd/com/ps1 etc.\n- Windows executable permission is more about dynamically linked libraries (dlls) etc.\n- It's not widely used.\n- Does not correspond to executability concept in Unix.\n- Every time user grabs a `+x`ed file from ftp/git/unzip/samba, we lose that\n  unix bit on windows, because it's not the same thing.\n\nQuoting from [SAMBA book](http://www.oreilly.com/openbook/samba/book/ch05_03.html):\n\n> “Note that there is no bit to specify that a file is executable. DOS and Windows NT filesystems identify executable files by giving them the extensions .EXE, .COM, .CMD, or .BAT. [...] Samba can preserve these bits by reusing the executable permission bits of the file on the Unix side - if it is instructed to do so. Mapping these bits, however, has an unfortunate side-effect: if a Windows user stores a file in a Samba share, and you view it on Unix with the ls -al command, some of the executable bits won't mean what you'd expect them to.”\n### Root cause 2: Go Windows Problems\n\nGolang's os.Stat implementation is **very poor**:\n- It cannot figure out executability. (because it's not a file attribute on\n  Windows but an ACL setting –found out through a separate syscall, not implemented in golang)\n- It returns same permissions for all user/group/others sections:\n  - Because there's no group/everybody concept on Windows as in chmod bits.\n  - That's why it returns `-rw-rw-rw-` for all files on Windows and that's the permission what files copied to a docker build context gets by default.\n  - This is dangerous (imagine a private key being set to `-rw-rw-rw-` permissions by default, that's the state we're in now).\n\nWe are hoping to address problems in golang in the long term. (fingers crossed)\n## Solution\n1. We are going to clear all 'group/others' bits: `-???------`\n   - Since we're clearing `r` from group/others, this might break multi-user docker images.\n2. We are going to copy r/w bits we got from golang os.Stat(). \n3. **We are going to add `+x` to all files packaged from windows.** :boom:\n   - This way we can at least have `+x` on all files and can support executing `./binary` or `./script.sh` from shell.\n   - This may break some apps that decline `+x`ed files as input. I haven't really heard of anything like that,\n     must be a really rare case. Even so user can have `RUN chmod -x` to remove that bit manually\n   - Security issue: We are making **everything** executable. Could be. :exclamation: Please discuss.\n4. We are going to add a notice for windows CLI: `WARNING: Permission bits on files added to image might not be correctly set.` :new:\n### Proposed Fix\n\nWe modify the perm bits of the tar header created via `tar.FileInfoHeader`\nwith a simple platform-specific helper method. (see code)\n\nWe are also going to change the expected permission string\non the integration-cli test cases when executed on windows.\n### Result\n\nWe will end up with a docker image in which all files copied from\nWindows filesystem have (1) grp/others bits cleared (2) `+x` added\nregardless of it should have `+x` because it does no harm.\n\nBehavior on Linux/Mac CLIs are unchanged.\n\n---\n\nI really appreciate your input on this. That's the only thing left prevents us from getting green on Windows client CI.\n\ncc: @ewindisch @crosbymichael @cpuguy83 @tianon @tiborvass @jfrazelle @icecrime @johngossman @sachin-jayant-joshi @jhowardmsft\n"},{"labels":["enhancement1"],"text":"Is it possible to have docker being aware when the Dockerfile was changed?\nFor example, if I have a Dockerfile inside a directory I would like to at least be warned that the Dockerfile was changed for the docker container that I'm trying to run.\nTime line example:\n- Create a Dockerfile\n- Build using `docker build something`\n- Run that image creating a container called `foo`, by doing `docker run --name foo something`\n- Change that Dockerfile\n- Run the same image `docker run --name bar something` shows: \"Warning: Dockerfile was changed the image `something` could be outdated\", but the container is created no matter what.\n"},{"labels":["enhancement1"],"text":"## Abstract\n\nThe intention of this proposal is to present a storage management API for Docker. This does not rework the internals of Docker but lays the groundwork for new storage primitives, Docker plugins, and alternate implementations by external storage providers. This aligns with spirit of treating storage as entities managed outside of containers as outlined in this wonderful PR  https://github.com/docker/docker/pull/8484.\n\nThis introduces a storage entity named \"dVol\" (Docker Volume) as a directly-addressable top-level entity. _dVols_ have their own properties, such as class of service, size etc and are attached to containers as data volumes as well as _image_ volumes. The nomenclature _dVol_ is used to disambiguate with the current semantics of \"Volumes\", which is only applicable to data volumes in Docker. In its actual implementation, it may merge with the docker volumes.\n\nThis proposal identifies a set of functionality provided by dVol and specifies an API to be implemented by third party storage providers. It outlines usage from Docker CLI and graphdriver plugins, and specifies interactions with the scheduler, so that scheduling of containers is aware of storage requirements, \n\nThe implementation of dVols could come from scale-out distributed block solution, direct attached disks, EBS in AWS, a distributed file system, or a local filesystem. A dVol driver implemented to manage local filesystem directories can ensure backward compatibility with existing use of docker volumes.\n## Background and Motivation\n\nStorage for docker _images_ comes from a shared namespace in a single filesystem on the host. The lifetime of modifications to the image root fs is tied to the lifetime of the container. The visibility and accessibility is restricted to a single node, and a container is forced to use the format and features of the host.\n\nDocker also give the ability to setup _data_ volumes. These are directories within the namespace of a container that can be shared by other containers. These can be directories within the host filesystem that is mounted in the container namespace.  The lifetime of data volumes is that of the last container that references it. The visibility and accessibility is restricted to a single node.\n\nThis PR addresses the issue around decoupling a container's namespace from the local host's filesystem thereby allowing for 3rd party storage providers to provide alternate implementations for hosting a container's data and image.  Such an implementation may be local or remote, for example on a shared SAN or NAS.  This will pave the way for hosting stateful containers, and providing specialized storage services for stateful containers, such as tiering, caching services and snapshots.\n## dVols\n\ndVols are opaque entities, addressed by globally unique ID and tagged with user-defined labels. dVols provide storage for both data as well as image. They encapsulate underlying device composition (SSD/SATA) and geographic device location including data-center/rack/machine. They have initial capacities, possibly thin provisioned, max usage can be enforced and may optionally be resized. dVols implement storage operations such as snapshots, new dVols can be created from existing snapshots. You can enumerate dVols in the system and their associations to containers. You can enumerate snapshots by dVol ID or by tag. dVols have stats and can have alerts. All the storage operations are at the granularity of dVols and when dVols are associated with a container image, the storage operations are now at container granularity. \n\nAt a high level, third party storage providers implementing dVols can enable:\n- Fast Loading of container images\n- Mobility for stateful containers\n- HA and fault tolerance for data\n- Storage operations (such as snapshots and clones) at container granularity\n### API\n\n``` go\ntype DVolDriver interface {\n\n    // Create a new dVol for the specific DVolConfig.\n    // Returns a system label that uniquely identifies the dVol in the system\n    Create(locator DVolLocator, config DVolConfig) (dVolLabel SystemLabel, err error)\n\n    // CreateFromSnap same as Create except that the dVol is initialized from a snapshot\n    CreateFromSnap(locator DVolLocator, config DVolConfig, snapLabel SystemLabel) (dVolLabel SystemLabel, err error)\n\n    // Attach dVol to the host.\n    // On success the devicePath specifies location where the device is exported\n    // Returns an error if the dVol is already attached or if dVolLabel doesn't exist\n    Attach(dVolLabel SystemLabel) (devicePath string, err error)\n\n    // Detach dVol from the host.\n    // Returns an error if the dVol is not attached.\n    Detach(dVolLabel SystemLabel) (err error)\n\n    // Inspect dVol.\n    Inspect(dVolLabel SystemLabel) (dVol DVol, err error)\n\n    // Delete dVol.\n    // This will return an error if there are any snaps associated with the dVol.\n    Delete(dVolLabel SystemLabel) (err error)\n\n    // Enumerate dVols that map to the dVolLocator. Locator fields may be regexp.\n    // If locator fields are left blank, this will return all dVols.\n    // If err is set to ErrEagain then the call should be retried with a zero cursor\n    Enumerate(locator DVolLocator, cursor *Cursor, dVols []DVol) (count int, err error)\n\n    // Snapshot specified dVol. IO to the underlying dVol must be quiesced before\n    // calling this function.\n    // On success, a system generated label is returned.\n    Snapshot(dVolLabel SystemLabel, snapLabel string) (snap SystemLabel, err error)\n\n    // SnapDelete deletes specified snapshot.\n    SnapDelete(snapLabel SystemLabel) (err error)\n\n    // SnapInspect inspect snapshot.\n    SnapInspect(snapLabel SystemLabel) (snap DVolSnap, err error)\n\n    // SnapEnumerate enumerate snaps for specified dVol\n    // Count indicates the number of snaps populated.\n    // If err is set to ErrEagain then the call should be retried with a zero cursor\n    SnapEnumerate(locator DVolLocator, cursor *Cursor, snaps []DVolSnap) (count int, err error)\n\n    // Stats Retrieve dVol stats\n    Stats(dVolLabel SystemLabel) (stats DVolStats, err error)\n\n    // Alerts Retrieve dVol alerts\n    Alerts(dVolLabel SystemLabel) (stats DVolAlerts, err error)\n}   \n```\n#### DVol Identfiers\n\nSystemLabel is a system generated globally unique identifier for dVols and snapshots. \n#### DVol Locators\n\nA DiskLocator is a user generated string identifying a dVol, these need not be unique and can be used in search/enumerate functions. This is typically the container imageid:label. This punts the management and mapping of container image dVols to the DVolDriver.\n#### DVolConfig\n\nThis is the specification of the backing datastore. It contains the following fields:\n- Size: Thinly provisioned size of the volume\n- HALevel : An integer value specifying the number of nodes that are allowed to fail and yet data be available. A value of 0 implies that data is not erasure coded. Failure of a single node will lead to data unavailability.\n- Tiering: A boolean value specifying if tiering is required across SSD/SATA\n- DeviceComposition: specifies preference for SATA/SSD or a combination of both.\n- CoS: A number between 1..9 specifying class of service relative to IO on other dVols\n- Dedupe: A boolean value specifying if dedupe is to be enabled.\n\nBased on feedback from this PR, we plan to submit a separate proposal for what a DVolConfig would look like and some suggestions on where this information could go.  We suspect that this would go with other information that a container would need, for example around networking properties\n#### DVol\n\nA runtime summary of DVol, gives information about hosts that contain the data, free space, and attached container. This information is used by scheduling and diagnosis/monitoring software. \n### Usage\n\nThe frontend for dVols management is an augmented version of 'docker volume' CLI in proposal https://github.com/docker/docker/pull/8484. The CLI would take an additional argument of --dvol-provider <port> that delegates storage management to an external provider. Once a dVol is attached on the host filesystem, it can then be mounted in the container's namespace.  This will also allow for a container to have its own filesystem that is separate from the host's filesystem.\n\nIn the case of container images, the creation/attach/detach of dVols is done via the graphdriver interface. A simple implementation uses the local filesystem for the container rootfs and the dVol becomes the uppermost writeable layer of the container filesystem. One way to achieve this is a modified version of the overlayfs driver that creates the dVol as the 'upper' layer. The lifetime of the dVol is not tied to the lifetime of the container. All the operations available via 'docker volume' is available on the container image dVol. \n### Scheduler Interaction\n\nSchedulers cannot be storage agnostic, they are fundamentally interconnected and need to work together to ensure an optimal runtime environment. Here is why:\n- Data locality: Ideally, containers should be scheduled to run on the same node where their data lives. dVols can be queried to list their underlying node and device topology to serve as predicates for container scheduling.\n- Data containers: Containers sharing dVol need to be scheduled on the same machine. This restriction doesn't exist if the underlying dVol is implemented as a distributed filesystem.\n- Inter-container IO resource management: Another input to the scheduler is learned IO patterns and workload inference. Armed with this information, the scheduler can distribute containers based on their workload for optimal distribution of IO across the aggregate spindles in the cluster while keeping the data access local.\n\nThe information above is inferred from a call to 'Inspect(dVolLabel SystemLabel)'\n### Image Format\n\nInformation that goes into DVolConfig needs to come from somewhere. It can come from a higher level specification of what storage a container requires to run such as _docker compose_ or from the image format or from both with compose overriding the image format. The proposal here is to include a DVolConfig specification in the image json including filesystem and blocksize preferences.  \n### Backward compatibility\n\nA dVol driver implementation that carves out volumes as directories in the local filesystem can provide an implementation that is  backward compatible to existing docker volume implementation.\n### Future work\n\nThe proposal is a starting point and we hope clarity and details will emerge from a community discussion. One of the pending items is to group together containers for storage related operations:\n- Container Links:\n  The storage driver or a higher entity needs to understand related containers.  This will allow for end users to apply storage actions on a set of related containers.  For example, the user can take a \"compose\" level snapshot, which will snap all containers in the same compose specification in a consistent state.  \n### Reference implementation\n\nComing Soon!\n"},{"labels":[null,"enhancement1"],"text":"Container with a restart policy (eg `--restart always`) is always restarted on daemon restart, even if the container has already been put to the stopped state manually.\n\nBackground: when I update containers I'd like to keep old containers around for a while in order to do a rollback if needed. Now, should the daemon restart, all versions of the containers start again.\n\n```\n> docker run -d --restart always --name r0 busybox sleep 1000\n> docker stop r0\n> docker ps | grep r0 | wc -l\n0\n> sudo service docker restart\n> docker ps | grep r0 | wc -l\n1\n```\n\nTested with `v1.4.1` and `v1.5.0`.\n\nI could help out fixing this if it gets confirmed as a bug.\n"},{"labels":["enhancement1"],"text":"Right now the `docker version` output is rather inconsistent.\n\nFor example:\n\n```\nClient version: 1.5.0\nClient API version: 1.17\nGo version (client): go1.3\nGit commit (client): a8a31ef\nOS/Arch (client): linux/amd64\nServer version: 1.5.0\nServer API version: 1.17\nGo version (server): go1.3\nGit commit (server): a8a31ef\n```\n\nThe first 2 lines are prefixed with \"Client\", but then the next 3 lines have \"(client)\" suffixed. Then the pattern repeats for the server lines. I think it would be better to use the same style for all the lines.\n\nSome previews of what different formats might look like:\n### 1 - prefix\n\n```\nClient version: 1.5.0\nClient API version: 1.17\nClient go version: go1.3\nClient Git commit: a8a31ef\nClient OS/Arch: linux/amd64\nServer version: 1.5.0\nServer API version: 1.17\nServer Go version: go1.3\nServer Git commit: a8a31ef\nServer OS/Arch: linux/amd64\n```\n### 2 - suffix\n\n```\nVersion (client): 1.5.0\nAPI version (client): 1.17\nGo version (client): go1.3\nGit commit (client): a8a31ef\nOS/Arch (client): linux/amd64\nVersion (server): 1.5.0\nAPI version (server): 1.17\nGo version (server): go1.3\nGit commit (server): a8a31ef\nOS/Arch (server): linux/amd64\n```\n### 3 - sectioned\n\n```\nClient\n  Version:     1.5.0\n  API version: 1.17\n  Go version:  go1.3\n  Git commit:  a8a31ef\n  OS/Arch:     linux/amd64\nServer\n  Version:     1.5.0\n  API version: 1.17\n  Go version:  go1.3\n  Git commit:  a8a31ef\n  OS/Arch:     linux/amd64\n```\n### 4 - tabular\n\n```\n              Client        Server\nVersion:      1.5.0         1.5.0\nAPI version:  1.17          1.17\nGo version:   go1.3         go1.3\nGit commit:   a8a31ef       a8a31ef\nOS/Arch:      linux/amd64   linux/amd64\n```\n\nPersonally I vote for the tabular format. Followed by sectioned. Prefixing looks okish, but suffix is just messy.\n"},{"labels":[null,"enhancement1"],"text":"Hey, I was working towards neighbour proxy support for IPv6 environments (https://tools.ietf.org/html/rfc4389). Specifically I need to add the global address allocated to the container as a neighbour proxy on the host. In theory I could build this using events, though there may be a race if the container wants external network right away. The better solution would be to add this to docker itself so when an IP is allocated we (optionally) add a neighbour entry, and similarly remove it when the IP is released.\n\nThoughts if this is something docker itself should have support for?\n"},{"labels":["enhancement1"],"text":"It would be great to be able to report metrics from an app to the container internally for those metrics to be accessible via the standard stats api interface external to the container.\n"},{"labels":[null,"enhancement1"],"text":"As the complexity of docker continues to grow, it is becoming increasingly difficult to diagnose the many issues that are reported daily. While there are some guidelines we have when reporting issues (daemon log dump, `docker info`, `docker version`, etc), anything more specific requires the user to manually collect info that may be difficult to find.\n\nIt may be beneficial to provide a simple tool that users can run to collect basic info about their docker installation and can also be used to collect broader information about their system and network configuration to better diagnose issues. The user could then simply attach the output of this tool to a github issue so that maintainers/contributors may inspect it and better understand the issue and more quickly find a resolution.\n\n@docker/maintainers what kind of info (outside of the basics like version, and `docker info`) do you suggest be collected by such a tool, and do you think we could start an effort to implement such a diagnostic tool?\n"},{"labels":["enhancement1"],"text":"After working with docker for a while now, I constantly find myself cleaning up old containers. If you have 2 or 3 docker based projects, each with several containers this can become a big annoyance. You have to wade through 100s of containers and very carefully find those, which are no longer required.\n\nThere was a discussion about a clean command in #928 - but with no outcome. I think, one of the difficulties was to identify no longer needed containers. IMO the main difficulty is really, that each and every container has a name. If \"throwaway\" containers (`fig run ...`, `docker run ...`) wouldn't get a name by default, it would be very clear, which containers can be deleted safely.\n"},{"labels":[null,"enhancement1"],"text":"So `docker rmi --no-prune test` would _never_ delete the layer `test` points to, even if it's the only remaining reference.\n\nThe current behaviour of deleting the topmost layer means there is no way (that I can think of) to untag an image with a single reference without doing a dance of moving the tag onto a referenced image before `docker rmi`.\n"},{"labels":[null,"enhancement1"],"text":"See the second problem from https://github.com/docker/docker/issues/10614#issuecomment-74160895.\n\nCurrently only the first one is used. If node that owns the first IP dies, then registry looks unavailable for docker server, even if there are more alive nodes.\n\nSee https://github.com/bobrik/buzzer for an example.\n"},{"labels":["enhancement1"],"text":"It looks like the API for Docker cp (`/containers/:id/copy`) natively writes tar format, but I don't see the ability to get the raw tar stream with the docker binary.  It would be great if `docker cp` (or a new command) would stream the tar format directly to stdout.  Similar to what `tar -cf - dir` would do.\n\nWe use docker to produce build artifacts that get uploaded to a continuous deployment system and it'd be great to just be able to do this `docker cp container:/output | curl -XPUT -d @- URL`\n\nI see docker export, but that looks like it can only export the entire container.\n"},{"labels":["enhancement1"],"text":"The team behind [App Container](https://github.com/appc/spec) (appc) is seeking feedback on whether there are any modifications required to the spec to allow it to work natively in Docker. \n## Background\n\nApp Container (appc) is a specification for running applications in containers. We think App Container represents the next logical iteration in what a container image format, runtime engine, and discovery protocol should look like. Full details of the spec can be found in the [GitHub repo](github.com/appc/spec), in particular in the following two files:\n- [SPEC](https://github.com/appc/spec/blob/master/SPEC.md) - contains the latest and complete version of the specification\n- [README](https://github.com/appc/spec/blob/master/README.md) - contains background information on some of the motivations and principles of the project\n\nappc is young but has already seen a variety of implementations well before it has reached 1.0 (stable) status. Before we call it 1.0, we are requesting feedback from the Docker community on what might need to be modified in the spec in order for it to be supported natively in Docker. If there are any issues with the spec preventing Docker from supporting it, we would like to discuss and address them now.\n## Current Status\n\nTo test the compatibility of App Container and Docker, we have started to implement App Container Image and Image Discovery support into Docker itself. This work has already reached a functional state, which can be seen in the PR https://github.com/docker/docker/pull/10776\nThis should be considered somewhat experimental but we believe this prototype demonstrates that native support for appc within Docker is viable and worth pursuing.\n## Future Work\n\nThere are a number of interesting features that could be brought into the Engine by exploring further integration with App Container. For example, a fully implemented version of the appc spec as part of the Docker Engine would \"close out\" these proposals with code:\n- Labels/Tags of images: https://github.com/docker/docker/pull/9882 https://github.com/docker/docker/pull/8955\n- Container Groups: https://github.com/docker/docker/issues/8781\n- Container Identity: https://github.com/docker/docker/pull/9498 with [appc metadata service](https://github.com/appc/spec/blob/master/SPEC.md#app-container-metadata-service)\n- Separation of transport and naming: https://github.com/docker/docker/issues/6805\n- Consistent image IDs: https://github.com/docker/docker/issues/6959\n\nEach of these topics (labels, container restarts, and container identity) is addressed and specified in the current v0.3.0 release of the spec. \n\nAnother point of interest is that the appc spec goes beyond giving a container a static public identity but also attempts to give a cryptographic identity, similar to how a host has an SSH public key.\n\nWe look forward to discussing these ideas in depth, but want to gauge the level of interest before committing a lot of effort towards building these features into the Engine. As mentioned earlier, the ACI running PR https://github.com/docker/docker/pull/10776 is just the first step.\n"},{"labels":[null,"enhancement1"],"text":"Currently `docker stats` uses a top-like (curses) interface, but I think it would be far more useful to have historical output.\n\nFor example:\n\n```\n# docker stats deadbeef012 5 3\n          CPU%   Mem%  Mem(M)  Limit(M)  NetIn(K)  NetOut(K)\n12:47:04  32.4    5.4    54.3    100.0        387     32,344\n12:47:09  36.2    5.9    59.2    100.0        455     75,923\n12:47:14  21.2    4.2    41.9    100.0        147     12,036\n```\n\nThis output is very easy to scan through (the current stats output is rather cluttered and hard to read), and shows you how the container is trending.\nIf you want to monitor multiple containers, it would simply add another column after the time for container ID.\nThis is very similar to the behavior of existing statistics utilities such as `sar` and `iostat`. This also makes the output able to be parsed by scripts.\n\nThe command line format is `docker stats CID [INTERVAL [COUNT]]`, where `INTERVAL` is the period over which to gather stats, and `COUNT` is how many intervals to gather before exiting.\n"},{"labels":[null,"enhancement1"],"text":"The docker distribution team is working on finalizing a [generic application content manifest](https://github.com/docker/distribution/pull/62) and for the new distribution tools to be used by Docker, we'll need to define an application media type for images.\n\nAn example media type would be:\n\n```\napplication/vnd.docker.container.image.params.v1+json\n```\n\nwith a format similar to that of the [docker image runconfig field](https://github.com/docker/docker/blob/802802b7812b7c73aefec4e922cad82f8dcddd4c/image/spec/v1.md#container-runconfig-field-descriptions)\n\n``` json\n{\n    \"user\": \"alice\",\n    \"group\": \"staff\",\n    \"cpuShares\": 8,\n    \"memory\": 2048,\n    \"memorySwap\": 4096,\n    \"workingDirectory\": \"/home/alice\",\n    \"ports\": [\n        {\n            \"port\": 8080,\n            \"protocol\": \"tcp\"\n        }\n    ],\n    \"volumes\": [\n        \"/var/job-result-data\",\n        \"/var/log/my-app-logs\"\n    ],\n    \"entrypoint\": [\n        \"/bin/my-app-binary\"\n    ],\n    \"command\": [\n        \"--foreground\",\n        \"--config\",\n        \"/etc/my-app.d/default.cfg\"\n    ],\n    \"environment\": {\n        \"DEBUG\": \"True\",\n        \"PATH\": \"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n        \"RETRO\": \"False\"\n    }\n}\n```\n"},{"labels":["enhancement1",null],"text":"## Issue: hack/ symbolic link to project/ causes issues on Windows\n\nWhen cloning the repo on Windows, as (for historical reasons) hack/ is now a symbolic link to project/, the symlink doesn’t get setup correctly. Windows git ends up treating hack as a 7 character file containing the word ‘project’, so it all starts going wrong quickly... In reality the hack/ directory (or symlink) is redundant.\n\nThere is a pretty messy workaround for git which is still error prone at http://stackoverflow.com/questions/5917249/git-symlinks-in-windows/16754068#16754068.\n## Proposal\n\nLooking at the build scripts, it seems just about everything is still referring to hack/. I think to make things easier going forward for supporting both Linux and Windows, the hack directory (or link as it is now) can be removed after files referring to hack/\\* are modified to their project/\\* equivalent.\n\nI believe the list of files involved here from a first-pass scan is:\n\nDocker/CHANGELOG.MD: \nDocker/Dockerfile\nDocker/Makefile\nDocker/builder/parser/testfiles/docker/Dockerfile\nDocker/builder/parser/testfiles/docker/result\nDocker/contrib/mkimage-debootstrap.sh\nDocker/dockerversion/dockerversion.go\nDocker/project/README.md\nDocker/project/RELEASE-CHECKLIST.md\ndocker/project/dind\nDocker/project/install.sh\nDocker/project/make.sh\nDocker/project/make/README.md\nDocker/project/make/test-unit\nDocker/project/release.sh\nDocker/vendor/MAINTAINERS\nDocker/vendor/src/github.com/docker/docker/hack/make/.validate\n"},{"labels":["enhancement1"],"text":"# Summary\n\nWe'd like to add support for using immutable image identifiers when pulling images from a v2 registry, creating containers, and running containers. \n# Background\n- #9015\n- https://github.com/docker/docker-registry/issues/804\n- https://github.com/docker/distribution/issues/46\n# Use case\n\nWhen I create a container, I may specify an image such as `mysql:latest`. When the image is pulled, `latest` is resolved to a particular image at that point in time. If I later want to add more containers (e.g. possible read slaves in the MySQL case), ideally all the new containers would use the exact same image as my first container. Using a tag isn't sufficient as the tag is mutable.\n# V2 registry support\n\nAs part of https://github.com/docker/distribution/issues/46, the v2 registry will be adding support for retrieving an image manifest for a particular digest. This feature gives us what we need, as long as the Docker CLI and Engine support it too.\n# Proposed CLI/Engine changes\n\nWe'll need to provide a means to reference an image by its digest. One possible example might be\n\n```\nnamespace/repository@digest\n```\n\nWe'll need to make sure the following commands continue to work as they currently do, as well as with an optional digest:\n- `docker pull`\n- `docker create`\n- `docker run`\n\nWhen listing images via `docker images`, we could default to displaying only the \"current\" values for each image and tag. An optional flag could enable displaying all values for each image and tag; namely, this would show 1 entry for each image/tag/digest combination.\n# Questions\n\n_What about v1 registry support?_\nIt's not likely we'll be able to support this\n\n_If I create an image locally via `docker tag` or `docker commit`, can I refer to it by tag + digest?_\nAs proposed in https://github.com/docker/distribution/issues/46, the registry is responsible for determining an image's digest and assigning it to the image. For an image that has not yet been pushed to a v2 registry, it may not be possible to refer to it by tag + digest. This is unlikely to be a significant issue, as the use case for tag + digest is consistent deployments using images pulled from registries. Or, if the community thinks this should be supported, we can revisit what component(s) are responsible for calculating digests.\n"},{"labels":["enhancement1",null],"text":"While docker stats is extremely useful, when a container exits the stats continue to wait for input, the connection is not closed. If you try to connect to container stats, Daemon accepts the connection, however does not respond any messages, I believe the default behaviour should be:\n1. If the stats was run before container started, and container stops & streaming stops, the stats Remote API, the connection should be closed.\n2. If the stats was run after container ended, the connection should be closed with proper HTTP header.\n\nExample of connecting to stats remote API after container has exited\n\n```\n$ curl -v http://localhost:4500/containers/00fa5e2f1767d611a8edb4c2b535b7f69e678dba72d999d03c5282dfb57b5b2b/stats\n* Hostname was NOT found in DNS cache\n*   Trying 127.0.0.1...\n* Connected to localhost (127.0.0.1) port 4500 (#0)\n> GET /containers/00fa5e2f1767d611a8edb4c2b535b7f69e678dba72d999d03c5282dfb57b5b2b/stats HTTP/1.1\n> User-Agent: curl/7.37.1\n> Host: localhost:4500\n> Accept: */*\n> \n... nothing happens\n```\n"},{"labels":[null,null,"enhancement1"],"text":"While having `docker run` automatically pull images that don't exist locally is nice for first-time users (since they can just `docker run some-image` and see the magic happen), it can be very frustrating for more experienced users who want to control the images they have and run on their host, especially since a pull cannot be cancelled except by restarting the entire daemon.  A stray `docker run` of the wrong image in the wrong terminal, for example, is a very frustrating situation to be in, especially since you then get to wait for that image to download (or not wait and then try to remember to delete it afterwards).\n\nIn other cases (especially scripted ones), `docker pull some-image && docker run some-image` emerges as a very common pattern.  For consistency with the new `docker build --pull`, I propose the addition of `docker run --pull` (ideally implemented daemon-side like `docker build --pull` is), which will default to `false` and thus change:\n\n``` console\n$ docker run --rm lolimage\nUnable to find image 'lolimage:latest' locally\nPulling repository lolimage\nFATA[0002] Error: image library/lolimage:latest not found \n```\n\ninto:\n\n``` console\n$ docker run --rm lolimage\nFATA[0002] Error: image library/lolimage:latest not found \n```\n\nSince this is a broader proposal than #10639 (deprecating an existing \"feature\"), I opted to make a new one instead of just commenting there.\n"},{"labels":[null,"enhancement1"],"text":"Issue:\nNormalization was added to tags on set/get/delete which strips a prefix of \"library/\" before setting or getting the tag.  With this change \"docker rmi\" is unable to remove a tag for an existing image named \"library/*\" and \"docker tag\" will strip \"library/\" before saving.\n\nExpectation:\nWhile the library namespace is reserved for official images on the hub, the docker cli should not prevent creation of images with this prefix.\n\nSee 3 diffs starting at \nhttps://github.com/docker/docker/commit/568f86eb186731b907b659e4ec64bda21c2fe31d#diff-9459cc42eeb37e6b37bcb473ce0b3d21R178\n\nping @dkjer\n"},{"labels":["enhancement1",null],"text":"## Background\n\nFollowing the Docker and Microsoft announcement in October 2014 (http://news.microsoft.com/2014/10/15/dockerpr/), and the recent PR https://github.com/docker/docker/pull/9113 for the port of the Docker client portion to Windows, this proposal is for the initial work necessary for the Docker daemon to compile on Windows.\n## Problem Statement\n\nThe existing daemon code base does not compile cross-platform for Windows. This is largely due to a number of ‘Linuxisms’ and system calls which do not have matching Windows counterparts. \nLinux specific calls will be factored out by various techniques such as filename_Linux.go and filename_Windows.go; through use of runtime.GOOS conditions; and/or through build tags. \n\nA simple example of the type of factoring would be removing support of the Unix HTTP namespace on Windows (but keeping it on Linux). Windows would support the TCP HTTP namespace only.\n\nIn addition, dummy no-op drivers will be added for future support of Windows containers.\n## Expectations\n\nAt the end of the series of PRs relating to this proposal, it is expected that the Docker Daemon will initialise and listen for requests on a TCP port on a Windows platform. It will be largely non-functional except for responding to “version” and “info”. \n\nThe functionality of the Docker daemon on Linux platforms will be unchanged.\n"},{"labels":[null,"enhancement1"],"text":"A Ceph (http://ceph.com/) volume could be implemented with something like ( see https://github.com/docker/docker/pull/8484 )\n\n```\ndocker volumes create --driver ceph --driver-opt foo=bar --name myimage rbd:image:/path/in/docker ...\n```\n\nwhich would map **rbd:image** via the Ceph kernel rbd driver ( http://ceph.com/docs/master/rbd/rbd-ko/ ) with\n\n```\n rbd map image\n```\n\nmount it somewhere\n\n```\nmount /dev/rbd0 /var/lib/docker/ceph/image\n```\n\nand then be used with\n\n```\ndocker ... --volume-from myimage/path/in/docker ...\n```\n\nIt requires support for https://github.com/docker/docker/issues/7249 ( for instance https://github.com/cpuguy83/docker/commit/448a1b0c31943084edb6a867a44ccdec49fcf1f0 )\n\nThe Ceph volume driver could use https://github.com/noahdesu/go-ceph or the rbd command line.\n\nThis is different from Ceph volume storage ( https://github.com/docker/docker/issues/7249 ).\n"},{"labels":[null,"enhancement1"],"text":"It would be good if I could supply a flag to `docker rm` to signal that I don't care if the container doesn't exist, rather than have it fail (and then parsing the error message to make sure it failed with \"No such container\").\n\n`-f` would fit with `rm`, but it's not exactly forcing anything - perhaps something more explicit like `--ignore-missing` would be better (and may be applicable to more commands in the future).\n\n---\n\nRationale:\n- `docker run --rm`, for various reasons, is not always reliable - a common workaround is to run `docker rm <container>` before I run `docker run --rm <container>`, to ensure that a previous unclean exit doesn't prevent me from starting a container with the same name.\n- **even when you're not using `--rm`**, and are just using well-known container names, you often have reason to \"remove container <name> if it exists, ignore otherwise\". e.g when using `--detached` you can't use `--rm` anyway, so you need to perform manual cleanup of stopped containers at some point. Most methods of scripting this are racey, which is why I think `rm` should handle it internally.\n"},{"labels":[null,"enhancement1"],"text":"[Managing data in containers](https://docs.docker.com/userguide/dockervolumes/) recommends using `--volumes-from` and a shared Docker volume container to handle backup and restore with commands like\n\n```\n$ sudo docker run --volumes-from dbdata -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata\n$ sudo docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n$ sudo docker run --volumes-from dbdata2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar\n```\n\nBut I would venture to say that `exec` is a more modern approach, and allows you to clean and speed up the backup pipeline significantly. To wit:\n### Dump a directory to tar\n\n```\n$ sudo docker exec -i \"$any_container\" tar -cf- /target | tar -t\n```\n### Backup a directory from one container to another\n\n```\n$ sudo docker exec -i container_a tar -cf- /source_path | sudo docker exec -i container_b tar -C /dest_path -xf-\n```\n\nThese approaches will work effectively with both Union and volume paths.\n\nI'm happy to submit a pull request, but I wanted to bounce the idea around first because it would represent a significant change to the existing documentation.\n"},{"labels":["enhancement1"],"text":"Currently using --net=container:foo is problematic as it requires a running container. Thus it is not possible to restart the container foo and one has to be very careful with startup order to ensure that the container foo starts and initializes its network before running another container.\n\nIt would be nice if --net=container:foo would not require the foo running. Rather docker can create foo's network namespace and allow other containers to join it. When foo later starts, it would just join that network namespace. Similarly, when foo exits and starts again, it should join the original namespace if other container still use it rather than creating a new  one again.\n\nAs a workaround with the current docker one can use an extra container that just sleeps forever in its main process and all relevant containers join its network. However this still does not solve the ordering problem as now one has to ensure that this extra container properly starts up before the rest. In addition this just wastes resources.\n"},{"labels":[null,null,"enhancement1"],"text":"If you want to remove a link between containers, you can use `docker rm --link`. The current documentation says:\n\n```\n $ docker help rm\n\nUsage: docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers\n\n  -f, --force=false      Force the removal of a running container (uses SIGKILL)\n  -l, --link=false       Remove the specified link and not the underlying container\n  -v, --volumes=false    Remove the volumes associated with the container\n```\n\nI eventually figured out that you need to use the syntax `docker rm -l HOSTCON/LINKCON`, but I'm not sure how I was meant to know that. It doesn't feel symmetrical to the syntax used to create a link or other commands which typically use : as a separator.\n\nAlso, I'm really not convinced `docker rm` is the right place for the functionality - all other flags remove the container including `-v`. Could this be moved to a `docker unlink` subcommand or similar and the documentation improved?\n"},{"labels":["enhancement1"],"text":"Attempting to declare multiple variables that reference previously declared variables on one line doesn't seem to work, for example this declaration will create variables that contain strings instead of expanded variable definitions:\n\n```\nENV JAVA_VERSION=jdk1.7.0_75 \\\n    JAVA_HOME=/usr/$JAVA_VERSION \\ \n    JAVA_BIN=$JAVA_HOME/bin \\\n    JRE_HOME=$JAVA_HOME/jre\n```\n\nAfter building a container with the above ENV declarations the environment will contain:\n\n```\nJAVA_VERSION=jdk1.7.0_75\nJAVA_HOME='/usr/$JAVA_VERSION'\nJAVA_BIN='$JAVA_HOME/bin'\nJRE_HOME='$JAVA_HOME/jre'\n```\n\nWhere I would expect those variables to have been expanded.\n\nOn the other hand creating variables using multiple ENV statements will work and variables will contain the expanded variables:\n\n```\nENV JAVA_VERSION jdk1.7.0_75\nENV JAVA_HOME /usr/$JAVA_VERSION\nENV JAVA_BIN $JAVA_HOME/bin\nENV JRE_HOME $JAVA_HOME/jre\n```\n\nAfter building a container with the above ENV declarations the environment will contain:\n\n```\nJAVA_VERSION=jdk1.7.0_75\nJAVA_HOME=/usr/jdk1.7.0_75\nJAVA_BIN=/usr/jdk1.7.0_75/bin\nJRE_HOME=/usr/jdk1.7.0_75/jre\n```\n\nIt wasn't apparent in the documentation that this should or should not work, and the second method causes 3 extra intermediate containers to be built. As the statements are very similar it would be nice to have then behave similarly, or the difference be documented.\n"},{"labels":["enhancement1"],"text":"```\nContainers: 29\nImages: 294\nStorage Driver: overlay\nExecution Driver: native-0.2\nKernel Version: 3.18.1-031801-generic\nOperating System: Ubuntu 14.04.1 LTS\nCPUs: 2\nTotal Memory: 3.791 GiB\nName: callisto\nID: ZWU6:UVBQ:HL5R:AAB5:GKYQ:2MSR:UPQW:TN6H:32C4:JJFX:WLWA:YPNZ\nUsername: bobrik\nRegistry: [https://index.docker.io/v1/]\nWARNING: No swap limit support\n```\n\nRunning container as usual:\n\n```\ndocker run --rm -it ubuntu:14.04\n```\n\nUsing `docker-exec` from @jpetazzo to enter the container, trying to use `strace` (it works):\n\n```\nroot@callisto ~ $ docker-enter 3980451ef011\n-bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\nroot@3980451ef011:/# strace -f -p 1\nProcess 1 attached\nread(0,\n```\n\nTrying `docker exec` (getting `Operation not permitted` for `ptrace`):\n\n```\n~ λ docker exec -it 3980451ef011 /bin/bash\nroot@3980451ef011:/# strace -f -p 1\nstrace: attach: ptrace(PTRACE_ATTACH, ...): Operation not permitted\nCould not attach to process.  If your uid matches the uid of the target\nprocess, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try\nagain as the root user.  For more details, see /etc/sysctl.d/10-ptrace.conf\n```\n\nIt would be great to have something like `--privileged` flag for `docker exec` to run commands like `strace` in container. This battery should be included, if you know what I mean.\n"},{"labels":[null,null,"enhancement1"],"text":"It is bizarre to me that there is a containers/resize endpoint, but that those parameters are not available to be set on container creation or start. In addition, _calling resize doesn't even work_ - you need to restart the container as well, making the whole flow for creating a TTY of a certain size nigh impossible in the usual Docker flow.\n"},{"labels":[null,null,"enhancement1"],"text":"Generally, a docker command instantiation makes several http requests to a running registry. Processing these requests in the registry server to sessionize the request flow is a non-trivial problem. While we have a proposed `Docker-Command` header in https://github.com/dmcgowan/docker/pull/18, it would be smart to include a unique token for each command execution that is added in a header to support trivial sessionization.\n\nUse cases of these session identifiers include producing usable metrics for private registries or added to outgoing webhooks (docker/distribution#42) for later processing.\n\nThe header would consist of the command that was run (not the entire cli, ie \"push\" or \"pull\") and a session id generated at the start of each instantiation of the docker client process. An example of such a header might be as follows:\n\n```\nDocker-Command: push 3eed4669-d1e8-4753-9420-998e530386b1\n```\n\nThe header could be added to all requests, from the client to the engine and beyond. If taken that far, such a header could support distributed debugging or begin to enable [dapper](http://research.google.com/pubs/pub36356.html)-like analysis.\n"},{"labels":[null,null,"enhancement1",null],"text":"This is a feature request to add the `--add-host=[]` (currently availble for `docker run`) to `docker build` also. This would be very handy.\n"},{"labels":[null,"enhancement1"],"text":"By experimenting I discovered that in Docker 1.3 I can replace Docker's /etc/hosts management in a container via adding -v .../custom_hosts_for_container:/etc/hosts . This will supply a custom /etc/hosts to the container hiding one provided by default.\n\nIs it supported solution that will continue to work in future?  Or is this is something that works just by an accident and could be broken in later versions of Docker?\n"},{"labels":["enhancement1"],"text":"## The Docker Vault\n\n(aka The Art of Injecting Ethereal Data into Containers)\n\nThis proposal is a docs-based follow up to #6075 and #6697, with some\nclarifications and improvements.\n### Purpose\n\nThe purpose for the \"Docker vault\" is to allow containers (both created in an\nintermediate build step or created explicitly) to access data that is considered\nto be \"secret\" while not allowing said data to be stored in images.\n\nEssentially this makes the general purpose of the Docker vault to allow users to\ninject arbitrary files into a container context while being assured that said\nfiles will not be leaked into image layers if that container is committed.\n\nThis allows for certain use-cases to be accomplished with ease, such as:\n- Injecting keys or information required to generate a final image (which is\n  allowed to be publicly available), while not storing said keys (which are\n  _not_  allowed to be publicly available) in the intermittent layers.\n- Injecting keys or other information required for the running of a Docker\n  container (such as SSL private keys) when said keys are _not_ allowed to be\n  publicly available.\n### Overview\n\nData injected into a container using the Docker value will **NOT** (under any\ncircumstances) be saved when an image is created from the container. The data\nis (to all intents and purposes) ethereal: you can only see it in a container\nthat has access to it, it cannot be saved into an image.\n\nContainers can be given access to this data, and will be able to access this\ndata throughout their lifetime (i.e. purging files from the Docker value will\nnot remove it from running containers).\n\nThe data in the vault is conceptually stored in \"boxes\", where a \"box\" can be\nconsidered a collection of files that are related in some fashion. Each box is\ngiven a name, and containers are given access to specific boxes. An image can\nhint what boxes it requires (just as it can hint the volumes it requires) and\nboxes can be \"aliased\" in a container (you can tell a container that a box named\n`a` is actually a box named `b` which the image expects).\n\nBox names can consist of any characters except \"`/`\" and \"`:`\".\n\nIt is important to note that vault operations will **NEVER** affect a running\ncontainer, because a running container must remain consistent throughout its\nlifetime.\n\nFiles in the Docker vault persist across restarts of the Docker daemon.\n\nContainers access files they have been given access to by accessing the files\nin `/run/vault/<box>/...`. Boxes _can_ store directories too. All files and\ndirectories in `/run/vault/` are owned by UID and GID `0`.\n\nModifications of any kind to a box inside a container will **not** be reflected\nin the vault's stored boxes. To this end, boxes are mounted `ro` inside a\ncontainer (and `remount`ing them `rw` will still **not** allow you to modify\nthe vault's stored boxes inside a container).\n### Usage\n\nThere are several ways to access the Docker vault functionality:\n- `docker vault ...`: add, query, and delete data from the Docker vault.\n- `docker create --inject`: inject files from the Docker vault into a container.\n- `docker run --inject`: inject files from the Docker vault into a container.\n- `docker build --inject`: inject files from the Docker vault into the\n  intermediate containers during the build.\n- The `Dockerfile` syntax explained below.\n#### `docker vault`\n\nThis subcommand has three classes of subcommands, with 6 subcommands in all:\n\nManagement (creates and destroys boxes):\n- `docker vault create [options] <box>...`\n- `docker vault destroy [options] <box>...`\n\nModification (adds files to and removes files from a box):\n- `docker vault add [options] <box> <file>...`\n- `docker vault remove [options] <box> <file>...`\n\nQuerying (lists information or accesses data inside boxes or the boxes\nthemselves):\n- `docker vault list [options] [<box>...]`\n- `docker vault read [options] <box>[:<file>]...`\n\nIt also has the following aliases (for the purposes of Unix-like simplicity):\n- `docker valut ls` => `docker vault list`\n- `docker vault rm` => `docker vault remove`\n- `docker vault cat` => `docker vault read`\n\n> **NOTE**: These could just be used as the proper commands and the long-form\n> ones ignored...\n\nIt is very important to note that **NONE** of the `docker vault` subcommands\nwill affect running containers (whether or not they are using a box affected by\nthe vault operation). Containers use copies of vault data, not references to the\nvault data itself -- in order to maintain consistency of a single container's\nlifetime.\n##### `docker vault create [options] <box>...`\n\nThis subcommand creates a new box called \"`<box>`\" with no files inside it. If\na box with the given name already exists, this command will emit an error and do\nnothing.\n\nOptions:\n- None.\n##### `docker vault destroy [options] <box>...`\n\nThis subcommand obliterates the box called \"`<box>`\" and any files stored\nwithin it.\n\nOptions:\n- `-f`, `--force`: ignore errors if the given box name does not exist.\n##### `docker vault add [options] <box> <file>...`\n\nThis subcommand adds the given list of files to the given box. The file names\nare preserved when adding the files to the box.\n\nIf the box does not exist, the command will emit an error and do nothing.\n\nIf one of the paths in the given list does not exist, the command will emit an\nerror and continue execution.\n\nIf the given path points to a symlink, the symlink itself is copied verbatim.\n\nIf one of the path components in the path is a symlink, the symlink is followed\nas though the box root was the root filesystem (it is scoped to the box).\n\nIf the path has some directory components, these will be reflected when the box\nis injected into a container. In other words, boxes can store directories.\nHowever, the path will be sanitised, so relative paths (`../a/b/c`) and absolute\npaths `/a/b/c` will not be reflected. In both cases the paths would be precisely\nidentical to `a/b/c`.\n\nIf the given path points to a directory, the command will emit an error and\ncontinue execution. If the `-r` flag is set, then all of the files and\ndirectories in that directory are also added to the box (as if their full paths\nwere also included in the command). If a directory with the given name already\nexists, then the directories are merged.\n\nOptions:\n- `-r`, `--recursive`: recurses directories given, adding all of the contents of\n  the directory to the box in addition to the directory itself.\n##### `docker vault remove [options] <box> <file>...`\n\nThis subcommand removes the specified files from the given box.\n\nIf the path doesn't exist inside the box, then the command will emit an error\nand continue execution.\n\nIf one of the path components in the path is a symlink, the symlink is followed\nas though the box root was the root filesystem (it is scoped to the box).\n\nIf the path is a directory, then the command will emit an error and continue\nexecution unless the `-r` flag is set. If the `-r` flag is set, then the\ndirectory and its contents are recursively removed.\n\nOptions:\n- `-r`, `--recursive`: recurses directories given, removing all of the contents\n  of the directories from the box in addition to the directory itself.\n##### `docker vault list [options] [<box>...]`\n\nThis subcommand lists all files and directories stored inside the given boxes.\n\nIf no boxes are given, `docker vault` will list the files in every box stored in\nthe Docker vault.\n\nIf a given box does not exist, the command will emit an error and continue\nexecution.\n\nOptions:\n- `-b`, `--boxes`: only print the name of each box, not their contents.\n- `-f FORMAT`, `--format=FORMAT`: formats each line with the given format\n  string.\n- `-r PATTERN`, `--pattern=PATTERN`: only print entries where the file\n  paths match the given regular expression.\n##### `docker vault read [options] <box>:<file>...`\n\nThis subcommand reads the contents of each file specified in the command line\nand prints them to `stdout`. No information is printed about which box or file\nthe data came from.\n\nIf a path of box doesn't exist, the command emits an error and continues\nexecution.\n\nIf one of the path components in the path is a symlink, the symlink is followed\nas though the box root was the root filesystem (it is scoped to the box).\n\nIf a path is a directory, the command emits an error and continue execution\nunless the `-r` flag is set. If the `-r` flag is set, then the directory is\nrecursed and all of its contents are\n\nOptions:\n- None.\n#### `docker create [--inject <box>:[<alias>]]...`\n\nThis option to `docker create` allows you to inject boxes into a container on\nits creation.\n\nIf an alias is specified, then the box is injected into `/run/vault/<alias>`.\nOtherwise, the box is injected into `/run/vault/<box>`.\n#### `docker run [--inject <box>:[<alias>]]...`\n\nThis option to `docker run` allows you to inject boxes into a container on its\ncreation.\n\nIf an alias is specified, then the box is injected into `/run/vault/<alias>`.\nOtherwise, the box is injected into `/run/vault/<box>`.\n#### `docker build [--inject <box>:[<alias>]]...`\n\nThis option to `docker build` allows you to inject boxes into each of the\nintermediate build containers during image creation. These boxes will (of\ncourse) not be stored in the resultant image.\n\nIf an alias is specified, then the box is injected into `/run/vault/<alias>`.\nOtherwise, the box is injected into `/run/vault/<box>`.\n#### Dockerfile\n\nImages can hint what boxes they expect in order to run (much like how volume\nhinting works). If a hint is not fulfiled, then an empty box is mounted\ninstead.\n\nEssentially the syntax has two forms (to mirror the `VOLUME` instruction):\n\n``` dockerfile\nBOX <box>\nBOX [\"<box>\"...]\n```\n\nThe first format is a legacy format, only allowing for one box name to be\nspecified. The second is the newer format, and it accepts a JSON array of\nboxes.\n### Internals\n\nThe following documents the following internals:\n- Changes to the RESTful API.\n- Changes to the container and image information.\n#### RESTful API Changes\n\nSeveral new endpoints will be added as a result of this functionality:\n- `PUT /vault/<box>/`\n- `DELETE /vault/<box>/`\n- `PUT /vault/<box>/<path>`\n- `DELETE /vault/<box>/<path>`\n- `GET /vault/`\n- `GET /vault/<box>/`\n- `GET /vault/<box>/<path>`\n\nAnd several modified by this functionality:\n- `POST /containers/<name>/create`\n- `POST /build`\n\nAll of which are readibly apparent if you look at the docs for the command-line.\n#### Container and Image Changes\n\nBasically, both the container and image structures need to be updated to store:\n- Hinted boxes (images).\n- Injected boxes (containers).\n\nBoth of which are readily apparent if you look at the docs for the command-line.\n\n/cc @shykes (this is a long one)\n"},{"labels":[null,null,"enhancement1"],"text":"There are already quite a number of issues related to this (listed below), to the point where I'm creating a new meta-issue: we need a bridge management API and commands with clear functions and docs.  The way it stands today, either your Docker bridge networking works automatically or it doesn't work at all, and if it doesn't it's not fixable short of hacking the source.  \n\nI recently experienced this at LinuxConf.AU using Docker 1.4.1.   While 172.\\* networks are not frequently used in the USA, they are the default for internal corporate networks in Australia, so Docker's bridge network conflicted with the conference network, preventing the Dockers I intended to do demos on from reaching the internet.  I then wasted an entire day trying to change the default bridge network to a different subnet -- with the help of a Linux networking expert -- without success.  While the --bid switch does allow you to change the bridge network, for example, containers you start don't recognize the change, and Docker doesn't actually set up the new bridge on the host (you have to do that entirely manually).   As you might imagine, this has been a significant inhibitor to Docker adoption in AU/NZ.\n\nI could file a set of minor issues with each step of trying to fix the Docker bridge that failed, but that would be missing the point; the core issue is that Docker assumes that its default bridge setup will simply work, and no real management tools to change that network exist; if you have to change bridges, you have to graduate to Docker hacker first.  This is wrong.\n\nRelated issues:\nhttps://github.com/docker/docker/issues/9986\nhttps://github.com/docker/docker/issues/9729\nhttps://github.com/docker/docker/issues/10096\nhttps://github.com/docker/docker/issues/6442\nhttps://github.com/docker/docker/issues/6155\nhttps://github.com/docker/docker/issues/3909\n"},{"labels":[null,"enhancement1"],"text":"Following discussion on https://github.com/docker/machine/issues/2, as well as debate on docker compose to know if this should be part of docker client or a separate executable, this proposal is about a way to easily provide some extra tools to docker command line.\n\ndocker client would delegate any command it doesn't handle by itself to an executable `docker-<invoked_command>`. This allows to run `docker machine ...` but distribute this feature as `docker-machine` executable and prevent conflict with BSD `machine`.\n\nOther benefits are :\n- it makes it easy for contributors party to propose new features as such executable, and have more people test it before it can be considered as an official docker feature. \n- it makes it possible to split docker into dedicated executables\n"},{"labels":["enhancement1"],"text":"Currently, as we know, all the users in the group \"docker\" can manipulate docker as if they have root privileges. In addition, all the instances are in a global scope. so when i want to list containers, in fact I  list all  users'  instances. But sometimes, in fact, always, I just want to list my containers, I really don't care others. So I think it may be better to append user info to the containers.When the user want to  do a list(exec `docker ps`), just shows the containers owned by the user rather than all the containers.\nOf course, you can list all containers just by passing one or more flags, because in fact the docker currently can‘t Completely isolate the containers based user or group as we can just escape the docker to show others instances by calling system commands(like `ps aux`).\n"},{"labels":["enhancement1"],"text":"Before you dismiss this off hand - there are legitimate use cases where this feature can save the user hours, and entice him to be a good Docker citizen - and I will list them.\n\nI know the philosophy of Dockerfiles it to make them as portable as possible. But a bigger part of the philosophy is to \"don't repeat yourself\" and \"don't reinvent the wheel\". There are two use cases where the security concern for not having this becomes a moot point, and where a simple Dockerfile command can save a lot of complexity.\n\nCase #1: You're in a secure enviroment, without internet access, and you have a complete local cache of an Ubuntu/CentOS/other repo, from which you'd like to build your docker images. \n\nUse case 1:\nIn your Dockerfile, you have easily removable lines:\nBINDMOUNT yum.repos.d:/etc/yum.repos.d  # which contains the 3 lines necessary to install from CD\nBINDMOUNT /media/CentOS-CD:/media/CentOS-CD (this is where your CD image is mounted - the / could even be relative from your current Dockerfile path, for security)\nRUN yum -y install _lots of stuff_\n\nResult: Your Dockerfile may not be portable, but you can use it to test locally in your offline environment. \n\nBUT - if you want to make your Dockerfile portable, you simply remove the two BINDMOUNT lines. In fact, if another environment ignores them altogether, your following commands will still work because the default repository config will not be overloaded with a config pointing to your locally mounted CD, but will still point to the default online sources, which will work as per normal. \n\nCurrent alternative: \n1) You have to change the repo config to pull from a non-standard DNS - MANUALLY - or you have to hijack DNS so it pulls from your local repo.\nAND 2) You have to run a local webserver\nAND 3) You have to expose the CD/offline repo files in a manner consistent with those of an on-line repository for the package mangement system in question\n\nFootnotes: \n1) Additionally your image needs tools to download from a webserver, eg wget or equivalent built into it's package mangement system\n2)AND the package mangement system needs to be part of the image \n\nWith the suggested solution, you could potentially bind-mount an entire /usr/local/bin tree with package management tools that need not be present in the resulting image - yielding a smaller, and more secure image - as if and when compromised, a hacker won't have easy access to your package management system to load the tools he needs to compromise the host.\n\nUse case 2:\nYou're on a terrible 3G connection, and you'd like to work on Dockerfiles in an offline enviroment, without the added burden of running your own DNS + Webserver + pristine offline copy of the whole online repo.\n\nMotivation:\nThe current approach throws sand in the face of anyone trying to be a constructive citizen in the Docker commnity, from the confines or poor or offline connectivity... and forces them to use their own hacked-together solutions. \n\nYou don't need to search far to find at least 5 much uglier solutions than the one proposed, that people are actively promoting. Those in secure/offline environments are left with no choice but to manually construct docker images from tar commands and shell files, while the whole Dockerfile infrastructure goes complete unused as it has no use to them, and consequently wouldn't see any contributions from them, unless the above could be supported. \n\nAlternately, the same could be accomplished had \"docker build\" supported the -v option from \"docker run\". \n"},{"labels":[null,"enhancement1"],"text":"Currently the `docker build` process runs under the docker daemon's cgroup, which means it has access to potentially unlimited CPU, Memory, and Disk bandwidth.  We would like to be able to constrain a docker build to a limited set of resources (similar to containers being run via `docker run`).  While today `docker build` currently runs with fairly high privileges, as things like user namespaces are added we will be increasing the security to where ordinary system users will be able to docker build more frequently.  \n\nI can think of two main user scenarios around constraining the resources a build can use:\n1. I'm a user on the command line, and my administrator allows me to docker build on top of images that are not running as the root user.  My administrator would like my CPU, memory, and disk usage to be governed by the same limits as my normal user account.\n2. I'm hosting docker builds for my company via an API, and I want to limit the resources that any individual build gets so that I can ensure aggressive or malicious builds don't cause the whole server to fall over.\n\nThe first use case would be that ideally docker build uses the cgroup the client PID is under to also run the build - resources are then shared between my session and the build, and are accounted for under my cgroup structure.\n\nThe second use case is a bit broader - the implementer of the hosting API might want to specify the exact cgroup based on attributes of the user, or simply have all docker builds default to a specific cgroup template.\n\nThere might be three increasing complexity features:\n1. On daemon startup, provide a flag for a cgroup template (or other resource spec) which all docker builds should be contained under\n2. Allow a cgroup or cgroup template (or resource spec) to be provided as an API argument on the build API, to limit the resources available for execution\n3. Have the docker build CLI automatically send (or alternatively fetch via the unix socket credentials) the current cgroup of the CLI process (or default resource spec of the user calling the command) and use that as the default value to the daemon.\n\nIn our case, we're trying to host docker builds in a reasonably resource constrained fashion, and moreover, to do it from a container (which already has cgroup limits set).  We'd prefer for the build we run to inherit the limits of the caller so that it's easy to reason about the limits of the build (the \"builder\" container gets X CPU, so the docker build simply shares that).\n\nDo others have a similar need or see the benefit in a feature like this in docker build?  We wanted to get feedback before pulling together a pull request.\n"},{"labels":["enhancement1"],"text":"Now that #10080 is in, we're requesting something like --pid=container:id -- we already support --net and --ipc in this mode.\n\nThe use case is Kubernetes concept of pods - tightly coupled containers that work together.  We believe that containers in a pod should be able to signal each other, for example.\n"},{"labels":[null,null,null,"enhancement1"],"text":"I use the Docker API, and when I run this request, the tags doesn't return.\n\n```\nGET /images/{id}/json\n```\n\nThis information is given when I list all images.\n\nWhat's the best practices to get image's tags information ?\n"},{"labels":["enhancement1"],"text":"This is a continuation of a request & discussion in #6687\nThe primary request is to allow disabling the mount namespace inside a container. The use case is so that one container can mount a volume (real volume, not a docker volume), and then use docker volumes to share the mount with another container.\nThere are a few cases where you might want to do this.\n- The most obvious use case is when your container doesn't have the utilities necessary to perform the mount (`mount.*` missing, FUSE daemon missing, etc). You could bake the tools into the application image, but keeping them separate lets the application image remain portable.\n- A related use case is so that you can run a FUSE daemon in it's own container.\n- Another use case is if you want to share this filesystem with multiple containers simultaneously.\n\nCurrently if you mount something on top of a docker volume inside a container, and you share that volume with another container, the mount appears empty.\n\nSince the processes inside the container need to see their root as the root of the image, we can instead use a simple `chroot` when the mount namespace is disabled.\n"},{"labels":["enhancement1"],"text":"The idea here is piped|configs.\n\n--labels=/usr/bin/generate_json_or_yml_nvp_thunk.sh |\n\nThis is nice because different folks could create their own script files to detect:\n\ncluster/rack awareness\ngpu capabilities\nover provisioning mechanics ...\n\ndiscussion can be found here: https://github.com/docker/swarm/issues/239\n"},{"labels":["enhancement1"],"text":"Gccgo build for non x86 arch may require changes to Dockerfile that makes docker build difficult from upstream for all archs.\nTo enable the build, one idea is to have\n1.  Default directory for Dockerfile for the archs that do not need any changes (hack/arch/default)\n2. Whichever archs need separate Dockerfile can have hack/arch/<myarch>/Dockerfile\n3. Corresponding changes to Makefile to copy the Dockerfile from appropriate place.\n\nNote: [ my early effort was to not move the current Dockerfile and give \n docker build -t \"$(DOCKER_IMAGE)\"  <path-to-archspecific-dockerfile>\nbut above idea failed since built image will fail to have hack/dind wrapper\n"},{"labels":["enhancement1"],"text":"## Proposal: Network Driver\n\n**THIS PROPOSAL IS A WORK IN PROGRESS**\n\nThis proposal brings new functionality and new interfaces to Docker's\nnetworking implementation. Some things are still in flux, but our goal is to\nget this approved as there is a sizeable effort internally to make this happen.\nWe need reviewers and comments on this proposal from the community.\n\nYou can see some of the work [here](https://github.com/docker/docker-network)\nbut this is a work in progress, and its implementation (and the maturity of it)\nshould not reflect the state of this document.\n### Concepts\n\nMany concepts will be used throughout this and several proposals that are in\nprocess.  You may see these repeated with slightly different verbiage or may\ncontain at-length descriptions of how these interfaces will be implemented.\n\nHere's a basic diagram about how the networking itself operates:\n\n![network extensions diagram](https://cloud.githubusercontent.com/assets/18923/5805948/d36826f4-9fc8-11e4-848b-13d28a23a4cf.png)\n\nThe following includes a description of the interface and its name. The\nsub-bullets provide practical examples of these components with the existing\nimplementation of Docker as an example.\n- Driver: executable code which is triggered when certain operations are\n  requested.\n- Extension: A collection of Drivers that supply different portions of\n  functionality to Docker.\n- State: A key/value store of parameterized data for consumption and mutation by a Driver.\n  - [libpack](https://github.com/docker/libpack) is our initial state\n    implementation, but we are also evaluating\n    [ecc](https://github.com/socketplane/ecc) and others.\n- Sandbox: An isolated environment \n  - libcontainer's functionality -- for now, this is more or less a standard docker container.\n- Endpoint: An addressable endpoint used for communication over a specific network. Endpoints join exactly one network and are expected to create a method of network communication for a container. Endpoints are garbage collected when they no longer belong to any Sandboxes.\n  - veth pair\n- Network: A collection of endpoints that are able to communicate to each other. These networks are intended to be isolated from each other and do not cross communicate. Networks house endpoints which can communicate with each other.\n  - Our ethernet bridge, iptables rules we use.\n### Container Network Model (or CNM)\n\nThe container network model is a few axioms about how docker wishes to supply\ninteroperation between networks and containers.\n1. All containers on a specific network can communicate with each other freely.\n2. Multiple networks are the way to segment traffic between containers and\n   should be supported by all drivers.\n3. Multiple endpoints per container are the way to join a container to\n   multiple networks.\n4. An endpoint is added to a sandbox to provide it with network connectivity.\n\nThis has a few consequences:\n- Network-based service discovery will replace on-disk and ENV discovery. This\n  allows discovery to be scoped and more flexible for external implementations.\n  Implementation of this is still TBD.\n- Links will be deprecated or only exist on the default network.\n### Notion of a Default Network\n\nSince Docker is a tool heavily used by both operations and development\npersonnel to different goals respective to their skill sets, it is critical to\nhave a functioning \"out of the box\" implementation for people to use with ease.\nDocker will create a \"default\" network (named `default`) for the use of basic\nnetworking.\n### Networks and Endpoints\n\nEndpoints are a part of a Network. The network is (at least in the simplebridge implementation) isolated, but drivers may implement the notion of a network however they choose.\n\nDocker's new system will allow for N networks, but a 'default' network will be\ncreated as a convenience for users, and with the default driver it will\nfunction similarly to the existing network solution now.\n\nMultiple endpoints can be created for a single container, and bound to them at\nthe same time. The endpoints may live on different networks and may all belong\nto one container.\n\nAgain, Endpoints as a part of different networks should not be able to communicate with each other in the default implementation. It's expected that network operators would program any bridging between two networks.\n### Workflow for a Network Driver\n\nAt boot, a network driver will be given a replay of its state; this will allow\nthe driver to return to being in sync with the state of the system, to create\nnew networks, etc. How replays are handled by drivers is intentionally\nundefined.\n\nA network can be requested to be created. In the workflow below, the network\nassumes to be created already.\n\nA network driver will be asked at `docker run` (in order, for a given network):\n1. To create an endpoint within the network\n2. To join an endpoint to a sandbox\n\nThe driver will also provide port mapping/expose functionality (see below for\nAPI) and communicate with service discovery (TBD).\n### Network API\n#### Driver abstract interface:\n\n``` go\ntype Driver interface {\n  // Restore from state\n  Restore(netstate state.State) error\n\n  // Create a new network with the network id and supplied parameters\n  AddNetwork(netid string, params []string) error\n\n  // Remove a network using the network ID.\n  RemoveNetwork(netid string) error\n\n  // Retrieve a network by ID.\n  GetNetwork(id string) (Network, error)\n\n  // List returns the IDs of available networks\n  ListNetworks() []string\n}\n```\n#### Network abstract interface:\n\n_(note that Link and Unlink here are merely for convenience and do not require\nan alternative implementation)_\n\n``` go\n// A network is a perimeter of IP connectivity between network services.\ntype Network interface {\n  // Id returns the network's globally unique identifier\n  Id() string\n\n  // List of endpoints that belong to this network.\n  Endpoints() []Endpoint\n\n  // Link makes the specified sandbox reachable as a named endpoint on the network.\n  // If the endpoint already exists, the call will either fail (replace=false), or\n  // unlink the previous endpoint.\n  //\n  // For example mynet.Link(mysandbox, \"db\", true) will make mysandbox available as\n  // \"db\" on mynet, and will replace the other previous endpoint, if any.\n  //\n  // The same sandbox can be linked to multiple networks.\n  // The same sandbox can be linked to the same network as multiple endpoints.\n  Link(s sandbox.Sandbox, name string, replace bool) (Endpoint, error)\n\n  // Unlink removes the specified endpoint, unlinking the corresponding sandbox from the\n  // network.\n  Unlink(name string) error\n}\n```\n#### Endpoint abstract interface:\n\n``` go\n// An endpoint represents a particular member of a network, registered under a certain name\n// and reachable over IP by other endpoints on the same network.\ntype Endpoint interface {\n  // The name of the endpoint.\n  Name() string\n\n  // Expose a port over the network. Publish it as the port to the host if\n  // requested.\n  Expose(portspec string, publish bool) error\n\n  // The network this endpoint belongs to.\n  Network() Network\n}\n```\n### `docker net` tool\n\n`docker net` is the vehicle we're pushing to manipulate networks. The\nbasic commands are described below:\n- `create`: create a network\n- `destroy`: destroy a network\n- `join`: join a container to a network\n- `leave`: remove a container from a network\n\nThere will be a forthcoming UI extension to `docker run` which also selects a\nnetwork at run time. This interface is currently to be determined.\n### Our initial implementation\n\nOur implementation is very similar to docker's existing implementation, which\nwe are dubbing `simplebridge`. This driver creates a bridge for each network,\nand a veth pair for each endpoint. Networks may contain a set of vxlan peers\nwhich will be attached to the bridge to ensure network connectivity to get\nmulti-host links.\n"},{"labels":[null,null,"enhancement1"],"text":"Sometimes you have really important containers, like data only containers, that you don't want accidentally deleted.  `docker rm -f $(docker ps -qa)` is just too easy to type sometimes and then, \"oh !@#!\", it's all gone.  (Well not really because you can find your data in `/var/lib/docker/vfs`).  Anyhow, I propose we add the following:\n\n`docker run/create --lock=true ...`\n`docker lock CID`\n`docker unlock CID`\n\nIf one was to do `docker rm CID` and CID was locked it wouldn't delete and exit with an error code.  Start/stop/restart should still work.  This flag is the equivalent of EC2's `DisableApiTermination` flag.  In order to delete a locked container you must first run `docker unlock CID`.\n"},{"labels":["enhancement1"],"text":"Every Image comes up with different tag name with their respective versions or with updated Container image.So can we have Tag variable  to a Dockerfile to get the Exact image instead getting all the images associated.\n\nExample\n\nDockerfile\n\n```\nFROM dockerfile/ubuntu:<VERSION>\n```\n\n```\ndocker build  --tag--version 12.04\n```\n"},{"labels":["enhancement1"],"text":"From a discussion on the docker-user mailing list:\n\nIt would be nice if the Dockerfile could be included in the image itself.\n- it could be used by docker hub to display the source of the image\n- if you pull an image from some other registry it would give you some idea of how the image was built\n\n@erikh raised the concern that this could be bad for images with secrets.\n\nThis could be implemented as a new flag to `docker build` which would optionally allow you to include the file. That way any image with secrets could omit the flag, for the old behaviour.\n\n`docker build --include-dockerfile` or something like that?\n"},{"labels":["enhancement1"],"text":"## Issue statement\n\nVolumes today can come in different flavors, but always with the underlying idea of _persistent data_. The recent submission of a `--tmpfs` PR (#9586) raised interesting questions:\n- Is `tmpfs` meaningful on other systems?\n- Should this be related to volumes somehow?\n\nThis proposal only aims at offering a place to discuss this particular aspect of volumes management and hopefully providing UX input for #8484.\n## Proposal\n\nA volume should be about **granting special properties to a directory inside a container**. That special meaning depends on the nature of the volume being mounted:\n- Persistent\n- Bound to a host directory\n- Ephemeral\n- ...\n\nThe key aspect of the proposal is that it allows to discuss _properties_ rather than _particular implementation_: `tmpfs` doesn't exist on some systems while the notion of ephemerality is universal.\n## Proposed UX\n\nWhat is true for **all volumes types**:\n- Volumes are managed with the set of top-level commands specified in #8484\n- The `docker volume create` command has a `--type=(persistent|host|ephemeral|...)` argument\n- Volumes are mounted using `docker run -v <volume_name_or_id>:/path/in/container/[:mode]`\n- Volumes can be created at container run time using `docker run` command line arguments:\n  - Type of the new volume is specified with `--volume-type=(persistent|host|ephemeral|...)`\n  - Name of the new volume is specified with the optional `--volume-name` flag\n\nEach type of volume might require **specific parameters** to be passed at creation time (such as the host path in case of a host-bound volume):\n- For `docker volume create` args are appended as colon separated values to the `--type` flag\n- For `docker run` command args are appended as colon separated values to the `--volume-type` flag\n- In both cases, this list of positional arguments is passed to the volume strategy for interpretation\n## Examples\n\nThe two following examples are equivalent:\n\n```\n$ docker volume create --type=host --host-path=/path/to/host/dir/ --name bidule\n$ docker run -v bidule:/path/in/container:ro -ti busybox\n```\n\n```\n$ docker run -v /path/in/container:ro --volume-name=bidule --volume-type host:/path/to/host/dir/ -ti busybox\n```\n\nPing @cpuguy83 @crosbymichael @shykes\n"},{"labels":["enhancement1"],"text":"I raised a discussion here\n\nhttps://groups.google.com/forum/#!topic/docker-dev/AlbF1ij5oY0\n\n<code>docker commit</code> command accept a <code>-m</code> parameter which allow users to add some messages to their image, just like which people would do while commit their code.\n\nBut anyway these messages are not read out when a user issue a <code>docker history</code> command to that image. \n\nBy inspecting into source code, I found these messages were surely stored with images, so I made a patch to fetch them out. \n\n``` patch\nroot@966e1e937644:/go/src/github.com/docker/docker# git diff 739d917..HEAD\ndiff --git a/api/client/commands.go b/api/client/commands.go\nindex a96089b..04d0496 100644\n--- a/api/client/commands.go\n+++ b/api/client/commands.go\n@@ -1036,7 +1036,7 @@ func (cli *DockerCli) CmdHistory(args ...string) error {\n\n\n        w := tabwriter.NewWriter(cli.out, 20, 1, 3, ' ', 0)\n        if !*quiet {\n-               fmt.Fprintln(w, \"IMAGE\\tCREATED\\tCREATED BY\\tSIZE\")\n+               fmt.Fprintln(w, \"IMAGE\\tCREATED\\tCREATED BY\\tSIZE\\tCOMMENT\")\n        }\n\n        for _, out := range outs.Data {\n@@ -1055,7 +1055,8 @@ func (cli *DockerCli) CmdHistory(args ...string) error {\n\n                        } else {\n                                fmt.Fprintf(w, \"%s\\t\", utils.Trunc(out.Get(\"CreatedBy\"), 45))\n                        }\n-                       fmt.Fprintf(w, \"%s\\n\", units.HumanSize(out.GetInt64(\"Size\")))\n+                       fmt.Fprintf(w, \"%s\\t\", units.HumanSize(out.GetInt64(\"Size\")))\n+                        fmt.Fprintf(w, \"%s\\n\", out.Get(\"Comment\"))\n                } else {\n                        if *noTrunc {\n                                fmt.Fprintln(w, outID)\ndiff --git a/graph/history.go b/graph/history.\ngo\nindex 2030c4c..853d6c6 100644\n--- a/graph/history.go\n+++ b/graph/history.go\n@@ -36,6 +36,7 @@ func (s *TagStore) CmdHistory(job *engine.Job) engine.Status {\n\n                out.Set(\"CreatedBy\", strings.Join(img.ContainerConfig.Cmd, \" \"))\n                out.SetList(\"Tags\", lookupMap[img.ID])\n                out.SetInt64(\"Size\", img.Size)\n+                out.Set(\"Comment\", img.Comment)\n                outs.Add(out)\n                return nil\n        })\n\nroot@966e1e937644:/go/src/github.com/docker/docker#\n```\n\nWith this patch you can have a COMMENT column in the output of <code>docker history</code> command.\n\n``` shell\nroot@localhost docker]# ./docker history 2ac9d1098bf1\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n2ac9d1098bf1        12 weeks ago        /bin/bash                                       241.4 MB            Add Git and SVN\n88d178232033        12 weeks ago        /bin/bash                                       186 MB              Add JDK1.7\n4e5e740790d1        3 months ago        /bin/bash                                       535 MB              \n88b42ffd1f7c        5 months ago        /bin/sh -c #(nop) ADD file:1fd8d7f9f6557cafc7   373.7 MB            \nc69cab00d6ef        5 months ago        /bin/sh -c #(nop) MAINTAINER Lokesh Mandvekar   0 B                \n511136ea3c5a        18 months ago                                                       0 B                 Imported from -\n\n\n```\n"},{"labels":[null,"enhancement1"],"text":"I want use docker -c cpushares ，but I have a  problem，it is cpu numa  architecture。\n\nwhat is cpu numa  \n\n```\nhttp://en.wikipedia.org/wiki/Non-uniform_memory_access \n```\n\nit means if docker cupshares  maybe  stretch over NUMA node  lead to cpu  performance  decrease\n\nI don’t use --cpuset=“” ,because it is not  flexible\n\n```\nWhat should I do？\n```\n\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                24\nOn-line CPU(s) list:   0-23\nThread(s) per core:    2\nCore(s) per socket:    6\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 62\nStepping:              4\nCPU MHz:               2600.024\nBogoMIPS:              5205.39\nVirtualization:        VT-x\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              15360K\nNUMA node0 CPU(s):     0-5,12-17\nNUMA node1 CPU(s):     6-11,18-2\n"},{"labels":[null,"enhancement1"],"text":"This is relevant for a container which wants to run some network service as well as an avahi daemon, for example:\n\nhttps://github.com/pmoust/squid-deb-proxy/blob/master/Dockerfile\nhttps://github.com/pmoust/squid-deb-proxy/blob/master/start.sh\n\nObviously the avahi daemon could be run outside the container (volume-in the services folder), or in a separate avahi-only container, or I could run all of this on a bridged pipework interface, but for my circumstances, I'd prefer to have a simple, standalone container which advertises its services using its own instance of the daemon.\n\nSince avahi-daemon uses SO_REUSEPORT, I believe this would be possible if docker had an option to also use it.\n"},{"labels":[null,"enhancement1"],"text":"This is a request for a docker build feature to be able to override on the commandline what is defined in the Dockerfile for FROM.  The driving use case is primarily in support of creating hotfixes, along the lines of the git-flow model of hotfixes:\n\nhttp://nvie.com/posts/a-successful-git-branching-model/\n\n...where we need to revert to an older version of a base image but build with new code.\n\nFROM lines in a Dockerfile are rarely pinned to version numbers, and therefore typically infer \":latest\" as their version identifier.  There are times when we identify bugs in the current releases of those base images while still needing to create interim build artifacts.  Alternatively, we have to create special releases of a base image to address an issue for some period of time (my/baseimage:latest v. the/baseimage:latest).  In all of these cases it is desirable to not modify Dockerfile source materials but simple invoke different builds and tests.\n\nThis could of course also be used to change support structures around an image build also.  For example you could have publicly shared Dockerfile repos specifying \"FROM centos\" and locally override builds to use \"--from rhel\".\n"},{"labels":[null,"enhancement1"],"text":"While trying to expose an incoming SCTP port for my container, I found out that something like `-p x:y/sctp` is not yet supported. It would be great to have the flexibility of exposing ports for any protocol for a container, although SCTP currently has the highest priority on my wish list.\n"},{"labels":[null,"enhancement1"],"text":"what we have:\ndocker has a great feature: ONBUILD\n\nwhat we need:\nhow one can be sure that the dockerfile is working?\n- build success\n- test success!\n\nhow to do it:\n- implement a new keyword: ASSERT or TEST or ONVISIT or what-ever\n- this command should give the client all the answers he ask from his dockerfile.\n\nbenefits:\n- no need to actually test the dockerfile locally or build a new testing-system \n\ngod bless\n"},{"labels":["enhancement1"],"text":"Hi all,\n\nI just wanted to say that I strongly wish all the containers that were last running would restart on the next boot by default.\nYes, there is the auto restart flag, but would rather like them auto-restart as the \"default\" and not have to add the autorestart flag to run the containers. \nThis has bitten me quite many times in the past and I just don't see how having auto-restart disabled as default makes Docker any more secure (I'd think this makes it more insecure due to being more accident prone).\n"},{"labels":[null,null,"enhancement1"],"text":"When you use `docker build` and don't tag it with the `-t` option, it will get built and given no tag. When developing an image, you end up having to put the image name and version in a separate build file as a wrapper around calling `docker build`.\n\nA nice feature would be automatically getting a default tag from a file in the context similar to the `.dockerignore` file. If no tag is specified on the command line, docker could read the tag from this file as a default.\n\nI propose that this file be called `.dockertag` and it should just be a simple one-line file with the tag name.\n"},{"labels":[null,"enhancement1"],"text":"Currently we can only inspect a single image or container. It would be great as part of the listing of all images or containers to be able to specify a parameter, something like '/images/json?inspect=true' which will return all images/containers in their 'inspected format' instead of the summary we get now. Thoughts?\n"},{"labels":["enhancement1"],"text":"At the beginning of Dockerfile I have ADD addedfile. In a git repository I have two branches: A and B. In each branch I have a different version of addedfile. Each time I switch branch from A to B and then back to A, Docker detects a change in file metadata and rebuilds the image (which takes 15 minutes).\n\nHow can I ignore metadata changes?\nIs there any quick fix or workaround for this?\n"},{"labels":[null,null,"enhancement1"],"text":"It would be nice if \"docker images -f\" could also filter by images that were not pushed to the repository after its creation whether by commit, build or import / load.\n\n``` sh\ndocker images -f 'not-pushed=true'\n```\n"},{"labels":[null,null,"enhancement1"],"text":"docker refuses to authenticate to a private registry over http even with the `--insecure-registry` enabled.\n\n```\n# docker push localhost:8000/scratch\nThe push refers to a repository [localhost:8000/scratch] (len: 1)\nSending image list\nPushing repository localhost:8000/scratch (1 tags)\n\n2014/12/08 16:42:12 HTTP code 401, Docker will not send auth headers over HTTP.\n```\n"},{"labels":[null,"enhancement1"],"text":"The purpose here is to speed up build iterations, especially when working with large contexts ('large' means hundreds of MB, possibly a GB).\n\nThere are some ways to mitigate the problem such as #6579 (.dockerignore to prevent uploading unnecessary files) and #5369 (to avoid copying source/intermediate files into the resulting image), but the next bottleneck is the transfer of the useful context itself. The current implementation requires to upload and process it completely for every build, even if only few files were changed since the previous build.\n\nThere have been several suggestions to mount an external volume at build (#8394 #1191 #8757), but they raise security and reproductibility issues. \n# Proposal\n\nThe change consist of updating the `POST /build` operation to allow sending the context in two steps. The first step would only include the files metadata and the sha256 sum of their content. The second step would provide the actual content of files (if requested by the server).\n\nA build could take place as follows:\n1. The client computes the sha256 sum of every regular file present in the context\n2. The client sends its `/build` request. The query includes an option to enable server-side caching (eg. `cache_context=true`). The body is a 'light' tarball of the context in which the content of regular files is replaced with their sha256 sum, except the Dockerfile which is transferred as is.\n   \n   ```\n   POST /build&cache_context=true HTTP/1.1\n   Content-Type: application/tar\n   \n   [...context tar archive, but with the content of regular files replaced\n    with their sha256 sum...]\n   ```\n3. In the response, the first chunk sent by the server includes a \"miss\" item listing all files that are missing from the cache. In case of 100% cache hits, the \"miss\" must be present and empty. Then the build proceeds normally.\n   \n   ```\n   HTTP/1.1 200 OK\n   Content-Type: application/json\n   \n   {\"miss\": [ ...list_of_missing_sha256_keys... ]}\n   {\"stream\":\"Step 1...\n    ...}\n   ```\n4. The client parses the response.\n   - If the first chunk does not include a \"miss\" item, then the client must assume that caching is not supported. It must abort build and restart a new build with caching disabled.\n   - If the \"miss\" item is empty, nothing particular happens.\n   - If the \"miss\" item is not empty, then the client creates a tarball with the missing files indexed by their sha256sum and  opens a second HTTP connection to upload it. In the tarball, the files should be stored in the same order as in the list sent by the server.\n     \n     ```\n     POST /build/upload HTTP/1.1\n     Content-Type: application/tar\n     \n     [...tarball of the missing files indexed by their sha256 sum...]\n     ```\n5. The server must verifiy the sha256 sums of the files before adding them to the cache, and then performs the build.\n   \n   ```\n   HTTP/1.1 200 OK\n   ```\n\nThis change would allow several optimisations:\n- once a file has its sha256 sum validated by the server, it can be cached for subsequent builds.\n- when launching a new build with a slightly modified context, only the updated files need to be uploaded\n- ADD/COPY of cached files will not require re-hashing on server-side (the server can trust its internal cache)\n- if the storage driver is 'btrfs', all server-side copies (CACHE->CONTEXT and CONTEXT->CONTAINER) can rely on the copy-on-write abilities of the filesystem (cp --reflink)\n- in case of a rebuild, hashing files should be faster on client-side since they are likely still cached in memory by the OS (if needed it is even possible to cache the sums on client-side and re-hash the files only if their mtime is changed)\n- the server can start the build eagerly, before the context is fully uploaded\n- the server can parse the Dockerfile eagerly and build the list of cache misses accordingly:\n  - so that files needed first are uploaded first\n  - so that unused files are not uploaded at all\n\nImplementation considerations:\n- before adding a file into the cache, the server MUST verify the sha256 sum (otherwise this would be a security hole)\n- the proposal does not address how the cache is managed. As for now we can just assume it is a basic LRU cache hosted by the docker engine. Sharing cache with a cluster of daemons or with an external storage facility is out of the scope.\n- the size of the cache should be configurable at runtime (eg. 'docker -d --context-cache=1G') the default should be rather small\n- instead of replacing the content of files with their sha256 sum in the tarball, it could be better to include the list of cached files as a file inside the archive, with a special name (eg. .docker-cached-files), the question is open\n"},{"labels":[null,null,null,"enhancement1"],"text":"Currently there is no way to attach to an existing exec. If I run `docker exec -di my_container bash`, there is no way to attach to this process, using the command line or the API.\n\nWhat are the steps needed to have such a feature implemented (maybe `docker exec attach :exec_id`)?\n"},{"labels":[null,"enhancement1"],"text":"Hi,\n\nI've been trying to create a pull request to add a new flag to the `docker ps` command. I was trying to add `-c/--colors` to show:\n- red for the exited containers.\n- green for the running ones.\n- yellow for the restarting.\n\nThe PR was kinda easy, but the problem is that `text/tabwriter` from the Golang standard library doesn't seem to allow the special chars needed, for example: `\\x1b[0m`. With \"doesn't allow\" I mean that the colors are added to the output, but the length of them is added as well and those ANSI codes are invisible which makes the output look weird:\n\n![Imgur](http://i.imgur.com/ktcJqKu.png)\n\nPerhaps this is going to be an issue in the future and the library should be changed? I couldn't manage to escape the lengths with the current situation and I don't think that it's possible (I double checked with some people and they agree). If you think that there is a solution without changing the library or that the change doesn't worth a change of library, please, let me know.\n"},{"labels":["enhancement1"],"text":"Hello,\n\nBecause Docker creates its own firewall rules, a restart of firewall managing service means, that all previously added Docker firewall rules are flushed. Would it be great if Docker knew how to react to it and recreate the rules again ?\n\nMost distributions use iptables service, but for example RHEL-7 and Fedora use Firewalld [1] service/daemon by default. It has a D-Bus interface and it's possible to detect that it's been restarted/reloaded.\n\nI already have some patches we can start with, I'll create pull request shortly.\nIt's my first go code so please have some patience with me.\n\nThank you !\n\n[1] http://www.firewalld.org/\n"},{"labels":[null,"enhancement1"],"text":"While starting a container, Docker internally does rootfs mount using AUFS. During rootfs preparation, a new read-write directory is created by default and this directory is NOT configurable. By making this directory configurable, one can create a read-write layer for the container, capturing all the changes to the container into a specified directory. This directory can either be part of the host VM's disk or could be on an attached device.\n\nThe following are the benefits offered by creating such a configuration:\n- Changes are directory agnostic, operates at a layer and captures entire container changes\n- Retains the directory structure as it gets created within the container\n- Since the changes are captured onto a mounted directory, data can be recovered even after container crash\n- If the mounted directory is an external device, data can be recovered even if the VM crashes\n\nTo achieve this, we propose \"-–volume-layer\" option to ‘docker run’ command  which enables docker to create a writeable (rw) container layer.\nThis enables docker to mount as shown below\n_mount –t aufs –o br=volume-layer-dir:ro-dirs. –o udba=reval none /rootfs_\n\nHow is this different from docker volumes (-v or --volume)?\nDocker volume enables mounting a local directory to a specific container directory. When there are multiple directories within a container to be externalized it becomes difficult to manage multiple volumes. Where as with volume layer all the changes are captured into the specified read-write directory.\n\nVolume layer and docker volumes can co-exist\n\nWe have been using this approach in WaveMaker Cloud (www.wavemaker.com/cloud), to be able to manage container data on our PaaS.\n"},{"labels":["enhancement1"],"text":"Docker events are limited as of now to untag and delete. For the purposes of inventory management as well as trouble shooting, it would be good to include additional events for image download as well as image updates. Ranjan\n"},{"labels":[null,"enhancement1",null],"text":"Docker Remote API already provides a WebSocket endpoint to attach to a container's tty with a WebSocket connection (`/containers/(id)/attach/ws` — although not documented).\n\nIt would be great to have this functionality for execs as well. It would let people attach to exec ttys with standard WebSocket libraries and totally avoid HTTP hijacking.\n"},{"labels":["enhancement1"],"text":"Similar to #7447, but different.\n\nRequest: the ability to add supplemental groups to a container without changing the current GID.\n\nThe use case is kubernetes pods with shared volumes - different containers in the pod can set different USER directives in their Dockerfiles, but they want to share on-disk files.  How we handle this internally is to allocate a GID per pod, and put all containers into that group, making the volume mode g+s for that GID.\n\nWould docker folks be amenable to a --extra-group flag (and requisite plumbing)?\n\n@vishh @jnagal @vmarmol \n"},{"labels":[null,null,"enhancement1"],"text":"Currently, one can specify --privileged and/or -v (volumes) for 'docker run'. It would be great if these options were also available for 'docker start'.\n\nThis would allow one to attach new volumes or 'host directories' to an already existing container.\n\nI searched the manual, but if I missed any option which already allows doing this, I apologize. Please let me know.\n"},{"labels":[null,"enhancement1"],"text":"It is too complex to set up authenticated https binding. Using ssh subsystem functionality would mean no additional authentication is needed. You can get std in and out and tunnel it to existing http calls, but both client and server have to understand this.\n"},{"labels":[null,"enhancement1"],"text":"Currently, as I understand it, specifying no --dns-search will result in Docker copying the paths from /etc/resolv.conf on the host into the container.\n\nIf you specify any --dns-search options then Docker will put just those into /etc/resolv.conf on the container.\n\nI suggest there should be a way to prepend the --dns-search options so they are searched first, but the ones from the host are still searched if those do not result in a match.\n\nMotivation: if the set-up on my host has a slew of search paths so programs I run can find services scattered across a large organisation, e.g.:\n\n```\nsearch london.internal.mycompany.com\nsearch internal.mycompany.com\nsearch mycompany.com\nsearch internal.mycompanysoldname.com\nsearch somecompanyweaquired.com\n```\n\nand when running in containers I want to add one more domain, e.g. containerservices.local\n"},{"labels":[null,"enhancement1"],"text":"# Volume based container interface spec\n\nThe overall goal is to define a standardized interface for applications and containers for communication, data persistence, providing configuration and more. Enabling further decoupling and modularization in accordance to the service oriented philosophy within Docker, where each individual tool should be decoupled.\n\nDefinitions:\n\n```\nBase Container = Container running the actual process/application\nHost Volume = Volume provided by host node\nData Volume Container = Container exposing plain Volume\nSide Container = Container exposing Volume with special logic\nHelper Container = Container providing helper services (logging, backup etc.)\nVaaS = Volume as a Service\n```\n\n---\n## 1. Specification\n### 1. Directory structure\n\nThe specification proposes \"/con\" as directory for container specifics. It is located under the root directory \"/\" to provide a common endpoint for all container needs. For each usage area an explicit subdirectory is proposed. The overall structure proposed in the first iteration is:\n\n```\n/con/proc [ro]\n/con/data [r+w]\n/con/log [r+w]\n/con/configuration [ro]\n/con/var [r+w]\n/con/context [ro]\n/con/secret [ro]\n/con/seed [ro] \n```\n\n**Defining \"/con\" as new directory for container specific things/volumes**\n### 2. Exchange method\n\nSpecifying a common way to exchange data is the first step in making containers and their data more modular and reusable. The basis of the interface rests on the UNIX feature \"Everything is a file\". Furthermore the method to inject these files is based on volumes. Each specific sub-directory of \"/con\" and if necessary sub-sub-directories have their own volume associated/mounted.  \n**Defining the interface as the injection of everything as a file via volumes.**\n### 3. Fallback\n\nFor ease of use and modularity each image has to be able to work with mounted volumes or the default directories. A MariaDB image for example does not need a mounted volume at \"/con/data\". In using the default directory instead of a volume that could be lost after a restart, therefore a message should be exposed to inform the user that his choice might have unwanted implications.  \n**Define the fallback as writing inside the container and exposing warnings.**\n## 2. Volume Providers\n### 1. One Container - simple - unrecommended\n\nBake everything into one container. This is greatly discouraged as it does not take decoupling into account.\n### 2. Host Volume - simple - unrecommended\n\nThe simplest solution to enable volumes are host volumes. Host directories can easily be mounted into a container via host volumes. This enables a fast working state for development setups. It is great for development, but relying on one specific host is not considered production ready and should not be used widely.\n### 3. Data Volume Container - simple\n\nWith Data Volume Containers more production ready step is provided. Data Volume Containers are minimal containers exposing plain volumes. This data is therefore versioned and can be easily deployed with docker tools. It is even easier to describe relationships with Kubernetes and other scheduling or deployment tools. It is still quite simple, but removes the drawback of being node specific. Using a private registry or decoupling credentials and sensitive data is encouraged and a standardized method will be provided within this proposal.  \n**Warning: Data Volume Containers could contain sensitive data. Pushing to a public registry should be considered carefully.**\n### 4. Side Container - advanced\n\nHost Volumes and Data Volume Containers are simple, but on the other hand they are quite inflexible. How they react and how they take in information is mostly static. For many use cases you want to define templates or logic instead of a static file.\nFor these more advanced, more dynamic use cases Side Containers are proposed. The same interface (\"Everything as a file\" + volumes) can be used, but they provide the ability for additional logic. Basically they are Data Volume Containers with added logic. They expose the same volumes, but to not provide the finished files within the image. They most likely contain code or a tool providing the ability for generation, storage or further logic within the Side Container. The resulting files are then exposed as volumes.\nThis enables the use of the same Base Container with different Side Containers. It basically makes it possible to switch from manual/static files from Data Volume Containers to custom or generic tools for generating files.\n### 5. VaaS - advanced + integrated\n\nWith the number of volumes comes more complexity. The interface will not grow more complex. Neither will the components themselves, but with the number of volumes the complexity of deployments is likely to increase. To reduce complexity and provide a common guide for service providers for closer integration Volume as a Service (VaaS) is proposed. \nWithin the VaaS offering Side Containers with platform specific logic could be provided. The first step for something like this is already done in Kubernetes with git based Volumes (kubernetes#1945). Providing VaaS either by providing bindings to different tools such as git or by providing platform specific Side Containers can reduce the deployment complexity.\n## 3. Areas of usage\n### /con/proc [ro]\n\nExposing container specific data at \"/con/proc\" as files in the way \"/proc\" is already doing it.\nIt enables applications inside a container to optimize itself for at runtime provided resources. Examples: cache size, java heap size etc. How this data is injected/provided is not in the scope of this proposal.\n\"/con/proc\" should use the same file format as \"/proc\" so it will not conflict with it later, when \"/proc\" is provided from the kernel. Symlinking it then would be a possible solution.\n### /con/data [r+w]\n\nAs defined storage location \"/con/data\" is available for persistent data. It can either use Data Volume Containers or Side Containers, which can use additional logic to provide a bridge between the exposed volume and persistent storage. This could be done by running a Ceph client or some binding code for different cloud storage providers. This not only enables the migration from simple persistent storage to distributed storage, but enables easy migration between different storage backends. Switch the Ceph Side Container with a Glusterfs Side Container for example and much more. \n### /con/log [r+w]\n\nThe \"/con/log\" endpoint is a data endpoint specifically for logs. It is available for writing logs or pipe them to another container. This could be accomplished via a simple Data Volume Container. Side Container could add additional logic, but the usual way would be to use the Data Volume Container from a Helper Container, which rotates and ships logs to a central log database.\n### /con/configuration [ro]\n\nProviding configurations for an application would use the \"/con/configuration\" endpoint. It should provide some defaults within the container and for production setup be mounted to a volume provided by Data Volume Containers (static files) or Side Containers (dynamic generation). Side Containers can bring additional logic with tools such as etcd, confd, etc.. These can be used to generate the configuration from more dynamic sources, but still use the same interface. This enables the use of the same Base Container with different Side Containers. It basically makes it possible to switch from manual configuration files, to etcd based ones, to consul based ones in a matter of minutes.\n\nFor tools relying heavily on ENVs an additional file is proposed to be located at \"/con/configuration/ENV\". It will be executed at runtime and provide the Base Container with ENVs.\n### /con/var [r+w]\n\nEnabling further use cases via a more general endpoint. \"/con/var\" can either be mounted as a single volume or be split into one volume for each subdirectory.\n## /con/context [ro]\n\n\"/con/context\" as standard place to keep docker specific scripts created during builds such as entrypoint scripts etc.\n### /con/secret [ro]\n\n\"/con/secret\" is available as endpoint for more sensitive data such as credentials, vpn-key, ssl-key, etc.. This volume should be stored in a memory backed location as sensitive information shouldn't be written do disk. Data is exposed decrypted at this endpoint. Using a Data Volume Container and no encryption is considered the simplest solution, but has security implications and therefore should not be used without careful consideration. Additionally Side Containers can expose decrypted data into \"/con/secrets\" and decrypt data with tools such as stoker, crypt, gpg, etc.. The ENV file located at \"/con/secret/ENV\" will be executed at runtime to create a bridge for ENV using applications.\nThe most advanced step would be to use a VaaS system including long term private key storage, short term token generation, authentication with token and finally providing the decrypted secret data as volume.\n### /con/seed [ro]\n\nFor decryption purposes and for more advanced secret distibution system \"/con/seed\" is reserved for a short lived token, a private key file or another seed mechanism. This seed can be used to request data to \"/con/secret\" in a VaaS scenario or be provided to the Side Container for decryption purposes.\n## 4. Other Concepts\n### Configuration distribution for Helper and Side Containers\n\nOnly one layer of Side Containers should be used. Configurations should be provided via static methods to Side and Helper Containers. This could be done via Data Volume Containers or VaaS.\n### Secret distribution for Helper and Side Containers\n\nDistributing secrets via \"/con/secret\" is only advisable at the base layer. Secrets for Side and Helper Containers could be delivered via the same volume. Only advisable to reduce complexity. For more elaborate setups, which try to separate credentials for each container, a VaaS based solution would be more sufficient.\n### Error/warning message example\n\nA message could be triggered, when a DEFAULT file is inside default non volume directory. One example for error messages generated from an entrypoint.sh would be:\n\n```\ndelimiter=\">>>\"\n\nif [ -a \"/con/configuration/DEFAULT\" ]; then\n    echo >&2 \"Default configuration directory in use.\"\n    echo >&2 \"Configurations can be provided via volume at /con/configuration.\"\n    echo >&2 \"mysqld configuration will be read from /con/configuration/my.cnf.\"\n    echo >&2 \"ENVs will be read from /con/configuration/ENV.\"\n    echo >&2 \"$delimiter\"\nfi\n\nif [ -a \"/con/secret/DEFAULT\" ]; then\n    echo >&2 \"Default secret directory in use.\"\n    echo >&2 \"Keys/secrets can be provided via volume at /con/secret.\"\n    echo >&2 \"Secure ENVs will be read from /con/secret/ENV.\"\n    echo >&2 \"$delimiter\"\nfi\n\nif [ -a \"/con/data/DEFAULT\" ]; then\n    echo >&2 \"WARNING: Non persistent data directory in use! Data loss on restart!\"\n    echo >&2 \"Please substitute /con/data with a volume.\"\n    echo >&2 \"Use a Data Volume Container or another method to mount a volume.\"\n    echo >&2 \"$delimiter\"    \nfi\n```\n\nThis would enable testing images, without the need to add volumes. Furthermore it gives the ability to work on specific volumes one by one until all needed volumes are integrated and can be launched in production. A more elaborate template for warnings has to be made, but this provide a reasonable foundation.\n\n---\n## Collaboration for usable official images\n\nWith this proposal the groundwork is layed to enable modular and interchangeable images. There needs to be a collaborative work to provide official Base Container images and official Side Container images. The detailed process and collaboration would blow the scope of this proposal and should be discussed in other forms.\n"},{"labels":["enhancement1"],"text":"I wish `docker create` were idempotent like `docker start`, and that `docker run` could also work this way somehow. What I want is to make sure a container is running: if it's not running, run it, and if it is running, just let it continue (and don't report an error).\n\nI attempted (poorly) to describe this on twitter:\n\nhttps://twitter.com/krarick/status/535300342317973505\n\nBasically I wish I could issue a single `docker run -d` command and have it act like `launchctl start`.\n\nI can do what I want with the current CLI:\n\n```\nset -e\n\nif ! docker inspect -f 1 foo &>/dev/null\nthen docker create --name foo bar\nfi\ndocker start foo\n```\n\nIdeally I would like to get the same behavior when I write:\n\n```\nset -e\n\ndocker create --idempotent --name foo bar\ndocker start foo\n```\n\nwhich could then be further condensed:\n\n```\ndocker run -d --idempotent --name foo bar\n```\n\nI'm just surprised this isn't a more common situation and that docker doesn't already do this conveniently in one command.\n\ncc @kelseyhightower\n"},{"labels":["enhancement1"],"text":"## Named remote volumes\n\nThis Proposal assumes that some variation on [`docker volumes` cli cmd](https://github.com/docker/docker/issues/8484)\nis merged.\n\nThis is also a follow on proposal for [fuse based Remote Shared Volumes](https://github.com/docker/docker/issues/7249)\n### UI changes\n\nAdd extra parameters to `docker volumes create`:\n\n```\n-s/--storage-driver=(*vfs*|vbox|rsync|nfs|sshfs|samba|fuse|andrewfs|*) - filesystem type to mount (vfs is what we use atm)\n-o/--options=\"mount options\"\n-p/--password - prompt for password\n--create - set up the share (some storage drivers may be able to create the share - like the vbox user share)\n```\n\nFor example:\n\n```\ndocker volumes create --name data --path //server/data --storage-driver smb --options \"rw,relatime,vers=1.0,sec=ntlmssp,cache=strict,username=sven,domain=SVENS-MAC-MINI,uid=1000,forceuid,gid=100,forcegid,addr=10.10.10.14,file_mode=0755,dir_mode=0755,nounix,noperm,rsize=61440,wsize=65536,actimeo=1\" --password\n> Enter smb password for `sven`:\n```\n\nTo define the current boot2docker Users share:\n\n```\ndocker volumes create --name home --path $HOME --storage-driver vbox --create\n# fuse based\ndocker volumes create --name home --path $HOME --storage-driver fuse --create\n```\n\nand then using the aliased volume \n\n```\ndocker run -d -v home:/home/sven ubuntu bash\n```\n\nWhat the user is doing, is setting up an alias to a possibly remote resource.\nWhen an alias is used, its cfg needs to be sent to the daemon, so it can\ninstanciate the volume, and persist that connection as well as possible.\n\nSome kind of restart policy needs to be worked out when the resource is\nnot available.\n### Remote Docker daemon volumes\n\nWhen the docker client detects that its talking to a non-local deamon, it can create the necessary volume on the fly for both `docker build` and `docker run`, choosing whatever reasonable defaults work for that platform. (`docker hosts` will give us more meta info to make the right choices)\n### Config store\n\nThe alias->type and settings needs to be stored both on the client side, and on any daemon that has containers that use it.\n\nThe config needs to be semi-secure, as it will container passwords in some \ninstances.\n### Possible implementation\n\nImplement storage drivers which mount into `/var/lib/docker/<driver>`. \n\nThe major change is to allow the Docker daemon to use more than one storage driver at any one time.\n\nFrom the little code reading i've done, the basic Storage driver could be enough, and I think we have to make the volume mounting into the container be recursive.\n## awesome side effect :)\n\nThis means that we won't be creating a new API for volumes to be coded against, we're using the existing one - so a [Gluster](https://github.com/docker/docker/issues/8881), [Ceph](https://github.com/docker/docker/issues/8854) driver can be implmented now, and will be accessible as a volume backend as soon as we implement the extra `/volume` API endpoints.\n"},{"labels":[null,"enhancement1"],"text":"Both me and a colleague came up against this recently. We had assumed that the `--env-file` that we passed to `docker run` would be equivalent format to an environment file that could be sourced in bash.\n\nHowever, currently it looks like it's just a split on the `=` ?\nhttps://github.com/docker/docker/blob/master/opts/envfile.go#L26-L27\n\nIt would be cool if we could have one file that both bash and Docker could interpret with the same quotes/escaping structure. Anyone got any thoughts?\n"},{"labels":[null,"enhancement1"],"text":"_Sorry for this not being a doc first proposal: I’d like to go through the design before proceeding with the documentation itself (it introduces a new magic file, hence probably a whole new documentation page)._\n### Issue statement\n\nDocker needs to provide a repeatable and well defined mechanism for repositories which builds into multiple image. Existing proposals:\n- #7284\n- #2112\n\nThese have been opened for some time, and have reached the point where tempers flaring and amount of noise overwhelms the signal. The thinking behind this new proposal was driven by the following remarks collected from previous debates:\n1. We want `docker build` to behave consistently, [without repository-specific knowledge](https://github.com/docker/docker/issues/2112#issuecomment-39763037)\n2. Through 1, we want to allow Docker Hub [automated builds](https://github.com/docker/docker/issues/2112#issuecomment-60597439) to use this new feature\n3. We do not want a repository from which _n_ images are built to contain [_n_ `Dockerfile`s at its root](https://github.com/docker/docker/issues/2112#issuecomment-47986440)\n4. We should provide an easy fix for this common problem, and it seems to me that new `Dockerfile` verbs are more difficult to introduce\n### Proposal\n\nWe introduce a new magic file at the root of the repository to act as the `Dockerfile` of all `Dockerfile`s:\n- It could be named `DockerfileIndex` (as I believe `Dockerfiles` is too error prone and confusing)\n- It has precedence over a regular `Dockerfile`, meaning that a `docker build` invoked in a directory containing both a `Dockerfile` and a `DockerfileIndex` interprets the `DockerfileIndex` only\n- Its content is a simple list of `Dockerfile`s' relative paths\n\n```\nDockerfile                  # Build the default Dockerfile located at the root\n./SubDirectoryA/Dockerfile  # Build SubDirectoryA\n./SubDirectoryB/Dockerfile  # Build SubDirectoryB\n```\n\nWith the above `DockerfileIndex`, the invocation of `docker build -t basename:tag .` at the root of the repository is equivalent to the following sequence of commands:\n\n```\ndocker build -t basename:tag .\ncd SubDirectoryA && docker build -t basename/SubDirectoryA:tag .\ncd SubDirectoryB && docker build -t basename/SubDirectoryB:tag .\n```\n\nNotes:\n- The first command only exists because the default `Dockerfile` is itself listed on the first line of the `DockerfileIndex` (there is nothing specific to this `Dockerfile` nor to the root)\n- With the V2 registry, an image name is not restricted to a “two-components” path anymore, so “username/image/subimage” is a valid identifier (and in our case, this scoping sounds reasonable)\n- The sequence of commands is provided only as a mean to illustrate the behavior: this doesn't imply that `DockerfileIndex` interpretation would occur on the client side with multiple `build` API calls\n- As the sequence of commands implies, each build context is scoped to the location of the `Dockerfile`: this is Docker current behavior, and I also strongly believe it is the least astonishing one.\n#### Different builds with a single context\n\nTo allow for multiple builds using a same context, we require multiple `Dockerfile` in a **single** directory: these different `Dockerfile` would be disambiguated using suffixes, such as `Dockerfile.A` and `Dockerfile.B`. Given `DockerfileIndex`:\n\n```\nDockerfile.A\nDockerfile.B\n```\n\nInvocation of `docker build -t basename:tag .` would result in images `basename/A:tag` and `basename/B:tag` to be created. Image name is scoped by `Dockerfile` location first and filename suffix second, which means that `SubDirectoryA/Dockerfile.Type1` build would produce image `basename/SubDirectoryA/Type1`.\n### Possible extensions\n\nThese probably doesn’t have to come in a first version:\n- Ability to trigger a specific sub-build from the `DockerfileIndex` location through the use of an additional `docker build` flag (`--only` was suggested).\n### History\n- 2014-17-11: add context specification, clarify cascading tag, promote \"same context builds\" to a first class use case\n"},{"labels":[null,"enhancement1"],"text":"I raised this discussion here \n\nhttps://forums.docker.com/t/command-to-remove-all-unused-images/20\n\nTo recap we often find ourselves with systems saturated with unused images, it would be super awesome if we had a simple way of cleaning up. \n\nEg:\n\n```\ndocker rmi --unused\n# remove all images that are not referenced by active containers\n```\n\nThis is tricky to script as you need to walk through dependencies, the workaround is just to ignore the errors from `rmi`, but it feels unclean. \n"},{"labels":[null,"enhancement1"],"text":"Hi all,\n\nI understand that docker re-mounts /proc for containers. I would like to add the hidepid=2 option to that (re)mount, but have not found any configuration option to do this. I understand I'd probably be able to do this in a privileged container, but that introduces too high a risk as a trade-off.\n\nIs there any way I can supply additional parameters to /proc being mounted in the container?\n"},{"labels":[null,"enhancement1"],"text":"Currently, an Image ID maps directly to a file system layer. It also is directly associated with the container config which was used to commit that file system layer and the config to use when starting a new container using this image. This is not ideal for various reasons:\n- binding an image's randomly generated ID to file system content rather than using content-addressable layers\n- creating unnecessary file system layers for every step while building an image (typically, only ADD/COPY/RUN/ONBUILD Dockerfile commands will produce file system changes)\n- Image history is constructed by following a chain of Parent IDs, requiring opening and reading a separate file for every layer.\n\nHere is the current image format as stored by Docker:\n\n```\ntype Image struct {\n    ID              string\n    Parent          string\n    Comment         string\n    Created         time.Time\n    Container       string\n    ContainerConfig runconfig.Config\n    DockerVersion   string\n    Author          string\n    Config          *runconfig.Config\n    Architecture    string\n    OS              string\n    Size            int64\n}\n```\n\nWhile I do not have any formal proposal yet, I've discussed various ideas with project maintainers and other members of the #distribution team. So far, our best idea is to separate the current `image.Image` struct type into 2 structs:\n\n```\ntype Image struct {\n    name struct {\n        repository string\n        tag        string\n    }\n\n    Created        time.Time\n    DockerVersion  string\n    Maintainers    []string\n    Architecture   string\n    OS             string\n\n    Config         *runconfig.Config\n\n    FSLayers []string  // Tarsums of file system layers\n    History  []string\n}\n\ntype FileSystemSnapshot struct {\n    ID              string  // Randomly Generated IDs which are specific\n    Parent          string  // to the storage driver on the local host\n\n    Checksum       string\n    ContentAddress string\n\n    Created         time.Time\n    ContainerConfig runconfig.Config\n    DockerVersion   string\n\n    VirtualSize     int64\n    LayerDiffSize   int64\n}\n```\n\nThe point of this issue is not to propose a solution, but to discuss issues with the current image format and discuss ideas for an alternative, next generation format to be used by the docker engine.\n"},{"labels":["enhancement1"],"text":"To support chunked or streaming tarsum calculations, it would be desirable to be able to save tarsum state mid-computation. This would allow a tarsum calculation to be saved, then resumed later when the rest of the data is available, similar to other kinds of hash calculations.\n\nThe main application for such a change would be to minimize local file buffering and storage layer round trips in the new version of the registry when accepting layer uploads.\n\nPossible API additions include the following methods:\n\n```\n// State is an opaque representation of a partial tarsum calculation.\ntype State <some trivially serializable type, maybe string>\n\n// ResumeTarSum continues a tarsum calculation on the provided reader using state. The \n// reader will Seek to the position stored in state and the computation will be resumed.\nResumeTarSum(rd io.ReadSeeker, state State) (TarSumInterface, error)\n```\n\nAnd add a state method to `TarSumInterface`:\n\n```\ntype TarSumInterface interface {\n    io.Reader\n    GetSums() map[string]string\n    Sum([]byte) string\n    Version() Version\n\n    // State returns an opaque state that can be used to resume a partial tarsum calculation. The\n    // returned state is valid even if the tarsum calculation reach io.EOF on the underlying reader.\n    State() State\n}\n```\n\n@vbatts @jlhawn \n"},{"labels":["enhancement1"],"text":"# Proposal: JSON Registry API V2.1\n## Abstract\n\nThe docker registry is a service to manage information about docker images and enable their distribution. While the current registry is usable, there are several problems with the architecture that have led to this proposal. For relevant details, please see the following issues:\n- docker/docker#8093\n- docker/docker-registry#612\n\nThe main driver of this proposal are changes to the docker the image format, covered in docker/docker#8093. The new, self-contained image manifest simplifies the image definition and the underlying backend layout. To reduce bandwidth usage, the new registry will be architected to avoid uploading existing layers and will support resumable layer uploads.\n\nWhile out of scope for this specification, the URI layout of the new API will be structured to support a rich Authentication and Authorization model by leveraging namespaces.\n\nFurthermore, to bring docker registry in line with docker core, the registry is written in Go.\n## Scope\n\nThis proposal covers the URL layout and protocols of the Docker Registry V2 JSON API. This will affect the docker core registry API and the rewrite of docker-registry.\n\nThis includes the following features:\n- Namespace-oriented URI Layout\n- PUSH/PULL registry server for V2 image manifest format\n- Resumable layer PUSH support\n- V2 Client library implementation\n\nWhile authentication and authorization support will influence this specification, details of the protocol will be left to a future specification. Other features marked as next generation will be incorporated when the initial support is complete. Please see the road map for details.\n## Use Cases\n\nFor the most part, the use cases of the former registry API apply to the new version. Differentiating uses cases are covered below.\n### Resumable Push\n\nCompany X's build servers lose connectivity to docker registry before completing an image layer transfer. After connectivity returns, the build server attempts to re-upload the image. The registry notifies the build server that the upload has already been partially attempted. The build server responds by only sending the remaining data to complete the image file.\n### Resumable Pull\n\nCompany X is having more connectivity problems but this time in their deployment datacenter. When downloading an image, the connection is interrupted before completion. The client keeps the partial data and uses http `Range` requests to avoid downloading repeated data.\n### Layer Upload De-duplication\n\nCompany Y's build system creates two identical docker layers from build processes A and B. Build process A completes uploading the layer before B. When process B attempts to upload the layer, the registry indicates that its not necessary because the layer is already known.\n\nIf process A and B upload the same layer at the same time, both operations will proceed and the first to complete will be stored in the registry (Note: we may modify this to prevent dogpile with some locking mechanism).\n### Access Control\n\nCompany X would like to control which developers can push to which repositories. By leveraging the URI format of the V2 registry, they can control who is able to access which repository, who can pull images and who can push layers.\n## Dependencies\n\nInitially, a V2 client will be developed in conjunction with the new registry service to facilitate rich testing and verification. Once this is ready, the new client will be used in docker to communicate with V2 registries.\n## Proposal\n\nThis section covers proposed client flows and details of the proposed API endpoints. All endpoints will be prefixed by the API version and the repository name:\n\n```\n/v2/<name>/\n```\n\nFor example, an API endpoint that will work with the `library/ubuntu` repository, the URI prefix will be:\n\n```\n/v2/library/ubuntu/\n```\n\nThis scheme will provide rich access control over various operations and methods using the URI prefix and http methods that can be controlled in variety of ways.\n\nClassically, repository names have always been two path components where each path component is less than 30 characters. The V2 registry API does not enforce this. The rules for a repository name are as follows:\n1. A repository name is broken up into _path components_. A component of a repository name must be at least two characters, optionally separated by periods, dashes or underscores. More strictly, it must match the regular expression `[a-z0-9]+(?:[._-][a-z0-9]+)*` and the matched result must be 2 or more characters in length.\n2. The name of a repository must have at least two path components, separated by a forward slash.\n3. The total length of a repository name, including slashes, must be less the 256 characters.\n\nThese name requirements _only_ apply to the registry API and should accept a superset of what is supported by other docker community components.\n## API Methods\n\nA detailed list of methods and URIs are covered in the table below:\n\n| Method | Path | Entity | Description |\n| --- | --- | --- | --- |\n| GET | `/v2/` | Check | Check that the endpoint implements Docker Registry API V2. |\n| GET | `/v2/<name>/tags/list` | Tags | Fetch the tags under the repository identified by `name`. |\n| GET | `/v2/<name>/manifests/<tag>` | Manifest | Fetch the manifest identified by `name` and `tag`. |\n| PUT | `/v2/<name>/manifests/<tag>` | Manifest | Put the manifest identified by `name` and `tag`. |\n| DELETE | `/v2/<name>/manifests/<tag>` | Manifest | Delete the manifest identified by `name` and `tag`. |\n| GET | `/v2/<name>/blobs/<digest>` | Blob | Retrieve the blob from the registry identified by `digest`. |\n| HEAD | `/v2/<name>/blobs/<digest>` | Blob | Check if the blob is known to the registry. |\n| POST | `/v2/<name>/blobs/uploads/` | Blob Upload | Initiate a resumable blob upload. If successful, an upload location will be provided to complete the upload. Optionally, if the `digest` parameter is present, the request body will be used to complete the upload in a single request. |\n| GET | `/v2/<name>/blobs/uploads/<uuid>` | Blob Upload | Retrieve status of upload identified by `uuid`. The primary purpose of this endpoint is to resolve the current status of a resumable upload. |\n| HEAD | `/v2/<name>/blobs/uploads/<uuid>` | Blob Upload | Retrieve status of upload identified by `uuid`. This is identical to the GET request. |\n| PATCH | `/v2/<name>/blobs/uploads/<uuid>` | Blob Upload | Upload a chunk of data for the specified upload. |\n| PUT | `/v2/<name>/blobs/uploads/<uuid>` | Blob Upload | Complete the upload specified by `uuid`, optionally appending the body as the final chunk. |\n| DELETE | `/v2/<name>/blobs/uploads/<uuid>` | Blob Upload | Cancel outstanding upload processes, releasing associated resources. If this is not called, the unfinished uploads will eventually timeout. |\n\nAll endpoints should support aggressive http caching, compression and range headers, where appropriate. Details of each method are covered in the following sections.\n\nThe new API will attempt to leverage HTTP semantics where possible but may break from standards to implement targeted features.\n### Errors\n\nActionable failure conditions, covered in detail in their relevant sections, will be reported as part of 4xx responses, in a json response body. One or more errors will be returned in the following format:\n\n```\n{\n    \"errors:\" [{\n            \"code\": <error identifier>,\n            \"message\": <message describing condition>,\n            \"detail\": <unstructured>\n        },\n        ...\n    ]\n}\n```\n\nThe `code` field will be a unique identifier, all caps with underscores by convention. The `message` field will be a human readable string. The optional `detail` field may contain arbitrary json data providing information the client can use to resolve the issue.\n\nThe error codes encountered via the API are enumerated in the following table:\n\n| Code | Message | Description | HTTPStatusCodes |\n| --- | --- | --- | --- |\n| `UNKNOWN` | unknown error | Generic error returned when the error does not have an             API classification. | Any |\n| `DIGEST_INVALID` | provided digest did not match uploaded content | When a blob is uploaded, the registry will check that    the content matches the digest provided by the client. The error may    include a detail structure with the key \"digest\", including the   invalid digest string. This error may also be returned when a manifest    includes an invalid layer digest. | 400, 404 |\n| `SIZE_INVALID` | provided length did not match content length | When a layer is uploaded, the provided size will be    checked against the uploaded content. If they do not match, this error    will be returned. | 400 |\n| `NAME_INVALID` | manifest name did not match URI | During a manifest upload, if the name in the manifest         does not match the uri name, this error will be returned. | 400, 404 |\n| `TAG_INVALID` | manifest tag did not match URI | During a manifest upload, if the tag in the manifest        does not match the uri tag, this error will be returned. | 400, 404 |\n| `NAME_UNKNOWN` | repository name not known to registry | This is returned if the name used during an operation is      unknown to the registry. | 404 |\n| `MANIFEST_UNKNOWN` | manifest unknown | This error is returned when the manifest, identified by          name and tag is unknown to the repository. | 404 |\n| `MANIFEST_INVALID` | manifest invalid | During upload, manifests undergo several checks ensuring           validity. If those checks fail, this error may be returned, unless a    more specific error is included. The detail will contain information    the failed validation. | 400 |\n| `MANIFEST_UNVERIFIED` | manifest failed signature verification | During manifest upload, if the manifest fails signature     verification, this error will be returned. | 400 |\n| `BLOB_UNKNOWN` | blob unknown to registry | This error may be returned when a blob is unknown to the         registry in a specified repository. This can be returned with a   standard get or if a manifest references an unknown layer during    upload. | 400, 404 |\n| `BLOB_UPLOAD_UNKNOWN` | blob upload unknown to registry | If a blob upload has been cancelled or was never       started, this error code may be returned. | 404 |\n\nWhile the client can take action on certain error codes, the registry may add new error codes over time. All client implementations should treat unknown error codes as `UNKNOWN`, allowing future error codes to be added without breaking API compatibility. For the purposes of the specification error codes will only be added and never removed.\n### API Version Check\n\nA minimal endpoint, mounted at `/v2/` will provide version support information based on its response statuses. The request format is as follows:\n\n```\nGET /v2/\n```\n\nIf a `200 OK` response is returned, the registry implements the V2(.1) registry API and the client may proceed safely with other V2 operations. Optionally, the response may contain information about the supported paths in the response body. The client should be prepared to ignore this data.\n\nIf a `401 Unauthorized` response is returned, the client should take action based on the contents of the \"WWW-Authenticate\" header and try the endpoint again. Depending on access control setup, the client may still have to authenticate against different resources, even if this check succeeds.\n\nIf `404 Not Found` response status, or other unexpected status, is returned, the client should proceed with the assumption that the registry does not implement V2 of the API.\n### Pulling An Image\n\nAn \"image\" is a combination of a JSON manifest and individual layer files. The process of pulling an image centers around retrieving these two components.\n\nThe first step in pulling an image is to retrieve the manifest. For reference, the relevant manifest fields for the registry are the following:\n\n| field | description |\n| --- | --- |\n| name | The name of the image. |\n| tag | The tag for this version of the image. |\n| fsLayers | A list of layer descriptors (including tarsum) |\n| signature | A JWS used to verify the manifest content |\n\nFor more information about the manifest format, please see [docker/docker#8093](https://github.com/docker/docker/issues/8093).\n\nWhen the manifest is in hand, the client must verify the signature to ensure the names and layers are valid. Once confirmed, the client will then use the tarsums to download the individual layers. Layers are stored in as blobs in the V2 registry API, keyed by their tarsum digest.\n\nThe API details follow.\n#### Pulling an Image Manifest\n\nThe image manifest can be fetched with the following url:\n\n```\nGET /v2/<name>/manifests/<tag>\n```\n\nThe \"name\" and \"tag\" parameter identify the image and are required.\n\nA `404 Not Found` response will be returned if the image is unknown to the registry. If the image exists and the response is successful, the image manifest will be returned, with the following format (see docker/docker#8093 for details):\n\n```\n{\n   \"name\": <name>,\n   \"tag\": <tag>,\n   \"fsLayers\": [\n      {\n         \"blobSum\": <tarsum>\n      },\n      ...\n    ]\n   ],\n   \"history\": <v1 images>,\n   \"signature\": <JWS>\n}\n```\n\nThe client should verify the returned manifest signature for authenticity before fetching layers.\n#### Pulling a Layer\n\nLayers are stored in the blob portion of the registry, keyed by tarsum digest. Pulling a layer is carried out by a standard http request. The URL is as follows:\n\n```\nGET /v2/<name>/blobs/<tarsum>\n```\n\nAccess to a layer will be gated by the `name` of the repository but is identified uniquely in the registry by `tarsum`. The `tarsum` parameter is an opaque field, to be interpreted by the tarsum library.\n\nThis endpoint may issue a 307 (302 for <HTTP 1.1) redirect to another service for downloading the layer and clients should be prepared to handle redirects.\n\nThis endpoint should support aggressive HTTP caching for image layers. Support for Etags, modification dates and other cache control headers should be included. To allow for incremental downloads, `Range` requests should be supported, as well.\n### Pushing An Image\n\nPushing an image works in the opposite order as a pull. After assembling the image manifest, the client must first push the individual layers. When the layers are fully pushed into the registry, the client should upload the signed manifest.\n\nThe details of each step of the process are covered in the following sections.\n#### Pushing a Layer\n\nAll layer uploads use two steps to manage the upload process. The first step starts the upload in the registry service, returning a url to carry out the second step. The second step uses the upload url to transfer the actual data. Uploads are started with a POST request which returns a url that can be used\nto push data and check upload status.\n\nThe `Location` header will be used to communicate the upload location after each request. While it won't change in the this specification, clients should use the most recent value returned by the API.\n##### Starting An Upload\n\nTo begin the process, a POST request should be issued in the following format:\n\n```\nPOST /v2/<name>/blobs/uploads/\n```\n\nThe parameters of this request are the image namespace under which the layer will be linked. Responses to this request are covered below.\n##### Existing Layers\n\nThe existence of a layer can be checked via a `HEAD` request to the blob store API. The request should be formatted as follows:\n\n```\nHEAD /v2/<name>/blobs/<digest>\n```\n\nIf the layer with the tarsum specified in `digest` is available, a 200 OK response will be received, with no actual body content (this is according to http specification). The response will look as follows:\n\n```\n200 OK\nContent-Length: <length of blob>\n```\n\nWhen this response is received, the client can assume that the layer is already available in the registry under the given name and should take no further action to upload the layer. Note that the binary digests may differ for the existing registry layer, but the tarsums will be guaranteed to match.\n##### Uploading the Layer\n\nIf the POST request is successful, a `202 Accepted` response will be returned with the upload URL in the `Location` header:\n\n```\n202 Accepted\nLocation: /v2/<name>/blobs/uploads/<uuid>\nRange: bytes=0-<offset>\nContent-Length: 0\n```\n\nThe rest of the upload process can be carried out with the returned url, called the \"Upload URL\" from the `Location` header. All responses to the upload url, whether sending data or getting status, will be in this format. Though the URI format (`/v2/<name>/blobs/uploads/<uuid>`) for the `Location` header is specified, clients should treat it as an opaque url and should never try to assemble the it. While the `uuid` parameter may be an actual UUID, this proposal imposes no constraints on the format and clients should never impose any.\n##### Upload Progress\n\nThe progress and chunk coordination of the upload process will be coordinated through the `Range` header. While this is a non-standard use of the `Range` header, there are examples of [similar approaches](https://developers.google.com/youtube/v3/guides/using_resumable_upload_protocol) in APIs with heavy use. For an upload that just started, for an example with a 1000 byte layer file, the `Range` header would be as follows:\n\n```\nRange: bytes=0-0\n```\n\nTo get the status of an upload, issue a GET request to the upload URL:\n\n```\nGET /v2/<name>/blobs/uploads/<uuid>\nHost: <registry host>\n```\n\nThe response will be similar to the above, except will return 204 status:\n\n```\n204 No Content\nLocation: /v2/<name>/blobs/uploads/<uuid>\nRange: bytes=0-<offset>\n```\n\nNote that the HTTP `Range` header byte ranges are inclusive and that will be honored, even in non-standard use cases.\n##### Monolithic Upload\n\nA monolithic upload is simply a chunked upload with a single chunk and may be favored by clients that would like to avoided the complexity of chunking. To carry out a \"monolithic\" upload, one can simply put the entire content blob to the provided URL:\n\n```\nPUT /v2/<name>/blobs/uploads/<uuid>?digest=<tarsum>[&digest=sha256:<hex digest>]\nContent-Length: <size of layer>\nContent-Type: application/octet-stream\n\n<Layer Binary Data>\n```\n\nThe \"digest\" parameter must be included with the PUT request. Please see the _Completed Upload_ section for details on the parameters and expected responses.\n\nAdditionally, the download can be completed with a single `POST` request to the uploads endpoint, including the \"size\" and \"digest\" parameters:\n\n```\nPOST /v2/<name>/blobs/uploads/?digest=<tarsum>[&digest=sha256:<hex digest>]\nContent-Length: <size of layer>\nContent-Type: application/octet-stream\n\n<Layer Binary Data>\n```\n\nOn the registry service, this should allocate a download, accept and verify the data and return the same  response as the final chunk of an upload. If the POST request fails collecting the data in any way, the registry should attempt to return an error response to the client with the `Location` header providing a place to continue the download.\n\nThe single `POST` method is provided for convenience and most clients should implement `POST` + `PUT` to support reliable resume of uploads.\n##### Chunked Upload\n\nTo carry out an upload of a chunk, the client can specify a range header and only include that part of the layer file:\n\n```\nPATCH /v2/<name>/blobs/uploads/<uuid>\nContent-Length: <size of chunk>\nContent-Range: <start of range>-<end of range>\nContent-Type: application/octet-stream\n\n<Layer Chunk Binary Data>\n```\n\nThere is no enforcement on layer chunk splits other than that the server must receive them in order. The server may enforce a minimum chunk size. If the server cannot accept the chunk, a `416 Requested Range Not Satisfiable` response will be returned and will include a `Range` header indicating the current status:\n\n```\n416 Requested Range Not Satisfiable\nLocation: /v2/<name>/blobs/uploads/<uuid>\nRange: 0-<last valid range>\nContent-Length: 0\n```\n\nIf this response is received, the client should resume from the \"last valid range\" and upload the subsequent chunk. A 416 will be returned under the following conditions:\n- Invalid Content-Range header format\n- Out of order chunk: the range of the next chunk must start after the \"last  valid range\" from the last response.\n\nWhen a chunk is accepted as part of the upload, a `202 Accepted` response will be returned, including a `Range` header with the current upload status:\n\n```\n202 Accepted\nLocation: /v2/<name>/blobs/uploads/<uuid>\nRange: bytes=0-<offset>\nContent-Length: 0\n```\n##### Completed Upload\n\nFor an upload to be considered complete, the client must submit a `PUT` request on the upload endpoint with a digest parameter. If it is not provided, the download will not be considered complete. The format for the final chunk will be as follows:\n\n```\nPUT /v2/<name>/blob/uploads/<uuid>?digest=<tarsum>[&digest=sha256:<hex digest>]\nContent-Length: <size of chunk>\nContent-Range: <start of range>-<end of range>\nContent-Type: application/octet-stream\n\n<Last Layer Chunk Binary Data>\n```\n\nOptionally, if all chunks have already been uploaded, a `PUT` request with a `digest` parameter and zero-length body may be sent to complete and validated the upload. Multiple \"digest\" parameters may be provided with different digests. The server may verify none or all of them but _must_ notify the client if the content is rejected.\n\nWhen the last chunk is received and the layer has been validated, the client will receive a `201 Created` response:\n\n```\n201 Created\nLocation: /v2/<name>/blobs/<tarsum>\nContent-Length: 0\n```\n\nThe `Location` header will contain the registry URL to access the accepted layer file.\n###### Digest Parameter\n\nThe \"digest\" parameter is designed as an opaque parameter to support verification of a successful transfer. The initial version of the registry API will support a tarsum digest, in the standard tarsum format. For example, a HTTP URI parameter might be as follows:\n\n```\ntarsum.v1+sha256:6c3c624b58dbbcd3c0dd82b4c53f04194d1247c6eebdaab7c610cf7d66709b3b\n```\n\nGiven this parameter, the registry will verify that the provided content does result in this tarsum. Optionally, the registry can support other other digest parameters for non-tarfile content stored as a layer. A regular hash digest might be specified as follows:\n\n```\nsha256:6c3c624b58dbbcd3c0dd82b4c53f04194d1247c6eebdaab7c610cf7d66709b3b\n```\n\nSuch a parameter would be used to verify that the binary content (as opposed to the tar content) would be verified at the end of the upload process.\n\nFor the initial version, registry servers are only required to support the tarsum format.\n##### Canceling an Upload\n\nAn upload can be cancelled by issuing a DELETE request to the upload endpoint. The format will be as follows:\n\n```\nDELETE /v2/<name>/blobs/uploads/<uuid>\n```\n\nAfter this request is issued, the upload uuid will no longer be valid and the registry server will dump all intermediate data. While uploads will time out if not completed, clients should issue this request if they encounter a fatal error but still have the ability to issue an http request.\n##### Errors\n\nIf an 502, 503 or 504 error is received, the client should assume that the download can proceed due to a temporary condition, honoring the appropriate retry mechanism. Other 5xx errors should be treated as terminal.\n\nIf there is a problem with the upload, a 4xx error will be returned indicating the problem. After receiving a 4xx response (except 416, as called out above), the upload will be considered failed and the client should take appropriate action.\n\nThe following table covers the various error conditions that may be returned after completing a layer upload:\n\n| Code | Message |\n| --- | --- |\n| DIGEST_INVALID | provided digest did not match uploaded content |\n| SIZE_INVALID | provided size did not match content size |\n\nNote that the upload url will not be available forever. If the upload uuid is unknown to the registry, a `404 Not Found` response will be returned and the client must restart the upload process.\n#### Pushing an Image Manifest\n\nOnce all of the layers for an image are uploaded, the client can upload the image manifest. An image can be pushed using the following request formats:\n\n```\nPUT /v2/<name>/manifests/<tag>\n\n{\n   \"name\": <name>,\n   \"tag\": <tag>,\n   \"fsLayers\": [\n      {\n         \"blobSum\": <tarsum>\n      },\n      ...\n    ]\n   ],\n   \"history\": <v1 images>,\n   \"signature\": <JWS>,\n   ...\n}\n```\n\nThe `name` and `tag` fields of the response body must match those specified in the URL.\n\nIf there is a problem with pushing the manifest, a relevant 4xx response will be returned with a JSON error message. The following table covers the various error conditions and their corresponding codes:\n\n| Code | Message |\n| --- | --- |\n| NAME_INVALID | Manifest name did not match URI |\n| TAG_INVALID | Manifest tag did not match URI |\n| MANIFEST_INVALID | Returned when an invalid manifest is received |\n| MANIFEST_UNVERIFIED | Manifest failed signature validation |\n| BLOB_UNKNOWN | Referenced layer not available |\n\nFor the `UNKNOWN_LAYER` error, the `detail` field of the error response will have an \"unknown\" field with information about the missing layer. For now, that will just be the tarsum. There will be an error returned for each unknown blob. The response format will be as follows:\n\n```\n{\n    \"errors:\" [{\n            \"code\": \"UNKNOWN_LAYER\",\n            \"message\": \"Referenced layer not available\",\n            \"detail\": {\n                \"unknown\": {\n                    \"blobSum\": <tarsum>\n                 }\n            }\n        },\n        ...\n    ]\n}\n```\n#### Listing Image Tags\n\nIt may be necessary to list all of the tags under a given repository. The tags for an image repository can be retrieved with the following request:\n\n```\nGET /v2/<name>/tags/list\n```\n\nThe response will be in the following format:\n\n```\n200 OK\nContent-Type: application/json\n\n{\n    \"name\": <name>,\n    \"tags\": [\n        <tag>,\n        ...\n    ]\n}\n```\n\nFor repositories with a large number of tags, this response may be quite large, so care should be taken by the client when parsing the response to reduce copying.\n### Deleting an Image\n\nAn image may be deleted from the registry via its `name` and `tag`. A delete may be issued with the following request format:\n\n```\nDELETE /v2/<name>/manifests/<tag>\n```\n\nIf the image exists and has been successfully deleted, the following response will be issued:\n\n```\n202 Accepted\nContent-Length: None\n```\n\nIf the image had already been deleted or did not exist, a `404 Not Found` response will be issued instead.\n## Roadmap\n- [X] Write Registry REST API V2 proposal\n  - [ ] Solicit feedback\n- [ ] Implement V2 API server\n  - [X] Basic Layer API\n  - [X] Basic Image API\n  - [ ] Resumable upload support\n- [ ] Implement V2 API client\n- [ ] Implement API compliance tests\n- [ ] Port docker core to use client from registry project for v2 pushes\n## Reviewers\n- @dmp42 \n- @dmcgowan \n- @jlhawn \n- Docker Community\n"},{"labels":[null,"enhancement1"],"text":"## Problem\n\nA container’s interface is effectively expressed by expected link names (aliases), volume mount points, and exposed network ports. This expression is left to implementation, and that requires users to inspect the images they use and the software inside of them. Since one of the best reasons to use Docker is abstraction of technical detail this seems like a great candidate for improvement.\n\nCurrently if a user wants to compose containers, they would use commands like these:\n\n```\ndocker run -d  --name=”web” webimage\ndocker run -d  --name=”monitor”  --link web:http monitorimage\n```\n\nArguments of the “link” flag takes the form <container_name>:<alias> where alias is the name that the software running inside the consumer container uses. The problem is that a new user has no idea what alias the software inside of monitorimage is expecting, and monitorimage cannot anticipate the alias that will be specified by the user. The best the image author can do is code to a specific alias and document that alias somewhere. But this still requires the user to dig into the image docs, or inspect the image itself.\n\nThis same issue applies to the —volumes-from flag that allows a user to map the volumes in one container into a new container. While the mechanism works, it requires that the author of the second container anticipates the mount points of any volumes that should be wired in.\n## Solution: Interfaces and Dependencies\n\nToday Docker lacks interface definitions. A container type system would eliminate the research required to build links between producer and consumer containers.\n\nConsider an example. An HTTP interface is defined as follows:\n\n```\n\"Http\": {\n    \"Config\": {\n        \"ExposedPorts\": {\n            \"80/tcp\": {}\n        },\n    }\n}\n```\n\nThis defines an interface named HTTP that can be used to describe any container that exposes port 80 on its network stack. Most web server images I’ve seen already expose port 80, and so there are already several image implementations of this interface. For the remainder of this article, suppose we have one such image named, “webimage.” These interface definitions would need their own repositories and distribution mechanism, similar to the infrastructure in place for images today. \n\nNow consider a consumer. Maybe this is a simple monitor that runs periodically to check the state of a linked web server. Today, an image built for such a purpose would have to assume what name the linked image will be bound to, and then some where in its code or configuration note that it should connect to that host on port 80. These assumptions are currently difficult to communicate without any form of dependency declaration. Typical class declarations in typed object oriented programming languages include a set of typed dependencies. For example the Java Monitor class depends on a HttpService member:\n\n```\nclass Monitor {\n private HttpService service;\n}\n```\n\nApplying the same dependency declaration style to Docker containers would create a few specific benefits. Lets see what such a declaration might look like. We’ll use the following Dockerfile to create an image called, “monitorimage.”\n\n```\nFROM busybox:latest\nADD webmonitor.sh /webmonitor.sh\nINJECT Http\nCMD /webmonitor.sh\n```\n\nUsing the proposed INJECT directive, an image author can clearly communicate to users what assumptions the software contained will make. In this case, the containers built from, “monitorimage” will depend on a linked image that will have the http alias and expose port 80. \n\nThe second benefit is that it would enable validation. On instantiation of such a container, the linking system can validate that the referenced target conforms to the Http interface, and then link that target to the, “http” name inside of the container. \n\nWith interface modeling and typed dependency description in place, the user might build such links with the following commands:\n\n```\ndocker run -d --name=”web” webimage\ndocker run -d --name=”monitor” --inject web monitorimage\n```\n\nThe user can know to build their second run command in this way with minimal knowledge of, “monitorimage.” With all of this metadata in place, Docker could provide rich error messages on instantiation rather than making a user wait for a runtime failure. Today if I were to launch a container from monitorimage without wiring in the required webimage on the correct alias, Docker would allow it and I would discover my error because the software wouldn’t work. With this injection system in place, Docker could provide helpful errors like:\n\nI failed to inject a container:\n\n```\ndocker run -d --name=“monitor” monitorimage\n[ERROR] failed to start: monitor; missing required Http dependency.\n```\n\nI injected a container that does not implement Http:\n\n```\ndocker run -d --name=”database” databaseimage\ndocker run -d --name=“monitor” --inject database monitorimage\n[ERROR] failed to start: monitor; database does not implement Http \n```\n\nThis would avoid and trivialize triage of a whole class of problems that users encounter when trying to wire together containers.\n## Thinking Bigger: Type Based Autowiring\n\nWith interface definition and dependency declaration in place, you are free to build further wiring simplifications. While there is power and flexibility in explicit wiring of containers, often there will only be a single container that implements a specific interface running on a single box. In these cases, it would be possible to anticipate the link that must be built. \n\n```\ndocker run -d --name=“web” webimage\ndocker run -d --name=“monitor” --wired monitorimage\n```\n\nIn fact, with autowiring in place the helpful names become less important so we could simplify to:\n\n```\ndocker run -d webimage\ndocker run -d --wired monitorimage\n```\n\nNow that is a powerful and simple expression. What’s better is the great error messages that you can produce in the event that there are multiple candidates for wiring.\n\n```\ndocker run -d webimage\ndocker run -d webimage\ndocker run -d  --wired monitorimage\n[ERROR] failed to start from: monitorimage; there are multiple candidates for the required Http dependency.\n```\n## Features\n- Interface definitions in JSON\n- Duck typed, not declarative implementation (ie. a container that exposes port 80 implements some HTTP interface)\n- INJECT Dockerfile directive\n- Add dependency interface list to image metadata\n- Validation of injected containers against specified interface\n- Type based container autowiring through introspection of running containers\n- Interface definition registry?\n"},{"labels":[null,"enhancement1"],"text":"## Native Docker Multi-Host Networking\n### TL;DR Practical SDN for Docker\n\nAuthors: @dave-tucker, @mavenugo and @nerdalert.\n- [Background](#background)\n- [Problem Statement](#problem-statement)\n- [Solution Components](#solution-components)\n- [Proposal](#proposal)\n- [Single Host Network Deployment Scenarios](#single-host-network-deployment-scenarios)\n- [Multi Host Network Deployment Scenarios](#multi-host-network-deployment-scenarios)\n- [Summary](#summary)\n## Background\n\nApplication virtualization will have a significant impact on the future of data center networks. Compute virtualization has driven the edge of the network into the server and more specifically the virtual switch. The compute workload efficiencies derived from Docker containers will dramatically increase the density of network requirements in the server. Scaling this density will require reliable network fundamentals, while also ensuring the developer has as much or little interaction with the network as is desired. \n\nA tightly coupled and native integration to Docker will ensure there is a base functionality that capable of integrating into the vast majority of data center network architectures today and help reduce the barriers to Docker adoption for the user. Just as important for the diverse user base, is making Docker networking dead simple for the to integrate, provision and troubleshoot.\n\nThe first step is a Native Docker Networking solution today that can handle Multi-Host environment which scales to production requirements and that works well with the existing network deployments / operations. \n## Problem Statement\n\nThough there are a few existing multi-host networking solutions, they are currently designed more as over-the-top solutions on top of Docker that either:\n1. Address a specific use case\n2. Address a specific orchestration system deployment\n3. Do not scale to the production requirements\n4. Do not work well with existing production network and operations.\n\nThe core of this proposal is to bring multi-host networking as a native part of Docker that handles most of the use-cases, scales and works well with the existing production network and operations. With this provided as a native Docker solution, every orchestration system can enjoy the benefits alike.\n\nThere are three ways to approach multi-host networking in docker:\n1. NAT-based : Just hide the containers behind the docker host IP address. Job Done.\n2. IP-based Each container should have it’s own unique IP address\n3. Hybrid. A mix of the above\n\n**NAT-based**\n\nThe first option (NAT-based) works by hiding the the containers behind a Docker Host IP address. The TCP port exposed by a given Docker container is mapped to an unique port on the Host machine. \n\nSince the mapped host port has to be unique, containers using well-known port numbers are therefore forced to use ephemeral ports. This adds complexity in network operations, network visibility, troubleshooting and deployment.\n\nFor example, the configuration of a front-end load-balancer for a DNS service hosted in a Docker cluster.\n\n_Service Address:_\n- 1.2.3.4:53\n\n_Servers:_\n- 10.1.10.1:65321\n- 10.36.45.2:64123\n- 10.44.3.1:54219\n\nIf you have firewalls or IDS/IPS devices behind the load-balancer, these also need to know that the DNS service is being hosted on these devices and port numbers. \n\n**IP-based**\n\nThe second option (IP-based) works by assigning unique IP-Addresses to the containers and thus avoiding the need to do Port-mapping, and solving issues with downstream load-balancers and firewalls by using well-known ports in pre-determined subnets. \nHowever, this exposes different sets of issues.\n-   **_Reachability**_: Which containers are on which host?*\n  - GCE uses a /24 per host for this reason, but solutions outside of GCE will require an overlay network like Flannel\n  - Even a GCE style architecture will make firewall management difficult\n- **Flexible Addressing / IP Address Management** (IPAM)*\n  - Who assigns IP addresses to containers\n    - Static? A flag in `docker run`?\n    - DHCP/IPAM? A proper DHCP server or IPAM solution?\n    - Docker? A local DHCP solution using Docker?\n    - Orchestration System? via `docker run` or another API?\n- **Deployability and migration concerns**\n  - Some clouds do not play well with routers (like EC2)\n## Proposal\n\nWe are proposing a Native Multi-Host networking solution to Docker that handles various production-grade deployment scenarios and use cases. \n\nThe power of Docker is its simplicity, yet it scales to the demands of hyper-scale deployments. The same cannot be said today for the native networking solution in Docker. This proposal aims to bridge that gap. The intent is to implement a production-ready reliable multi-host networking solutions that is native to Docker while remaining laser focused on the user friendly needs of the developers environment that is at the heart of the Docker transformation.\n\nThe new edge of the network is the vSwitch. The virtual port density that application virtualization will drive is an even larger multiplier then the explosion of virtual ports created by OS virtualization. This will create port density far beyond anything to date. In order to scale, the network cannot be seen as merely the existing physical spine/leaf 2-tier physical network architecture but also incorporate the virtual edge. Having Docker natively incorporate clear scalable architectures will avoid the all too common problem of the network blocking innovation.\n## Solution Components\n\n**1. Programmable vSwitch**\n\nTo implement this solution we require a programmable vSwitch.\nThis will allow us to configure the necessary bridges, ports and tunnels to support a wide range of networking use cases.\n\nOur initial focus will be to develop an API to implement the primitives required of the vSwitch for multi-host networking with a focus on delivering an implementation for Open vSwitch first.\n\nThis link, [_WHY-OVS_](https://github.com/openvswitch/ovs/blob/master/WHY-OVS.md) covers the rational for choosing OVS and why it is important to the Docker ecosystem and virtual networking as a whole. Open vSwitch has a mature Kernel Data-Plane (upstream since 3.7) with a rich set of features that addresses the requirements of mult-host. In addition to the data-plane performance and functionality, Open vSwitch also has an integrated management-plane called OVSDB that abstracts the Switch as a Database for the applications to make use of.\n\nWith this proposal the native implementation in Docker will:\n- Provide an API for implementing Multi-Host Networking\n- Provide an implementation for an Open vSwitch datapath\n- Implement native control plane to address the scenarios mentioned in this proposal.\n\n**2. Network Integration**\n\nThe various scenarios that we will deal with in this proposal range between existing Port-Mapping solution to VXLAN based Overlays to Native underlay network-integration. There are real deployment scenarios for each of these use-cases / scenarios.\n\nFacilitate the common application HA scenario of a service needing a 1:1 NAT mapping between the container’s back-end ip-address and a front-end IP address from a routable address pool. Alternatively, the containers can also be reachable globally depending on the users IP addressing strategy.\n\n**3. Flexible Addressing / IP Address Management (IPAM)**\n\nIn a multi-host environment, IP Addressing Strategy becomes crucial. Some of the Use-cases, as we will see, will also require reasonable IPAM in place. This discussion will also lead to the production-grade scale requirements of Layer2 vs Layer3 networks.\n\n**4. Host Discovery**\nThough it is obvious, it is important to mention the Host Discovery requirements that is inherent for any Multi-host solution. We believe that such Host/Service Discovery mechanism is a generic requirement and is not specific to the Multi-Host networking needs and as such we are backing the Docker Clustering proposal for this purpose. \n\n**5. Multi-Tenancy**\nAnother important consideration is to provide the architectural white-space for Multi-Tenancy solutions that may either be introduced in Docker Natively or by external orchestration systems.\n## Single Host Network Deployment Scenarios\n- Parity with existing Docker Single-Host solution\n\nThis is the native Single-Host Docker Networking model as of today. This is the most basic scenario that the solution that we are proposing must address seamlessly. This scenario brings in the basic Open vSwitch integration into Docker which we can build on top of for the Multi-Host scenarios that follows.\n\n**Figure - 1**\n\n![*Figure - 1*](https://cloud.githubusercontent.com/assets/1711674/4904734/ca8083e8-6449-11e4-8ff7-4cb6fb251a4f.png)\n- Addition of Flexible Addressing\n\nThis scenario adds a Flexible Addressing scheme to the basic single-host use-case where we can provide IP addressing from one of many different sources\n\n**Figure -  2**\n\n![*Figure - 2*](https://cloud.githubusercontent.com/assets/1711674/4904747/deff6dde-6449-11e4-84a6-6a89f196686a.png)\n## Multi Host Network Deployment Scenarios\n\nThis following scenarios enables backend Docker containers to communicate with one another across multiple hosts. This fulfills the need for high availability applications to survive beyond a single node failure.\n- Overlay Tunnels (VXLAN, GRE, Geneve, etc.)\n\nFor environments which need to abstract the physical network, overlay networks need to create a virtual datapath using supported tunneling encapsulations (VXLAN, GRE, etc). It is just as important for these networks to be as reliable and consistent as the underlying network. Our experience leads us towards using similar consistency protocol such as a tenant aware BGP in order to achieve the worry free environment developers and operators desire. This also presents an evolvable architecture if a tighter coupling into the native network is of value in the future.\n\nThe overlay datapath is provisioned between tunnel endpoints residing in the Docker host which gives the appearance of all hosts within a given provider segment being directly connected to one another as depicted in the following Diagram 3.\n\n**Figure -  3**\n\n![*Figure -  3*](https://cloud.githubusercontent.com/assets/1711674/4904762/0f15a47a-644a-11e4-9077-7695c9210550.png)\n\nAs a new container comes online, the prefix is updated in the routing protocol announcing its location via a tunnel endpoint. As the other Docker hosts receive the updates the forwarding is installed into OVS for which tunnel endpoint the host resides. When the host is deprovisioned, the similar process occurs and tunnel endpoint Docker hosts remove the forwarding entry for the deprovisioned container.\nUnderlay Network Integration\n- Underlay Network integration\n\nThe backend can also simply be bridged into a networks broadcast domain and rely on upstream networking to provide reachability. Traditional L2 bridging has significant scaling issues but it is still very common in many data centers with flat VLAN architectures to facilitate live workload migrations of their VMs.\n\nThis model is fairly critical for DC architectures that require a tight coupling of network and compute as opposed to a ships in the night design of overlays abstracting the physical network.\n\nThe underlay network integration can be designed with some specific network architecture in mind and hence we see models like Google Compute where every host is assigned a dedicated Subnet & each pod gets an ip-address from that subnet.\n\n**Figure -  4** - Dedicated one Static Subnet per Host*\n\n![*Figure -  4*](https://cloud.githubusercontent.com/assets/1711674/4904780/288df77c-644a-11e4-8a1a-5052a5084186.png)\n\nThe entire backend container space can be advertised into the underlying network for IP reachability. IPv6 is becoming attractive for many in this scenario due to v4 constraints.\n\nBy extending L3 to the true edge of the network in the vSwitch it enables a proven network scale while still retaining the ability to perform disaggregated network services on the edge. Extending gateway protocols to the host will play a significant role in scaling a tight coupling to the network architecture.\n\nAlternatively, Underlay integration can also provide Flexible addressing combined with /32 host-updates to the network in order to provide the subnet flexibility.\n\n**Figure -  5**\n\n![*Figure -  5*](https://cloud.githubusercontent.com/assets/1711674/4904787/4690b7be-644a-11e4-974e-e1874d26e567.png)\n## Summary\n\nImplementing the above solution provides a flexible, scalable, multi-host networking as a native part of Docker. This implementation adds a strong networking foundation that is intent on providing an evolvable network architecture for the future.\n"},{"labels":["enhancement1"],"text":"I want to create tag names like `myapplication:1.1.0-beta+2c4f57c` but I get the following error:\n\n`2014/11/04 12:52:53 Error response from daemon: Illegal tag name (1.1.0-beta+2c4f57c): only [A-Za-z0-9_.-] are allowed, minimum 2, maximum 30 in length`\n\nBecause I want to follow the versioning guidelines of semantic versioning (http://semver.org/) I need to be able to add the + sign in the tag name\n"},{"labels":[null,"enhancement1"],"text":"Recently at work was prompted at work to tail the docker daemon logs to investigate an condition (which was sorted out) I had the daemon running on `debug` mode and I found some information missing that could have been useful in debugging, to explain what I mean let me elaborate: \n\nOur Docker use case can be termed as command and control where we have a set of master instances talking to a bunch of minions who instruct the minions to perform image/container management (pull, push, start, kill) to aid, debug and potentially leave a log-trail of command origin it will be very useful to list the source of the command to the docker daemon for example current daemon logs look like:\n\n![image](https://cloud.githubusercontent.com/assets/311217/4878297/474f709e-6307-11e4-8604-ddd648c1bc29.png)\n\nThese log fragments provide what command was executed but does not provide clue about its origin as in whether it came from localhost or remote host. I also saw that the future releases will contain a enhanced and a more parsable log format #8761 which is awesome but I feel that providing the source where the command came from will be useful as well in establishing an log-trail about container management. \n"},{"labels":[null,"enhancement"],"text":"Our servers inside corporate network can not connect to internet directly, and https proxy is not an option, so we use nexus to make registry proxies of popular registries like Docker Hub, gcr.io and etc.  \r\n\r\nCurrently we can set \"registry-mirrors\" of Docker Hub through daemon.json, then we can pull images from proxy directly using same image name:\r\n\r\n`docker pull mysql:8`\r\n\r\nBut for registries like gcr, we must address proxy url in pull command:\r\n\r\n`docker pull nexus_ip:registry_port/google_containers/pause:3.2`\r\n\r\nwhile pulling same image on internet:\r\n\r\n`docker pull k8s.gcr.io/google_containers/pause:3.2`\r\n\r\nIt's a bit of inconvenience, cause some scripts or config files should be rewrite for intranet environment, while they where written on internet env.\r\n\r\nHow about allowing \"registry-mirrors\" of gcr.io and other popular registries?\r\n\r\nCurrently \"registry-mirrors\" setting:\r\n\r\n`{\r\n  \"registry-mirrors\": [\"https://mirror.of.dockerhub\"]\r\n}`\r\n\r\nWe could change it to:\r\n\r\n`{\r\n  \"registry-mirrors\": {\r\n  \"https://hub.docker.com/\": \"https://mirror.of.dockerhub\",\r\n  \"k8s.gcr.io\": \"mirror.of.gcr\"\r\n  }\r\n}`\r\n\r\nIs this possible? Could this be done?\r\n\r\n"},{"labels":[null,"enhancement",null],"text":"The docker-compose extra_hosts feature does not add host entries to the hosts file on Windows like it does for Linux.\r\n\r\nIt should."},{"labels":[null,"enhancement"],"text":"Just opening this for discussion, following https://twitter.com/ibuildthecloud/status/1303204483757875206?s=20\r\n\r\nThe pkg/locker package is a small package and, looking at git history, has low maintenance (last changes were made 3 years ago). Similar to (e.g.) the https://github.com/moby/sys and https://github.com/moby/term packages, we can consider moving the package to its own repository, to allow easier consumption of the package without having to pull in the whole docker repository and/or dealing with the CalVer versioning issues (which doesn't work well with Go modules).\r\n\r\n\r\n/cc @ibuildthecloud @seemethere @cpuguy83 @kolyshkin "},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nWhen creating Windows containers, there are a few things to keep in mind:\r\n\r\n- you cannot start containers which have images with a newer OS version than the host (e.g: you cannot start a container with the image ``mcr.microsoft.com/windows/servercore:1903`` on a Windows Server 2019 (1809) node).\r\n- without Hyper-V Isolation, you cannot create containers which have images with a much older OS version than the host (e.g.: you cannot start the image ``mcr.microsoft.com/windows/servercore:1903`` on a Windows Server 1909 node).\r\n\r\nThis is currently documented here: https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/version-compatibility?tabs=windows-server-2004%2Cwindows-10-2004\r\n\r\nCurrently, ``docker manifest annotate`` doesn't allow its users to also add information about an image's OS version:\r\n\r\n```\r\ndocker manifest annotate --help\r\n\r\nUsage:  docker manifest annotate [OPTIONS] MANIFEST_LIST MANIFEST\r\n\r\nAdd additional information to a local image manifest\r\n\r\nOptions:\r\n      --arch string           Set architecture\r\n      --os string             Set operating system\r\n      --os-features strings   Set operating system feature\r\n      --variant string        Set architecture variant\r\n```\r\n\r\nTypically, the ``os.version`` information is useful for Windows nodes to pull the most appropriate Windows image from the manifest list.\r\n\r\nWhen creating the manifest list with images that have been built on Windows nodes and pushed, that information is included in the manfiest list by default. However, if a Windows image is built with ``docker buildx``, that information is no longer included. The only way to actually include this information inside the manifest list before pushing it is to edit the manifest list file itself manually:\r\n\r\n```\r\nIMAGE_TAG=\"claudiubelu/busybox:1.29-sample\"\r\n\r\n# The images in the manifest list are stored locally. The folder / file name is almost the same,\r\n# with a few changes.\r\nmanifest_image_folder=$(echo \"docker.io/${IMAGE_TAG}\" | sed \"s|/|_|g\" | sed \"s/:/-/\")\r\n\r\n# create the manifest list and annotate the OS and arch.\r\ndocker manifest create --amend \"${IMAGE_TAG}\" \"${IMAGE_TAG}-windows-amd64-1809\" \"${IMAGE_TAG}-windows-amd64-1903\"\r\ndocker manifest annotate --os \"windows\" --arch \"amd64\" \"${IMAGE_TAG}\" \"${IMAGE_TAG}-windows-amd64-1809\"\r\ndocker manifest annotate --os \"windows\" --arch \"amd64\" \"${IMAGE_TAG}\" \"${IMAGE_TAG}-windows-amd64-1903\"\r\n\r\necho \"The 'os.version' information will not be present.\"\r\ndocker manifest inspect \"${IMAGE_TAG}\"\r\n\r\n# Getting the full OS version from the original image this was built from.\r\nBASEIMAGE=\"mcr.microsoft.com/windows/nanoserver:1809\"\r\nfull_version=$(docker manifest inspect ${BASEIMAGE} | grep \"os.version\" | head -n 1 | awk '{print $2}') || true\r\n\r\n# At the moment, docker manifest annotate doesn't allow us to set the os.version, so we'll have to\r\n# it ourselves. The manifest list can be found locally as JSONs.\r\nsed -i -r \"s/(\\\"os\\\"\\:\\\"windows\\\")/\\0,\\\"os.version\\\":$full_version/\" \\\r\n    \"${HOME}/.docker/manifests/${manifest_image_folder}/${manifest_image_folder}-windows-amd64-1809\"\r\n\r\necho \"The 'os.version' information has been added to the 1809 image in the manifest list. The other one should be added as well before pushing the manifest list.\"\r\ndocker manifest inspect \"${IMAGE_TAG}\"\r\n\r\n```\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create windows images with ``docker buildx``\r\n2. Create manifest list with those images.\r\n3. Inspect the manifest list, see that there is no ``os.version`` on the images included in the manfiest list.\r\n4. Try to add the ``os.version`` through the docker CLI.\r\n\r\n**Describe the results you received:**\r\n\r\nThe manifest list did not contain the necessary ``os.version.`` information, and it couldn't be added through the docker CLI.\r\n\r\n**Describe the results you expected:**\r\n\r\nDocker CLI should be able to add the ``os.version`` information through ``docker manifest annotate``.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           19.03.6\r\n API version:       1.40\r\n Go version:        go1.12.17\r\n Git commit:        369ce74a3c\r\n Built:             Fri Feb 28 23:45:43 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          19.03.6\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.12.17\r\n  Git commit:       369ce74a3c\r\n  Built:            Wed Feb 19 01:06:16 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.3.3-0ubuntu1~18.04.2\r\n  GitCommit:\r\n runc:\r\n  Version:          spec: 1.0.1-dev\r\n  GitCommit:\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\ndocker info\r\nClient:\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Build with BuildKit (Docker Inc., v0.4.1-25-ge24e04b)\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 105\r\n Server Version: 19.03.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version:\r\n runc version:\r\n init version:\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.3.0-1034-azure\r\n Operating System: Ubuntu 18.04.4 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 7.749GiB\r\n Name: ubuntu\r\n ID: GSNQ:BJWA:TNPW:FHXF:KXYF:YRVY:BAOP:YNPI:7MRD:JRYC:HF4B:ONL5\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Username: claudiubelu\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nIn 0b4b0a7b5d5de8cb575b666312fceaa2cd58e658, some metadata fields for partial messages were introduced, however as of now they are not used by all plugins and especially the journald driver.\r\n\r\nhttps://github.com/moby/moby/blob/master/daemon/logger/journald/journald.go#L108-L110\r\n\r\nOnly a `CONTAINER_PARTIAL_MESSAGE` indicator is set, but the other fields are omitted.\r\n\r\nIn various issues, it was suggested to use the fluentd concat plugin or similar approaches to reassemble those messages. However they become hard to use when a certain log volume is present, as the downstream services can no longer reliably detect message boundaries. Having the metadata fields present will greatly improve that situation.\r\n\r\n**Describe the results you expected:**\r\n\r\nAll data from the [PartialLogMetaData](https://github.com/moby/moby/commit/0b4b0a7b5d5de8cb575b666312fceaa2cd58e658#diff-2e3c49be523e0817fcf5ed3927f7155fR32) struct present in journald logs.\r\n\r\nIf needed, i can create a PR for this, but first I wanted to hear some feedback, as this might not be desired for any reson."},{"labels":[null,"enhancement"],"text":"Docker is rejecting trailing comments in `FROM` directives. Can we please make sure that comments are parsed better?\r\n\r\nSome users are also noting that comments interact poorly with backslashed `RUN` directives. For that one, I'm not sure if Docker can provide a better behavior. But for simple `FROM` lines, Docker should definitely be able to handle trailing comments."},{"labels":[null,"enhancement"],"text":"\r\nDocker pull large image from Private registry, EOF error #37751\r\n\r\nWhen the large image pull fails, the attempt to download starts all over from byte zero vs starting from the location that it ran into problems.\r\n\r\nWhich means that it tries repeatedly and then keeps failing.\r\nAlso consuming large quantities of band width."},{"labels":["enhancement",null,null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n`docker start` failed with error message:    \r\nError response from daemon: no such file or directory\r\nError: failed to start containers: test\r\n\r\nThe reason is `/dev/sda` device is not exist in the host which used it in the container when create.  This message is not so accurate which confused ops to find the reason if not  familiar with the source code.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker create --name test --device /dev/sda:/dev/sda --device-read-bps /dev/sda:1mb busybox`（note: the host not have the `/dev/sda` device,  and can use `/dev/xxx` replace to simulate this  phenomenon）\r\n2. `docker start test` \r\n\r\n**Describe the results you received:**\r\n\r\nError response from daemon: no such file or directory\r\nError: failed to start containers: test\r\n\r\n**Describe the results you expected:**\r\nError response from daemon: failed to stat \"/dev/sda\": no such file or directory\r\nError: failed to start containers: test\r\n\r\nMay be can update functions  getBlkioThrottleDevices in daemon_unix.go with `errors.Wrapf` to wrap more value information for ops use.\r\n```go\r\n...\r\nif err := unix.Stat(weightDevice.Path, &stat); err != nil {\r\n      return nil, errors.Wrapf(err, \"failed to stat %q\", d.Path)\r\n}\r\n...\r\n```\r\n**docker version**\r\n19.03.5"},{"labels":[null,"enhancement"],"text":"Hi,\r\n\r\n`Docker version 19.03.11, build 42e35e6`\r\n\r\nIf `/etc/docker/daemon.json` is empty.\r\nReally empty!\r\nLike:\r\n` `\r\n\r\nThen docker fails to start and fails to provide any meaningful message as to why it can't start.\r\n\r\nAt the moment there are 2 ways to fix this.\r\n1. Make settings.json contain an empty json object:\r\n`{}`\r\n\r\n2. Remove `/etc/docker/daemon.json`\r\n\r\nCheers,\r\nMark"},{"labels":[null,"enhancement"],"text":"**Description**\r\nUnlike containers, images and networks volumes prune subcommand seems to handle only `label` and `label!` filters.\r\nFor example thi command doesn't work:\r\n\r\n```\r\n$ docker system prune --force --filter \"until=3h\"  --volumes\r\nERROR: The \"until\" filter is not supported with \"--volumes\"\r\n```\r\nIt would be nice for all resources to share the same filterset or to have an explanation in the doc on why `until` filter cannot be implemented also for volumes.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           19.03.12-ce\r\n API version:       1.40\r\n Go version:        go1.14.4\r\n Git commit:        48a66213fe\r\n Built:             Wed Jul  1 17:05:50 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          19.03.12-ce\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.14.4\r\n  Git commit:       48a66213fe\r\n  Built:            Wed Jul  1 17:05:26 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.3.4.m\r\n  GitCommit:        d76c121f76a5fc8a462dc64594aea72fe18e1178.m\r\n runc:\r\n  Version:          1.0.0-rc91\r\n  GitCommit:        24a3cf88a7ae5f4995f6750654c0e2ca61ef4bb2\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 23\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 21\r\n Images: 20\r\n Server Version: 19.03.12-ce\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: d76c121f76a5fc8a462dc64594aea72fe18e1178.m\r\n runc version: 24a3cf88a7ae5f4995f6750654c0e2ca61ef4bb2\r\n init version: fec3683\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.7.8-arch1-1\r\n Operating System: Arch Linux\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 7.672GiB\r\n Name: laptop\r\n ID: EGXP:ACFG:HZQA:ESDJ:TPN6:JXO5:F2JT:DGXI:X526:6TGN:DDKE:AZSL\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nWhen the docker host can no longer reach cloudwatch (e.g. due to a network outage), log events are (almost silently) dropped by the awslogs driver. A log entry to mark the failure is added to the docker daemon logs on the host.\r\n\r\nAn improvement to this fallback would be to also leave a trace of this in the aws cloudwatch logs. The next batch of log events could be prefixed with a message indicating how many messages total have been dropped. When connectivity to cloudwatch is restored, leaving a trace of the failure would help assess how many log entries are missing.\r\n\r\nA further improvement would be to buffer logs to a circular buffer on disk during a network outage. And then uploading the old logs that were missed when connectivity resumes.\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n\r\n**Steps to reproduce the issue:**\r\n1. configure a container with the awslogs log driver. \r\n2. make cloudwatch unreachable from the docker host.\r\n3. let the awslogs driver fail to submit a batch or two\r\n4. restore network connectivity to cloudwatch\r\n5. notice how the logstream in cloudwatch does not have an indication that log events are missing.\r\n\r\n**Describe the results you received:**\r\n\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\nJust a small entry in the log stream that says how many events have been dropped.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           19.03.6-ce\r\n API version:       1.40\r\n Go version:        go1.13.4\r\n Git commit:        369ce74\r\n Built:             Fri May 29 04:01:26 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          19.03.6-ce\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.13.4\r\n  Git commit:       369ce74\r\n  Built:            Fri May 29 04:01:57 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.3.2\r\n  GitCommit:        ff48f57fc83a8c44cf4ad5d672424a98ba37ded6\r\n runc:\r\n  Version:          1.0.0-rc10\r\n  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 1\r\n Server Version: 19.03.6-ce\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ff48f57fc83a8c44cf4ad5d672424a98ba37ded6\r\n runc version: dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n init version: fec3683\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 4.14.177-139.254.amzn2.x86_64\r\n Operating System: Amazon Linux 2\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 9.91GiB\r\n Name: ip-10-1-59-52.us-west-2.compute.internal\r\n ID: O7SX:3R4H:ZCJQ:QVH7:2HIL:CVE6:XY6X:HXWA:IDBC:NFZB:F4IL:V5EM\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nrunning on an instance on AWS EC2."},{"labels":[null,null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nIn api document `/version` response looks like wrong.\r\nhttps://github.com/moby/moby/blob/master/docs/api/v1.40.yaml#L7715\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. `curl --unix-socket /var/run/docker.sock localhost/version`\r\n\r\n**Describe the results you received:**\r\n```\r\n{\r\n  \"Platform\": {\r\n    \"Name\": \"Docker Engine - Community\"\r\n  },\r\n  \"Components\": [\r\n    {\r\n      \"Name\": \"Engine\",\r\n      \"Version\": \"19.03.12\",\r\n      \"Details\": {\r\n        \"ApiVersion\": \"1.40\",\r\n        \"Arch\": \"amd64\",\r\n        \"BuildTime\": \"2020-06-22T15:44:07.000000000+00:00\",\r\n        \"Experimental\": \"false\",\r\n        \"GitCommit\": \"48a66213fe\",\r\n        \"GoVersion\": \"go1.13.10\",\r\n        \"KernelVersion\": \"5.3.0-62-generic\",\r\n        \"MinAPIVersion\": \"1.12\",\r\n        \"Os\": \"linux\"\r\n      }\r\n    },\r\n    ...\r\n  ],\r\n  \"Version\": \"19.03.12\",\r\n  \"ApiVersion\": \"1.40\",\r\n  \"MinAPIVersion\": \"1.12\",\r\n  \"GitCommit\": \"48a66213fe\",\r\n  \"GoVersion\": \"go1.13.10\",\r\n  \"Os\": \"linux\",\r\n  \"Arch\": \"amd64\",\r\n  \"KernelVersion\": \"5.3.0-62-generic\",\r\n  \"BuildTime\": \"2020-06-22T15:44:07.000000000+00:00\"\r\n}\r\n```\r\n\r\n**Describe the results you expected:**\r\nLooks like document is wrong.\r\nWe should update document?\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.12\r\n API version:       1.40\r\n Go version:        go1.13.10\r\n Git commit:        48a66213fe\r\n Built:             Mon Jun 22 15:45:36 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.12\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.13.10\r\n  Git commit:       48a66213fe\r\n  Built:            Mon Jun 22 15:44:07 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.2.13\r\n  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429\r\n runc:\r\n  Version:          1.0.0-rc10\r\n  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.12\r\n API version:       1.40\r\n Go version:        go1.13.10\r\n Git commit:        48a66213fe\r\n Built:             Mon Jun 22 15:45:36 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.12\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.13.10\r\n  Git commit:       48a66213fe\r\n  Built:            Mon Jun 22 15:44:07 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.2.13\r\n  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429\r\n runc:\r\n  Version:          1.0.0-rc10\r\n  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n[I] docker.vim )\r\n[I] docker.vim ) docker info\r\nClient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 2\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 52\r\n Server Version: 19.03.12\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7ad184331fa3e55e52b890ea95e65ba581ae3429\r\n runc version: dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n init version: fec3683\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.3.0-62-generic\r\n Operating System: Ubuntu 18.04.4 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 31.09GiB\r\n Name: thinkpad\r\n ID: EKKH:UC2C:RDMA:JDP2:KDIG:GW2W:T2H3:S3LT:MWNI:D5YG:X24P:K2DC\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Username: skanehira\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nOS: Ubuntu 18.04.01"},{"labels":[null,"enhancement"],"text":"## TL;DR\r\n\r\n- Likely related to the security fix in runc, containers now require a bigger memory-footprint to start (see https://github.com/opencontainers/runc/issues/1980).\r\n- Trying to start a container with less than 6m of memory will fail, and produce a confusing error message.\r\n- Containers that fail to start because of this limit, will not be marked as `OOMKilled`, which makes it harder for users to find the cause of the failure\r\n    - can we detect this situation? is it because the shim was killed?\r\n- Docker defines a minimum memory-limit of 4m, which no longer is adequate\r\n\r\nWe should probably look into the memory-consumption needed, and try to reduce this overhead. Possibly alternative approaches could help (see \"possible enhancements for consideration\" at the end).\r\n\r\n\r\n## Test environment\r\n\r\nSteps were executed on a DigitalOcean machine (1 CPU, 2GB memory). Details below\r\n\r\n> **Note**: this was running on a machine without swap, so each of the below was printing\r\n>\r\n> ```\r\n> WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.\r\n> ```\r\n>\r\n> I removed those warnings for readability. I later re-configured the machine to enable swap (described further down)\r\n\r\n<details>\r\n    <summary>docker version and docker info</summary>\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.12\r\n API version:       1.40\r\n Go version:        go1.13.10\r\n Git commit:        48a66213fe\r\n Built:             Mon Jun 22 15:45:36 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.12\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.13.10\r\n  Git commit:       48a66213fe\r\n  Built:            Mon Jun 22 15:44:07 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.2.13\r\n  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429\r\n runc:\r\n  Version:          1.0.0-rc10\r\n  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 2\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 2\r\n Images: 10\r\n Server Version: 19.03.12\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7ad184331fa3e55e52b890ea95e65ba581ae3429\r\n runc version: dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n init version: fec3683\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 4.15.0-66-generic\r\n Operating System: Ubuntu 18.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 1\r\n Total Memory: 1.947GiB\r\n Name: ubuntu-s-1vcpu-2gb-ams3-01\r\n ID: 7VHE:PVY6:PXSQ:APMB:RC3E:ITUA:RWAJ:6EZK:QLLC:PHT6:DIIJ:5A6K\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n</details>\r\n\r\n## Problem description\r\n\r\nFor some time, we defined a minimum limit for `--memory` limits (mostly based on some \"trial and error\" to see what would be a reasonable minimum). Currently that limit is 4MB\r\n\r\n```bash\r\ndocker run --rm --memory=3m alpine echo success\r\ndocker: Error response from daemon: Minimum memory limit allowed is 4MB.\r\nSee 'docker run --help'.\r\n```\r\n\r\nWhile looking at https://github.com/docker/for-linux/issues/1054 on a Ubuntu 18.04 machine (kernel `4.15.0-66-generic`), I noticed that this limit is too low, and that low values now fail with a confusing error message (possibly related to security fixes in runc now causing more memory to be consumed during creation/starting the container).\r\n\r\nTrying to find what the minimum was, here's trying to set 4m, 5m, 6m limits:\r\n\r\n```bash\r\nrun --rm --memory=4m alpine echo success\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"process_linux.go:449: container init caused \\\"process_linux.go:415: setting cgroup config for procHooks process caused \\\\\\\"failed to write \\\\\\\\\\\\\\\"4194304\\\\\\\\\\\\\\\" to \\\\\\\\\\\\\\\"/sys/fs/cgroup/memory/docker/1254c8d63f85442e599b17dff895f4543c897755ee3bd9b56d5d3d17724b38d7/memory.limit_in_bytes\\\\\\\\\\\\\\\": write /sys/fs/cgroup/memory/docker/1254c8d63f85442e599b17dff895f4543c897755ee3bd9b56d5d3d17724b38d7/memory.limit_in_bytes: device or resource busy\\\\\\\"\\\"\": unknown.\r\nERRO[0000] error waiting for container: context canceled\r\n\r\ndocker run --rm --memory=5m alpine\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"process_linux.go:449: container init caused \\\"process_linux.go:415: setting cgroup config for procHooks process caused \\\\\\\"failed to write \\\\\\\\\\\\\\\"5242880\\\\\\\\\\\\\\\" to \\\\\\\\\\\\\\\"/sys/fs/cgroup/memory/docker/723b06ffb440d3b3629ba80cc160ee1aacb8aaa6e583f4b2289d4f649404c0e9/memory.limit_in_bytes\\\\\\\\\\\\\\\": write /sys/fs/cgroup/memory/docker/723b06ffb440d3b3629ba80cc160ee1aacb8aaa6e583f4b2289d4f649404c0e9/memory.limit_in_bytes: device or resource busy\\\\\\\"\\\"\": unknown.\r\nERRO[0000] error waiting for container: context canceled\r\n\r\ndocker run --rm --memory=6m alpine echo success\r\nsuccess\r\n```\r\n\r\nCleaned up error message:\r\n\r\n```\r\nsetting cgroup config for procHooks process caused\r\nfailed to write \"4194304\" to \"/sys/fs/cgroup/memory/docker/1254c8d63f85442e599b17dff895f4543c897755ee3bd9b56d5d3d17724b38d7/memory.limit_in_bytes\":\r\nwrite /sys/fs/cgroup/memory/docker/1254c8d63f85442e599b17dff895f4543c897755ee3bd9b56d5d3d17724b38d7/memory.limit_in_bytes: device or resource busy \"\": unknown.\r\n```\r\n\r\nNext, finding a more \"exact\" limit (is it 6m exactly?), some interesting bits:\r\n\r\n`5.1m` did not print the cgroup-failure, but looks like the process did not come up; notice that no container-ID is printed (which could be a bug in our code?)\r\n\r\n```bash\r\ndocker run --rm --memory=5.1m alpine echo success\r\ndocker: Error response from daemon: cannot start a stopped process: unknown.\r\n```\r\n\r\nTrying again, without `--rm`, so that I can inspect the container's state:\r\n\r\n```bash\r\ndocker run --memory=5.1m alpine echo success\r\n\r\ndocker ps -a\r\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n96cf0c635fd2        alpine              \"echo success\"      3 seconds ago       Created                                 objective_bohr\r\n\r\ndocker container inspect --format='{{json .State}}' objective_bohr | jq .\r\n\r\n{\r\n  \"Status\": \"created\",\r\n  \"Running\": false,\r\n  \"Paused\": false,\r\n  \"Restarting\": false,\r\n  \"OOMKilled\": false,\r\n  \"Dead\": false,\r\n  \"Pid\": 0,\r\n  \"ExitCode\": 128,\r\n  \"Error\": \"cannot start a stopped process: unknown\",\r\n  \"StartedAt\": \"0001-01-01T00:00:00Z\",\r\n  \"FinishedAt\": \"0001-01-01T00:00:00Z\"\r\n}\r\n```\r\n\r\nSo, the container was not marked as `OOMKilled`\r\n\r\n\r\nIn the 5.1m ... 5.8m range, I also often get a `context canceled`\r\n\r\n```bash\r\ndocker run --rm --memory=5.1m alpine echo success\r\ndocker: Error response from daemon: cannot start a stopped process: unknown.\r\nERRO[0000] error waiting for container: context canceled\r\n```\r\n\r\nAnd the corresponding logs (`/var/log/syslog`). Relevant information;\r\n\r\n```\r\nTask in /docker/4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e killed as a result of limit of /docker/4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e\r\nmemory: usage 5220kB, limit 5220kB, failcnt 26\r\nmemory+swap: usage 0kB, limit 9007199254740988kB, failcnt 0\r\nkmem: usage 440kB, limit 9007199254740988kB, failcnt 0\r\nMemory cgroup stats for /docker/4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e: cache:0KB rss:4780KB rss_huge:0KB shmem:0KB mapped_file:0KB dirty:0KB writeback:0KB inactive_anon:0KB active_anon:4768KB inactive_file:0KB active_file:0KB unevictable:0KB\r\n[ pid ]   uid  tgid total_vm      rss pgtables_bytes swapents oom_score_adj name\r\n[15121]     0 15121   103754     2473   143360        0             0 runc:[2:INIT]\r\nMemory cgroup out of memory: Kill process 15121 (runc:[2:INIT]) score 1864 or sacrifice child\r\nKilled process 15121 (runc:[2:INIT]) total-vm:415016kB, anon-rss:4720kB, file-rss:5172kB, shmem-rss:0kB\r\noom_reaper: reaped process 15121 (runc:[2:INIT]), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB\r\n```\r\n\r\n\r\n\r\n<details>\r\n\r\n```\r\nJul  1 08:12:09 ubuntu-s-1vcpu-2gb-ams3-01 dockerd[7455]: time=\"2020-07-01T08:12:09.867865476Z\" level=warning msg=\"Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.\"\r\nJul  1 08:12:09 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.323768] docker0: port 1(veth6de25f3) entered blocking state\r\nJul  1 08:12:09 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.323771] docker0: port 1(veth6de25f3) entered disabled state\r\nJul  1 08:12:09 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.324087] device veth6de25f3 entered promiscuous mode\r\nJul  1 08:12:09 ubuntu-s-1vcpu-2gb-ams3-01 networkd-dispatcher[854]: WARNING:Unknown index 115 seen, reloading interface list\r\nJul  1 08:12:09 ubuntu-s-1vcpu-2gb-ams3-01 systemd-udevd[15081]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.\r\nJul  1 08:12:09 ubuntu-s-1vcpu-2gb-ams3-01 systemd-udevd[15081]: Could not generate persistent MAC address for veth551ed8f: No such file or directory\r\nJul  1 08:12:09 ubuntu-s-1vcpu-2gb-ams3-01 systemd-udevd[15084]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.\r\nJul  1 08:12:09 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.333033] IPv6: ADDRCONF(NETDEV_UP): veth6de25f3: link is not ready\r\nJul  1 08:12:09 ubuntu-s-1vcpu-2gb-ams3-01 systemd-udevd[15084]: Could not generate persistent MAC address for veth6de25f3: No such file or directory\r\nJul  1 08:12:09 ubuntu-s-1vcpu-2gb-ams3-01 containerd[7309]: time=\"2020-07-01T08:12:09.981585296Z\" level=info msg=\"shim containerd-shim started\" address=\"/containerd-shim/moby/4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e/shim.sock\" debug=false pid=15095\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 systemd-networkd[3946]: veth6de25f3: Gained carrier\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 systemd-networkd[3946]: docker0: Gained carrier\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.650910] eth0: renamed from veth551ed8f\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.651164] IPv6: ADDRCONF(NETDEV_CHANGE): veth6de25f3: link becomes ready\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.651188] docker0: port 1(veth6de25f3) entered blocking state\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.651190] docker0: port 1(veth6de25f3) entered forwarding state\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663278] runc:[2:INIT] invoked oom-killer: gfp_mask=0x14000c0(GFP_KERNEL), nodemask=(null), order=0, oom_score_adj=0\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663279] runc:[2:INIT] cpuset=4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e mems_allowed=0\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663285] CPU: 0 PID: 15121 Comm: runc:[2:INIT] Not tainted 4.15.0-66-generic #75-Ubuntu\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663286] Hardware name: DigitalOcean Droplet, BIOS 20171212 12/12/2017\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663286] Call Trace:\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663296]  dump_stack+0x63/0x8e\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663299]  dump_header+0x71/0x285\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663301]  oom_kill_process+0x21f/0x420\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663302]  out_of_memory+0x2b6/0x4d0\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663306]  mem_cgroup_out_of_memory+0xbb/0xd0\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663307]  mem_cgroup_oom_synchronize+0x2e8/0x320\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663309]  ? mem_cgroup_css_online+0x40/0x40\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663310]  pagefault_out_of_memory+0x36/0x7b\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663314]  mm_fault_error+0x90/0x180\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663315]  __do_page_fault+0x46b/0x4b0\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663317]  ? __schedule+0x256/0x880\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663319]  do_page_fault+0x2e/0xe0\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663321]  ? async_page_fault+0x2f/0x50\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663324]  do_async_page_fault+0x51/0x80\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663325]  async_page_fault+0x45/0x50\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663327] RIP: 0033:0x7f77a63f30dd\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663328] RSP: 002b:00007fff84b6ba40 EFLAGS: 00010202\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663329] RAX: 000000000000ef61 RBX: 00000000000000f1 RCX: 000055e3eeff8fb0\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663330] RDX: 00000000000000f1 RSI: 0000000000000000 RDI: 000055e3eeff90a0\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663330] RBP: ffffffffffffffa8 R08: 000055e3eeff59e0 R09: 0000000000000000\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663331] R10: 000055e3eefe7010 R11: 0000000000000000 R12: 000000000000000d\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663331] R13: 00007f77a6749ca0 R14: 00007f77a6749c40 R15: 0000000000000000\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663332] Task in /docker/4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e killed as a result of limit of /docker/4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663337] memory: usage 5220kB, limit 5220kB, failcnt 26\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663337] memory+swap: usage 0kB, limit 9007199254740988kB, failcnt 0\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663338] kmem: usage 440kB, limit 9007199254740988kB, failcnt 0\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663338] Memory cgroup stats for /docker/4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e: cache:0KB rss:4780KB rss_huge:0KB shmem:0KB mapped_file:0KB dirty:0KB writeback:0KB inactive_anon:0KB active_anon:4768KB inactive_file:0KB active_file:0KB unevictable:0KB\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663362] [ pid ]   uid  tgid total_vm      rss pgtables_bytes swapents oom_score_adj name\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663408] [15121]     0 15121   103754     2473   143360        0             0 runc:[2:INIT]\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.663409] Memory cgroup out of memory: Kill process 15121 (runc:[2:INIT]) score 1864 or sacrifice child\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.666109] Killed process 15121 (runc:[2:INIT]) total-vm:415016kB, anon-rss:4720kB, file-rss:5172kB, shmem-rss:0kB\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.671888] oom_reaper: reaped process 15121 (runc:[2:INIT]), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 containerd[7309]: time=\"2020-07-01T08:12:10.316214199Z\" level=info msg=\"shim reaped\" id=4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 dockerd[7455]: time=\"2020-07-01T08:12:10.328018040Z\" level=info msg=\"ignoring event\" module=libcontainerd namespace=moby topic=/tasks/delete type=\"*events.TaskDelete\"\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 systemd-networkd[3946]: veth6de25f3: Lost carrier\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.742548] docker0: port 1(veth6de25f3) entered disabled state\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.745452] veth551ed8f: renamed from eth0\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.757493] docker0: port 1(veth6de25f3) entered disabled state\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 networkd-dispatcher[854]: WARNING:Unknown index 114 seen, reloading interface list\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.762798] device veth6de25f3 left promiscuous mode\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 kernel: [49223.762808] docker0: port 1(veth6de25f3) entered disabled state\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 networkd-dispatcher[854]: ERROR:Unknown interface index 114 seen even after reload\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 dockerd[7455]: time=\"2020-07-01T08:12:10.402854115Z\" level=error msg=\"4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e cleanup: failed to delete container from containerd: no such container\"\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 dockerd[7455]: time=\"2020-07-01T08:12:10.412135381Z\" level=error msg=\"Handler for POST /v1.40/containers/4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e/start returned error: cannot start a stopped process: unknown\"\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 dockerd[7455]: time=\"2020-07-01T08:12:10.426702914Z\" level=error msg=\"failed to process event\" container=4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e error=\"could not find container 4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e: No such container: 4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e\" event=exit event-info=\"{4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e 4d42d19b32596a652c03346bbe2d437d09b7b6cf5218bb31329b70938eaca88e 15121 137 2020-07-01 08:12:10.285461759 +0000 UTC false <nil>}\" module=libcontainerd namespace=moby\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 systemd-udevd[15167]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 systemd-udevd[15167]: link_config: could not get ethtool features for veth551ed8f\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 systemd-udevd[15167]: Could not set offload features of veth551ed8f: No such device\r\nJul  1 08:12:10 ubuntu-s-1vcpu-2gb-ams3-01 systemd-networkd[3946]: docker0: Lost carrier\r\n```\r\n\r\n</details>\r\n\r\n\r\nTrying with `--network=host` (skipping creation of a network namespace), didn't help; memory limit still looks to be too low:\r\n\r\n```bash\r\ndocker run --rm --network=host --memory=5.1m alpine echo success\r\ndocker: Error response from daemon: cannot start a stopped process: unknown.\r\nERRO[0000] error waiting for container: context canceled\r\n```\r\n\r\nFor 5.9m, I got an interesting one. Unfortunately I wasn't able to reproduce it after, and didn't capture logs (weren't able to find them back);\r\n\r\n```bash\r\ndocker run --rm --memory=5.9m alpine echo success\r\nstandard_init_linux.go:211: exec user process caused \"argument list too long\"\r\n```\r\n\r\n(whaaa?)\r\n\r\nWith 5.9m or up, things went _mostly_ ok;\r\n\r\n```bash\r\ndocker run --rm --memory=5.9m alpine echo success\r\nsuccess\r\n```\r\n\r\n### updating memory limit after starting\r\n\r\nSo, the problem looks to be the memory consumption when _starting_ the container.\r\n\r\n```bash\r\ndocker run -dit --name mycontainer --memory=8m alpine\r\nacdd326419f0898be63b0463cfc81cd17fb34d2dae6f8aa3768ee6a075ca5c86\r\n```\r\n\r\nOnce the container is started, it's consuming little memory:\r\n\r\n```bash\r\nCONTAINER ID        NAME                CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS\r\nacdd326419f0        mycontainer         0.00%               460KiB / 8MiB       89.84%              1.12kB / 0B         0B / 0B             1\r\n```\r\n\r\nAnd we can reduce the memory-limit to go as low as 512k:\r\n\r\n```bash\r\necho 4194304 > /sys/fs/cgroup/memory/docker/acdd326419f0898be63b0463cfc81cd17fb34d2dae6f8aa3768ee6a075ca5c86/memory.limit_in_bytes\r\necho 2097152 > /sys/fs/cgroup/memory/docker/acdd326419f0898be63b0463cfc81cd17fb34d2dae6f8aa3768ee6a075ca5c86/memory.limit_in_bytes\r\necho 1048576 > /sys/fs/cgroup/memory/docker/acdd326419f0898be63b0463cfc81cd17fb34d2dae6f8aa3768ee6a075ca5c86/memory.limit_in_bytes\r\necho 524288  > /sys/fs/cgroup/memory/docker/acdd326419f0898be63b0463cfc81cd17fb34d2dae6f8aa3768ee6a075ca5c86/memory.limit_in_bytes\r\n```\r\n\r\nAt this point, the container is still running fine (within limits);\r\n\r\n```\r\nCONTAINER ID        NAME                CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS\r\nacdd326419f0        mycontainer         0.00%               460KiB / 512KiB     89.84%              1.12kB / 0B         0B / 0B             1\r\n```\r\n\r\nGoing to 256k, an error occurred, and the memory limit cannot be updated;\r\n\r\n```bash\r\necho 262144 > /sys/fs/cgroup/memory/docker/acdd326419f0898be63b0463cfc81cd17fb34d2dae6f8aa3768ee6a075ca5c86/memory.limit_in_bytes\r\n-bash: echo: write error: Device or resource busy\r\n\r\ncat /sys/fs/cgroup/memory/docker/acdd326419f0898be63b0463cfc81cd17fb34d2dae6f8aa3768ee6a075ca5c86/memory.limit_in_bytes\r\n524288\r\n```\r\n\r\nSo, looks like the kernel refuses to set a limit < amount of actual memory used in the cgroup\r\n\r\nTrying if stopping and re-starting a container with the low limit works (or if memory use is needed to _start_ the container);\r\n\r\n```bash\r\ndocker kill mycontainer\r\nmycontainer\r\n\r\ndocker start mycontainer\r\nmycontainer\r\n```\r\n\r\nThat didn't work, because the cgroup is reset to the container's config when starting again;\r\n\r\n```\r\nCONTAINER ID        NAME                CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS\r\nacdd326419f0        mycontainer         0.00%               468KiB / 8MiB       5.71%               906B / 0B           0B / 0B             1\r\n```\r\n\r\nTrying to update the container's limit to 4M before killing;\r\n\r\n```bash\r\ndocker container update --memory=4m mycontainer\r\nmycontainer\r\n\r\ndocker kill mycontainer\r\nmycontainer\r\n\r\ndocker start mycontainer\r\nError response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"process_linux.go:449: container init caused \\\"process_linux.go:415: setting cgroup config for procHooks process caused \\\\\\\"failed to write \\\\\\\\\\\\\\\"4194304\\\\\\\\\\\\\\\" to \\\\\\\\\\\\\\\"/sys/fs/cgroup/memory/docker/acdd326419f0898be63b0463cfc81cd17fb34d2dae6f8aa3768ee6a075ca5c86/memory.limit_in_bytes\\\\\\\\\\\\\\\": write /sys/fs/cgroup/memory/docker/acdd326419f0898be63b0463cfc81cd17fb34d2dae6f8aa3768ee6a075ca5c86/memory.limit_in_bytes: device or resource busy\\\\\\\"\\\"\": unknown\r\nError: failed to start containers: mycontainer\r\n```\r\n\r\nSo, looks like _starting_ the container requires more than 4M, so this is not a solution.\r\n\r\n\r\n### Does swap help?\r\n\r\nNo, enabling swap does not help. I tried enabling swap limit support:\r\n\r\n```bash\r\ncat /etc/default/grub |grep GRUB_CMDLINE_LINUX=\r\nGRUB_CMDLINE_LINUX=\"cgroup_enable=memory swapaccount=1\"\r\n```\r\n\r\nAfter enabling swap and rebooting the machine, the problem is the same;\r\n\r\n```bash\r\ndocker run --rm --memory=4m alpine echo success\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"process_linux.go:449: container init caused \\\"process_linux.go:415: setting cgroup config for procHooks process caused \\\\\\\"failed to write \\\\\\\\\\\\\\\"4194304\\\\\\\\\\\\\\\" to \\\\\\\\\\\\\\\"/sys/fs/cgroup/memory/docker/10ef86ea7cd32cd775f0e6b1a9fd882a0b0d4191f1817b1375d478ce2cbac90d/memory.limit_in_bytes\\\\\\\\\\\\\\\": write /sys/fs/cgroup/memory/docker/10ef86ea7cd32cd775f0e6b1a9fd882a0b0d4191f1817b1375d478ce2cbac90d/memory.limit_in_bytes: device or resource busy\\\\\\\"\\\"\": unknown.\r\n\r\ndocker run --rm --memory=4m --memory-swap=16m alpine echo success\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"process_linux.go:449: container init caused \\\"process_linux.go:415: setting cgroup config for procHooks process caused \\\\\\\"failed to write \\\\\\\\\\\\\\\"4194304\\\\\\\\\\\\\\\" to \\\\\\\\\\\\\\\"/sys/fs/cgroup/memory/docker/6772c052387d1a4a6321c97cc2a4d1f2e5a3908e89510e433eae3aecc36b7438/memory.limit_in_bytes\\\\\\\\\\\\\\\": write /sys/fs/cgroup/memory/docker/6772c052387d1a4a6321c97cc2a4d1f2e5a3908e89510e433eae3aecc36b7438/memory.limit_in_bytes: device or resource busy\\\\\\\"\\\"\": unknown.\r\n```\r\n\r\n\r\n### possible enhancements for consideration\r\n\r\nWondering if it would be useful to allow lower memory limits, and to achieve that, we could;\r\n\r\n1. create and start container (with a minimum limit of (e.g.) `memory-limit as defined by user + 6m` (accounting for the overhead of starting the container))\r\n2. after container is started update the limit to the specified value (e.g. `512k`)\r\n\r\nNot sure if worth looking into (and if it would be racy)\r\n"},{"labels":[null,"enhancement",null,null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n`--add-host` don't work on windows container currently.\r\nthere is a issue in https://github.com/docker/for-win/issues/1455\r\n\r\n**Steps to reproduce the issue:**\r\n1.Run `docker run --rm -it --add-host me:127.0.0.1 mcr.microsoft.com/windows/nanoserver:1909 ping me`\r\n\r\n**Describe the results you received:**\r\nPing request could not find host me. Please check the name and try again.\r\n\r\n**Describe the results you expected:**\r\nping works OK.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.8\r\n API version:       1.40\r\n Go version:        go1.12.17\r\n Git commit:        afacb8b\r\n Built:             Wed Mar 11 01:23:10 2020\r\n OS/Arch:           windows/amd64\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.8\r\n  API version:      1.40 (minimum version 1.24)\r\n  Go version:       go1.12.17\r\n  Git commit:       afacb8b\r\n  Built:            Wed Mar 11 01:37:20 2020\r\n  OS/Arch:          windows/amd64\r\n  Experimental:     true\r\n Kubernetes:\r\n  Version:          Unknown\r\n  StackAPI:         Unknown\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker Application (Docker Inc., v0.8.0)\r\n  buildx: Build with BuildKit (Docker Inc., v0.3.1-tp-docker)\r\n  mutagen: Synchronize files with Docker Desktop (Docker Inc., testing)\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 10\r\n Server Version: 19.03.8\r\n Storage Driver: windowsfilter (windows) lcow (linux)\r\n  Windows:\r\n  LCOW:\r\n Logging Driver: json-file\r\n Plugins:\r\n  Volume: local\r\n  Network: ics internal l2bridge l2tunnel nat null overlay private transparent\r\n  Log: awslogs etwlogs fluentd gcplogs gelf json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Default Isolation: hyperv\r\n Kernel Version: 10.0 19041 (19041.1.amd64fre.vb_release.191206-1406)\r\n Operating System: Windows 10 Pro Version 2004 (OS Build 19041.329)\r\n OSType: windows\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 31.83GiB\r\n Name: okhowang-PC5\r\n ID: V33J:D6JS:DB5H:UUEQ:HRA6:4RCJ:HWM3:ADHH:OVZP:CSZJ:O5ZT:3K4G\r\n Docker Root Dir: C:\\ProgramData\\Docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: true\r\n Insecure Registries:\r\n  dockerimage.isd.com\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n Product License: Community Engine\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nphysical"},{"labels":[null,"enhancement",null],"text":"I ran into this when trying to test a failure condition in a container. I ran this container with the minimum amount of memory and no swap expecting to see it be OOM killed but instead of the `docker run` command seems to die in a weird way and it leaves the container in 'Created'\r\n\r\n# Repro\r\n```\r\nroot@bbuzbee-VirtualBox:~# docker run --memory=4MB --memory-swappiness=0 docker.elastic.co/beats/filebeat:7.8.0\r\nWARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.\r\ndocker: Error response from daemon: OCI runtime start failed: cannot start a container that has stopped: unknown.\r\nERRO[0000] error waiting for container: context canceled \r\nroot@bbuzbee-VirtualBox:~# docker ps -a\r\nCONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS              PORTS               NAMES\r\n5d1a52b47f2c        docker.elastic.co/beats/filebeat:7.8.0   \"/usr/local/bin/dock…\"   2 seconds ago       Created                                 nervous_goldwasser\r\n```\r\n\r\n\r\n# Docker version\r\n\r\n```\r\nroot@bbuzbee-VirtualBox:~# docker version\r\nClient: Docker Engine - Community\r\n Version:           19.03.10\r\n API version:       1.40\r\n Go version:        go1.13.10\r\n Git commit:        9424aeaee9\r\n Built:             Thu May 28 22:16:49 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.10\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.13.10\r\n  Git commit:       9424aeaee9\r\n  Built:            Thu May 28 22:15:20 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.2.13\r\n  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429\r\n runc:\r\n  Version:          1.0.0-rc10\r\n  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n"},{"labels":[null,"enhancement"],"text":"When I tested moby cli-integration-test on the mips64le device,\r\nDockerHubPullSuite.TestPullAllTagsFromCentralRegistry failed.\r\nThe error message is shown below.\r\n```\r\n----------------------------------------------------------------------\r\nFAIL: docker_cli_pull_test.go:194: DockerHubPullSuite.TestPullAllTagsFromCentralRegistry\r\n\r\ndocker_cli_pull_test.go:196:\r\n    s.Cmd(c, \"pull\", \"dockercore/engine-pull-all-test-fixture\")\r\ndocker_hub_pull_suite_test.go:73:\r\n    c.Assert(err, checker.IsNil, check.Commentf(\"%q failed with errors: %s, %v\", strings.Join(arg, \" \"), out, err))\r\n... value *exec.ExitError = &exec.ExitError{ProcessState:(*os.ProcessState)(0xc420675000), Stderr:[]uint8(nil)} (\"exit status 1\")\r\n... \"dockercore/engine-pull-all-test-fixture\" failed with errors: Using default tag: latest\r\nlatest: Pulling from dockercore/engine-pull-all-test-fixture\r\nno matching manifest for unknown in the manifest list entries\r\n, exit status 1\r\n-----------------------------------------------------------------------\r\n```\r\n\r\nThe root cause is that the image dockercore/engine-pull-all-test-fixture does not support for mips64le.\r\nCould we make the image dockercore/engine-pull-all-test-fixture support for mips64le.\r\nPlease refer to the following link.\r\nhttps://github.com/moby/moby/pull/34837\r\n\r\n"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nWhen specifying the message format with the Splunk logging driver as raw, like this, `--log-opt splunk-format=raw` there are a few prefixes, such as Container ID, env variable or label that get prefixed to the JSON formatted log message.\r\n\r\nWe have found that the experience we're looking for is to ONLY ingest the JSON formatted *message* without any data prefixed to it.  This would allow us to use the native splunk `sourcetype` of `json` for the logs coming from our Docker containers.  \r\n\r\nThe issue we're facing now is having to use a custom transform/splunk sourcetype to essentially regex off all of the prefixed information that shows up before the JSON formatted message.  This custom transform in Splunk causes a good deal of overhead work for the Indexers that must take place on _every_ log message sent by our Docker containers.\r\n\r\nIn order to maintain backwards compatibility, perhaps a new message format of `--log-opt splunk-format=raw-message` that would ONLY send a JSON payload of the actual log message to splunk (no prefixes).\r\n\r\n**For example:**\r\nHere is a `splunk-format=raw` payload that the docker container sends to Splunk.  You can see the Container ID (`ec950baea86f`) is prefixed.  We would like instead for the Docker logging driver for splunk to send the message with **no prefixes**, only the JSON data:\r\n```\r\nec950baea86f {\"request\":\"GET /ent-service/service-status HTTP/1.1\",\"loggerFqcn\":\"com.bst.valves.Log4j2JsonAccessLogValve\",\"config_env\":\"us-west-2_qa\",\"level\":\"INFO\",\"CONID\":null,\"responseHeader\":{\"conversation-id\":\"\",\"correlation-id\":\"9f507674-e01b-4156-be7e-bac10bfef845\"},\"label\":[\"\\\"\",\"\\\"\"],\"service_version\":\"2.1.72\",\"requestAttribute\":{\"buClientId\":null,\"Swid\":null},\"byteSent\":7,\"threadName\":\"https-jsse-nio-8443-exec-4\",\"service\":\"entitlement\",\"host\":\"10.0.0.0\",\"requestHeader\":{\"conversation-id\":null,\"correlation-id\":null,\"X-Forwarded-For\":null},\"loggerName\":\"Log4j2JsonAccessLogValve\",\"user\":null,\"timeMillis\":1593107616320,\"CID\":\"9f507674-e01b-4156-be7e-bac10bfef845\",\"httpStatusCode\":200,\"elapsedTime\":\"0.005\"}\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.2\r\n API version:       1.40\r\n Go version:        go1.12.8\r\n Git commit:        6a30dfc\r\n Built:             Thu Aug 29 05:26:49 2019\r\n OS/Arch:           windows/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.2\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.12.8\r\n  Git commit:       6a30dfc\r\n  Built:            Thu Aug 29 05:32:21 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.2.6\r\n  GitCommit:        894b81a4b802e4eb2a91d1ce216b8817763c29fb\r\n runc:\r\n  Version:          1.0.0-rc8\r\n  GitCommit:        425e105d5a03fabd737a126ad93d62a9eeede87f\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 34\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 33\r\n Images: 55\r\n Server Version: 19.03.2\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 894b81a4b802e4eb2a91d1ce216b8817763c29fb\r\n runc version: 425e105d5a03fabd737a126ad93d62a9eeede87f\r\n init version: fec3683\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 4.9.184-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 1.934GiB\r\n Name: docker-desktop\r\n ID: ORVS:HV4U:SFRN:22LV:NILV:4UP5:CWCF:LEJH:JB67:7ZEY:HNB7:FGD3\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: true\r\n  File Descriptors: 36\r\n  Goroutines: 55\r\n  System Time: 2020-06-26T03:04:24.1411096Z\r\n  EventsListeners: 1\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n Product License: Community Engine\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nOur containers run in AWS ECS and send their logs to an instance of Splunk which also runs in AWS (Splunk 8.0.1)."},{"labels":["enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nThe client/ package does not provide ServiceInspect, but provides ServiceInspectWithRaw. This is unequal with other features of the package (e.g. services)\r\n"},{"labels":[null,"enhancement"],"text":"Sorry to open an issue similar to dozens of others, but this one is not a duplicatedof any of those (yes, I check all of them to say that. In fact, I checked every single post throughout the whole internet about running docker in Android (or at least I think so)). Here's some considerations regarding this issue:\r\n\r\n1. I do understand the requirements to run docker on a device;\r\n1. I do have a custom kernel that supports running docker. I'm even able to run docker containers inside a chrooted environment within an GNU/Linux rootfs. That proves my kernel fully supports docker;\r\n1. I'm also already able to run docker directly in Android (no chroot and no VM emulation with qemu or whatever)\r\n\r\nEven though I'm already able to run dockerd, containerd, runc, dockercli, tini and all other requirements, and here's the prove:\r\n\r\n<img src=\"https://user-images.githubusercontent.com/9597536/84588107-097d3180-ae14-11ea-8b74-5362a7f21ac9.jpg\" width=\"500\">\r\n\r\n(this is docker running in my Redmi Note 7 in termux terminal emulator)\r\n\r\nit doesn't serve of anything, since there's no container that supports the Android OS. This becomes clear when you try to `docker run` pretty much any container and all you get is `no matching manifest for android/arm64 in the manifest list entries` error:\r\n\r\n![Screenshot_20200531-203509_Termux](https://user-images.githubusercontent.com/9597536/83365923-57695280-a39b-11ea-83c8-39cf3f131afd.png)\r\n\r\nDespite aarch64 is supported by the majority of containers, the Android OS is not. So, If you then try to run a Linux version of that container with the flag `--platform=linux/arm64` you'll get the `operating system is not supported` error:\r\n\r\n![Screenshot_20200531-211606_Termux](https://user-images.githubusercontent.com/9597536/83366227-2a1da400-a39d-11ea-9bd1-fdddc2426d69.png)\r\n\r\nIf you then patch docker source code to ignore this error and continue anyway (because like me, you don't agree that Android can't run Linux container), you'll finally get a segmentation fault:\r\n\r\n![Screenshot_20200531-211226_Termux](https://user-images.githubusercontent.com/9597536/83366135-aa8fd500-a39c-11ea-9d69-b818fb14fb83.png)\r\n\r\nSo, my issue is, more specifically, about the containers starting to support Android or, more feasibly, docker starting to make Android support Linux containers."},{"labels":[null,null,"enhancement"],"text":"We have been already using shim v2 (`containerd-shim-runc-v2`) on cgroup v2 hosts:\r\n\r\nhttps://github.com/moby/moby/blob/aaf470eca7b588aa19e6681bff8bf08d17be1bf2/daemon/daemon_unix.go#L1759-L1761\r\n\r\nWe should use shim v2 on cgroup v1 hosts as well.\r\n\r\nI think we can safely switch the default, but if we fear potential regression, probably we should have a new daemon flag like `--containerd-shim-api=(1|2)` which defaults to `1`.\r\n\r\n"},{"labels":[null,null,"enhancement"],"text":"Hi there,\r\n\r\nI am running a Docker Swarm setup with 1 master and 2 worker nodes. Each node has defined a config-only network configuration for a large /22 network in our production network infrastructure with its own /24 IP-range. For swarm-usage each node has a network based on this configuration.\r\n\r\nIf I deploy a service to my Swarm I choose this network to connect my containers to it, so that I can access them from any host in our production network via their IP address. \r\n\r\nThe problem I have is, that each container automatically connects to the docker_gwbridge network also. Just after I manually disconnect the containers from the docker_gwbridge network, they are reachable from outside. Before that I have no chance to reach them via ping or ssh.\r\n\r\nI really need these containers to be reachable from our normal network and normal hosts in it. And I also need them to use a IP address of the nodes IP-range our the /22 network.\r\n\r\n\r\nThe config-only network(s) are created by the following way:\r\n\r\n`docker network create --config-only --subnet=10.XXX.12.0/22 --gateway=10.XXX.12.1 --ip-range=10.XXX.13.0/24 --attachable -o parent=enpX swarm-net-config`\r\n\r\nThe network used by the swarm then is created on the master node as followed:\r\n\r\n`docker network create -d macvlan --scope swarm --attachable --config-from swarm-net-config swarm-net`\r\n"},{"labels":[null,null,"enhancement"],"text":"**Description**\r\n\r\nThe command `docker pull` executed on a armv6 Raspberry Pi One (\"Model B Plus Rev 1.2\") running a 32-bit Raspbian OS will fail to pull the correct docker image \r\ndefined in a manifest as soon as a 'linux/arm/v7' manifest entry exists.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `uname -m && docker run --rm arm32v6/alpine:3.8 sh -c 'cat /etc/*release | grep PRETTY_NAME'`\r\n2. `uname -m && docker run --rm arm32v6/alpine:3.9 sh -c 'cat /etc/*release | grep PRETTY_NAME'`\r\n3. `uname -m && docker run --rm alpine:3.8 sh -c 'cat /etc/*release | grep PRETTY_NAME'`\r\n4. `uname -m && docker run --rm alpine:3.9 sh -c 'cat /etc/*release | grep PRETTY_NAME'`\r\n\r\n**Describe the results you received:**\r\n```\r\n$ uname -m && docker run --rm arm32v6/alpine:3.8 sh -c 'cat /etc/*release | grep PRETTY_NAME'\r\narmv6l\r\nPRETTY_NAME=\"Alpine Linux v3.8\"\r\n$ uname -m && docker run --rm arm32v6/alpine:3.9 sh -c 'cat /etc/*release | grep PRETTY_NAME'\r\narmv6l\r\nPRETTY_NAME=\"Alpine Linux v3.9\"\r\n$ uname -m && docker run --rm alpine:3.8 sh -c 'cat /etc/*release | grep PRETTY_NAME'\r\narmv6l\r\nPRETTY_NAME=\"Alpine Linux v3.8\"\r\n$ uname -m && docker run --rm alpine:3.9 sh -c 'cat /etc/*release | grep PRETTY_NAME'\r\narmv6l\r\n$ echo $?\r\n139\r\n```\r\nFor steps 1,2 and 3, I was expecting these results. However, at step 4, the 'docker run' command did not produce any output and exits \r\nwith a 139 return code.\r\n\r\n**Describe the results you expected:**\r\n\r\nFor step 4, I was expecting the same result as in step 2: \r\n```\r\narmv6l\r\nPRETTY_NAME=\"Alpine Linux v3.9\"\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nThere is no \"linux/arm/v7\" image on alpine:3.8 manifest:\r\n```\r\n$ docker run --rm mplatform/mquery alpine:3.8Image: alpine:3.8\r\n * Manifest List: Yes\r\n * Supported platforms:\r\n   - linux/amd64\r\n   - linux/arm/v6\r\n   - linux/arm64\r\n   - linux/386\r\n   - linux/ppc64le\r\n   - linux/s390x\r\n```\r\n\r\nBut alpine:3.9 manifest defines a linux/arm/v7 entry:\r\n```\r\n$ docker run --rm mplatform/mquery alpine:3.9\r\nImage: alpine:3.9\r\n * Manifest List: Yes\r\n * Supported platforms:\r\n   - linux/amd64\r\n   - linux/arm/v6\r\n   - linux/arm/v7\r\n   - linux/arm64\r\n   - linux/386\r\n   - linux/ppc64le\r\n   - linux/s390x\r\n```\r\n\r\nIf you prefer, with `docker manifest inspect`:\r\n```\r\n$ DOCKER_CLI_EXPERIMENTAL=enabled docker manifest inspect alpine:3.8\r\n[...]\r\n\"digest\": \"sha256:dabea2944dcc2b86482b4f0b0fb62da80e0673e900c46c0e03b45919881a5d84\",\r\n         \"platform\": {\r\n            \"architecture\": \"arm\",\r\n            \"os\": \"linux\",\r\n            \"variant\": \"v6\"\r\n         }\r\n[...]\r\n\r\n$ DOCKER_CLI_EXPERIMENTAL=enabled docker manifest inspect alpine:3.9\r\n[...]\r\n{\r\n         \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\r\n         \"size\": 528,\r\n         \"digest\": \"sha256:7a3d88cbc7e2d6c0213deaf2d006933c9f5905c4eb7846b703a66fc6504000b7\",\r\n         \"platform\": {\r\n            \"architecture\": \"arm\",\r\n            \"os\": \"linux\",\r\n            \"variant\": \"v6\"\r\n         }\r\n      },\r\n      {\r\n         \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\r\n         \"size\": 528,\r\n         \"digest\": \"sha256:cfd8b55d209956f63c8fcc931f5c6874984e5e0ffdcb8f45ba9085f190385d73\",\r\n         \"platform\": {\r\n            \"architecture\": \"arm\",\r\n            \"os\": \"linux\",\r\n            \"variant\": \"v7\"\r\n         }\r\n      },\r\n[...]\r\n```\r\n\r\nAn evidence that dockerd 19.03.8 pulls the (wrong) linux/arm/v7 images variant:  \r\n```\r\n$ docker pull arm32v7/alpine:3.9\r\n$ docker images | grep \"alpine\"\r\narm32v6/alpine        3.8                                                   e60f4bcd6c89        4 months ago        4.01MB    # OK\r\nalpine                3.8                                                   e60f4bcd6c89        4 months ago        4.01MB    # OK\r\narm32v6/alpine        3.9                                                   4fe9c9b2a0c4        4 weeks ago         4.68MB    # OK\r\narm32v7/alpine        3.9                                                   9df0ff5446fc        4 weeks ago         3.72MB    # OK\r\nalpine                3.9                                                   9df0ff5446fc        4 weeks ago         3.72MB    # NOK: should be '4fe9c9b2a0c4' and not '9df0ff5446fc' ! \r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.8\r\n API version:       1.40\r\n Go version:        go1.12.17\r\n Git commit:        afacb8b\r\n Built:             Wed Mar 11 01:35:24 2020\r\n OS/Arch:           linux/arm\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.8\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.12.17\r\n  Git commit:       afacb8b\r\n  Built:            Wed Mar 11 01:29:22 2020\r\n  OS/Arch:          linux/arm\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.2.13\r\n  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429\r\n runc:\r\n  Version:          1.0.0-rc10\r\n  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 2\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 34\r\n Server Version: 19.03.8\r\n Storage Driver: overlay2\r\n  Backing Filesystem: <unknown>\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: 2zsjxcgeymlaecu00y8j0i1uw\r\n  Is Manager: true\r\n  ClusterID: t5ftctfw0dlnxbve8ilm7yo0x\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 192.168.0.114\r\n  Manager Addresses:\r\n   192.168.0.114:2377\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7ad184331fa3e55e52b890ea95e65ba581ae3429\r\n runc version: dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n init version: fec3683\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 4.19.97+\r\n Operating System: Raspbian GNU/Linux 10 (buster)\r\n OSType: linux\r\n Architecture: armv6l\r\n CPUs: 1\r\n Total Memory: 432.4MiB\r\n Name: white\r\n ID: QDNQ:Y3WR:ALAX:BXDG:JRKM:VBXW:SEC3:LX7D:LLFH:GG2O:EHK6:GG4G\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\nWARNING: No cpu cfs quota support\r\nWARNING: No cpu cfs period support\r\nWARNING: No cpuset support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nThis test was ran on a \"Raspberry Pi Model B Plus Rev 1.2\".\r\nImportant device info: it is an armhf v6 (and not v7 !) device, even if its 'CPU architecture' is set to 7 ! \r\n```\r\n$ dpkg --print-architecture\r\narmhf\r\n$ uname -m\r\narmv6l\r\n$ cat /proc/cpuinfo | grep \"CPU architecture\"\r\nCPU architecture: 7\r\n```\r\n"},{"labels":["enhancement"],"text":"I was wondering if it's already possible to filter containers by their network alias (aliasName.networkName)? So far, I've not been able to find it in the API docs.\r\n\r\nI would've expected something like\r\n\r\n`GET /v1.xx/containers/json?all=1&before=8dfafdbc3a40&filters={\"alias\":[\"aliasName.networkName\"]} HTTP/1.1` to exist.\r\n\r\nIf this functionality does not exist, is it likely this would get merged if I implement this (if at all possible?), or does this go against any design ideals for moby/docker?"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nWhen running this command show an activity indicator.\r\n\r\n`docker cp`\r\n\r\nWhen running it could some sort of spinner like this.\r\n\r\nhttps://www.shellscript.sh/tips/spinner/spinner.gif\r\n\r\nIf progress doesn't start or fails midstream the spinner would stop."},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nWith the recent glibc 2.31 I have the chronyd (containerized) error:\r\n<details><summary>details</summary>\r\n\r\n```\r\nstrace -f -vv -y chronyd -d\r\n...\r\nfstat(5</run/chrony/chronyd.pid>, {st_dev=makedev(0xfd, 0x2), st_ino=403464, st_mode=S_IFREG|0644, st_nlink=1, st_uid=0, st_gid=0, st_blksize=4096, st_blocks=0, st_size=0, st_atime=1588773723 /* 2020-05-06T14:02:03.384790676+0000 */, st_atime_nsec=384790676, st_mtime=1588773723 /* 2020-05-06T14:02:03.393790684+0000 */, st_mtime_nsec=393790684, st_ctime=1588773723 /* 2020-05-06T14:02:03.393790684+0000 */, st_ctime_nsec=393790684}) = 0\r\nwrite(5</run/chrony/chronyd.pid>, \"229\\n\", 4) = 4\r\nclose(5</run/chrony/chronyd.pid>)       = 0\r\nuname({sysname=\"Linux\", nodename=\"c719b2c6036a\", release=\"5.5.19-un-def-alt1\", version=\"#1 SMP PREEMPT Fri Apr 24 08:59:34 UTC 2020\", machine=\"x86_64\", domainname=\"(none)\"}) = 0\r\nprctl(PR_CAPBSET_READ, CAP_SYS_TIME)    = 1\r\nprctl(PR_CAPBSET_READ, CAP_SYS_TIME)    = 1\r\nclock_adjtime(CLOCK_REALTIME, 0x7fffa6c9e5e0) = -1 EPERM (Operation not permitted)\r\nwrite(2</dev/pts/0>, \"2020-05-06T14:02:03Z \", 212020-05-06T14:02:03Z ) = 21\r\nwrite(2</dev/pts/0>, \"Fatal error : adjtimex(0x8001) f\"..., 64Fatal error : adjtimex(0x8001) failed : Operation not permitted\r\n) = 64\r\nclose(4<socket:[785246]>)               = 0\r\nexit_group(1)                           = ?\r\n+++ exited with 1 +++\r\n\r\n```\r\n\r\n</details>\r\n\r\nSince https://sourceware.org/git/?p=glibc.git;a=commit;h=c81aa64e81826c308c4ecbd1982fa086ca6e647b glibc changed its implementation from `adjtimex` to `clock_adjtime`.\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. glibc 2.31 within container\r\n2. run container with `--cap-add CAP_SYS_TIME`\r\n3. run `chronyd -d`\r\n\r\n**Describe the results you received:**\r\nChronyd fails to adjust the time(yes, host's time).\r\n\r\n**Describe the results you expected:**\r\nChronyd adjusts the time (yes, host's time).\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n$ docker --version\r\nDocker version 19.03.8-ce, build \r\n```\r\n\r\n<details>\r\n<summary>**Output of `docker info`:**</summary>\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 5\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 67\r\n Server Version: dev\r\n Storage Driver: overlay2\r\n  Backing Filesystem: <unknown>\r\n  Supports d_type: true\r\n  Native Overlay Diff: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: d76c121f76a5fc8a462dc64594aea72fe18e1178\r\n runc version: d736ef14f0288d6993a1845745d6756cfc9ddd5a\r\n init version: N/A (expected: fec3683b971d9c3ef73f284f176672c44b448662)\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.5.19-un-def-alt1\r\n Operating System: ALT 8.2 Server (december)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 7.77GiB\r\n Name: docker.test\r\n ID: 2YPH:FFJJ:5WV7:OPQE:JF6E:QJQ2:3FMU:7V2Z:6ERA:Z2LS:YZTK:SC66\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Username: stanislavlevin\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n</details>\r\n\r\nWith the custom seccomp profile, I get it.\r\n```diff\r\ndiff --git a/profiles/seccomp/default.json b/profiles/seccomp/default.json\r\nindex e9f54af4d4..2233a07bb0 100644\r\n--- a/profiles/seccomp/default.json\r\n+++ b/profiles/seccomp/default.json\r\n@@ -65,6 +65,8 @@\r\n                                \"chmod\",\r\n                                \"chown\",\r\n                                \"chown32\",\r\n+                               \"clock_adjtime\",\r\n+                               \"clock_adjtime64\",\r\n                                \"clock_getres\",\r\n                                \"clock_getres_time64\",\r\n                                \"clock_gettime\",\r\n@@ -813,4 +815,4 @@\r\n                        \"excludes\": {}\r\n                }\r\n        ]\r\n-}\r\n\\ No newline at end of file\r\n+}\r\ndiff --git a/profiles/seccomp/seccomp_default.go b/profiles/seccomp/seccomp_default.go\r\nindex 8247ac5a1d..84c005ec2b 100644\r\n--- a/profiles/seccomp/seccomp_default.go\r\n+++ b/profiles/seccomp/seccomp_default.go\r\n@@ -58,6 +58,8 @@ func DefaultProfile() *types.Seccomp {\r\n                                \"chmod\",\r\n                                \"chown\",\r\n                                \"chown32\",\r\n+                               \"clock_adjtime\",\r\n+                               \"clock_adjtime64\",\r\n                                \"clock_getres\",\r\n                                \"clock_getres_time64\",\r\n                                \"clock_gettime\",\r\n```"},{"labels":["enhancement"],"text":"I'm working on an application for which it would be helpful to detect whether `nvidia-container-toolkit` is installed, so that the application can tell whether it will be able to start GPU-enabled containers later on. The best way I know of now is to actually start a container with GPUs requested (something equivalent to `docker run --gpus 0 debian true`) and see if it fails with a particular error (\"could not select device driver\").\r\n\r\nThat's not a _huge_ cost, but having to look for a specific error message is odd and the whole process slows down application startup, may require an image to be downloaded (which may be difficult in some installation scenarios), and adds complexity to the code (it's using the API, so it has to explicitly check if the image is present, pull it if not, create the container, and start the container).\r\n\r\nWith `nvidia-docker2`, it is possible to simply query `/info` and check whether \"nvidia\" is in the list of runtimes, but I don't see an equivalent for `nvidia-container-toolkit`, and I was hoping that something similar could be added or to find out if there is something that I missed.\r\n\r\n(Apologies if this should've gone to https://github.com/NVIDIA/nvidia-docker, but I figured this is more about querying Docker's capabilities generally and not actually that specific to the NVIDIA packages. I'll be happy to take the issue over there if the maintainers here think otherwise.)"},{"labels":[null,"enhancement",null],"text":"Allow initiating a log rotation by signalling to moby's service.\r\nLike [rsyslog](https://github.com/syslog-ng/syslog-ng/blob/master/contrib/fedora-packaging/syslog-ng.logrotate#L4) and other services allow, sending a signal to the service will trigger a log rotation.\r\n\r\nThis is helpful for 2 reasons:\r\n\r\n1. To re-open the log file if something has changed in the underlying file-system like a mount etc.\r\n2. We'll be able to use linux's `logrotate` cron job to rotate moby's logs, allowing us to use the same system-wide log rotation configuration for all services."},{"labels":[null,"enhancement"],"text":"In multistage builds, `COPY --from` invokes the whole `TarUntar` code path. Unfortunately, this actually copies the file data. On filesystems with copy-on-write semantics (btrfs, xfs, etc), this is wasteful.\r\n\r\nSemantically, `TarUntar` is correct and will work on all platforms. Practically, it is very unexpected on copy-on-write filesystems that copying a file actually copies the data. `TarUntar` is also much slower than simply doing the equivalent of `cp --reflink=auto; chown; chmod;`.\r\n\r\nThis also holds for non-multistage builds, but there the sources could come from anywhere. In multistage copies, the builder can reliably determine that a reflink copy will succeed."},{"labels":[null,"enhancement",null],"text":"During the development of container applications, most time is spent on doing iterative code changes, rebuilding code with `docker build` and then running it with `docker run`. \r\n\r\nOne of the performance bottlenecks in this flow is image creation. Every time you need to invoke `docker run` you need to point it to an image. Image creation is slow because images are immutable content-addressable structures defined by tarballs. So to create an image, we need to convert its snapshots to tarballs and then create an image config based on them. In moby case, we then extract the tar metadata from tarballs to disk and discard the actual duplicate data. In containerd storage, this is actually even much slower as containerd keeps the original data, and therefore all layers need to be gzip compressed as well. As a rough estimation 1 sec per 100MB is spent on waiting for this (in some cases more). If your images get larger, this problem gets more and more important. Some languages that keep a lot of source files like Nodejs are also more affected by this than others.\r\n\r\nThis all results in significant developer overhead compared to host development that can't be optimized out this a smarter Dockerfile. Although almost all of the time, these images as discarded as soon as another code change has been made.\r\n\r\nBuildkit has already made some steps to solve this. Compared to the legacy builder, no image is created after every command, and only the very final build result is converted to an image. Eg. for multi-stage builds, no images are created for intermediate stages, already resulting in a better experience. If another output type is specified with `--output`, no image is created at all. Buildkit runs its internal containers directly from snapshots without an image config.\r\n\r\nThis ability should be exposed to the `docker build` users as well. Images should only be needed for releases and distribution. For development, we can run containers directly from the build cache. This also makes storage management much easier as build-cache can be managed with GC and intelligent `prune` commands while when we create an image, it is never released until the user runs a command to delete it.\r\n\r\nFor implementation, there are two possibilities. One is to make the build targets available to `docker run`. With this method, when a `Solve()` request with the gateway API returns, it carries an ID that can be used for `docker run <build-result-id>`. When container is created, dockerd will recognize that ID and ask a snapshot from the builder to get the rootfs. Once both the gateway is released and container is deleted, resources can be released from build cache.\r\n\r\nAnother implementation is to do this only in BuildKit side, using https://github.com/moby/buildkit/issues/749, which allows running arbitrary commands with BuildKit mounts. The benefit of this approach is that it can be used with all BuildKit configurations. A particularly interesting one is using `docker buildx` with the docker container driver. Currently, this method requires using `--load` if you want to make an image available to `docker run`, so it is even slower. The downside of using `Exec` method in BuildKit is that we will not be able to support all the flags that `docker run` has, only the very minimal ones. Usually, this should be ok for development though.\r\n\r\nIdeally, both of the methods could be implemented, but BuildKit `exec` approach seems more urgent.\r\n\r\nOnce this is done, another level of optimization would be to use it together with `MergeOp` https://github.com/moby/buildkit/issues/1431 that also would eliminate the overhead of `COPY` operations duplicating source code after every change, significantly improving working with bigger images."},{"labels":["enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\nOn pruning containers, images, or networks, we currently have `until` filter, which filters those objects based on timestamp and prunes only the older objects. Isn't it better to have `since` filter, which prunes the newer objects?\r\n\r\n**Describe the results you expected:**\r\nWith `since` filter, we can prune newer containers/images/networks than the specified timestamp.\r\n"},{"labels":[null,"enhancement",null],"text":"In light of the Github API changes:\r\nhttps://developer.github.com/changes/2/ (https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/)\r\n\r\nthis won't really work in the near future:\r\n\r\n```Dockerfile\r\nARG token\r\nADD \"<url>?access_token=$token\"  .\r\n```\r\n\r\ninstead we are gonna have to do:\r\n\r\n```Dockerfile\r\nRUN curl -H \"Authorization: token $token\" \"url\"  >  bar\r\n```\r\n\r\nis there a way to use a header with ADD or should we just use curl?\r\n"},{"labels":[null,"enhancement"],"text":"This is somewhat of a carryover from Linux, but exec errors are simply the worst to debug. If your entrypoint is a shell script whose shebang interpreter is not present in the container, then exec complains but fails to clarify WHICH interpreter is missing. This leads to many lost hours as DevOps scramble to understand the nature of the error.\r\n\r\nSimilarly, if you make the mistake of trying to run an alt libc (eg musl) binary from a GNU container, then Docker yields a generic exec error. No mention of any particular missing SO file.\r\n\r\nAlso, many chmod oddities in Docker land. Exec won't tell you when the entrypoint lacks executable bits.\r\n\r\nAlso, chmod +x <script> sometimes silently fails to mutate the executable bit. Not sure if a Docker thing or a Kubernetes/Alpine thing. Oh well."},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nWhen trying to download container images from non-docker registries (k8s.gcr.io, quay.io, registry.centos.org), download-froxen-image-v2.sh returns \"curl: (22) The requested URL returned error: 401 Unauthorized\"\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. ```wget https://github.com/moby/moby/blob/19.03/contrib/download-frozen-image-v2.sh```\r\n2. ```chmod +x download-frozen-image-v2.sh```\r\n3. ```./download-frozen-image-v2.sh \"kube-apiserver\" k8s.gcr.io/kube-apiserver:v1.18.2```\r\n\r\n**Describe the results you received:**\r\n```curl: (22) The requested URL returned error: 401 Unauthorized```\r\n\r\n\r\n**Describe the results you expected:**\r\nI expected successful completion of the script with the output looking similar to:\r\n```\r\nDownloading 'k8s.gcr.io/kube-apiserver:v1.18.2' (1 layers)...\r\n\r\n######################################################################## 100.0%\r\n\r\nDownload of images into 'kube-apiserver' complete.\r\nUse something like the following to load the result into a Docker daemon:\r\n  tar -cC 'kube-apiserver' . | docker load\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nI am able to successfully pull images with download-frozen-image-v2.sh from Docker Hub\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.8\r\n API version:       1.40\r\n Go version:        go1.12.17\r\n Git commit:        afacb8b\r\n Built:             Wed Mar 11 01:27:04 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.8\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.12.17\r\n  Git commit:       afacb8b\r\n  Built:            Wed Mar 11 01:25:42 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.2.13\r\n  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429\r\n runc:\r\n  Version:          1.0.0-rc10\r\n  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nlient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 17\r\n  Running: 17\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 9\r\n Server Version: 19.03.8\r\n Storage Driver: overlay2\r\n  Backing Filesystem: <unknown>\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7ad184331fa3e55e52b890ea95e65ba581ae3429\r\n runc version: dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n init version: fec3683\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 3.10.0-957.27.2.el7.x86_64\r\n Operating System: CentOS Linux 7 (Core)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 3\r\n Total Memory: 7.638GiB\r\n Name: 1588216178.test\r\n ID: ZOYK:SR43:OIMO:ZT36:WBGK:ALKP:WDHV:4NYF:QPN5:GLKF:ZFPB:TPFY\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nDocker is not installed on the device that I am using download-frozen-image-v2.sh on."},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nThe /events endpoint does not have a filter for `task`.  This means to determine if a container would be ready to accept work for the service, the system needs to poll the task periodically to see if the service is running (or if had crashed).\r\n\r\n\r\n**Steps to reproduce the issue:**\r\n\r\nhttps://docs.docker.com/engine/api/v1.40/#operation/SystemEvents\r\n\r\n**Describe the results you received:**\r\n\r\nI did not see `type=task` as a possible filter\r\n\r\n**Describe the results you expected:**\r\n\r\nI expected to see `type=task` as a possible filter\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"When you set the entrypoint in a Dockerfile it clears the CMD set by the parent image.  It would be better if you could reset the CMD with a special \"inherit\" instruction to allow upstream changes to propagate automatically"},{"labels":[null,"enhancement"],"text":"I want to modify `$PATH` in docker building time. However `ARG` doesn't work.\r\n\r\nIf I use `ENV` the modified `$PATH`  would persist in docker image, which I don't want.\r\n\r\n        ```dockerfile\r\n        ARG bar=abc\r\n        ARG PATH=$PATH:$bar\r\n        # ARG wont expand $HOME\r\n        ARG HOME=$HOME:$bar\r\n        RUN echo $bar\r\n        RUN echo $PATH\r\n        RUN echo $HOME\r\n\r\n        # abc\r\n        # /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\r\n        # :abc\r\n        ```\r\n\r\nIs there any workaround to this?\r\n"},{"labels":["enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n`--privileged --cgroupns=private` on cgroup v1 should be supported.\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n```console\r\n$ docker run --rm --privileged --cgroupns=private hello-world\r\n```\r\n\r\n**Describe the results you received:**\r\n```\r\ndocker: Error response from daemon: privileged mode is incompatible with private cgroup namespaces on cgroup v1 host.  You must run the container in the host cgroup namespace when running privileged mode.\r\nSee 'docker run --help'.\r\n```\r\n\r\n\r\n**Describe the results you expected:**\r\nIt should work\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nNot sure why we didn't support  `--privileged --cgroupns=private` on cgroup v1.\r\nCouldn't figure out in https://github.com/moby/moby/pull/38377\r\n\r\n**Output of `docker version`:**\r\n```\r\nClient:\r\n Version:           20.03.0-dev\r\n API version:       1.41\r\n Go version:        go1.13.9\r\n Git commit:        2f494e118\r\n Built:             Tue Apr  7 14:28:04 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          dev\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.13.9\r\n  Git commit:       a6a47d1a49\r\n  Built:            Tue Apr  7 14:26:17 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          v1.3.0-468-g4660e4db\r\n  GitCommit:        4660e4dbb653b4d83ffbc7b0dd21c9c2d2f10ea3\r\n runc:\r\n  Version:          1.0.0-rc10+dev\r\n  GitCommit:        d3fdacb92f4e82b4916900483d6a94caa3869041\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n"},{"labels":[null,null,"enhancement"],"text":"Apologies if this has been discussed before - did a search and nothing leapt out at me.\r\n\r\nI've been using [@sudo-bmitch 's excellent wait script](https://github.com/sudo-bmitch/docker-stack-wait) and wondered if there would be any possiblity of some similar function being pulled into the core?  I realise the obvious answer is 'just keep using the script' - but it feels like something that would be welcome by a lot of people if it was more 'front and center' as part of the stack deploy command.\r\n\r\nUntil I was pointed at the script I'd find myself ssh'ing into a swarm manager and sitting running docker ps/docker service/etc commands to see how the deployment was going.  Which was less than ideal.\r\n\r\nAnyway - that was all really :-)\r\n"},{"labels":[null,"enhancement"],"text":"I haven't dug into this deeply, but we were discussing this in the maintainers meeting (/cc @cpuguy83).\r\n\r\nCurrently, the \"rootless\" install script (https://github.com/docker/docker-install/blob/master/rootless-install.sh) is designed such that no `root` permissions are needed to install everything for running docker in rootless mode.\r\n\r\nWhile this is _one_ scenario, another scenario could be that docker is installed the \"regular\" way, but now the user wants to either;\r\n\r\n1. disable the regular docker service, and instead run rootless, or\r\n2. run additional (rootless) instances\r\n\r\nWe should investigate if those scenarios are possible:\r\n\r\n- should additional things be added to the `.deb` / `.rpm` packages (so that users have everything they need if they want to give \"rootless\" mode a spin)\r\n    - if so, we could bundle them, e.g., in a `contrib` directory somewhere\r\n- would there be permission issues with files installed by the `.deb` / `.rpm` packages that need to be addressed?\r\n- (for 1. above); write instructions how to disable the regular `docker` (and `containerd`) services / systemd units\r\n- document how to create a new `docker context` so that the user can switch between the \"host\" (rootful) daemon and \"rootless\" daemon (if both are running) \r\n- other things?\r\n\r\nI think that having the _option_ to run docker in rootless mode (after doing a regular install) would be useful to have: it would make it easier to give it a try, and possibly lead to more users testing / trying it.\r\n\r\n@AkihiroSuda ptal (you most likely know what would be needed)\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nThe Reproducible Builds guideline points that build tools should make build timestamp configurable https://reproducible-builds.org/docs/source-date-epoch/\r\n\r\nWhile building docker image with `docker build` the image \"created\" property is set up to the docker daemon system timestamp. This is against reproducibility guidelines as even on the same host you're unable to build the same image again. It's even more difficult to do it on different hosts with docker daemon.\r\n\r\nThere is this cool [blogpost](https://maori.geek.nz/how-to-digest-a-docker-image-ca9fc7630b71) that describes the simplest ever docker image ending up having different digests when running on two different hosts or without cache.\r\n\r\n**Workarounds**\r\nThe JIB builds images on it's own, without using `docker build` command.\r\nThey create *.json and *.tar filles manually just to override \"Created\" property of the image.\r\nIt is described in their [FAQ](https://github.com/GoogleContainerTools/jib/blob/master/docs/faq.md#why-is-my-image-created-48-years-ago)\r\n\r\n**Describe the results you expected:**\r\nI'd love to have additional option in the build command like\r\n`docker build --sourceDateEpoch='1970-01-01 00:00:00.0' .`\r\nor just `--sourceDateEpoch=0` with default value to `system.time()`.\r\n\r\nThis would enable to reuse layers cache while building docker images from various build tools plugins like this https://github.com/sbt/sbt-native-packager/issues/1321 or described here https://github.com/GoogleContainerTools/jib/issues/2252\r\n\r\n\r\n"},{"labels":[null,null,"enhancement"],"text":"**Description**\r\n\r\nThe `MEMORY USAGE` and `MEM %` columns in `docker stats` don't include RAM used by `tmpfs` filesystems.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n1) Run\r\n    ```sh\r\n    docker run --shm-size=256MB --memory=256MB -it alpine sh -c \"\r\n            dd if=/dev/urandom of=/dev/shm/file bs=32M\r\n            ls -lh /dev/shm\r\n            read\r\n    \"\r\n    ```\r\n    and/or\r\n    ```sh\r\n    docker run --shm-size=256MB --memory=256MB -it alpine sh -c \"\r\n            dd if=/dev/urandom of=/tmp/file bs=32M\r\n            ls -lh /tmp\r\n            read\r\n    \"\r\n    ```\r\n2) Observe that the `dd` process is killed after exhausting the `memory`/`shm-size` limits and the RAM resident filesystem now contains a `200M`ish file:\r\n    ```\r\n    Killed\r\n    total 217M   \r\n    -rw-r--r--    1 root     root      216.6M Mar 20 05:23 file\r\n    ```\r\n3) Run `docker stats`\r\n\r\n**Describe the results you received:**\r\n```\r\nCONTAINER ID        NAME                   CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS\r\n8c601a0d8404        keen_elbakyan          0.00%               7.316MiB / 256MiB   2.86%               8.49kB / 0B         0B / 0B             1\r\n7066646c1a76        infallible_engelbart   0.00%               1.613MiB / 256MiB   0.63%               5.06kB / 0B         0B / 0B             1\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nThe `MEM USAGE` and `MEM %` columns in the above table to be around `200M`ish and `78%`ish.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nDocker version 19.03.7-ce, build 7141c199a2\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n<details>\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 21\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 19\r\n Images: 5\r\n Server Version: 19.03.7-ce\r\n Storage Driver: overlay2\r\n  Backing Filesystem: <unknown>\r\n  Supports d_type: true\r\n  Native Overlay Diff: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: nvidia runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: d76c121f76a5fc8a462dc64594aea72fe18e1178.m\r\n runc version: dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n init version: fec3683\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.4.24-1-MANJARO\r\n Operating System: Manjaro Linux\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 15.56GiB\r\n Name: ruro-laptop\r\n ID: XHD5:R5HL:34CY:3UJ2:4QD6:IP3P:FIOD:NUZX:ZBEE:XND6:OJ5P:J6RM\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```\r\n</details>\r\n\r\n**Additional details:**\r\n\r\nOutput of\r\n```sh\r\ncurl --unix-socket /var/run/docker.sock \"http://localhost/containers/<container-id>/stats?stream=0\" | python -m json.tool\r\n```\r\n\r\n<details>\r\n<summary>For the <code>/tmp</code> container</summary>\r\n\r\n```json\r\n{\r\n    \"read\": \"2020-03-20T05:24:03.754425567Z\",\r\n    \"preread\": \"2020-03-20T05:24:02.752857441Z\",\r\n    \"pids_stats\": {\r\n        \"current\": 1\r\n    },\r\n    \"blkio_stats\": {\r\n        \"io_service_bytes_recursive\": [],\r\n        \"io_serviced_recursive\": [],\r\n        \"io_queue_recursive\": [],\r\n        \"io_service_time_recursive\": [],\r\n        \"io_wait_time_recursive\": [],\r\n        \"io_merged_recursive\": [],\r\n        \"io_time_recursive\": [],\r\n        \"sectors_recursive\": []\r\n    },\r\n    \"num_procs\": 0,\r\n    \"storage_stats\": {},\r\n    \"cpu_stats\": {\r\n        \"cpu_usage\": {\r\n            \"total_usage\": 1841965540,\r\n            \"percpu_usage\": [\r\n                992402,\r\n                1665649,\r\n                1619827291,\r\n                2729087,\r\n                4554574,\r\n                5189767,\r\n                201081062,\r\n                5925708\r\n            ],\r\n            \"usage_in_kernelmode\": 1620000000,\r\n            \"usage_in_usermode\": 200000000\r\n        },\r\n        \"system_cpu_usage\": 2611727730000000,\r\n        \"online_cpus\": 8,\r\n        \"throttling_data\": {\r\n            \"periods\": 0,\r\n            \"throttled_periods\": 0,\r\n            \"throttled_time\": 0\r\n        }\r\n    },\r\n    \"precpu_stats\": {\r\n        \"cpu_usage\": {\r\n            \"total_usage\": 1841965540,\r\n            \"percpu_usage\": [\r\n                992402,\r\n                1665649,\r\n                1619827291,\r\n                2729087,\r\n                4554574,\r\n                5189767,\r\n                201081062,\r\n                5925708\r\n            ],\r\n            \"usage_in_kernelmode\": 1620000000,\r\n            \"usage_in_usermode\": 200000000\r\n        },\r\n        \"system_cpu_usage\": 2611719640000000,\r\n        \"online_cpus\": 8,\r\n        \"throttling_data\": {\r\n            \"periods\": 0,\r\n            \"throttled_periods\": 0,\r\n            \"throttled_time\": 0\r\n        }\r\n    },\r\n    \"memory_stats\": {\r\n        \"usage\": 234754048,\r\n        \"max_usage\": 268439552,\r\n        \"stats\": {\r\n            \"active_anon\": 0,\r\n            \"active_file\": 113635328,\r\n            \"cache\": 227082240,\r\n            \"dirty\": 197885952,\r\n            \"hierarchical_memory_limit\": 268435456,\r\n            \"hierarchical_memsw_limit\": 536870912,\r\n            \"inactive_anon\": 0,\r\n            \"inactive_file\": 113577984,\r\n            \"mapped_file\": 0,\r\n            \"pgfault\": 8976,\r\n            \"pgmajfault\": 0,\r\n            \"pgpgin\": 64152,\r\n            \"pgpgout\": 8749,\r\n            \"rss\": 0,\r\n            \"rss_huge\": 0,\r\n            \"total_active_anon\": 0,\r\n            \"total_active_file\": 113635328,\r\n            \"total_cache\": 227082240,\r\n            \"total_dirty\": 197885952,\r\n            \"total_inactive_anon\": 0,\r\n            \"total_inactive_file\": 113577984,\r\n            \"total_mapped_file\": 0,\r\n            \"total_pgfault\": 8976,\r\n            \"total_pgmajfault\": 0,\r\n            \"total_pgpgin\": 64152,\r\n            \"total_pgpgout\": 8749,\r\n            \"total_rss\": 0,\r\n            \"total_rss_huge\": 0,\r\n            \"total_unevictable\": 0,\r\n            \"total_writeback\": 0,\r\n            \"unevictable\": 0,\r\n            \"writeback\": 0\r\n        },\r\n        \"failcnt\": 19,\r\n        \"limit\": 268435456\r\n    },\r\n    \"name\": \"/keen_elbakyan\",\r\n    \"id\": \"8c601a0d84047b079b4e409c0fd3197efc1ce80bd964c472f49cbf769517ee5d\",\r\n    \"networks\": {\r\n        \"eth0\": {\r\n            \"rx_bytes\": 5610,\r\n            \"rx_packets\": 39,\r\n            \"rx_errors\": 0,\r\n            \"rx_dropped\": 0,\r\n            \"tx_bytes\": 0,\r\n            \"tx_packets\": 0,\r\n            \"tx_errors\": 0,\r\n            \"tx_dropped\": 0\r\n        }\r\n    }\r\n}\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>For the <code>/dev/shm</code> container</summary>\r\n\r\n```json\r\n{\r\n    \"read\": \"2020-03-20T05:32:09.684746601Z\",\r\n    \"preread\": \"2020-03-20T05:32:08.683481358Z\",\r\n    \"pids_stats\": {\r\n        \"current\": 1\r\n    },\r\n    \"blkio_stats\": {\r\n        \"io_service_bytes_recursive\": [],\r\n        \"io_serviced_recursive\": [],\r\n        \"io_queue_recursive\": [],\r\n        \"io_service_time_recursive\": [],\r\n        \"io_wait_time_recursive\": [],\r\n        \"io_merged_recursive\": [],\r\n        \"io_time_recursive\": [],\r\n        \"sectors_recursive\": []\r\n    },\r\n    \"num_procs\": 0,\r\n    \"storage_stats\": {},\r\n    \"cpu_stats\": {\r\n        \"cpu_usage\": {\r\n            \"total_usage\": 1494756157,\r\n            \"percpu_usage\": [\r\n                822498,\r\n                856679,\r\n                558496,\r\n                8284470,\r\n                200931734,\r\n                1192872,\r\n                1278601167,\r\n                3508241\r\n            ],\r\n            \"usage_in_kernelmode\": 1270000000,\r\n            \"usage_in_usermode\": 200000000\r\n        },\r\n        \"system_cpu_usage\": 2615624390000000,\r\n        \"online_cpus\": 8,\r\n        \"throttling_data\": {\r\n            \"periods\": 0,\r\n            \"throttled_periods\": 0,\r\n            \"throttled_time\": 0\r\n        }\r\n    },\r\n    \"precpu_stats\": {\r\n        \"cpu_usage\": {\r\n            \"total_usage\": 1494756157,\r\n            \"percpu_usage\": [\r\n                822498,\r\n                856679,\r\n                558496,\r\n                8284470,\r\n                200931734,\r\n                1192872,\r\n                1278601167,\r\n                3508241\r\n            ],\r\n            \"usage_in_kernelmode\": 1270000000,\r\n            \"usage_in_usermode\": 200000000\r\n        },\r\n        \"system_cpu_usage\": 2615616430000000,\r\n        \"online_cpus\": 8,\r\n        \"throttling_data\": {\r\n            \"periods\": 0,\r\n            \"throttled_periods\": 0,\r\n            \"throttled_time\": 0\r\n        }\r\n    },\r\n    \"memory_stats\": {\r\n        \"usage\": 234721280,\r\n        \"max_usage\": 268439552,\r\n        \"stats\": {\r\n            \"active_anon\": 0,\r\n            \"active_file\": 0,\r\n            \"cache\": 233029632,\r\n            \"dirty\": 0,\r\n            \"hierarchical_memory_limit\": 268435456,\r\n            \"hierarchical_memsw_limit\": 536870912,\r\n            \"inactive_anon\": 232894464,\r\n            \"inactive_file\": 0,\r\n            \"mapped_file\": 0,\r\n            \"pgfault\": 8976,\r\n            \"pgmajfault\": 0,\r\n            \"pgpgin\": 65604,\r\n            \"pgpgout\": 8741,\r\n            \"rss\": 0,\r\n            \"rss_huge\": 0,\r\n            \"total_active_anon\": 0,\r\n            \"total_active_file\": 0,\r\n            \"total_cache\": 233029632,\r\n            \"total_dirty\": 0,\r\n            \"total_inactive_anon\": 232894464,\r\n            \"total_inactive_file\": 0,\r\n            \"total_mapped_file\": 0,\r\n            \"total_pgfault\": 8976,\r\n            \"total_pgmajfault\": 0,\r\n            \"total_pgpgin\": 65604,\r\n            \"total_pgpgout\": 8741,\r\n            \"total_rss\": 0,\r\n            \"total_rss_huge\": 0,\r\n            \"total_unevictable\": 0,\r\n            \"total_writeback\": 0,\r\n            \"unevictable\": 0,\r\n            \"writeback\": 0\r\n        },\r\n        \"failcnt\": 30,\r\n        \"limit\": 268435456\r\n    },\r\n    \"name\": \"/infallible_engelbart\",\r\n    \"id\": \"7066646c1a76b421db4ccee09d9bf6e2802203b42509b4138913bcf16a5132b1\",\r\n    \"networks\": {\r\n        \"eth0\": {\r\n            \"rx_bytes\": 5737,\r\n            \"rx_packets\": 40,\r\n            \"rx_errors\": 0,\r\n            \"rx_dropped\": 0,\r\n            \"tx_bytes\": 0,\r\n            \"tx_packets\": 0,\r\n            \"tx_errors\": 0,\r\n            \"tx_dropped\": 0\r\n        }\r\n    }\r\n}\r\n```\r\n</details>\r\n\r\nAddressing similar issues:\r\n\r\n- This issue might be an unintended side effect of issue #10824 / https://github.com/docker/cli/pull/80. Not including caches in memory usage is correct, since they will be automatically freed, if memory is needed. Not including `tmpfs` is not correct since the kernel can't just delete these files and so exhausting this resource will result in dead processes.\r\n- Possibly related to #39004, but not likely imo."},{"labels":[null,"enhancement"],"text":"I was wondering if anyone had considered this simple additional feature:\r\n\r\n**I have this situation:**\r\n```\r\ndocker ps --format '{{.Names}}'\r\ncontainerVenus1\r\ncontainerVenus2\r\ncontainerVenus3\r\ncontainer1Pluto\r\ncontainer2Pluto\r\ncontainer3Pluto\r\n```\r\nand for example I want stop only Pluto containers.\r\n\r\n**how it currently works:**\r\n```\r\ndocker stop $(docker ps -a -q --filter 'name=Pluto')\r\n```\r\n\r\n**it would be more beautiful:**\r\n```\r\ndocker stop *Pluto\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"## Desired Behavior\r\nI need a `docker network rm --force` command that exits with success if the specified network doesn't exist.  This analogous to the `rm -f` shell command.\r\n```\r\n$ docker network rm --force nonexistent_network ; echo $?\r\n0\r\n```\r\n## Current Behavior\r\nCurrently, the command exits with status 1 if no networks were deleted.\r\n```\r\n$ docker network rm nonexistent_network ; echo $?\r\nError: No such network: nonexistent_network\r\n1\r\n```\r\n\r\n## Why I need this change\r\nI am calling this command in a cleanup script.  I need to be able to run the script repeatedly, without creating any networks in between runs.  The script needs to stop and report an error if `docker network rm` cannot do what it needs to do.  For example, if it cannot connect to dockerd.  But if the networks are already deleted, then the script needs to continue.  With the current behavior of `docker network rm`, the script must perform its own check for networks before calling `docker network rm `.  This is extra code.  Alternatively, the script can silently ignore failure of the command.  This has serious drawbacks."},{"labels":[null,"enhancement"],"text":"## Desired Behavior\r\nI need the `docker rm --force` command to exit with success if the specified containers don't exist.  This analogous to the `rm -f` shell command.\r\n```\r\n$ docker rm --force nonexistent_container ; echo $?\r\n0\r\n```\r\n## Current Behavior\r\nCurrently, the command exits with status 1 if no containers were deleted.\r\n```\r\n$ docker rm --force nonexistent_container ; echo $?\r\nError: No such container: nonexistent_container\r\n1\r\n```\r\n\r\n## Why I need this change\r\nI am calling this command in a cleanup script.  I need to be able to run the script repeatedly, without creating any containers in between runs.  The script needs to stop and report an error if `docker rm --force` cannot do what it needs to do.  For example, if it cannot connect to dockerd.  But if the containers are already deleted, then the script needs to continue.  With the current behavior of `docker rm --force`, the script must perform its own check for containers before calling `docker rm --force`.  This is extra code that I would like to delete."},{"labels":[null,"enhancement"],"text":"Based off this issue and the request by @theJeztah\r\nhttps://github.com/moby/moby/issues/25209\r\n\r\n\r\n    services:\r\n          computer_simulation:\r\n             ulimits:\r\n                 rtprio:\r\n                     soft: -1\r\n                     hard: -1\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nInstalling an already existing plugin produces an error.\r\n\r\nThis becomes an issue when running automation scripts (e.g. with Ansible), as the command returns a non-zero code\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Installs a plugin. For instance, _loki-docker-driver_:\r\n```bash\r\n~ docker plugin install  grafana/loki-docker-driver:v0.2.0 --alias loki --grant-all-permissions\r\nv0.2.0: Pulling from grafana/loki-docker-driver\r\n6ea6cc4c31c6: Download complete \r\nDigest: sha256:691cca27b0b79f7a8a81890dfa281382f4f667010f94856a35a2ae8c583f6a48\r\nStatus: Downloaded newer image for grafana/loki-docker-driver:v0.2.0\r\nInstalled plugin grafana/loki-docker-driver:v0.2.0\r\n```\r\nThis works fine.\r\n\r\n2. Reinstall the plugin again:\r\n```bash\r\n~ docker plugin install  grafana/loki-docker-driver:v0.2.0 --alias loki --grant-all-permissions\r\nError response from daemon: plugin loki:latest already exists\r\n```\r\nThis returns a non-zero value:\r\n```bash\r\n~ echo $?\r\n1\r\n```\r\n\r\n**Describe the results you received:**\r\nRefer to reproduction steps above.\r\n\r\n**Describe the results you expected:**\r\nThe command does not produce an error, and exit with a zero value.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.5\r\n API version:       1.40\r\n Go version:        go1.12.12\r\n Git commit:        633a0ea\r\n Built:             Wed Nov 13 07:22:34 2019\r\n OS/Arch:           darwin/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.5\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.12.12\r\n  Git commit:       633a0ea\r\n  Built:            Wed Nov 13 07:29:19 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.2.10\r\n  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339\r\n runc:\r\n  Version:          1.0.0-rc8+dev\r\n  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 56\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 55\r\n Images: 623\r\n Server Version: 19.03.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: b34a5c8af56e510852c35414db4c1f4fa6172339\r\n runc version: 3e425f80a8c931f88e6d94a8c831b9d5aa481657\r\n init version: fec3683\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 4.19.76-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 6\r\n Total Memory: 1.943GiB\r\n Name: docker-desktop\r\n ID: 5QEY:TZ5T:UC22:UK6P:PIS4:XD4P:VF3X:ZD57:JZZT:676V:7IF2:FWBJ\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: true\r\n  File Descriptors: 42\r\n  Goroutines: 59\r\n  System Time: 2020-03-06T21:46:58.830546274Z\r\n  EventsListeners: 3\r\n HTTP Proxy: gateway.docker.internal:3128\r\n HTTPS Proxy: gateway.docker.internal:3129\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n Product License: Community Engine\r\n```"},{"labels":[null,null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n`--domainname` doesn't work with rootless due to EPERM.\r\n\r\n**Steps to reproduce the issue:**\r\n`docker  run  -it --rm  --domainname foo alpine`\r\n\r\n**Describe the results you received:**\r\n```\r\ndocker: Error response from daemon: OCI runtime create failed: \r\ncontainer_linux.go:349: starting container process caused \"process_linux.go:449: \r\ncontainer init caused \\\"write sysctl key kernel.domainname: open \r\n/proc/sys/kernel/domainname: permission denied\\\"\": unknown.\r\n```\r\n\r\n**Describe the results you expected:**\r\nIt should work.\r\n\r\nTo avoid EPERM, runc should use `setdomainname(2)` rather than writing to `/proc/sys/kernel/domainname`.\r\nWe may need to amend OCI Runtime Spec.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n`--hostname` works .\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           20.03.0-dev\r\n API version:       1.41\r\n Go version:        go1.13.8\r\n Git commit:        7d407207\r\n Built:             Fri Mar  6 12:06:03 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          dev\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.13.8\r\n  Git commit:       8d0b2a044f\r\n  Built:            Fri Mar  6 12:03:10 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          v1.3.0-367-g936b7b00\r\n  GitCommit:        936b7b0043e4b90a7f4810ed91ce066257117178\r\n runc:\r\n  Version:          1.0.0-rc10+dev\r\n  GitCommit:        6503438fd6b0415bc146403b30b8a248b3346f52\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Build with BuildKit (Docker Inc., v0.3.1-31-g891d355)\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 2\r\n Server Version: dev\r\n Storage Driver: overlay2\r\n  Backing Filesystem: <unknown>\r\n  Supports d_type: true\r\n  Native Overlay Diff: false\r\n Logging Driver: json-file\r\n Cgroup Driver: none\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 936b7b0043e4b90a7f4810ed91ce066257117178\r\n runc version: 6503438fd6b0415bc146403b30b8a248b3346f52\r\n init version: fec3683\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n  rootless\r\n Kernel Version: 5.3.0-40-generic\r\n Operating System: Ubuntu 19.10\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 7.748GiB\r\n Name: suda-ws01\r\n ID: CWVR:KJQU:3CNT:IJF7:FMME:22Y7:GKFW:AFKJ:IVLQ:JOVW:3KZY:S25M\r\n Docker Root Dir: /home/suda/.local/share/docker\r\n Debug Mode: false\r\n Username: akihirosuda\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nWhen invoking `docker build` with a git URL, if the git reference being checked out contains a reference to a submodule that is not at the head of a branch or recent tag in the submodule's repo, the operation fails.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker build <git URL with a submodule reference not at a branch head or recent tag in the submodule repo>`\r\n\r\n**Describe the results you received:**\r\n```\r\nunable to prepare context: unable to 'git clone' to temporary context directory: error initializing submodules: Submodule 'Foo' (git@github.com:foo/bar.git) registered for path 'Foo'\r\nCloning into '/private/var/folders/86/ddj24jnj17jf1vjx2s_fmw8w0000gn/T/docker-build-git585701942/Foo'...\r\nerror: Server does not allow request for unadvertised object 34ad12fa10bfefc73e027940753d9d70845e4a89\r\nFetched in submodule path 'Foo', but it did not contain 34ad12fa10bfefc73e027940753d9d70845e4a89. Direct fetching of that commit failed.\r\n: exit status 1\r\n```\r\n\r\n**Describe the results you expected:**\r\n`Successfully built`\r\n\r\n**Additional information you deem important:**\r\nI believe this problem is caused by `docker build` cloning the requested git repository with the `--depth=0` flag.  The [documentation](https://docs.docker.com/engine/reference/commandline/build/#git-repositories) makes no mention of `--depth=0` being used.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.5\r\n API version:       1.40\r\n Go version:        go1.12.12\r\n Git commit:        633a0ea\r\n Built:             Wed Nov 13 07:22:34 2019\r\n OS/Arch:           darwin/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.5\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.12.12\r\n  Git commit:       633a0ea\r\n  Built:            Wed Nov 13 07:29:19 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.2.10\r\n  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339\r\n runc:\r\n  Version:          1.0.0-rc8+dev\r\n  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 16\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 16\r\n Images: 11\r\n Server Version: 19.03.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: b34a5c8af56e510852c35414db4c1f4fa6172339\r\n runc version: 3e425f80a8c931f88e6d94a8c831b9d5aa481657\r\n init version: fec3683\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 4.19.76-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 6\r\n Total Memory: 1.943GiB\r\n Name: docker-desktop\r\n ID: EYRW:KLGG:S7GY:M7YZ:FHTL:KX2P:Z4NJ:MJZG:BFQV:KAWB:FBC4:3O7O\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: true\r\n  File Descriptors: 36\r\n  Goroutines: 52\r\n  System Time: 2020-03-03T21:14:12.232032346Z\r\n  EventsListeners: 3\r\n HTTP Proxy: gateway.docker.internal:3128\r\n HTTPS Proxy: gateway.docker.internal:3129\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n Product License: Community Engine\r\n```"},{"labels":["enhancement"],"text":"First, delighted to see the Mirantis blog post on continuing support for swarm mode!  🎉🍾 @dperny \r\n\r\nHere service.go uses the namesgenerator package to create a random service name if the caller did not specify one: https://github.com/moby/moby/blob/9fee52d5441526450ce88934dfb01ed726b9e284/daemon/cluster/convert/service.go#L155-L160\r\n\r\nBased on the two word lists namesgenerator uses, it's only capable of generating <10<sup>5</sup> distinct names (and this has been trending downwards over time, as words get struck from the list for potentially causing offense). This implies that collisions are *expected* in most sets of just a few hundred random service names ([math](https://en.wikipedia.org/wiki/Birthday_problem#Cast_as_a_collision_problem)), causing intermittent failures in service creation. This has [happened to me](https://github.com/chanzuckerberg/miniwdl/issues/303) in an application creating many short-lived services, similar to the new Swarm Jobs feature, which I'd speculate might also be affected by this.\r\n\r\n[Elsewhere](https://github.com/moby/moby/blob/9fee52d5441526450ce88934dfb01ed726b9e284/daemon/names.go#L84-L92) namesgenerator is used in a retry loop -- something similar seems to be needed for service names creation as well. Or, namesgenerator could be changed to add more entropy."},{"labels":[null,null,"enhancement",null,null],"text":"**Description**\r\nDownload speed is very slow for route [/containers/{id}/archive](https://docs.docker.com/engine/api/v1.40/#operation/ContainerArchive) when using `Accept-Encoding: gzip, deflate` header for GET request. \r\n\r\nIt seems, that download stream is chunked into very little pieces (0x800/ 2048 bytes) (Marked with 0x800 and CRLF) when I observed stream with Wireshark.\r\nOn the contrast, chunk size is (0x8000 / 32 768) when using `Accept-Encoding: identity` or no header at all.\r\n\r\nIs this very low chunk size feature or bug? It makes downloading of big files extremely slow, as each chunk is processed separately, and there are 16x amount of chunks. (Or maybe encoding is slowing Docker engine a lot)\r\n\r\nI'm not forced to use encoding headers, but I came to this issue, since `docker-py` Python API is using default headers of `requests` library and those happens to include `Accept-Encoding: gzip, deflate`  \r\n\r\nFor example downloading file with size 1.1GB without headers, is done with around 1s in my computer locally. On the opposite, encoded download takes 28 seconds and this seems to not include decompression, when done with curl. I tried with both Unix sockets and IP protocol.\r\n\r\nMaybe there are other reasons as well for slowness, but I couldn't figure it out.\r\n\r\n**Steps to reproduce the issue:**\r\nDownload some big file from container by using route `/containers/{id}/archive`\r\ne.g. using curl with encoding headers (download file named as \"/out\", size of 1.1GB):\r\n```\r\n curl -v -H \"Accept-Encoding: gzip, deflate\"  \"http://0.0.0.0:2376/v1.40/containers/6e42935c65c90978f139736654d45e77ba342fa87bdc86b81dd12bf8a591645a/archive?path=%2Fout\" --output test.tar\r\n```\r\nThis takes around 28 seconds on my machine. Produces gzip compressed file\r\n\r\nDownload same file without encoding:\r\n```\r\ncurl -v  \"http://0.0.0.0:2376/v1.40/containers/6e42935c65c90978f139736654d45e77ba342fa87bdc86b81dd12bf8a591645a/archive?path=%2Fout\" --output test.tar\r\n```\r\nTakes around 1 second.\r\n\r\nParameter `--unix-socket \"/var/run/docker.sock\"` can be used for UNIX socket.\r\nI also tried with Python 'requests/urlib3' library and difference was about the same.\r\n\r\nWith header `Accept-Encoding: deflate`, it takes around 28 seconds as well.\r\nWith header `Accept-Encoding: gzip`, it takes around 28 seconds again.\r\n\r\n**Describe the results you received:**\r\nWeak download speed of encoded tar file from container\r\n\r\n**Describe the results you expected:**\r\nI expected that encoding makes it slower, but not almost 30 times slower.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           19.03.6-ce\r\n API version:       1.40\r\n Go version:        go1.13.7\r\n Git commit:        369ce74a3c\r\n Built:             Thu Feb 13 18:15:10 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          19.03.6-ce\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.13.7\r\n  Git commit:       369ce74a3c\r\n  Built:            Thu Feb 13 18:14:54 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.3.3.m\r\n  GitCommit:        d76c121f76a5fc8a462dc64594aea72fe18e1178.m\r\n runc:\r\n  Version:          1.0.0-rc10\r\n  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 31\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 31\r\n Images: 145\r\n Server Version: 19.03.6-ce\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: d76c121f76a5fc8a462dc64594aea72fe18e1178.m\r\n runc version: dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n init version: fec3683\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.5.4-arch1-1\r\n Operating System: Arch Linux\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 15.39GiB\r\n Name: workstation\r\n ID: REDACTED\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Username: REDACTED\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```"},{"labels":[null,null,"enhancement",null],"text":"The tmpfs API is quite old, since this was created we added a more generic mounts API.\r\nWe should migrate the tmpfs backend to use the same backend as the mounts API."},{"labels":[null,"enhancement"],"text":"```\r\ndocker logs\r\n```\r\n\r\noutputs the whole container history, which is non-sense for a container that has been alive for some time.\r\n\r\nHence as far as I'm concerned, I *always* have type `docker logs --tail=100 -f <container_name>` which is crazy long for something that feels like a default behavior.\r\n\r\nWhichever value different than `all` would work for me: eg. 10 ou 100.\r\n\r\nBut who wants to output 10k lines by default and block the process for 2/3mins?"},{"labels":[null,"enhancement"],"text":"A long long time ago (in this galaxy, though), we added support for container stats.\r\nThe API was designed to just push stats at an interval until the client disconnects.\r\n\r\nNot too much later, people were wanting to be able to just fetch a single set of stats and be done with it, so we added a query param `stream=false`, which makes dockerd hang up after a single stat is sent. Not the best naming but it did what it needed to do.\r\n\r\nBut then, oh no... now the docker CLI, when using `--no-stream`, always shows 0% CPU usage because there was only 1 CPU stat and nothing compare CPU usage with. It was decided to collect 2 stats instead of just 1 so that the client can calculate the usage.\r\n\r\nFast forward to where we are now....\r\n\r\nHere we have a problem. Any tool wanting to sample docker takes 2x as long (nearly 2 seconds) to collect per container than before the mentioned change (which admittedly was years ago).\r\nMeanwhile every sampling tool is just going to sample as often as it needs to and calculate usage from there and does not care about the primed stats.\r\n\r\nIt would have been nice if we changed the CLI to make 2 collections rather than expecting the engine to do this for them. It wasn't a breaking change to make the engine do 2 collections, but certainly seems (or at least requiring an API bump) to backtrack and only take 1 collection.\r\n\r\nI was hoping to make a patch to work around this problem but I can't really think of one that doesn't involve bumping the API... save for probably a long shot where we could change the engine to accept more than just true/false from the query param (`?stream=`)... but even this could only be taken advantage of by a custom client lib, not the official one because that is accepting a bool in the function arguments.\r\n\r\nSo opening this as an intention to fix this, and also to solicit feedback for ideas we could do possibly as a back-portable patch."},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\nBoth the default apparmor and seccomp profiles contain restrictions for **mount**. While seccomp allows it for CAP_SYS_ADMIN, apparmor blocks it altogether:   https://github.com/moby/moby/blob/2ebaeef943cc8c11d63a9175fd132792bcd7d376/profiles/seccomp/default.json#L573\r\nhttps://github.com/moby/moby/blob/2ebaeef943cc8c11d63a9175fd132792bcd7d376/profiles/apparmor/template.go#L35  \r\nBecause of this, even containers with CAP_SYS_ADMIN are unable to **mount**.  \r\n\r\nIs there a reason for limiting it in both configurations? I find that such a redundancy makes debugging more difficult and increases the chance of creating faulty configuration.\r\n\r\n\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Use a program that relies on **mount** in a container with CAP_SYS_ADMIN\r\n\r\n**Describe the results you received:**\r\n`permission denied`\r\n\r\n**Describe the results you expected:**\r\nA successful execution.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.3\r\n API version:       1.40\r\n Go version:        go1.12.10\r\n Git commit:        a872fc2f86\r\n Built:             Tue Oct  8 01:00:44 2019\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.3\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.12.10\r\n  Git commit:       a872fc2f86\r\n  Built:            Tue Oct  8 00:59:17 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.2.10\r\n  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339\r\n runc:\r\n  Version:          1.0.0-rc8+dev\r\n  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n\r\n```"},{"labels":[null,null,null,null,"enhancement"],"text":"The `.dockerignore` file has been around for a while, but there's still room for improvement. Opening this as a ticket, based on some notes I had, and some existing issues. I can break down this issue further (and open separate tickets where needed)\r\n\r\n#### Some related issues:\r\n\r\n- [ ] https://github.com/moby/moby/issues/40318 Verify parity between .dockerignore and .gitignore syntax\r\n- [ ] https://github.com/moby/moby/issues/30018 Exception patterns in .dockerignore do not support wildcard directories\r\n- [ ] https://github.com/moby/moby/issues/41433 wildcard directories not implemented correctly in dockerignore\r\n- [ ] https://github.com/moby/moby/issues/12843 Global .dockerignore\r\n- [ ] (re)consider nested .dockerignore files https://github.com/moby/moby/issues/20944, and https://github.com/moby/moby/pull/21020#issuecomment-210106299\r\n- [ ] https://github.com/moby/moby/issues/9294 \"Output the reason while cache need to be invalidated on docker build\"\r\n- [ ] https://github.com/moby/moby/issues/15771 COPY with excluded files is not possible\r\n\r\n#### Assorted issues / suggestions / topics\r\n\r\n- Improve performance where possible. Ignoring files should make builds smaller and _faster_, not cause builds to be slow.\r\n    - Would be great if it's possible to profile builds, and see how much time was taken by _excluding_ files. Having this would allow users to make better informed decisions wether or not `.dockerignore` is the right solution.\r\n    - The dockerignore file is often used to make the build-context smaller in order to speed up builds, but with the introduction of BuildKit, some of those may no longer be relevant (and `.dockerignore` could actually have a negative impact in some situations)\r\n- Make `.dockerignore` follow the _exact_ same patterns as `.gitignore` (which it was originally modeled after)\r\n    - `.gitignore` pattern reference: https://git-scm.com/docs/gitignore#_pattern_format\r\n    - if possible, run the .git test-suite / tests to verify this\r\n- Add a global `.dockerignore` to allow excluding common files (`.idea`, `Thumbs.db` files and such). See [moby#12843](https://github.com/moby/moby/issues/12843 Global .dockerignore)\r\n    - allow easy discovery _why_ a file or directory was ignored (which becomes more important if a global ignore file is active). Also see next bullet\r\n- Introduce a command similar to [`git check-ignore`](https://git-scm.com/docs/git-check-ignore) for debugging\r\n    - related: https://github.com/moby/moby/issues/9294 \"Output the reason while cache need to be invalidated on docker build\"\r\n- Note: recursive `.dockerignore` files were proposed at some point, but were considered a risky, breaking change (see https://github.com/moby/moby/issues/20944, and https://github.com/moby/moby/pull/21020#issuecomment-210106299)\r\n    - should this be reconsidered?\r\n- Improve and validate the [`.dockerignore` documentation](https://docs.docker.com/engine/reference/builder/#dockerignore-file)\r\n    - document the per-Dockerfile dockerignore (when using BuildKit and Docker 19.03 or up: https://github.com/moby/moby/issues/12886#issuecomment-480575928, https://github.com/moby/moby/issues/12886#issuecomment-518843764)\r\n    - improve docs by referring to / borrowing from `.gitignore` (where applicable) (see https://github.com/moby/moby/issues/40318 \"BUG: `.dockerignore` sometimes ignores `**`\")\r\n    - could be moved to a separate page if needed\r\n    - provide better examples (also for Windows platforms)\r\n    - best practices (what files to consider for excluding to optimize caching; when to use `.dockerignore`, when *not* to use and (e.g.) use BuildKit, optimize your Dockerfile, use multi-stage builds etc.)\r\n    - common pitfalls\r\n- (related) consider implementing `COPY --exclude` to allow specifying ignore-rules in the Dockerfile itself; this can help with situations where (e.g.) frequently changing files are mixed with \"cacheable\" files (proposal in https://github.com/moby/moby/issues/15771). Allowing such files to be `COPY`'d separately would give more control over layers/caching.\r\n- (Slightly related) improve errors produced due to files that were excluded. The current error for missing files is confusing (as it mentions the file path on the _daemon_ side), therefore making it hard to trace the error back to `.dockerignore` (see the discussion on https://github.com/docker/for-mac/issues/1922, and https://github.com/moby/moby/issues/37605)\r\n\r\n\r\n\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nWhen building images, sometimes `**` will be matched, other times it'll be ignored.\r\nChanging `.dockerignore` line, might influence matching of later lines.\r\n\r\n**Steps to reproduce the issue:**\r\nRun following script:\r\nNotice, that last line of `.dockerignore` is matched the second time, but it isn't matched the first time.\r\n```bash\r\n#!/usr/bin/env bash\r\n\r\nset -euo pipefail\r\n\r\ndockerignore() {\r\n  (\r\n    # There is a bug in docker standard build kit - this seems to fix it\r\n    export DOCKER_BUILDKIT=1\r\n    docker build --no-cache -t build-context -f - . 2>/dev/null <<EOF\r\n    FROM busybox\r\n    COPY . /build-context\r\n    WORKDIR /build-context\r\n    CMD find .\r\nEOF\r\n    docker run --rm build-context\r\n    docker image rm build-context > /dev/null\r\n  )\r\n}\r\n\r\nmsg() {\r\n  printf '##### %s #####\\n' \"${*}\"\r\n}\r\n\r\ntmpdir=\"$(mktemp -d)\"\r\ntrap 'rm -rf \"${tmpdir}\"' EXIT\r\n\r\ncd \"${tmpdir}\"\r\n\r\nmkdir -p nested-dir\r\ntouch {nested-dir/,}something.{allow,ignore}{1,2}\r\n\r\nmsg 'Real folder structure:'\r\nfind . \r\n\r\nmsg 'Applying first .dockerignore config:'\r\ntee .dockerignore <<BUGGED\r\n*\r\n!something.allow1\r\n!something.allow2\r\n!**/something.allow1\r\n!**/something.allow2\r\nBUGGED\r\n\r\nmsg 'Contents in first docker image:'\r\ndockerignore\r\n\r\n\r\nmsg 'Applying second .dockerignore config:'\r\ntee .dockerignore <<BUGGED\r\n*\r\n!something.allow1\r\n!something.allow2\r\n!nested-dir/something.allow1\r\n!**/something.allow2\r\nBUGGED\r\n\r\nmsg 'Contents in second docker image:'\r\ndockerignore\r\n```\r\n\r\n**Describe the results you received:**\r\nWhen using first `.dockerignore` contents, files I wanted to include from `nested-dir/` are not included.\r\n\r\n```\r\n##### Real folder structure: #####\r\n.\r\n./something.ignore2\r\n./something.allow2\r\n./nested-dir\r\n./nested-dir/something.ignore2\r\n./nested-dir/something.allow2\r\n./nested-dir/something.ignore1\r\n./nested-dir/something.allow1\r\n./something.ignore1\r\n./something.allow1\r\n##### Applying first .dockerignore config: #####\r\n*\r\n!something.allow1\r\n!something.allow2\r\n!**/something.allow1\r\n!**/something.allow2\r\n##### Contents in first docker image: #####\r\n.\r\n./something.allow2\r\n./something.allow1\r\n##### Applying second .dockerignore config: #####\r\n*\r\n!something.allow1\r\n!something.allow2\r\n!nested-dir/something.allow1\r\n!**/something.allow2\r\n##### Contents in second docker image: #####\r\n.\r\n./something.allow2\r\n./nested-dir\r\n./nested-dir/something.allow2\r\n./nested-dir/something.allow1\r\n./something.allow1\r\n```\r\n\r\n**Describe the results you expected:**\r\n`**` should always match `any number of directories (including zero)` as [per documentation.](https://docs.docker.com/engine/reference/builder/#dockerignore-file)\r\nAfter using the first `.dockerignore` contents, docker context should have selected files from `nested-dir/`\r\n\r\n```\r\n##### Real folder structure: #####\r\n.\r\n./something.ignore2\r\n./something.allow2\r\n./nested-dir\r\n./nested-dir/something.ignore2\r\n./nested-dir/something.allow2\r\n./nested-dir/something.ignore1\r\n./nested-dir/something.allow1\r\n./something.ignore1\r\n./something.allow1\r\n##### Applying first .dockerignore config: #####\r\n*\r\n!something.allow1\r\n!something.allow2\r\n!**/something.allow1\r\n!**/something.allow2\r\n##### Contents in first docker image: #####\r\n.\r\n./something.allow2\r\n./nested-dir\r\n./nested-dir/something.allow2\r\n./nested-dir/something.allow1\r\n./something.allow1\r\n##### Applying second .dockerignore config: #####\r\n*\r\n!something.allow1\r\n!something.allow2\r\n!nested-dir/something.allow1\r\n!**/something.allow2\r\n##### Contents in second docker image: #####\r\n.\r\n./something.allow2\r\n./nested-dir\r\n./nested-dir/something.allow2\r\n./nested-dir/something.allow1\r\n./something.allow1\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.5\r\n API version:       1.40\r\n Go version:        go1.12.12\r\n Git commit:        633a0ea838\r\n Built:             Wed Nov 13 07:29:52 2019\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.5\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.12.12\r\n  Git commit:       633a0ea838\r\n  Built:            Wed Nov 13 07:28:22 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.2.10\r\n  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339\r\n runc:\r\n  Version:          1.0.0-rc8+dev\r\n  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 6\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 6\r\n Images: 139\r\n Server Version: 19.03.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: b34a5c8af56e510852c35414db4c1f4fa6172339\r\n runc version: 3e425f80a8c931f88e6d94a8c831b9d5aa481657\r\n init version: fec3683\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 4.15.0-72-generic\r\n Operating System: Linux Mint 19.2\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 15.16GiB\r\n Name: Limonka\r\n ID: W7BY:I3ZZ:YGXV:G2PZ:4YIR:WKID:NEJR:E35Z:466L:NLT6:MKYF:ES52\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,null,null,"enhancement"],"text":"When using `host` mode networking, port-mapping / publishing ports is not possible.\r\n\r\nFor `docker run`, this combination was silently ignored in the past (https://github.com/moby/moby/issues/35500), but a warning was added in https://github.com/moby/moby/pull/35510;\r\n\r\n```bash\r\ndocker run -d --name foo --network=host --publish target=80,published=8000 nginx:alpine\r\nWARNING: Published ports are discarded when using host network mode\r\n```\r\n\r\nFor services, no such warning or error is currently in place, so the service is created \"successfully\", but actual deployment fails, producing a confusing error message (see https://github.com/moby/moby/issues/34215) \"container cannot be disconnected from host network or connected to host network\"\r\n\r\n\r\n```console\r\ndocker service create --name foo --network host --publish target=80,published=8000 nginx:alpine\r\n\r\n4tgz2472wtum44wsn4cegq0a3\r\noverall progress: 0 out of 1 tasks\r\n1/1: container cannot be disconnected from host network or connected to host ne…\r\n```\r\n\r\nWe should;\r\n\r\n- improve handling of this situation, and make this a hard _error_ when attempting to create a service using `host` mode networking, and attempting to publish/map a port\r\n- (related) consider making the same situation for `docker run` an _error_ as well, instead of a warning\r\n- make sure that this error is produced both when deploying a service using `docker service create`, `docker service update`, and when using `docker stack deploy`\r\n- if needed, take cross-platform situations into account (i.e., the manager may be running on a different OS than the worker (windows/linux)\r\n\r\n\r\nIdeally, these checks are performed by the _daemon_ so that the error is produced irregardless which tool is used to connect with the API.\r\n"},{"labels":[null,"enhancement"],"text":"For openrc init systems it would be good to have possibility to configure HTTP(S) proxy servers for docker daemon.\r\ne.g. for systemd there is documented approach https://docs.docker.com/config/daemon/systemd/#httphttps-proxy.\r\n"},{"labels":[null,"enhancement"],"text":"\r\n**Description**\r\n\r\nDepending on how docker image tar was saved it does not have tags recorded in it. So I use\r\n\r\n```\r\ndocker load -i image.tar\r\n```\r\n\r\nI get an image with out a tag.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Use\r\n```\r\ndocker load -i image.tar\r\n```\r\ncommand to load a image tar which does not have tag \r\n2. run\r\n```\r\ndocker images\r\n```\r\nand see that the docker image does not have a tag.\r\n\r\n3.\r\n\r\n**Describe the results you received:**\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.2\r\n API version:       1.40\r\n Go version:        go1.12.8\r\n Git commit:        6a30dfc\r\n Built:             Thu Aug 29 05:26:49 2019\r\n OS/Arch:           windows/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.2\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.12.8\r\n  Git commit:       6a30dfc\r\n  Built:            Thu Aug 29 05:32:21 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          v1.2.6\r\n  GitCommit:        894b81a4b802e4eb2a91d1ce216b8817763c29fb\r\n runc:\r\n  Version:          1.0.0-rc8\r\n  GitCommit:        425e105d5a03fabd737a126ad93d62a9eeede87f\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n"},{"labels":[null,null,"enhancement"],"text":"Currently, we're running tests without selinux enabled. Would be good to make sure stuff still goes with selinux enabled."},{"labels":[null,"enhancement"],"text":"The repository contains some code that was generated from `.proto` files, but we don't have a make target, nor a step in CI to check if those files need to be regenerated (for example when updating versions of gogo/protobuf or go/protobuf); see https://github.com/moby/moby/pull/40077#discussion_r336258155\r\n\r\nWe should add a make target to make it easy to regenerated those files, and a step in CI to verify the generated files "},{"labels":[null,null,"enhancement"],"text":"or at least an environment to test user namespace functionality after docker-ce packages have been built."},{"labels":[null,"enhancement",null],"text":"This test, and probably other tests that spin up new daemons, use the default configuration, which means they tried to setup a daemon/client to use; see \r\n\r\n- a `unix://` socket for exposing the API\r\n- use `--storage-driver=overlay2`\r\n\r\nBoth of those won't work on Windows, which means we're currently skipping those tests.\r\n\r\nWe should investigate if we can make those tests work on Windows as well, either by spinning up a daemon with different settings, or (depending on the test) by re-using a daemon for those tests. We don't run these tests in parallel, so re-using a daemon could be an option (if we properly clean-up state after each test).\r\n\r\nThere's some things to consider though;\r\n\r\n- the Linux daemons `docker load` a busybox image (and some optionally some other images) to be used for starting containers used during the tests. While this can be done fast (couple of seconds) on Linux, loading an image on Windows takes _minutes_, so is not a solution if we want to spin op a new daemon.\r\n- because of the above, we can start with tests that don't require containers to be created\r\n- re-use a daemon for other tests (see the paragraph higher up)\r\n\r\nSome details about the test-failures can be found in  https://github.com/moby/moby/pull/40155;\r\n\r\nhttps://ci.docker.com/public/blue/rest/organizations/jenkins/pipelines/moby/branches/PR-40154/runs/5/nodes/52/log/?start=0\r\n\r\n```\r\n00:14:14.623  --- FAIL: TestInfoDebug (0.01s)\r\n00:14:14.623      info_test.go:75: [dbe75bf7729f3] failed to start daemon with arguments [--containerd /var/run/docker/containerd/containerd.sock --data-root D:\\gopath\\src\\github.com\\docker\\docker\\bundles\\tmp\\TestInfoDebug\\dbe75bf7729f3\\root --exec-root C:\\windows\\TEMP\\dxr\\dbe75bf7729f3 --pidfile D:\\gopath\\src\\github.com\\docker\\docker\\bundles\\tmp\\TestInfoDebug\\dbe75bf7729f3\\docker.pid --userland-proxy=true --containerd-namespace dbe75bf7729f3 --containerd-plugins-namespace dbe75bf7729f3p --host unix://C:\\windows\\TEMP\\docker-integration\\dbe75bf7729f3.sock --storage-driver overlay2 --debug] : protocol not available\r\n00:14:14.623  === RUN   TestInfoInsecureRegistries\r\n00:14:14.623  --- FAIL: TestInfoInsecureRegistries (0.00s)\r\n00:14:14.623      info_test.go:100: [d3c745c16a39c] failed to start daemon with arguments [--containerd /var/run/docker/containerd/containerd.sock --data-root D:\\gopath\\src\\github.com\\docker\\docker\\bundles\\tmp\\TestInfoInsecureRegistries\\d3c745c16a39c\\root --exec-root C:\\windows\\TEMP\\dxr\\d3c745c16a39c --pidfile D:\\gopath\\src\\github.com\\docker\\docker\\bundles\\tmp\\TestInfoInsecureRegistries\\d3c745c16a39c\\docker.pid --userland-proxy=true --containerd-namespace d3c745c16a39c --containerd-plugins-namespace d3c745c16a39cp --host unix://C:\\windows\\TEMP\\docker-integration\\d3c745c16a39c.sock --debug --storage-driver overlay2 --insecure-registry=192.168.1.0/24 --insecure-registry=insecurehost.com:5000] : protocol not available\r\n00:14:14.623  === RUN   TestInfoRegistryMirrors\r\n00:14:14.623  --- FAIL: TestInfoRegistryMirrors (0.01s)\r\n00:14:14.623      info_test.go:124: [d277126ad0515] failed to start daemon with arguments [--containerd /var/run/docker/containerd/containerd.sock --data-root D:\\gopath\\src\\github.com\\docker\\docker\\bundles\\tmp\\TestInfoRegistryMirrors\\d277126ad0515\\root --exec-root C:\\windows\\TEMP\\dxr\\d277126ad0515 --pidfile D:\\gopath\\src\\github.com\\docker\\docker\\bundles\\tmp\\TestInfoRegistryMirrors\\d277126ad0515\\docker.pid --userland-proxy=true --containerd-namespace d277126ad0515 --containerd-plugins-namespace d277126ad0515p --host unix://C:\\windows\\TEMP\\docker-integration\\d277126ad0515.sock --debug --storage-driver overlay2 --registry-mirror=https://192.168.1.2 --registry-mirror=http://registry.mirror.com:5000] : protocol not available\r\n```\r\n\r\n"},{"labels":[null,"enhancement"],"text":"Just posting my notes here; we're trying to run more tests with different combinations of CLI and Daemon, which includes tests in the (frozen/deprecated) \"integration-cli\" suite.\r\n\r\nSome of those tests are dependent on CLI behaviour, so it's expected that those may fail, but a good occasion to review/update some of them so that we maintain coverage, without relying on CLI behavior. \r\n\r\n\r\n## Debugging TestInfoDebug\r\n\r\nhttps://github.com/moby/moby/blob/23b6b5a9ae0e16bdcf37d0ec9a5a8bee9cb38f97/integration-cli/docker_cli_info_test.go#L177-L193\r\n\r\n- Test is failing because output format changed\r\n- Test looks to not be very useful, as it seems to be making some wrong assumptions (see below)\r\n    - Only difference between daemon with debug enabled and debug disabled, is the \"debug true/false\" in the info output\r\n    - The daemon _always_ sends the debug information (NProc, NFd, NGoroutines, SystemTime, NEventsListener, DockerRootDir)\r\n    - The CLI _hides_ the fields above if it detects that the _daemon_ does not have debug enabled\r\n    - This seems wrong, as showing/hiding is a client-side decision, so it should be\r\n      (if at all) dependent on the _CLI_ running with `--debug` enabled.\r\n    - Note that this does _not_ apply to the `--format='{{ json . }}'` output,\r\n      which always shows all fields that are received from the daemon.\r\n    - Perhaps we should always show the information we received (especially\r\n      `\"System Time\"` looks to be useful, not just for debugging purposes)\r\n- The `docker info` output itself is already widely covered by unit tests in the\r\n  docker/cli repository, which test that information from a `Info` API response\r\n  is printed (and in the right format), both in JSON and \"pretty\" formats;\r\n  https://github.com/docker/cli/tree/a8ff7f821017ae3d5347392c82718d6cba221681/cli/command/system/testdata\r\n\r\n\r\n```bash\r\nmkdir results && cd results\r\n```\r\n\r\n### 17.06 client, 17.06 daemon (debug disabled)\r\n\r\n\r\n```bash\r\nexport cid=\"$(docker run -d --rm --privileged -v $(pwd)/results:/results -v /var/lib/docker docker:17.06-dind dockerd)\"\r\n\r\ndocker exec -it \"$cid\" sh\r\napk add --no-cache curl jq\r\ndocker         info > /results/17.06_cli_daemon.txt\r\ndocker --debug info > /results/17.06_cli_debug_daemon.txt\r\ndocker         info --format='{{ json .}}' | jq . > /results/17.06_cli_daemon.json\r\ndocker --debug info --format='{{ json .}}' | jq . > /results/17.06_cli_debug_daemon.json\r\ncurl  --unix-socket /var/run/docker.sock http://localhost/info | jq . > /results/17.06_curl_info_daemon.json\r\n```\r\n\r\n### 17.06 client, 17.06 daemon (debug enabled)\r\n\r\n```bash\r\nexport cid=\"$(docker run -d --rm --privileged -v $(pwd)/results:/results -v /var/lib/docker docker:17.06-dind dockerd --debug)\"\r\n\r\ndocker exec -it \"$cid\" sh\r\n\r\napk add --no-cache curl jq\r\ndocker         info > /results/17.06_cli_daemon_debug.txt\r\ndocker --debug info > /results/17.06_cli_debug_daemon_debug.txt\r\ndocker         info --format='{{ json .}}' | jq . > /results/17.06_cli_daemon_debug.json\r\ndocker --debug info --format='{{ json .}}' | jq . > /results/17.06_cli_debug_daemon_debug.json\r\ncurl  --unix-socket /var/run/docker.sock http://localhost/info | jq . > /results/17.06_curl_info_daemon_debug.json\r\n```\r\n\r\n### 19.03 client, 19.03 daemon (debug disabled)\r\n\r\n\r\n```bash\r\nexport cid=\"$(docker run -d --rm --privileged -v $(pwd)/results:/results -v /var/lib/docker docker:19.03-dind dockerd)\"\r\n\r\ndocker exec -it \"$cid\" sh\r\napk add --no-cache curl jq\r\ndocker         info > /results/19.03_cli_daemon.txt\r\ndocker --debug info > /results/19.03_cli_debug_daemon.txt\r\ndocker         info --format='{{ json .}}' | jq . > /results/19.03_cli_daemon.json\r\ndocker --debug info --format='{{ json .}}' | jq . > /results/19.03_cli_debug_daemon.json\r\ncurl  --unix-socket /var/run/docker.sock http://localhost/info | jq . > /results/19.03_curl_info_daemon.json\r\n```\r\n\r\n### 19.03 client, 19.03 daemon (debug enabled)\r\n\r\n```bash\r\nexport cid=\"$(docker run -d --rm --privileged -v $(pwd)/results:/results -v /var/lib/docker docker:19.03-dind dockerd --debug)\"\r\n\r\ndocker exec -it \"$cid\" sh\r\n\r\napk add --no-cache curl jq\r\ndocker         info > /results/19.03_cli_daemon_debug.txt\r\ndocker --debug info > /results/19.03_cli_debug_daemon_debug.txt\r\ndocker         info --format='{{ json .}}' | jq . > /results/19.03_cli_daemon_debug.json\r\ndocker --debug info --format='{{ json .}}' | jq . > /results/19.03_cli_debug_daemon_debug.json\r\ncurl  --unix-socket /var/run/docker.sock http://localhost/info | jq . > /results/19.03_curl_info_daemon_debug.json\r\n```\r\n\r\nResults are uploaded below (renamed `json` to `json.txt` because GitHub doesn't support uploading JSON):\r\n\r\n[17.06_cli_daemon_debug.json.txt](https://github.com/moby/moby/files/3743119/17.06_cli_daemon_debug.json.txt)\r\n[17.06_cli_daemon_debug.txt](https://github.com/moby/moby/files/3743120/17.06_cli_daemon_debug.txt)\r\n[17.06_cli_daemon.json.txt](https://github.com/moby/moby/files/3743121/17.06_cli_daemon.json.txt)\r\n[17.06_cli_daemon.txt](https://github.com/moby/moby/files/3743122/17.06_cli_daemon.txt)\r\n[17.06_cli_debug_daemon_debug.json.txt](https://github.com/moby/moby/files/3743123/17.06_cli_debug_daemon_debug.json.txt)\r\n[17.06_cli_debug_daemon_debug.txt](https://github.com/moby/moby/files/3743124/17.06_cli_debug_daemon_debug.txt)\r\n[17.06_cli_debug_daemon.json.txt](https://github.com/moby/moby/files/3743125/17.06_cli_debug_daemon.json.txt)\r\n[17.06_cli_debug_daemon.txt](https://github.com/moby/moby/files/3743126/17.06_cli_debug_daemon.txt)\r\n[17.06_curl_info_daemon_debug.json.txt](https://github.com/moby/moby/files/3743127/17.06_curl_info_daemon_debug.json.txt)\r\n[17.06_curl_info_daemon.json.txt](https://github.com/moby/moby/files/3743128/17.06_curl_info_daemon.json.txt)\r\n[19.03_cli_daemon_debug.json.txt](https://github.com/moby/moby/files/3743129/19.03_cli_daemon_debug.json.txt)\r\n[19.03_cli_daemon_debug.txt](https://github.com/moby/moby/files/3743130/19.03_cli_daemon_debug.txt)\r\n[19.03_cli_daemon.json.txt](https://github.com/moby/moby/files/3743131/19.03_cli_daemon.json.txt)\r\n[19.03_cli_daemon.txt](https://github.com/moby/moby/files/3743132/19.03_cli_daemon.txt)\r\n[19.03_cli_debug_daemon_debug.json.txt](https://github.com/moby/moby/files/3743133/19.03_cli_debug_daemon_debug.json.txt)\r\n[19.03_cli_debug_daemon_debug.txt](https://github.com/moby/moby/files/3743134/19.03_cli_debug_daemon_debug.txt)\r\n[19.03_cli_debug_daemon.json.txt](https://github.com/moby/moby/files/3743135/19.03_cli_debug_daemon.json.txt)\r\n[19.03_cli_debug_daemon.txt](https://github.com/moby/moby/files/3743136/19.03_cli_debug_daemon.txt)\r\n[19.03_curl_info_daemon_debug.json.txt](https://github.com/moby/moby/files/3743137/19.03_curl_info_daemon_debug.json.txt)\r\n[19.03_curl_info_daemon.json.txt](https://github.com/moby/moby/files/3743138/19.03_curl_info_daemon.json.txt)\r\n\r\n\r\n\r\n"},{"labels":[null,"enhancement"],"text":"Docker does not sees a new cpuset if hot added after provisioning a VM with less CPUs because it hard codes somehow its original cpuset in `/sys/fs/cgroup/cpuset/docker/cpuset.cpus`\r\n\r\nIn other words:\r\nBoxes that were provisioned with `N` CPUS and later you bumped the resources to `N+x` CPU cores, will experience the problem where Docker will only see N CPUs available in every container instead of N+x CPUs\r\n\r\nI believe containers should use the value from `docker info` => `CPUs: 8` (runtime) instead of hardcoding it during installation.\r\n\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a VM and provision it with 2 CPUs, install docker and everything.\r\n\r\n2. During installation docker writes the available cpuset at the moment (0-1) to `/sys/fs/cgroup/cpuset/docker/cpuset.cpus`\r\n\r\nAt this point `docker info` displays:\r\n```\r\nKernel Version: 3.10.0-514.26.2.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 11.69GiB\r\n```\r\n\r\n3. Eventually you figure out you need more resources, because your apps require so, so you add 6 more CPUs.\r\n\r\nIf you run `docker info` indeed it knows about your new 6 CPUS\r\n\r\n```\r\nKernel Version: 3.10.0-514.26.2.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 11.69GiB\r\n```\r\n### Host OS\r\nYour OS knows about them\r\n```\r\n$ nproc\r\n8\r\n```\r\n\r\nbut the namespaced cpuset in docker is not refreshed (obviously):\r\n```\r\n$ cat /sys/fs/cgroup/cpuset/docker/cpuset.cpus\r\n0-1\r\n```\r\n\r\n### Containers\r\nContainers doesn't know about these new cores as well\r\n```\r\n$ docker run -it centos:7 nproc\r\n2\r\n```\r\n\r\nIf I try to use **more** cpus than available it claims that it knows about the new cpuset\r\n```\r\n$ docker run -it --cpuset-cpus=0-10 centos:7 nproc\r\ndocker: Error response from daemon: Requested CPUs are not available - requested 0-10, available: 0-7.\r\n```\r\n\r\nBut if I try to use a range in between the available ones (0-3):\r\n\r\n```\r\n$ docker run -it --cpuset-cpus=0-3 centos:7 nproc\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused \"process_linux.go:275: applying cgroup configuration for process caused \\\"failed to write 0-3 to cpuset.cpus: write /sys/fs/cgroup/cpuset/docker/e55dce73f5174f16fb360f01a774163da027315d0a1f648268e3fca6368cdd4a/cpuset.cpus: permission denied\\\"\": unknown\r\n```\r\n\r\nWhich led me to [536](https://github.com/docker/for-linux/issues/536), even though this is related to 536 it is not fully related\r\n\r\n### The solution seems to be\r\n```\r\necho \"0-7\" > /sys/fs/cgroup/cpuset/docker/cpuset.cpus\r\n\r\n# or more \"dynamically\"\r\necho \"0-$(( $(nproc) - 1 ))\" >  /sys/fs/cgroup/cpuset/docker/cpuset.cpus\r\n\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nI don't think this affects a particular version of Docker, rather all versions\r\n\r\n<details><summary><bold>Output of `docker version`: </bold></summary>\r\n<p><br>\r\n\r\n```\r\nClient:\r\n Version:           18.09.1\r\n API version:       1.39\r\n Go version:        go1.10.6\r\n Git commit:        4c52b90\r\n Built:             Wed Jan  9 19:35:01 2019\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.1\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.10.6\r\n  Git commit:       4c52b90\r\n  Built:            Wed Jan  9 19:06:30 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n```\r\n<br>\r\n</p>\r\n</details>\r\n\r\n\r\n<details><summary><bold>Output of `docker info`: </bold></summary>\r\n<p><br>\r\n\r\n```\r\nContainers: 14\r\n Running: 9\r\n Paused: 0\r\n Stopped: 5\r\nImages: 10\r\nServer Version: 18.09.1\r\nStorage Driver: overlay\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: active\r\n NodeID: qskzvycajleyypxy56rjzt03n\r\n Is Manager: true\r\n ClusterID: c9bw8td1bw1zoc5ms77iwmkf3\r\n Managers: 3\r\n Nodes: 5\r\n Default Address Pool: 10.0.0.0/8  \r\n SubnetSize: 24\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 10.18.0.72\r\n Manager Addresses:\r\n  10.18.0.71:2377\r\n  10.18.0.72:2377\r\n  10.18.0.73:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 9754871865f7fe2f4e74d43e2fc7ccd237edcbce\r\nrunc version: 96ec2177ae841256168fcf76954f7177af9446eb\r\ninit version: fec3683\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.10.0-514.26.2.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 11.69GiB\r\nName: xxx.xxxxx.x\r\nID: DXTN:VFYO:IJUP:DCAX:LEVQ:VKL6:2MMK:GLOI:3RKF:5PAX:DWP7:SLO3\r\nDocker Root Dir: /usr/share/mobi/docker_data\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\nProduct License: Community Engine\r\n```\r\n\r\n<br>\r\n</p>\r\n</details>\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nNote: I really don't think this is an issue but I wanted to point it out if anyone ever faces something related to this."},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nFile 1\r\n\r\n```\r\nFROM base\r\n\r\nCOPY path/1/file tmp/\r\n```\r\nFile 2\r\n```\r\nFROM base\r\n\r\nCOPY path/2/file tmp/\r\n```\r\nIf both files are equal, and the only difference is the source path, then a build for File2 still does not use the build cache from File1. This seems too restricitve seeing that the result of both operations is or should be the same.\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. build file 1\r\n2. build file 2\r\n3. check whether cache was used\r\n\r\n**Describe the results you received:**\r\n\r\nBuild of file 2 copies the file and does not use the cached layer from file 1.\r\n\r\n**Describe the results you expected:**\r\n\r\nBuild of file 2 used the cache of file one for the COPY.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           18.09.0\r\n API version:       1.39\r\n Go version:        go1.11.1\r\n Git commit:        4d60db472b\r\n Built:             Thu Nov  8 21:14:51 2018\r\n OS/Arch:           windows/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.1\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.10.6\r\n  Git commit:       4c52b90\r\n  Built:            Wed Jan  9 19:41:57 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 65\r\n Running: 3\r\n Paused: 0\r\n Stopped: 62\r\nImages: 331\r\nServer Version: 18.09.1\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 9754871865f7fe2f4e74d43e2fc7ccd237edcbce\r\nrunc version: 96ec2177ae841256168fcf76954f7177af9446eb\r\ninit version: fec3683\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.14.92-boot2docker\r\nOperating System: Boot2Docker 18.09.1 (TCL 8.2.1)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 3.856GiB\r\nName: default\r\nID: COJA:SX7R:PMCY:232N:VE7A:MQ2A:7DH6:UQE6:XEZB:L2CU:VJZV:ETMO\r\nDocker Root Dir: /mnt/sda1/var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nHTTP Proxy: http://10.0.2.2:3128\r\nHTTPS Proxy: http://10.0.2.2:3128\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\n provider=virtualbox\r\nExperimental: true\r\nInsecure Registries:\r\n eu.artifactory.conti.de:7014\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\nProduct License: Community Engine\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nI see these results both on Windows with Docker Toolbox, as well as on Linux."},{"labels":["enhancement"],"text":"**Use Case**\r\nFor the purpose of CI/CD, I'd like be able to `bind mount` docker socket from Docker host into container under a different user (other than root) and specific permission. This lets us use docker from inside the Docker container for various CI/CD tasks as a user other than root.\r\n\r\nI expect something like running `docker run -v /var/run/docker.sock:/var/run/docker.sock/:<user>:<group>:<perm>`\r\n\r\nRelated to [#2259](https://github.com/moby/moby/issues/2259) but this one is a more specific case - it only deals with `bind mount` (not volume)"},{"labels":[null,"enhancement"],"text":"Tracking issue for some remaining linting issues to be fixed after https://github.com/moby/moby/pull/39668 is merged.\r\n\r\n```\r\nclient/hijack.go:85:16:                               SA1019: httputil.NewClientConn is deprecated: Use the Client or Transport in package net/http instead.  (staticcheck)\r\nintegration-cli/docker_api_attach_test.go:245:12:     SA1019: httputil.NewClientConn is deprecated: Use the Client or Transport in package net/http instead.  (staticcheck)\r\nintegration/plugin/authz/authz_plugin_test.go:180:7:  SA1019: httputil.NewClientConn is deprecated: Use the Client or Transport in package net/http instead.  (staticcheck)\r\nintegration/plugin/authz/authz_plugin_test.go:479:12: SA1019: httputil.NewClientConn is deprecated: Use the Client or Transport in package net/http instead.  (staticcheck)\r\n```\r\n\r\n\r\nThese warnings are currently suppressed in `hack/validate/golangci-lint.yml` (after https://github.com/moby/moby/pull/39668 is merged), so when this is fixed, those lines should be removed"},{"labels":[null,"enhancement"],"text":"Tracking issue for some remaining linting issues to be fixed after https://github.com/moby/moby/pull/39668 is merged.\r\n\r\n```\r\ndistribution/registry.go:84:3: SA1019: base.Dial is deprecated: Use DialContext instead, which allows the transport to cancel dials as soon as they are no longer needed. If both are set, DialContext takes priority.  (staticcheck)\r\nregistry/registry.go:188:3:    SA1019: base.Dial is deprecated: Use DialContext instead, which allows the transport to cancel dials as soon as they are no longer needed. If both are set, DialContext takes priority.  (staticcheck)\r\n```\r\n\r\n\r\nThese warnings are currently suppressed in `hack/validate/golangci-lint.yml` (after https://github.com/moby/moby/pull/39668 is merged), so when this is fixed, those lines should be removed"},{"labels":[null,"enhancement"],"text":"Xattrs is deprecated, so we should replace this with PAXRecords. IIRC, containerd already uses these, but we must take care to remain backward compatible\r\n\r\n```\r\npkg/archive/archive.go:407:3:       SA1019: hdr.Xattrs is deprecated: Use PAXRecords instead.  (staticcheck)\r\npkg/archive/archive.go:408:3:       SA1019: hdr.Xattrs is deprecated: Use PAXRecords instead.  (staticcheck)\r\npkg/archive/archive.go:661:26:      SA1019: hdr.Xattrs is deprecated: Use PAXRecords instead.  (staticcheck)\r\npkg/archive/archive_linux.go:47:7:  SA1019: hdr.Xattrs is deprecated: Use PAXRecords instead.  (staticcheck)\r\npkg/archive/archive_linux.go:48:12: SA1019: hdr.Xattrs is deprecated: Use PAXRecords instead.  (staticcheck)\r\n```\r\n\r\nThese warnings are currently suppressed in `hack/validate/golangci-lint.yml` (after https://github.com/moby/moby/pull/39668 is merged), so when this is fixed, those lines should be removed"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nNow to enable HTTP_PROXY / HTTPS_PROXY to pull images from proxy, you need add it to systemd configuration and **restart dockerd daemon**\r\nhttps://docs.docker.com/engine/reference/commandline/pull/#proxy-configuration\r\n\r\nIt's blocker for Production envs due to several points:\r\n1. Daemon restart lead to containers restart\r\n2. When proxy server is down you need change it to other proxy as fast as possible\r\n\r\n**Describe the results you expected:**\r\n\r\ndocker cli (docker-compose) should use values from env vars\r\n\r\n```\r\nexport HTTP_PROXY=xxx\r\nexport HTTPS_PROXY=xxx\r\ndocker pull XXX\r\ndocker-compose pull\r\n```\r\n"},{"labels":[null,null,"enhancement"],"text":"Currently `docker run --runtime=kata --privileged` is insecure because `--privileged` let OCI runtime mount host devices.\r\n\r\nRecently containerd/CRI got a new config option `privileged_without_host_devices` which disables mounting host devices for containers with `securityContext.privileged`.\r\nhttps://github.com/containerd/cri/pull/1225\r\n\r\nProbably we can port over this to Moby as `--privileged --security-opt privileged-without-host-devices`. The daemon probably should return an error if `--security-opt privileged-without-host-devices` is specified but `--privileged` is not specified."},{"labels":[null,"enhancement"],"text":"Currently, we run a bunch of tests on the moby/moby codebase for each PR and against the master branch of the repo after a PR is merged, but the results of the tests are in approximately [400KB of plain-text format](https://jenkins.dockerproject.org/view/Docker/job/Docker-PRs/55176/consoleFull). To find out which test failed, a user would have to view the console logs of the jenkins job and do a browser text search for `FAIL:`.\r\n\r\nWould be better to have the test results also reported in a structured format for consumption by report generators to make pretty graphs and tables and such. Having the test result in a [junit xml format](https://github.com/windyroad/JUnit-Schema) would allow jenkins UI to consume and generate reports with the [junit plugin](https://plugins.jenkins.io/junit).\r\n\r\nThe type of information we could easily find with a junit xml format output:\r\n* name of all the test case failures (assuming the jenkins job does not stop running tests upon first failure)\r\n* time it took to run each test case\r\n* summary of how many tests were run, how many passed, how many failed, and how many were skipped\r\n\r\nWould need to generate junit xml format for each of the 4 test frameworks in moby/moby:\r\n1. [unit](https://github.com/moby/moby/blob/86dad145e90a9e22d95a0092eb11348f8ed9922d/hack/test/unit) tests - uses [golang testing](https://golang.org/pkg/testing/) and can output junit xml by using [gotestsum](https://github.com/gotestyourself/gotestsum) to execute tests\r\n1. [integration](https://github.com/moby/moby/tree/86dad145e90a9e22d95a0092eb11348f8ed9922d/integration) tests - uses [gotest.tools](https://github.com/gotestyourself/gotest.tools) and can output junit xml by using [gotestsum](https://github.com/gotestyourself/gotestsum) to execute tests\r\n1. [integration-cli](https://github.com/moby/moby/tree/86dad145e90a9e22d95a0092eb11348f8ed9922d/integration-cli) tests - uses [go-check](https://github.com/go-check/check) and can output junit xml by using [go2xunit](https://github.com/tebeka/go2xunit)\r\n1. [docker-py](https://github.com/moby/moby/blob/86dad145e90a9e22d95a0092eb11348f8ed9922d/hack/make/test-docker-py) tests - uses [pytest](https://github.com/pytest-dev/pytest) and can output junit xml by setting [--junitxml](http://doc.pytest.org/en/latest/usage.html#creating-junitxml-format-files) flag\r\n\r\n## Progress:\r\n\r\n- [x] unit tests: ~#39637~ #39638\r\n  - [x] unit test reports for non-amd64 architectures #39719\r\n  - [ ] unit test reports for windows #39971\r\n- [x] integration #39724\r\n- [ ] integration (windows) https://github.com/moby/moby/pull/39998\r\n- [x] integration-cli #39911\r\n- [ ] integration-cli (windows) (no PR yet)\r\n- [x] docker-py #39716\r\n\r\n"},{"labels":[null,"enhancement"],"text":"Timings in the current situation (after some improvements were made)\r\n\r\ntaken from https://ci.docker.com/public/job/moby/job/master/53/flowGraphTable/ (master) and https://ci.docker.com/public/job/moby/7 (https://github.com/moby/moby/pull/39747) for Windows.\r\n\r\nDuration in the \"New\" column is the total time to run the Job (which may include some setting up, printing `docker info` / `docker version`)\r\n\r\n\r\nStatus             | Job                | Old   | New   | Notes\r\n-------------------|--------------------|-------|-------|-----------------------\r\n:white_check_mark: | janky / x86_64     | 01:55 | 01:01 | Run in parallel integration (00:53), and split unit/validation (21:00)\r\n:white_check_mark: | experimental       | 01:35 | -     | Removed; covered by x86_64\r\n:white_check_mark: | PowerPC (PR)       | 02:00 | 00:19 | Only \"integration\"\r\n:construction:     | PowerPC (master)   | 02:00 | 01:42 | Parallel: integration: 00:19, integration-cli: 01:29\r\n:white_check_mark: | z / s390x (PR)     | 02:00 | 00:13 | Only \"integration\"\r\n:construction:     | z / s390x (master) | 02:00 | 01:41 | Parallel: integration: 00:13, integration-cli: 01:34\r\n:construction:     | Windows RS1        | 01:15 | 01:44 | unit + integration + integration-cli: 01:42 (_slower_ than old Jenkins (on first run?), because we now pull the base image, instead of loading it from a tar)\r\n:white_check_mark: | Windows RS5        | 01:30 | 01:05 | unit + integration + integration-cli: 01:04\r\n\r\n-------------------------\r\n\r\n\r\nCurrently, our moby/moby PR checks take about 2 hours. I think we can cut that in half with a bit of parallelization. The Jenkinsfile PR #38565 and build nodes now hooked up, I think we can do this. Breaking out the `unit` test into a separate step in PR #39638 is an example of setting up a parallel step.\r\n\r\nI've analyzed the current things we run in a PR check to see what else we can do:\r\n\r\nThe PR checks run in parallel:\r\n* janky [~115 minutes](https://jenkins.dockerproject.org/job/Docker-PRs/buildTimeTrend)\r\n  https://github.com/moby/moby/blob/4fb5e9e696fb50d65966a4b6b3c7906046d7c6a4/hack/ci/janky#L5-L14\r\n* experimental [~95 minutes](https://jenkins.dockerproject.org/job/Docker-PRs-experimental/buildTimeTrend)\r\n  https://github.com/moby/moby/blob/4fb5e9e696fb50d65966a4b6b3c7906046d7c6a4/hack/ci/experimental#L7-L9\r\n* powerpc [~120 minutes](https://jenkins.dockerproject.org/job/Docker-PRs-powerpc/buildTimeTrend)\r\n  https://github.com/moby/moby/blob/4fb5e9e696fb50d65966a4b6b3c7906046d7c6a4/hack/ci/powerpc#L5-L6\r\n* z [~115 minutes](https://jenkins.dockerproject.org/job/Docker-PRs-s390x/buildTimeTrend)\r\n  https://github.com/moby/moby/blob/4fb5e9e696fb50d65966a4b6b3c7906046d7c6a4/hack/ci/z#L5-L6\r\n* windowsRS1 [~75 minutes](https://jenkins.dockerproject.org/job/Docker-PRs-WoW-RS1/buildTimeTrend)\r\n  https://github.com/moby/moby/blob/4fb5e9e696fb50d65966a4b6b3c7906046d7c6a4/hack/ci/windows.ps1#L810-L816\r\n* windowsRS5-process [~90 minutes](https://jenkins.dockerproject.org/job/Docker-PRs-WoW-RS5-Process/buildTimeTrend)\r\n  https://github.com/moby/moby/blob/4fb5e9e696fb50d65966a4b6b3c7906046d7c6a4/hack/ci/windows.ps1#L810-L816\r\n\r\nThe individual parts of the PR check have timings of approx:\r\n* hack/validate/default ~1 minute\r\n* hack/test/unit ~7 minutes\r\n* hack/make.sh binary-daemon ~1 minute\r\n* hack/make.sh dynbinary ~1 minute\r\n* hack/make.sh test-docker-py ~7 minute\r\n* hack/make.sh test-integration-flaky ~1 minute\r\n* hack/make.sh test-integration ~95 minutes\r\n  * integration ~10 minutes\r\n  * integration-cli ~85 minutes\r\n* hack/make.sh cross ~2 minutes\r\n\r\nI think the big one to parallelize into its own step would be the legacy `integration-cli` tests that take ~85 minutes. If we can also break that down further by test suite, we could then get to sub-1-hr tests."},{"labels":[null,"enhancement"],"text":"Perhaps a test suite specifically to exercise race detection: https://blog.golang.org/race-detector\r\n\r\nWould be good to setup a PR check for this, but if it takes a long time to run, then perhaps as a nightly job on master branch?"},{"labels":["enhancement",null],"text":"`State.OOMKilled` is never set when running `docker inspect` on a container on Windows, even if that container exceeded its memory limit. This is a known shortcoming; the test is [explicitly skipped on Windows](https://github.com/moby/moby/blob/b10cd5c0e6f479f131e33cf323b89212587fa38e/integration/container/kill_test.go). I'm aware that the \"OOM killer\" is Linux specific, but perhaps there's a way to communicate that a container exceeded its memory limit on Windows as well."},{"labels":[null,null,"enhancement",null],"text":"related:\r\n\r\n- https://github.com/moby/moby/issues/39572CI: configure apt-cacher / mirror for packages (or other solutions)\r\n- https://github.com/moby/moby/issues/36555 Feature: docker build --profile report for step duration and layer size\r\n\r\nThis is just a quick thought; happy to close if it's not useful;\r\n\r\nBuildKit allows [collecting detailed information about builds using opentracing](https://github.com/moby/buildkit#opentracing-support) (see https://github.com/moby/buildkit/pull/255), which would provide much more detailed information on build; example screenshot from that PR:\r\n\r\n<img width=\"1022\" alt=\"screen shot 2018-01-06 at 10 17 25 pm\" src=\"https://user-images.githubusercontent.com/585223/34647186-b524e952-f330-11e7-8bf1-b0f7edb0b77c.png\">\r\n\r\nShould we collect data about the `docker build` steps in our CI to keep track of performance improvements/regressions ?"},{"labels":[null,null,"enhancement",null],"text":"Also related: https://github.com/moby/moby/issues/39512\r\n\r\nCI is currently configured to use the `cdn-fastly.deb.debian.org` mirror. This was primarily done to fix flakiness in CI (where the upstream package repositories were down);\r\n\r\n- https://github.com/moby/moby/issues/21512 [Build process] Error fetching deb\r\n- https://github.com/moby/moby/pull/20665 Introduce `dockercore/buildbase` x64 base image\r\n- https://github.com/moby/moby/pull/21590 Allow specifying apt mirror in dockerfile\r\n- https://github.com/moby/moby/pull/28640 allow replacing httpredir mirror for arm\r\n\r\nWe should:\r\n\r\n### Investigate if we still should specify the mirror manually\r\n\r\nFrom the discussion in \r\n\r\n> I've got high hopes for http://deb.debian.org, but it's currently stretch-only, and isn't supported by debootstrap (yet), so we'd have to hard-code cdn-fastly.deb.debian.org for now, and I'm not sure how well-supported that'll stay without an official announcement that it ought to be used on debian-devel-announce. 😞\r\n\r\nLooking at the page at http://httpredir.debian.org, `deb.debian.org` already redirects to a mirror\r\n\r\n> If you hit the server behind deb.debian.org directly, either because you use an older apt or because you use a HTTP proxy that does not support SRV records, your requests will get HTTP redirected to one of the CDN instances. If you want to avoid the redirects, you can pick one instance directly. For instance, this also works in your sources.list:\r\n> ```\r\n> deb http://cdn-fastly.deb.debian.org/debian stable main\r\n> deb http://cdn-fastly.deb.debian.org/debian-security stable/updates main\r\n> \r\n> The redirection service is also available on HTTPS, so with the apt-transport-https package installed, you can use:\r\n> \r\n> ```\r\n> deb https://deb.debian.org/debian stable main\r\n> deb https://deb.debian.org/debian-security stable/updates main\r\n> ```\r\n>\r\n> This service is sponsored by Fastly and Amazon CloudFront.\r\n\r\n\r\n- Is `dn-fastly.deb.debian.org` the only mirror it redirects to?\r\n- Are there other mirrors that are faster for the machines we run on?\r\n  - i.e., for the AWS machines, is there a local/faster mirror that we currently exclude by manually overriding?\r\n  - the s390x/PowerPC/Windows machines run on different infrastructure; is this the optimum mirror for those?\r\n\r\n\r\n### Investigate if setting up `apt-cacher` would improve performance\r\n\r\nWe should investigate if setting up a apt-cacher / mirror for the packages we use improves speed; see comments on https://github.com/moby/moby/issues/21512#issuecomment-201551250\r\n\r\n> Another idea is to install our own apt-cacher, so that we can can cache the packages we use close to us, and limit the impact of a bad mirror. see https://help.ubuntu.com/community/Apt-Cacher-Server for more details.\r\n> ..\r\n>\r\n> https://docs.docker.com/engine/examples/apt-cacher-ng/\r\n\r\nWe'll have to profile the `docker build` steps to see how long they take (switching to `BuildKit` for the builds might give us more details about the builds through opentracing (https://github.com/moby/buildkit/pull/255), and we could collect metrics about that?)\r\n\r\n### Investigate using BuildKit (distributed) cache\r\n\r\nPerhaps instead of optimising at the package-manager level, taking advantage of BuildKit's improved caching might be a better solution. https://github.com/moby/moby/pull/20665 (Introduce `dockercore/buildbase` x64 base image) turned out to not be a good solution at the time, but things may have changed with BuildKit."},{"labels":[null,"enhancement"],"text":"Currently using `vfs`, should use `overlay2`. Hopefully brings performance improvements to running the PR checks. Using `overlay2` would be closer to production environment because that is the default graphdriver."},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nThe only resource limit for memory that you can pass via compose files is \"memory\".  Other limits are available via the command-line (https://docs.docker.com/config/containers/resource_constraints/#limit-a-containers-access-to-memory).\r\nThis means that it's not possible to apply these limits to Swarm services when using compose files to define the services.\r\n\r\nThe compose schema at \r\nhttps://github.com/docker/compose/blob/7ae632a9ee7530fcf81e212baa3e588f477ea862/compose/config/config_schema_v3.7.json shows that resource limits are limited (ha!) to \"memory\" and \"cpu\"\r\n\r\n**Steps to reproduce the issue:**\r\n1. Add \"memory-swap\" to the deploy: resources: limits: block in compose.\r\n2. Try deploying the service\r\n\r\n**Describe the results you received:**\r\n`memory-swap Additional property memory-swap is not allowed`\r\n\r\n**Describe the results you expected:**\r\nThat the limits which can be passed via the cli are accepted by compose.\r\n\r\n\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           18.09.6\r\n API version:       1.39\r\n Go version:        go1.10.8\r\n Git commit:        481bc77\r\n Built:             Sat May  4 02:35:27 2019\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.6\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.10.8\r\n  Git commit:       481bc77\r\n  Built:            Sat May  4 01:59:36 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 127\r\nServer Version: 18.09.6\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: active\r\n NodeID: l8hgak0hicao4h5a2b2vw5jrj\r\n Is Manager: true\r\n ClusterID: i1cfbjimxou9zs4lnepdnh4ls\r\n Managers: 1\r\n Nodes: 1\r\n Default Address Pool: 10.0.0.0/8\r\n SubnetSize: 24\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 10\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 10.43.106.72\r\n Manager Addresses:\r\n  10.43.106.72:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84\r\nrunc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30\r\ninit version: fec3683\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-151-generic\r\nOperating System: elementary OS 0.4.1 Loki\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 7.67GiB\r\nName: mootleberry\r\nID: DVYX:LEUX:EU5N:GEUC:JEJR:DZW3:5FPU:54QF:TWTM:RKB2:MYJD:YWI6\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: wheelybird\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\nProduct License: Community Engine\r\n\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nAll environments."},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nHello, I am an engineer working for ARM. \r\nI found there is not Jenkins CI run on arm64 platform, but in Jenkins inventory 2 machines are available: arm64v8-ubuntu-08 and arm64v8-ubuntu-09. \r\nIs there anything I can do to setup the CI."},{"labels":[null,null,"enhancement",null],"text":"This issue is closed related to issue  - https://github.com/moby/moby/issues/25526\r\n\r\nSwarm ingress should support injecting proxy protocol headers to preserve client information. Proxy protocol is a fully open standard and supported by most known network servers:\r\n1. https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt\r\n2. https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html\r\n3. https://cloud.google.com/load-balancing/docs/tcp/setting-up-tcp#proxy-protocol\r\n4. https://docs.openshift.com/container-platform/3.9/install_config/router/proxy_protocol.html\r\n5. https://docs.traefik.io/configuration/entrypoints/#proxyprotocol\r\n6. https://docs.nginx.com/nginx/admin-guide/load-balancer/using-proxy-protocol/\r\n7. https://aws.amazon.com/about-aws/whats-new/2013/07/30/elastic-load-balancing-now-supports-proxy-protocol/\r\n8. https://avinetworks.com/docs/16.3/proxy-protocol-support/\r\n9. https://blog.digitalocean.com/load-balancers-now-support-proxy-protocol/\r\n10. https://www.brightbox.com/blog/2019/03/12/proxy-protocol-load-balancer-api/\r\n\r\nKubernetes supports proxy protocol because it has pluggable ingress - both haproxy and nginx support it (e.g. https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#use-proxy-protocol)\r\n\r\nIn all fairness, there are some relevant pro and con discussions in Envoy for the same usecase\r\nhttps://github.com/envoyproxy/envoy/issues/4128 and\r\nhttps://github.com/envoyproxy/envoy/issues/1031\r\n\r\nProxy protocol is L4 and is therefore not constrained by L7 solutions like X-Forwarded-For. It is the most widely accepted L4 solution for maintaining source ip through a bunch of proxies.\r\n\r\n"},{"labels":[null,null,"enhancement"],"text":"**Description**\r\n\r\nStream the image directly to the tar writer instead of writing it to the file system first.\r\n\r\nDocker save currently uses disk space equivalent to the size of the image. This also slows it down compared to streaming the data to the tar writer.\r\n"},{"labels":[null,"enhancement"],"text":"I'm currently using containers for my workzone, and I'm just getting my hands on the new io-uring thingy in kernel 5.1 [here](http://git.kernel.dk/cgit/liburing/)\r\n\r\nAnd then I noticed that docker currently blocks the io-uring related requests :\r\n\r\n```\r\n/code/liburing/examples on  master! ⌚ 10:14:17\r\n$ ./io_uring-test io_uring-test.c\r\nqueue_init: Operation not permitted\r\n```\r\n\r\nWhile this test program runs without a problem on the host machine (I'm using a fedora30 host with docker-machine --drive generic)\r\n\r\nSo I thought this would be the place to suggest this feature"},{"labels":[null,null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nI've created a user defined attachable overlay swarm network. I can inspect it, but when i attempt to attach a container to it, i get the following error when running on the manager node:\r\n\r\n```\r\n$ docker network connect mrunner taskman_api.1.sp560iqxm3wszzgaylg1bc2y6 \r\nError response from daemon: network mrunner not found\r\n```\r\nThe network is defined and is attachable\r\n\r\n```\r\n$ docker network inspect mrunner\r\n[\r\n    {\r\n        \"Name\": \"mrunner\",\r\n        \"Id\": \"kviwxfejsuyc9476eznb7a8yw\",\r\n        \"Created\": \"2019-06-20T21:25:45.271304082Z\",\r\n        \"Scope\": \"swarm\",\r\n        \"Driver\": \"overlay\",\r\n        \"EnableIPv6\": false,\r\n        \"IPAM\": {\r\n            \"Driver\": \"default\",\r\n            \"Options\": null,\r\n            \"Config\": [\r\n                {\r\n                    \"Subnet\": \"10.0.1.0/24\",\r\n                    \"Gateway\": \"10.0.1.1\"\r\n                }\r\n            ]\r\n        },\r\n        \"Internal\": false,\r\n        \"Attachable\": true,\r\n        \"Ingress\": false,\r\n        \"ConfigFrom\": {\r\n            \"Network\": \"\"\r\n        },\r\n        \"ConfigOnly\": false,\r\n        \"Containers\": null,\r\n        \"Options\": {\r\n            \"com.docker.network.driver.overlay.vxlanid_list\": \"4098\"\r\n        },\r\n        \"Labels\": null\r\n    }\r\n]\r\n\r\n\r\n$ docker network ls\r\nNETWORK ID          NAME                DRIVER              SCOPE\r\n4a454d677dea        bridge              bridge              local\r\n95383b47ee94        docker_gwbridge     bridge              local\r\n249684755b51        host                host                local\r\nzgx0nppx33vj        ingress             overlay             swarm\r\nkviwxfejsuyc        mrunner             overlay             swarm\r\na30a12f8d7cc        none                null                local\r\nuftxcaoz9rzg        taskman_default     overlay             swarm\r\n\r\n```\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n```\r\n$ docker network connect mrunner taskman_api.1.sp560iqxm3wszzgaylg1bc2y6\r\nError response from daemon: No such container: taskman_api.1.sp560iqxm3wszzgaylg1bc2y6\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           18.09.6\r\n API version:       1.39\r\n Go version:        go1.10.8\r\n Git commit:        481bc77\r\n Built:             Sat May  4 02:35:57 2019\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.6\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.10.8\r\n  Git commit:       481bc77\r\n  Built:            Sat May  4 01:59:36 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 98\r\n Running: 3\r\n Paused: 0\r\n Stopped: 95\r\nImages: 360\r\nServer Version: 18.09.6\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: active\r\n NodeID: z5t0zxgpkpcf8s7ncpvxeeae3\r\n Is Manager: true\r\n ClusterID: 42z20jivdqscdyt06xu9rrj73\r\n Managers: 1\r\n Nodes: 1\r\n Default Address Pool: 10.0.0.0/8  \r\n SubnetSize: 24\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 10\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 10.105.46.39\r\n Manager Addresses:\r\n  10.105.46.39:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84\r\nrunc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30\r\ninit version: fec3683\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.15.0-51-generic\r\nOperating System: Ubuntu 18.04.2 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 15.46GiB\r\nName: pienso-jack\r\nID: 53ZQ:F4RN:HRSM:FKE3:7J5N:73DV:TTNK:K6XG:GMJK:DD6W:6SV4:6BDC\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\nProduct License: Community Engine\r\n\r\nWARNING: No swap limit support\r\n\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nTested on a Lenovo Thinkpad single node swarm on Ubuntu 18.04\r\nTested on docker swarm on OS X\r\n-- Same results on both \r\n"},{"labels":[null,null,"enhancement"],"text":"I just wanted to post this up because I know I've spoken to several people about this and all seem to at least agree just nothing seems to be happening to improve... basically we are in a bad situation with libnetwork:\r\n\r\n- Codebase is a mess (sorry, but it really really is), and few people are able to work on it/expertise in the lower levels of it are dwindling.\r\n- Reviews on the repo are not happening\r\n- Lacks many features (e.g. proper ipv6)\r\n- Slowest part of container startup (currently uses a re-exec and a runc hook to setup networking)\r\n- Most of the container networking space is focused on CNI.\r\n\r\nStarting this issue to so we can move the ball forward in this front and maybe even add a line item, once we have some level of agreement, to ROADMAP.md about what the plan to fix it is.\r\n\r\nI also want to say, a plan to fix does not mean that the maintainers will be doing the actual work, or that Docker Inc should be solely responsible. We should think about how can we get the community to engage (especially one weary of trying to contribute to libnetwork)."},{"labels":[null,"enhancement"],"text":"https://github.com/moby/moby/blob/master/contrib/check-config.sh#L194\r\n\r\nin Kernel 5.2 it's IP_NF_NAT (not NF_NAT_IPV4)"},{"labels":[null,"enhancement",null,null],"text":"**Description**\r\n\r\ndocker run's shm-size option is not honored while running linux containers in _Windows containers_ mode (LCOW).\r\n\r\n**Steps to reproduce the issue:**\r\n1. Enable LCOW\r\n2. `docker run --rm --shm-size=\"1g\" -it alpine /bin/sh`\r\n3. `df -h`\r\n\r\n**Describe the results you received:**\r\n/dev/shm is 481MB.\r\nFilesystem                Size      Used Available Use% Mounted on\r\noverlay                  19.7G     40.0K     18.7G   0% /\r\ntmpfs                    64.0M         0     64.0M   0% /dev\r\ntmpfs                   481.2M         0    481.2M   0% /sys/fs/cgroup\r\n**shm                     481.2M         0    481.2M   0% /dev/shm**\r\ntmpfs                   481.2M         0    481.2M   0% /proc/acpi\r\ntmpfs                    64.0M         0     64.0M   0% /proc/kcore\r\ntmpfs                    64.0M         0     64.0M   0% /proc/keys\r\ntmpfs                    64.0M         0     64.0M   0% /proc/timer_list\r\ntmpfs                    64.0M         0     64.0M   0% /proc/sched_debug\r\ntmpfs                   481.2M         0    481.2M   0% /sys/firmware\r\n\r\n**Describe the results you expected:**\r\nIf I switch back to Linux containers mode and running the same commands, the /dev/shm gets its expected size:\r\n`docker run --rm --shm-size=\"1g\" -it alpine /bin/sh`\r\n`df -h`\r\nFilesystem                Size      Used Available Use% Mounted on\r\noverlay                  58.4G      2.9G     52.5G   5% /\r\ntmpfs                    64.0M         0     64.0M   0% /dev\r\ntmpfs                   990.2M         0    990.2M   0% /sys/fs/cgroup\r\n/dev/sda1                58.4G      2.9G     52.5G   5% /etc/resolv.conf\r\n/dev/sda1                58.4G      2.9G     52.5G   5% /etc/hostname\r\n/dev/sda1                58.4G      2.9G     52.5G   5% /etc/hosts\r\n**shm                       1.0G         0      1.0G   0% /dev/shm**\r\ntmpfs                   990.2M         0    990.2M   0% /proc/acpi\r\ntmpfs                    64.0M         0     64.0M   0% /proc/kcore\r\ntmpfs                    64.0M         0     64.0M   0% /proc/keys\r\ntmpfs                    64.0M         0     64.0M   0% /proc/timer_list\r\ntmpfs                    64.0M         0     64.0M   0% /proc/sched_debug\r\ntmpfs                   990.2M         0    990.2M   0% /sys/firmware\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           18.09.2\r\n API version:       1.39\r\n Go version:        go1.10.8\r\n Git commit:        6247962\r\n Built:             Sun Feb 10 04:12:31 2019\r\n OS/Arch:           windows/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.2\r\n  API version:      1.39 (minimum version 1.24)\r\n  Go version:       go1.10.6\r\n  Git commit:       6247962\r\n  Built:            Sun Feb 10 04:28:48 2019\r\n  OS/Arch:          windows/amd64\r\n  Experimental:     true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 0\r\n Paused: 0\r\n Stopped: 1\r\nImages: 2\r\nServer Version: 18.09.2\r\nStorage Driver: windowsfilter (windows) lcow (linux)\r\n Windows:\r\n LCOW:\r\nLogging Driver: json-file\r\nPlugins:\r\n Volume: local\r\n Network: ics l2bridge l2tunnel nat null overlay transparent\r\n Log: awslogs etwlogs fluentd gelf json-file local logentries splunk syslog\r\nSwarm: inactive\r\nDefault Isolation: hyperv\r\nKernel Version: 10.0 17763 (17763.1.amd64fre.rs5_release.180914-1434)\r\nOperating System: Windows 10 Enterprise Version 1809 (OS Build 17763.503)\r\nOSType: windows\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 15.95GiB\r\nName: **REDACTED**\r\nID: D562:4FVF:M3ZU:ZXAU:VMEP:YBML:D56X:SIOR:5PQR:ZLNA:LUXG:K2HG\r\nDocker Root Dir: C:\\ProgramData\\Docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: -1\r\n Goroutines: 33\r\n System Time: 2019-06-02T22:29:19.2498729+02:00\r\n EventsListeners: 1\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\nProduct License: Community Engine\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nphysical"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nAs the question post [here](https://stackoverflow.com/questions/56339181/how-to-escape-colon-when-add-device-to-docker-container), if device name has `:` in it, `--device` cannot work.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n     docker run -it --rm --device /dev/serial/by-path/pci-0000:00:14.0-usb-0:8:1.0-port0 -v /dev:/dev ubuntu /bin/bash\r\n\r\n**Describe the results you received:**\r\n\r\n> invalid argument \"/dev/serial/by-path/pci-0000:00:14.0-usb-0:8:1.0-port0\" for \"--device\" flag: bad format for path: /dev/serial/by-path/pci-0000:00:14.0-usb-0:8:1.0-port0\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\nNo error.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           18.09.2\r\n API version:       1.39\r\n Go version:        go1.10.6\r\n Git commit:        6247962\r\n Built:             Sun Feb 10 04:13:47 2019\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.1\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.10.6\r\n  Git commit:       4c52b90\r\n  Built:            Wed Jan  9 19:02:44 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n\r\n```\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 12\r\n Running: 5\r\n Paused: 0\r\n Stopped: 7\r\nImages: 168\r\nServer Version: 18.09.1\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 9754871865f7fe2f4e74d43e2fc7ccd237edcbce\r\nrunc version: 96ec2177ae841256168fcf76954f7177af9446eb\r\ninit version: fec3683\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.15.0-29-generic\r\nOperating System: Ubuntu 18.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 7.591GiB\r\nName: shubuntu1\r\nID: KPEF:JCF2:SO6Y:P4BN:NJTG:4QWN:D324:MUJP:3TM4:4VYC:K3VB:FUNV\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\nProduct License: Community Engine\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n"},{"labels":[null,"enhancement"],"text":"\r\n**Description**\r\n\r\n\r\nWould it be possible to specify differents ports used by Docker Swarm communications than the one by defaults?\r\n\r\n* 7946 TCP/UDP\r\n* 4789 UDP\r\n* 2377 TCP\r\n\r\nOn some environments, these ports can already be used by other processes.\r\n\r\n\r\n**Steps to reproduce the issue:**\r\n\r\nLaunch some processes which listen on port 7946 TCP for exemple on a multi hosts environment.\r\nOnce the Swarm Cluster initialized, the nodes are having issues to communicate\r\n\r\n**Describe the results you received:**\r\n\r\nGeneral issues with the Swarm communications\r\n\r\n**Describe the results you expected:**\r\n\r\nAt initialization of the swarm (or on docker engine configuration itself), it would be good to be able to specify on which ports we want Docker Swarm to communicate on.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           18.06.1-ce\r\n API version:       1.38\r\n Go version:        go1.10.3\r\n Git commit:        e68fc7a\r\n Built:             Tue Aug 21 17:24:56 2018\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          18.06.1-ce\r\n  API version:      1.38 (minimum version 1.12)\r\n  Go version:       go1.10.3\r\n  Git commit:       e68fc7a\r\n  Built:            Tue Aug 21 17:23:21 2018\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 6\r\n Running: 6\r\n Paused: 0\r\n Stopped: 0\r\nImages: 6\r\nServer Version: 18.09.6\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: active\r\n NodeID: fpff6d44df9tt0547kf80w920\r\n Is Manager: true\r\n ClusterID: sgfun10lqklr1auugkcylr6kt\r\n Managers: 3\r\n Nodes: 7\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 10\r\n Dispatcher:\r\n  Heartbeat Period: 30 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 10.3.34.34\r\n Manager Addresses:\r\n  10.3.34.21:2377\r\n  10.3.34.22:2377\r\n  10.3.34.34:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84\r\nrunc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30\r\ninit version: fec3683\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-87-generic\r\nOperating System: Ubuntu 16.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 3.859GiB\r\nName: xxxxxxx\r\nID: H6IH:27MG:FK4B:O2K3:HMWH:U3YE:YZ4P:BKMW:DBDE:UT4L:GPTD:PUCH\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n  xxxxxxxxxxxxxxxx\r\n  xxxxxxxxxxxxxxxx\r\nLive Restore Enabled: false\r\nProduct License: Community Engine\r\n```"},{"labels":[null,null,"enhancement"],"text":"When building images, let’s assume the Dockerfile and associated resources are found in the current working directory unless this path is explicitly overridden."},{"labels":["enhancement"],"text":"It would be very useful if `docker stats` command could show totals as well. There was an issue https://github.com/moby/moby/issues/18733 earlier but was declined. Not sure why.\r\n"},{"labels":["enhancement",null],"text":"It'd be very useful to have the capability to change the time in the container without changing the time on the host. This could be very helpful in testing scenarios like:\r\n\r\n- clock drifts in distributed systems\r\n- debugging time-related issues, such as expired SSL certificates\r\n- testing software for year-2038 compliance\r\n\r\nThe alternative right now is [libfaketime](https://github.com/wolfcw/libfaketime), but it has limitations (see libfaketime's compatibility issues) and can cause segmentation faults and unexpected errors."},{"labels":[null,null,"enhancement"],"text":"Tracking issue to get all the PR jobs to run in an environment where we have the latest docker-ce release with buildkit enabled.\r\n\r\nThis will make sure we're exercising the buildkit build functionality and we can then take advantage of the new build capabilities provided by buildkit (caching, new front-ends, whatnot).\r\n\r\ncc @dave-tucker @psftw @tonistiigi"},{"labels":[null,"enhancement"],"text":"ala https://docs.docker.com/engine/reference/commandline/ps/#filtering"},{"labels":[null,"enhancement"],"text":"Similar issue fixed earlier (commit: fa900bd30a0881772b8964ee4d91d791af5b13c8) has given rise to another issue where you can't install multiple group or packages since they . I have fixed the issue to have multiple -p and and -g options for multiple packages and groups and handle the spaces in the name properly. \r\n\r\nLet me know if can I raise a pull request for this change?"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nAn application that uses the `io_pgetevents` system call fails with -EPERM\r\n\r\n**Steps to reproduce the issue:**\r\n1. Build such an application (e.g. https://github.com/scylladb/seastar) in a container\r\n2. Run it\r\n3. Look at the pretty errors\r\n\r\n**Describe the results you received:**\r\nio_pgetevents() fails with -EPERM\r\n\r\n**Describe the results you expected:**\r\nio_pgetevents() does not fail with -EPERM\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n18.06.0-dev\r\n```\r\n\r\n"},{"labels":[null,null,"enhancement",null,null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nWhen building and pushing docker images in a shared environment (e.g. on a jenkins worker) there is always a possibility that between a build command and the following push command that someone else pulls the same tag as the one that is to be pushed.\r\n\r\nThis can result in a push of the other image instead of the one intended. The reason for this issue is that we can't set a source image when pushing, we have to use the same name on the local docker host as the name in the registry.\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. User 1: `docker build -t myregistry/an-image:stable .`\r\n2. User 2: `docker pull myregistry/an-image:stable`\r\n3. User 1: `docker push myregistry/an-image:stable` <- this will push the old image and not the one User 1 just built.\r\n\r\n\r\n**Suggestion**\r\n\r\nAdd an option to `docker push` that allows users to specify which source image they want to push.\r\nExample: `docker push --source <image-id-or-name> NAME[:TAG]`\r\n\r\nThis addition would allow users to minimize the risk of pushing the wrong image to the registry.\r\nAdding an option ensures backward compatibility of the command.\r\n"},{"labels":["enhancement"],"text":"**Description**\r\nAt present the number of event log entries is fixed to 256 (https://github.com/moby/moby/blob/master/daemon/events/events.go#L12). This request is to make the number of entries configurable. \r\n\r\n**Proposal** \r\nProvide a configuration option to dockerd (as a first stab `--events-limit int`) that default to 256 (current limit). \r\n\r\n**Considerations**\r\n\r\n1. Does there need to be an upper limit? "},{"labels":[null,"enhancement"],"text":"\r\n<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nThis is a follow up to issue# 14036 (https://github.com/moby/moby/issues/14036).  --env-file is a secure way to pass an environment that contains sensitive data.  Without it my data becomes shy, the cache flushes and all my bits go down the drain.\r\n\r\nI think the work-around is to `docker cp` the environment file and source it, but that means I also have to fix it up with quotes, escapes, etc.\r\n\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1.\r\n2.\r\n3.\r\n\r\n**Describe the results you received:**\r\n**Describe the results you expected:**\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nI like Chicken\r\n\r\n**Output of `docker version`:**\r\n**Output of `docker info`:**\r\n```\r\n#  docker --version && docker info\r\nDocker version 18.09.1, build 4c52b90\r\nContainers: 4\r\n Running: 0\r\n Jumping: 0\r\n Walking: 0\r\n Lying down: 0\r\n Paused: 0\r\n Stopped: 4\r\nImages: 69\r\nServer Version: 18.09.1\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 9754871865f7fe2f4e74d43e2fc7ccd237edcbce\r\nrunc version: 96ec2177ae841256168fcf76954f7177af9446eb\r\ninit version: fec3683\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-131-generic\r\nOperating System: Ubuntu 16.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 7.772GiB\r\nName: [censored]\r\nID: [censored]\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\nProduct License: Community Engine\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\ncreating a network using the `--ipv6` parameter also requires specifying a subnet to use. So that the ipv6 stack is added to the container without assigning a GUA, so that only the link local address is present.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nDocker version 18.09.2-ce, build 62479626f2\r\n```\r\n\r\nCurrent workaround to create a link local container is to add the ipam option for a fe80::/64 subnet, but that assigns two link local addresses.\r\n\r\nI think it should be able to create a container with only an link local address for e.g. unit test."},{"labels":[null,"enhancement"],"text":"Using `COPY --from` to copy an entire folder does not preserve the permissions of the top folder. Example:\r\n\r\n```\r\nFROM centos:7 AS prepare\r\n\r\nRUN mkdir -p /opt/test \\\r\n && echo \"Before chmod\" \\\r\n && ls -l /opt/ \\\r\n && touch /opt/test/file \\\r\n && chmod -R g=u /opt/test \\\r\n && echo \"After chmod\" \\\r\n && ls -l /opt/ \\\r\n && ls -l /opt/test/\r\n\r\nFROM centos:7\r\n\r\nCOPY --from=prepare /opt/test /opt/test\r\n\r\nRUN echo \"After copy\" \\\r\n && ls -l /opt/ \\\r\n && ls -l /opt/test/\r\n```\r\n\r\nAfter copy I expect:\r\n```\r\nls -l /opt/\r\ndrwxrwxr-x 2 root root 4096 Feb 11 22:06 test\r\n```\r\nAfter copy I have:\r\n```\r\nls -l /opt/\r\ndrwxr-xr-x 2 root root 4096 Feb 11 22:06 test\r\n```\r\n\r\nDocker version:\r\n```\r\nClient: Docker Engine - Community\r\n Version:           18.09.1\r\n API version:       1.39\r\n Go version:        go1.10.6\r\n Git commit:        4c52b90\r\n Built:             Wed Jan  9 19:34:26 2019\r\n OS/Arch:           windows/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.1\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.10.6\r\n  Git commit:       4c52b90\r\n  Built:            Wed Jan  9 19:41:49 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n```\r\nDocker info:\r\n```\r\nContainers: 6\r\n Running: 1\r\n Paused: 0\r\n Stopped: 5\r\nImages: 112\r\nServer Version: 18.09.1\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 9754871865f7fe2f4e74d43e2fc7ccd237edcbce\r\nrunc version: 96ec2177ae841256168fcf76954f7177af9446eb\r\ninit version: fec3683\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.125-linuxkit\r\nOperating System: Docker Desktop\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 6\r\nTotal Memory: 5.8GiB\r\nName: docker-desktop\r\nID: MXHC:RWZ7:3NKV:OCY4:EI6P:ZB4D:27NN:2SQE:3RDE:5MAO:VYQI:UW3R\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 54\r\n Goroutines: 101\r\n System Time: 2019-02-11T22:16:18.8130036Z\r\n EventsListeners: 1\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\nProduct License: Community Engine\r\n```"},{"labels":[null,null,"enhancement"],"text":"**Description**\r\nWhen executing `docker build .` in a directory wit a lot of files, it takes a long time until the message \"Sending build context to docker daemon\" is displayed. From a user perspective, the command  `docker build` seems to hang. \r\n\r\nAdding a message along the lines of  `Preparing to send the content of <context> to the docker daemon`  or `Preparing to send all contents of <context> to the docker daemon` could help to explain what is happening. `<context>` should be given as absolute path if local.\r\n\r\n\r\n**Steps to reproduce the issue:**\r\nAlternative 1:\r\n1. `cd $HOME`\r\n2. `touch Dockerfile` \r\n3. `docker build`\r\n\r\nAlternative 2:\r\n1. `docker build https://github.com/moby/moby.git` (This is not a valid repo for build, but shows the issue too)\r\n\r\n**Describe the results you received:**\r\nIn both alternatives, no output is produced for quite some while. It seems that `docker build` hangs or is unresponsive. \r\n\r\nThe output for Alternative 2 on Windows without a running docker daemon is:\r\n\r\n```\r\nPS > Get-Date; docker build https://github.com/moby/moby.git; Get-Date\r\nSunday, February 10, 2019 12:37:45 PM\r\n[error messages omitted ...] This error may also indicate that the docker daemon is not running.\r\nSunday, February 10, 2019 12:42:41 PM\r\n```\r\nIt takes 5 minutes for any output to show. This is worse for large local build contexts (150k files, 8GB ~ 20-30 minutes until output, depending on disk speed). \r\n\r\n**Describe the results you expected:**\r\n\"Something\" should happen. \r\n\r\nAdding a message like mentioned above would prepare the user for a potentially long-running operation or hint at potential issues with the usage of `docker build`. E.g. in#14172 users who execute `docker build .` in their `$HOME` directory) however in my case it was just a context with 150k files \r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nI had a large local build context (150k files, 8GB ) and it took ~20-30 minutes until the first output. The initial pass(?) of the buld context can be sped up by replacing the big file collection with a tar. In that case (100 files, still 8G) output was shown after ~30 seconds, \r\n\r\n**Output of `docker version`:**\r\n(docker daemon currently not running, this system is not setup for docker. I originally noticed the issue on another system but did not have the time to write this issue)\r\n```\r\nClient: Docker Engine - Community\r\n Version:           18.09.0\r\n API version:       1.39\r\n Go version:        go1.10.4\r\n Git commit:        4d60db4\r\n Built:             Wed Nov  7 00:47:51 2018\r\n OS/Arch:           windows/amd64\r\n Experimental:      false\r\n[docker daemon not running]\r\n```\r\n\r\n**Output of `docker info`:**\r\n(docker daemon currently not running, this system is not setup for docker. I originally noticed the issue on another system but did not have the time to write this issue)\r\n```\r\n[docker daemon not running]\r\n```"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nFrom time to time, in Docker Desktop, when we start our VM (which include Docker 18.09.1), dockerd fails to start with a containerd timeout error. Is it possible to change the timeout ? Any ideas how to prevent this ?\r\n\r\nOn our CI, we have this error twice a day. \r\n\r\n**Steps to reproduce the issue:**\r\n1. Start Docker Desktop\r\n\r\nDiagnostics tarball: [aftersuite.diagkit.zip](https://github.com/moby/moby/files/2795207/aftersuite.diagkit.38.zip)\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\n time=\"2019-01-25T07:04:44.977151687Z\" level=debug msg=\"Listener created for HTTP on unix (/var/run/docker.sock)\"\r\n time=\"2019-01-25T07:04:45.162571112Z\" level=info msg=\"libcontainerd: started new containerd process\" pid=1674\r\n time=\"2019-01-25T07:04:45.162840398Z\" level=info msg=\"parsed scheme: \\\"unix\\\"\" module=grpc\r\n time=\"2019-01-25T07:04:45.162856961Z\" level=info msg=\"scheme \\\"unix\\\" not registered, fallback to default scheme\" module=grpc\r\n time=\"2019-01-25T07:04:45.410986427Z\" level=info msg=\"ccResolverWrapper: sending new addresses to cc: [{unix:///var/run/docker/containerd/containerd.sock 0  <nil>}]\" module=grpc\r\n time=\"2019-01-25T07:04:45.411073113Z\" level=info msg=\"ClientConn switching balancer to \\\"pick_first\\\"\" module=grpc\r\n time=\"2019-01-25T07:04:45.411704138Z\" level=info msg=\"pickfirstBalancer: HandleSubConnStateChange: 0xc420206230, CONNECTING\" module=grpc\r\n time=\"2019-01-25T07:04:54.569090623Z\" level=info msg=\"starting containerd\" revision=9754871865f7fe2f4e74d43e2fc7ccd237edcbce version=v1.2.2\r\n time=\"2019-01-25T07:04:54.569234870Z\" level=debug msg=\"changing OOM score to -500\"\r\n time=\"2019-01-25T07:04:54.782698053Z\" level=info msg=\"loading plugin \"io.containerd.content.v1.content\"...\" type=io.containerd.content.v1\r\n time=\"2019-01-25T07:04:54.784545823Z\" level=info msg=\"loading plugin \"io.containerd.snapshotter.v1.btrfs\"...\" type=io.containerd.snapshotter.v1\r\n time=\"2019-01-25T07:04:54.786567883Z\" level=warning msg=\"failed to load plugin io.containerd.snapshotter.v1.btrfs\" error=\"path /var/lib/docker/containerd/daemon/io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter\"\r\n time=\"2019-01-25T07:04:54.786612170Z\" level=info msg=\"loading plugin \"io.containerd.snapshotter.v1.aufs\"...\" type=io.containerd.snapshotter.v1\r\n time=\"2019-01-25T07:04:54.885911502Z\" level=warning msg=\"failed to load plugin io.containerd.snapshotter.v1.aufs\" error=\"modprobe aufs failed: \"modprobe: module aufs not found in modules.dep\\n\": exit status 1\"\r\n time=\"2019-01-25T07:04:54.885964794Z\" level=info msg=\"loading plugin \"io.containerd.snapshotter.v1.native\"...\" type=io.containerd.snapshotter.v1\r\n time=\"2019-01-25T07:04:54.889901357Z\" level=info msg=\"loading plugin \"io.containerd.snapshotter.v1.overlayfs\"...\" type=io.containerd.snapshotter.v1\r\n time=\"2019-01-25T07:04:54.891825699Z\" level=info msg=\"loading plugin \"io.containerd.snapshotter.v1.zfs\"...\" type=io.containerd.snapshotter.v1\r\n time=\"2019-01-25T07:04:55.021518729Z\" level=warning msg=\"failed to load plugin io.containerd.snapshotter.v1.zfs\" error=\"path /var/lib/docker/containerd/daemon/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter\"\r\n time=\"2019-01-25T07:04:55.021582732Z\" level=info msg=\"loading plugin \"io.containerd.metadata.v1.bolt\"...\" type=io.containerd.metadata.v1\r\n time=\"2019-01-25T07:04:55.025851643Z\" level=warning msg=\"could not use snapshotter zfs in metadata plugin\" error=\"path /var/lib/docker/containerd/daemon/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter\"\r\n time=\"2019-01-25T07:04:55.025888221Z\" level=warning msg=\"could not use snapshotter btrfs in metadata plugin\" error=\"path /var/lib/docker/containerd/daemon/io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter\"\r\n time=\"2019-01-25T07:04:55.025895800Z\" level=warning msg=\"could not use snapshotter aufs in metadata plugin\" error=\"modprobe aufs failed: \"modprobe: module aufs not found in modules.dep\\n\": exit status 1\"\r\n time=\"2019-01-25T07:04:55.166231705Z\" level=error msg=\"failed connecting to containerd\" error=\"failed to dial \\\"/var/run/docker/containerd/containerd.sock\\\": context deadline exceeded\" module=libcontainerd\r\n time=\"2019-01-25T07:04:55.266561864Z\" level=info msg=\"killing and restarting containerd\" module=libcontainerd pid=1674\r\n time=\"2019-01-25T07:04:55.267250447Z\" level=debug msg=\"received signal\" signal=user defined signal 1\r\n time=\"2019-01-25T07:04:55.268086716Z\" level=info msg=\"=== BEGIN goroutine stack dump ===\r\n goroutine 14 [running]:\r\n github.com/containerd/containerd/cmd/containerd/command.dumpStacks()\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/cmd/containerd/command/main_unix.go:78 +0x8c\r\n github.com/containerd/containerd/cmd/containerd/command.handleSignals.func1(0xc4200e9920, 0xc4200e98c0, 0x190aba0, 0xc4200b0010, 0xc420094660)\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/cmd/containerd/command/main_unix.go:53 +0x274\r\n created by github.com/containerd/containerd/cmd/containerd/command.handleSignals\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/cmd/containerd/command/main_unix.go:43 +0x8b\r\n\r\n goroutine 1 [syscall]:\r\n syscall.Syscall(0x4b, 0x3, 0x0, 0x0, 0x67d3e4, 0xc4200b4370, 0xc420364000)\r\n \t/usr/local/go/src/syscall/asm_linux_amd64.s:18 +0x5\r\n syscall.Fdatasync(0x3, 0x3, 0x0)\r\n \t/usr/local/go/src/syscall/zsyscall_linux_amd64.go:446 +0x42\r\n github.com/containerd/containerd/vendor/go.etcd.io/bbolt.fdatasync(0xc4203f6d20, 0x4000, 0x4000)\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/bolt_linux.go:9 +0x3f\r\n github.com/containerd/containerd/vendor/go.etcd.io/bbolt.(*DB).init(0xc4203f6d20, 0x0, 0xc4200845b0)\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/db.go:438 +0x1b4\r\n github.com/containerd/containerd/vendor/go.etcd.io/bbolt.Open(0xc4201035e0, 0x48, 0x1a4, 0x20cf680, 0xc420422ce8, 0x1, 0x0)\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/db.go:218 +0x240\r\n github.com/containerd/containerd/services/server.LoadPlugins.func2(0xc420357ab0, 0xc42023e720, 0x21, 0xc42032e620, 0x1e)\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/services/server/server.go:264 +0x49d\r\n github.com/containerd/containerd/plugin.(*Registration).Init(0xc4200b7900, 0xc420357ab0, 0xc4200b7900)\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/plugin/plugin.go:100 +0x3a\r\n github.com/containerd/containerd/services/server.New(0x190aba0, 0xc4200b0010, 0xc4204307e0, 0x1, 0xc4204dbc80, 0x0)\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/services/server/server.go:120 +0x557\r\n github.com/containerd/containerd/cmd/containerd/command.App.func1(0xc4201351e0, 0xc4201351e0, 0xc4204dbd07)\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/cmd/containerd/command/main.go:141 +0x67e\r\n github.com/containerd/containerd/vendor/github.com/urfave/cli.HandleAction(0x16f0900, 0x18e7090, 0xc4201351e0, 0xc4200e9860, 0x0)\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/vendor/github.com/urfave/cli/app.go:502 +0xca\r\n github.com/containerd/containerd/vendor/github.com/urfave/cli.(*App).Run(0xc4203cee00, 0xc4200b60f0, 0x5, 0x5, 0x0, 0x0)\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/vendor/github.com/urfave/cli/app.go:268 +0x60e\r\n main.main()\r\n \tgithub.com/containerd/containerd/cmd/containerd/main.go:33 +0x51\r\n\r\n goroutine 19 [syscall]:\r\n os/signal.signal_recv(0x18fa520)\r\n \t/usr/local/go/src/runtime/sigqueue.go:139 +0xa8\r\n os/signal.loop()\r\n \t/usr/local/go/src/os/signal/signal_unix.go:22 +0x24\r\n created by os/signal.init.0\r\n \t/usr/local/go/src/os/signal/signal_unix.go:28 +0x43\r\n\r\n goroutine 20 [chan receive]:\r\n github.com/containerd/containerd/vendor/github.com/golang/glog.(*loggingT).flushDaemon(0x20aa9a0)\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/vendor/github.com/golang/glog/glog.go:879 +0x8d\r\n created by github.com/containerd/containerd/vendor/github.com/golang/glog.init.0\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/vendor/github.com/golang/glog/glog.go:410 +0x205\r\n\r\n goroutine 15 [select, locked to thread]:\r\n runtime.gopark(0x18e9ed0, 0x0, 0x115f5ff, 0x6, 0x18, 0x1)\r\n \t/usr/local/go/src/runtime/proc.go:291 +0x120\r\n runtime.selectgo(0xc42005a750, 0xc420094720)\r\n \t/usr/local/go/src/runtime/select.go:392 +0xe56\r\n runtime.ensureSigM.func1()\r\n \t/usr/local/go/src/runtime/signal_unix.go:549 +0x1f6\r\n runtime.goexit()\r\n \t/usr/local/go/src/runtime/asm_amd64.s:2361 +0x1\r\n\r\n goroutine 16 [select]:\r\n github.com/containerd/containerd/vendor/github.com/docker/go-events.(*Broadcaster).run(0xc4200b7950)\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/vendor/github.com/docker/go-events/broadcast.go:117 +0x3c4\r\n created by github.com/containerd/containerd/vendor/github.com/docker/go-events.NewBroadcaster\r\n \t/tmp/tmp.uRi922615F/src/github.com/containerd/containerd/vendor/github.com/docker/go-events/broadcast.go:39 +0x1b1\r\n\r\n === END goroutine stack dump ===\"\r\n time=\"2019-01-25T07:04:55.567850463Z\" level=error msg=\"containerd did not exit successfully\" error=\"signal: killed\" module=libcontainerd\r\n panic: runtime error: invalid memory address or nil pointer dereference\r\n [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x157e8f0]\r\n\r\n goroutine 42 [running]:\r\n github.com/docker/docker/vendor/github.com/containerd/containerd.(*Client).Close(0x0, 0x0, 0x0)\r\n \t/go/src/github.com/docker/docker/vendor/github.com/containerd/containerd/client.go:536 +0x30\r\n github.com/docker/docker/libcontainerd/supervisor.(*remote).monitorDaemon(0xc4201ea1a0, 0x2766900, 0xc4201f1a00)\r\n \t/go/src/github.com/docker/docker/libcontainerd/supervisor/remote_daemon.go:321 +0x262\r\n created by github.com/docker/docker/libcontainerd/supervisor.Start\r\n \t/go/src/github.com/docker/docker/libcontainerd/supervisor/remote_daemon.go:90 +0x3fa\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           18.09.1\r\n API version:       1.39\r\n Go version:        go1.10.6\r\n Git commit:        4c52b90\r\n Built:             Wed Jan  9 19:33:12 2019\r\n OS/Arch:           darwin/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.1\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.10.6\r\n  Git commit:       4c52b90\r\n  Built:            Wed Jan  9 19:41:49 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n```\r\n"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nThe syslog logging driver currently always specifies `-` for the structured data portion of `rfc5424` formatted messages. I would like to see a `--log-opt` added to be able to specify structured data to be sent with each log line. Perhaps something like `--log-opt \"syslog-structured-data=foo@123 tag=name\"`\r\n\r\nAt Timber, though also common to other log providers that provide a syslog compliant-endpoint (e.g. Loggly), we require the API key to be specified as a structured data field to be used for authentication of incoming messages.\r\n\r\nWe currently maintain our [own logging driver](https://github.com/timberio/timber-docker-logging-driver), but are considering shifting to syslog forwarded messages instead.\r\n\r\nRelated to https://github.com/moby/moby/issues/20354 and https://github.com/moby/moby/issues/26937.\r\n\r\nI'm happy to take a stab at a PR, but wanted to open this first to see if it would be a desirable feature and in-case anyone else is considering it."},{"labels":[null,null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n`docker info` should print the default builder  (legacy or buildkit)\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker info`\r\n\r\n**Describe the results you received:**\r\n```\r\nDefault builder: BuildKit\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nNo such info in `Docker info`\r\n\r\n"},{"labels":["enhancement"],"text":"**Description**\r\n\r\nWhen using the [\"github.com/docker/docker/pkg/mount\"](https://godoc.org/github.com/docker/docker/pkg/mount) package's `Mounted` function, it falsely claims that a certain path is not mounted when in fact it is. It appears that a path containing spaces is not properly unescaped from `/proc/self/mountinfo` before being compared.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `mount /dev/sdx /mnt/foo\\ bar`\r\n2. call `mount.Mounted(\"/mnt/foo bar\")`\r\n\r\n**Describe the results you received:**\r\n\r\n`false, nil`\r\n\r\n**Describe the results you expected:**\r\n\r\n`true, nil`\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nThis is probably something to check for on non-Linux platforms too."},{"labels":[null,null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\nold image may have empty environment, and the build from this image, will  returns message:\r\n invalid environment variable:\r\n**Steps to reproduce the issue:**\r\n1.old image has environment like:\r\n\r\n```\r\n\"Env\": [\r\n\"\",\r\n\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\r\n\"container=docker\"\r\n],\r\n```\r\n\r\nand the code is return error:\r\n\r\n```go\r\nopts/opts.go\r\nfunc ValidateEnv(val string) (string, error) {\r\n    arr := strings.Split(val, \"=\")\r\n    if arr[0] == \"\" {\r\n        return \"\", fmt.Errorf(\"invalid environment variable: %s\", val)\r\n    }\r\n```\r\n\r\n\r\n**Describe the results you received:**\r\n invalid environment variable:\r\n\r\n**Describe the results you expected:**\r\ndid we need to support such old image?\r\nwe can filter empty variable to do that\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, [etc.):**] \r\n \r\npre issue is closed: (https://github.com/moby/moby/issues/30356)\r\n"},{"labels":[null,"enhancement",null],"text":"_Redirecting an issue from the Windows container docs (https://github.com/MicrosoftDocs/Virtualization-Documentation/issues/928)_\r\n\r\nCurrently build accepts cpushares, cpusetcpus, cpuperiod, cpuquota but not the corresponding Windows args of CpuCount, CpuPercent of CreateContainer.  This issue to track adding those options to build.\r\n\r\nRef:\r\nhttps://docs.docker.com/engine/api/v1.39/#operation/ImageBuild\r\nhttps://docs.docker.com/engine/api/v1.39/#operation/ContainerCreate\r\n\r\n"},{"labels":[null,null,"enhancement"],"text":"`docker system df` currently doesn't show the amount of space consumed by plugins (see https://github.com/moby/moby/issues/33775).\r\n\r\nGiven that `docker system df` is intended to show the amount of space used by docker, I think it would make sense to include information about plugins as well."},{"labels":[null,"enhancement"],"text":"\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. cd to location of Dockerfile\r\n2. `docker build`\r\n\r\n**Describe the results you received:**\r\n`\"docker build\" requires exactly 1 argument.`\r\n\r\n**Describe the results you expected:**\r\ntreat `docker build` as `docker build .`\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           18.09.0\r\n API version:       1.39\r\n Go version:        go1.10.4\r\n Git commit:        4d60db4\r\n Built:             Wed Nov  7 00:47:43 2018\r\n OS/Arch:           darwin/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.0\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.10.4\r\n  Git commit:       4d60db4\r\n  Built:            Wed Nov  7 00:55:00 2018\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n```\r\n\r\n\r\n"},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nDocker service creation should have an option to fail a service creation if a required named volume does not exist rather than create a named volume with the default driver.\r\n\r\nIt is probably a bad idea to implicitly create a volume if the requested volume cannot be found given that this will silently create the service which will in turn store its data on an ephemeral volume. That ephemeral volume will not be available if the service is scheduled and that would cause many issue in a production environment.\r\n\r\nAdding a volume option during service creation which would check if the volume exist before scheduling the service would solve this issue without breaking backwards compatibility. \r\n"},{"labels":[null,null,"enhancement",null,null],"text":"Long time since https://github.com/Nvveen/Gotty has been updated. Looks like a much newer library can almost drop in with https://github.com/morikuni/aec\r\n\r\nNot a high priority."},{"labels":["enhancement",null,null],"text":"**Description**\r\n\r\n`docker system df` nor `docker ps -s` report container logs disk usage. In case someone built up heavily-logging application, this may lead users to wander around trying to understand what eats their space. This is especially viable for docker for mac, since it is all dumped inside VM.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.05.0-ce\r\n API version:  1.29\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:09:06 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.05.0-ce\r\n API version:  1.29 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:09:06 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 12\r\n Running: 7\r\n Paused: 0\r\n Stopped: 5\r\nImages: 62\r\nServer Version: 17.05.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 261\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 9048e5e50717ea4497b757314bad98ea3763c145\r\nrunc version: 9c2d8d184e5da67c95d601382adf14862e4f2228\r\ninit version: 949e6fa                                                                                                                                                                                                                        \r\nSecurity Options:                                                                                                                                                                                                                            \r\n apparmor                                                                                                                                                                                                                                    \r\n seccomp                                                                                                                                                                                                                                     \r\n  Profile: default                                                                                                                                                                                                                           \r\nKernel Version: 4.15.0-42-generic                                                                                                                                                                                                            \r\nOperating System: Linux Mint 18.1                                                                                                                                                                                                            \r\nOSType: linux                                                                                                                                                                                                                                \r\nArchitecture: x86_64                                                                                                                                                                                                                         \r\nCPUs: 8                                                                                                                                                                                                                                      \r\nTotal Memory: 31.24GiB                                                                                                                                                                                                                       \r\nName: vortex                                                                                                                                                                                                                                 \r\nID: W5AM:NNFP:T467:Y7WS:TETO:NUXI:2DIH:2BV7:KYEJ:QU2C:SWMH:FNJQ                                                                                                                                                                              \r\nDocker Root Dir: /var/lib/docker                                                                                                                                                                                                             \r\nDebug Mode (client): false                                                                                                                                                                                                                   \r\nDebug Mode (server): false                                                                                                                                                                                                                   \r\nRegistry: https://index.docker.io/v1/                                                                                                                                                                                                        \r\nExperimental: false                                                                                                                                                                                                                          \r\nInsecure Registries:                                                                                                                                                                                                                         \r\n <redacted>                                                                                                                                                                                           \r\n 127.0.0.0/8                                                                                                                                                                                                                                 \r\nLive Restore Enabled: false                                                                                                                                                                                                                  \r\n                                                                                                                                                                                                                                             \r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nStandard linux CE distribution"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nI would like to have support for the secomp rule action: `SCMP_ACT_LOG` which is mentioned here: http://man7.org/linux/man-pages/man3/seccomp_rule_add.3.html\r\n\r\nIt would be cool to use for auto-generating minimal seccomp profiles for docker images\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. create a seccomp profile that starts with\r\n\r\n```json\r\n{\r\n  \"defaultAction\": \"SCMP_ACT_LOG\",\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\n```bash\r\ndocker: Error response from daemon: OCI runtime create failed: string SCMP_ACT_LOG is not a valid action for seccomp: unknown.\r\nERRO[0000] error waiting for container: context canceled\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nTo see a bunch of syscalls being logged\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           18.09.0\r\n API version:       1.39\r\n Go version:        go1.10.4\r\n Git commit:        4d60db4\r\n Built:             Wed Nov  7 00:47:43 2018\r\n OS/Arch:           darwin/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.0\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.10.4\r\n  Git commit:       4d60db4\r\n  Built:            Wed Nov  7 00:55:00 2018\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 0\r\n Paused: 0\r\n Stopped: 1\r\nImages: 102\r\nServer Version: 18.09.0\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: active\r\n NodeID: t2pt34ds01y6ph2ume0mi5pny\r\n Is Manager: true\r\n ClusterID: feqcitiv60rseyuvczyannv5r\r\n Managers: 1\r\n Nodes: 1\r\n Default Address Pool: 10.0.0.0/8\r\n SubnetSize: 24\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 10\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 192.168.65.3\r\n Manager Addresses:\r\n  192.168.65.3:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 468a545b9edcd5932818eb9de8e72413e616e86e\r\nrunc version: 69663f0bd4b60df09991c08812a60108003fa340\r\ninit version: fec3683\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.125-linuxkit\r\nOperating System: Docker for Mac\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 7.786GiB\r\nName: linuxkit-025000000001\r\nID: FBGL:KLA6:N43C:LPEF:XGKO:BC3A:24HH:3QA3:Q6PC:XW6D:NEH2:MLAW\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 42\r\n Goroutines: 173\r\n System Time: 2018-12-08T06:05:42.489959672Z\r\n EventsListeners: 2\r\nHTTP Proxy: gateway.docker.internal:3128\r\nHTTPS Proxy: gateway.docker.internal:3129\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\nProduct License: Community Engine\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nThe overlay driver's diff function is implemented by NaiveDiffDriver. The roLayer must be implemented like this, but whether mountLayer can be directly packaged with upperdir, it will perform better than NaiveDiffDriver's diff function.\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1.\r\n2.\r\n3.\r\n\r\n**Describe the results you received:**\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"\r\n**Description**\r\n\r\nIn the file `default.json` there are several allowed variants of the API function `personality()`. But one variant is missing that is needed in some cases. (read [how we faced with this issue](https://board.asm32.info/docker-support.256/) in our project)\r\n\r\nIt is the personality `PER_LINUX32_3GB` == `0x08000008` - a 32bit program with memory limited to 3GB. (0xc0000000)\r\n\r\nThis personality is rarely used, but fully safe, so IMHO it worth to be added to the official default security profile. \r\n\r\nRegards\r\n"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nThe volume createdAt time is not really the time of creation of the docker volume, but more the time the volume was last updated.\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker volume create testCtime\r\n2. docker volume inspect testCtime > ctime1.log\r\n3. docker run --rm -v testCtime:/data debian:stretch-slim touch /data/a\r\n4. docker volume inspect testCtime > ctime2.log\r\n5. diff ctime1.log ctime2.log\r\n\r\n**Describe the results you received:**\r\nThe docker volume creation time is updated based on when new files are written to the volume. So it's actually the change time rather than the creation time.\r\n```bash\r\n➜  docker volume create testCtime\r\ntestCtime\r\n➜  docker volume inspect testCtime > ctime1.log\r\n➜  docker run --rm -v testCtime:/data debian:stretch-slim touch /data/a\r\n➜  docker volume inspect testCtime > ctime2.log\r\n➜  diff ctime1.log ctime2.log\r\n3c3\r\n<         \"CreatedAt\": \"2018-11-26T15:00:06+01:00\",\r\n---\r\n>         \"CreatedAt\": \"2018-11-26T15:00:19+01:00\",\r\n```\r\n\r\n**Describe the results you expected:**\r\nThe creation time of the docker volume should have remained the one from ctime1.log and not change when files are added or removed from the volume. \r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nThe problem seems to be with the way the creation time is computed on linux systems.\r\nhttps://github.com/moby/moby/blob/0b7cb16dde4a20d024c7be59801d63bcfd18611b/volume/local/local_unix.go#L97\r\nIt uses  \r\n```go\r\nfileInfo.Sys().(*syscall.Stat_t).Ctim.Unix()\r\n```\r\ninstead of \r\n```go\r\nfileInfo.Sys().(*syscall.Statx_t).Btim.Unix()\r\n```\r\nwhich is desribed here: https://golang.org/src/cmd/vendor/golang.org/x/sys/unix/ztypes_linux_amd64.go\r\n\r\nThe only constraint though is that `statx` is available with kernel versions 4.11+\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           18.06.1-ce\r\n API version:       1.38\r\n Go version:        go1.10.4\r\n Git commit:        \r\n Built:             Sat Aug 25 18:30:58 2018\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          18.06.1-ce\r\n  API version:      1.38 (minimum version 1.12)\r\n  Go version:       go1.10.4\r\n  Git commit:       e68fc7a215d7133c34aa18e3b72b4a21fd0c6136\r\n  Built:            Sat Aug 25 18:30:40 2018\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 2\r\n Running: 0\r\n Paused: 0\r\n Stopped: 2\r\nImages: 849\r\nServer Version: 18.06.1-ce\r\nStorage Driver: overlay\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 8f54c750c67e87a3b1c218208bbd5cc427c653e9 (expected: 468a545b9edcd5932818eb9de8e72413e616e86e)\r\nrunc version: N/A (expected: 69663f0bd4b60df09991c08812a60108003fa340)\r\ninit version: N/A (expected: )\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 4.18.16-96.current\r\nOperating System: Solus 3.9999\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 19.56GiB\r\nName: solus\r\nID: YHOL:PG7Q:YL6F:5XRA:7RYL:6RWA:VNGC:VVIX:BAQH:FTKU:HG4G:32CY\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: syeupload\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nNone"},{"labels":[null,null,"enhancement"],"text":"[`contrib/apparmor`](https://github.com/moby/moby/tree/master/contrib/apparmor) provides Apparmor profile for `dockerd` itself.\r\n\r\nHowever, it has not been maintained since Feb 2016.\r\n\r\nWe should update the profile and also make it more easy to use.\r\n"},{"labels":[null,"enhancement"],"text":"Hello,\r\n\r\nWe are working with docker and we aggregate our logs from various sources into a single log stream. Sometimes the log messages arrive out of order and we need to sort them. We have a problem sorting the logs from docker that are formatted using the RFC5424 format in the syslog driver. When logs look like this:\r\n\r\n```\r\n2018-11-23T21:02:39.57518Z\r\n2018-11-23T21:02:39.575182Z\r\n2018-11-23T21:02:39.575183Z\r\n```\r\n\r\nthey sort like this:\r\n\r\n```\r\n2018-11-23T21:02:39.575182Z\r\n2018-11-23T21:02:39.575183Z\r\n2018-11-23T21:02:39.57518Z\r\n```\r\n\r\nThis is because the format truncates trailing zeroes, and Z comes after all numbers in sort order.\r\n\r\nThis can be solved by changing the RFC5424 golang format from `2006-01-02T15:04:05.999999Z07:00` to `2006-01-02T15:04:05.000000Z07:00`. There is no text in the RFC that dictates that trailing zeros be truncated or not truncated.\r\n\r\n I am thinking a change can be made that introduces a new format for rsyslog `rfc5424micro-fw` for \"fixed width\". If anyone else thinks this is a good idea I can make a pull request. If there's a better way that doesn't involve making a new format option, I'm open to suggestions.\r\n\r\nThanks!\r\n"},{"labels":[null,null,"enhancement",null],"text":"When building with `BuildKit`, the `--no-cache` option is (partially) ignored;\r\n\r\nBuild a Dockerfile;\r\n\r\n```bash\r\nDOCKER_BUILDKIT=1 docker build -t bla https://github.com/thaJeztah/go-top.git\r\n[+] Building 29.1s (8/8) FINISHED                                                                                                                                                                                          \r\n => [internal] load git source https://github.com/thaJeztah/go-top.git                                                                                                                                                1.3s\r\n => [internal] load metadata for docker.io/library/golang:1.10-alpine                                                                                                                                                 2.3s\r\n => CACHED [internal] helper image for file operations                                                                                                                                                                0.0s\r\n => [build 1/3] FROM docker.io/library/golang:1.10-alpine@sha256:df75dc8049519981ef51a6e37fe4e79c0cfe9d5a0432ba9b36afe1a1df835f3b                                                                                    17.9s\r\n => => resolve docker.io/library/golang:1.10-alpine@sha256:df75dc8049519981ef51a6e37fe4e79c0cfe9d5a0432ba9b36afe1a1df835f3b                                                                                           0.0s\r\n => => sha256:d830d821f1ea39178886c4853e25ec07369f037cb758e07c3cbab832eabc9a14 1.36kB / 1.36kB                                                                                                                        0.0s\r\n => => sha256:2f577258bd101df535742773fc8d03fe9ea0350e8e132ab55da7dbbc508c89f2 3.80kB / 3.80kB                                                                                                                        0.0s\r\n => => sha256:2d13b02809266f700fe9d5a5002b4b8731fd5c8ff9218240bdc69005e678b158 78.11MB / 78.11MB                                                                                                                     14.3s\r\n => => sha256:0934a9e519360feba9d140548eebdab7cab66c55d04de2bde94a3dffa79a1122 125B / 125B                                                                                                                            1.1s\r\n => => sha256:df75dc8049519981ef51a6e37fe4e79c0cfe9d5a0432ba9b36afe1a1df835f3b 2.04kB / 2.04kB                                                                                                                        0.0s\r\n => => extracting sha256:2d13b02809266f700fe9d5a5002b4b8731fd5c8ff9218240bdc69005e678b158                                                                                                                             2.6s\r\n => => extracting sha256:0934a9e519360feba9d140548eebdab7cab66c55d04de2bde94a3dffa79a1122                                                                                                                             0.0s\r\n => [build 2/3] COPY . .                                                                                                                                                                                              1.6s\r\n => [build 3/3] RUN go install -v ./...                                                                                                                                                                               3.1s\r\n => [stage-1 1/1] COPY --from=build /go/bin/go-top /                                                                                                                                                                  2.7s\r\n => exporting to image                                                                                                                                                                                                0.0s\r\n => => exporting layers                                                                                                                                                                                               0.0s\r\n => => writing image sha256:e7320ea8c5feabd7e31e4f084fd69f52a4a44325caee01beedd6a0d7f004c198                                                                                                                          0.0s\r\n => => naming to docker.io/library/bla                                                                                                                                                                                0.0s\r\n```\r\n\r\nBuild the same image again, but this time with `--no-cache`;\r\n\r\n```bash\r\nDOCKER_BUILDKIT=1 docker build -t bla --no-cache https://github.com/thaJeztah/go-top.git\r\n[+] Building 7.9s (8/8) FINISHED                                                                                                                                                                                           \r\n => CACHED [internal] load git source https://github.com/thaJeztah/go-top.git                                                                                                                                         0.0s\r\n => [internal] load metadata for docker.io/library/golang:1.10-alpine                                                                                                                                                 0.9s\r\n => CACHED [build 1/3] FROM docker.io/library/golang:1.10-alpine@sha256:df75dc8049519981ef51a6e37fe4e79c0cfe9d5a0432ba9b36afe1a1df835f3b                                                                              0.0s\r\n => CACHED [internal] helper image for file operations                                                                                                                                                                0.0s\r\n => [build 2/3] COPY . .                                                                                                                                                                                              1.4s\r\n => [build 3/3] RUN go install -v ./...                                                                                                                                                                               2.7s\r\n => [stage-1 1/1] COPY --from=build /go/bin/go-top /                                                                                                                                                                  2.7s\r\n => exporting to image                                                                                                                                                                                                0.0s\r\n => => exporting layers                                                                                                                                                                                               0.0s\r\n => => writing image sha256:8a8b64198a7f7da426fd5783ae0b0d8958cd154424a060ca2b37a4a17ec0510c                                                                                                                          0.0s\r\n => => naming to docker.io/library/bla                                                                                                                                                                                0.0s\r\n```\r\n\r\nNotice that various steps show `CACHED`\r\n\r\n\r\n\r\nReproduced on Docker 18.09.0;\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           18.09.0\r\n API version:       1.39\r\n Go version:        go1.10.4\r\n Git commit:        4d60db4\r\n Built:             Wed Nov  7 00:47:43 2018\r\n OS/Arch:           darwin/amd64\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.0\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.10.4\r\n  Git commit:       4d60db4\r\n  Built:            Wed Nov  7 00:55:00 2018\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n```"},{"labels":[null,null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\ndockerd start failed when daemon config with some log driver options\r\n\r\n\r\n**Steps to reproduce the issue:**\r\n1.install docker-ce (17.03 up to latest version) on ubuntu 16.04 LTS\r\n2.config the `/etc/docker/daemon.json` as following:\r\n```\r\n{\r\n  \"log-driver\": \"json-file\",\r\n  \"log-opts\": {\r\n    \"max-size\": \"10m\"\r\n  }\r\n}\r\n```\r\n3.restart dockerd by `sudo service docker restart`, and it works\r\n4.if change `/etc/docker/daemon.json` to:\r\n```\r\n{\r\n  \"log-driver\": \"json-file\",\r\n  \"log-opts\": {\r\n    \"max-size\": \"10m\",\r\n    \"max-file\": 3\r\n  }\r\n}\r\n```\r\nrestart dockerd will get error:\r\n```\r\n-- Subject: Unit docker.socket has begun start-up\r\n-- Defined-By: systemd\r\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\r\n--\r\n-- Unit docker.socket has begun starting up.\r\nNov 21 15:55:58 ubuntu systemd[1]: Listening on Docker Socket for the API.\r\n-- Subject: Unit docker.socket has finished start-up\r\n-- Defined-By: systemd\r\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\r\n--\r\n-- Unit docker.socket has finished starting up.\r\n--\r\n-- The start-up result is done.\r\nNov 21 15:55:58 ubuntu systemd[1]: Starting Docker Application Container Engine...\r\n-- Subject: Unit docker.service has begun start-up\r\n-- Defined-By: systemd\r\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\r\n--\r\n-- Unit docker.service has begun starting up.\r\nNov 21 15:55:58 ubuntu dockerd[16761]: unable to configure the Docker daemon with file /etc/docker/daemon.json\r\nNov 21 15:55:58 ubuntu systemd[1]: docker.service: Main process exited, code=exited, status=1/FAILURE\r\nNov 21 15:55:58 ubuntu systemd[1]: Failed to start Docker Application Container Engine.\r\n-- Subject: Unit docker.service has failed\r\n-- Defined-By: systemd\r\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\r\n--\r\n-- Unit docker.service has failed.\r\n--\r\n-- The result is failed.\r\nNov 21 15:55:58 ubuntu systemd[1]: docker.service: Unit entered failed state.\r\nNov 21 15:55:58 ubuntu systemd[1]: docker.service: Failed with result 'exit-code'.\r\nNov 21 15:55:58 ubuntu sudo[16726]: pam_unix(sudo:session): session closed for user root\r\nNov 21 15:58:35 ubuntu sudo[16769]:   shaoan : TTY=pts/0 ; PWD=/home/shaoan ; USER=root ; COMMAND=/usr/bin/vi\r\nNov 21 15:58:35 ubuntu sudo[16769]: pam_unix(sudo:session): session opened for user root by shaoan(uid=0)\r\nNov 21 16:01:39 ubuntu sudo[16769]: pam_unix(sudo:session): session closed for user root\r\n```\r\n\r\n\r\nIs the parameter `max-file` not supported in in `daemon.json` config? or a bug with docker daemon?\r\n\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\n(This is relaying an issue [reported on stackoverflow](https://stackoverflow.com/q/53252818/596285))\r\n\r\nIf an older install of runc remains on the docker host, the containerd.io package will fail to upgrade.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\nAttempt to upgrade docker-ce to 18.09 on Ubuntu while an older runc package remains.\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\n$ sudo apt-get install docker-ce\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\ndocker-ce is already the newest version (5:18.09.0~3-0~ubuntu-xenial).\r\nYou might want to run 'apt-get -f install' to correct these:\r\nThe following packages have unmet dependencies.\r\n docker-ce : Depends: containerd.io but it is not going to be installed\r\nE: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).\r\n```\r\n\r\nForcing the install of containerd.io results in:\r\n\r\n```\r\nThe following additional packages will be installed:\r\n  containerd.io\r\nThe following NEW packages will be installed\r\n  containerd.io\r\n0 to upgrade, 1 to newly install, 0 to remove and 1 not to upgrade.\r\n1 not fully installed or removed.\r\nNeed to get 0 B/19.9 MB of archives.\r\nAfter this operation, 87.6 MB of additional disk space will be used.\r\nDo you want to continue? [Y/n] Y\r\n(Reading database ... 523620 files and directories currently installed.)\r\nPreparing to unpack .../containerd.io_1.2.0-1_amd64.deb ...\r\nUnpacking containerd.io (1.2.0-1) ...\r\ndpkg: error processing archive /var/cache/apt/archives/containerd.io_1.2.0-1_amd64.deb (--unpack):\r\n trying to overwrite '/usr/sbin/runc', which is also in package runc 1.0.0~rc2+docker1.13.1-0ubuntu1~16.04.1\r\ndpkg-deb: error: subprocess paste was killed by signal (Broken pipe)\r\nErrors were encountered while processing:\r\n /var/cache/apt/archives/containerd.io_1.2.0-1_amd64.deb\r\nE: Sub-process /usr/bin/dpkg returned an error code (1)\r\n```\r\n\r\nNote the older install of `runc 1.0.0~rc2+docker1.13.1-0ubuntu1~16.04.1` in the above output.\r\n\r\n**Describe the results you expected:**\r\n\r\nThe containerd.io package should force the removal of older runc packages to avoid the conflict.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\nNot available.\r\n\r\n**Output of `docker info`:**\r\n\r\nNot available.\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"Is there a better way to show the port column? I mean, like, show in one column, rather than one line?\r\n![dport](https://user-images.githubusercontent.com/44765750/47984157-a512d000-e110-11e8-8634-36f8a408f8a0.png)\r\n\r\nExpect:\r\n![dport2](https://user-images.githubusercontent.com/44765750/47984248-033fb300-e111-11e8-9768-62092eac797f.png)\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nThe journald log driver currently does not satisfy the [`SizedLogger`](https://github.com/moby/moby/blob/533e07afbe6683cf2a2033c1836e2008547c278b/daemon/logger/logger.go#L83) interface. Users of this driver often known what the correct buffer size is for their journald instance, and there is a [default line max](https://www.freedesktop.org/software/systemd/man/journald.conf.html#LineMax=) that is different than docker's default.\r\n\r\nThe issue of configurable log line length has come up before, but all of those issues seems fairly stale. I'm afraid of reopening a can of worms, so I wanted to keep this issue as scoped as possible. The interface already exists, another [log driver](https://github.com/moby/moby/blob/533e07afbe6683cf2a2033c1836e2008547c278b/daemon/logger/awslogs/cloudwatchlogs.go#L338) uses it, adding configuration to the journald driver to support it is straightforward, and seems like it would be valuable.\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Configure a docker daemon to use the journald log driver\r\n1. Run a container that logs a line between 16k and 48k in size\r\n\r\n**Describe the results you received:**\r\nThe long log line is split between multiple journal entries\r\n\r\n**Describe the results you expected:**\r\nThe long log line is written as a single journal entry"},{"labels":[null,null,"enhancement"],"text":"**Description**\r\n\r\nAfter ingesting a secret inside the container, it would be useful to remove the secret file to minimize the risk. This would make it possible to reduce the attack window on a secret to only the container startup routine.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\nCreate a test secret:\r\n\r\n```\r\n$ echo hello secret | docker secret create test_secret -  \r\n```\r\n\r\nDeploy a service using this secret:\r\n\r\n```\r\n$ cat docker-compose.yml\r\nversion: '3.6'\r\nsecrets:\r\n  test_secret:\r\n    external: true\r\nservices:\r\n  busybox:\r\n    image: busybox\r\n    command: tail -f /dev/null\r\n    secrets:\r\n    - source: test_secret\r\n      target: /run/secrets/my_secret\r\n      uid: '0'\r\n      gid: '0'\r\n      mode: 0700\r\n\r\n$ docker stack deploy -c docker-compose.yml secret\r\n```\r\n\r\nExec into the container, read the secret, and try to destroy it:\r\n\r\n```\r\n$ docker ps -l                  \r\nCONTAINER ID        IMAGE               COMMAND               CREATED             STATUS              PORTS    NAMES\r\nfaa0c56a9480        busybox:latest      \"tail -f /dev/null\"   2 seconds ago       Created    secret_busybox.1.kb0q06xcceiacc9th0zhvyzia                                                              \r\n\r\n$ docker exec -it faa /bin/sh\r\n/ # cd /run/secrets/                                           \r\n/run/secrets # ls -al                                      \r\ntotal 12                                                            \r\ndrwxr-xr-x    2 root     root          4096 Oct 26 12:08 . \r\ndrwxr-xr-x    3 root     root          4096 Oct 26 12:08 ..        \r\n-rwx------    1 root     root            13 Oct 26 12:08 my_secret\r\n/run/secrets # cat my_secret                                    \r\nhello secret                                                      \r\n/run/secrets # rm my_secret                                 \r\nrm: remove 'my_secret'? y                                      \r\nrm: can't remove 'my_secret': Device or resource busy               \r\n/run/secrets # echo >my_secret\r\n/bin/sh: can't create my_secret: Read-only file system\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\nThe delete commands show the secret is mounted with a read only tmpfs filesystem that doesn't allow the secret to be deleted or overwritten:\r\n\r\n```\r\n/run/secrets # rm my_secret                                 \r\nrm: remove 'my_secret'? y                                      \r\nrm: can't remove 'my_secret': Device or resource busy               \r\n/run/secrets # : >my_secret\r\n/bin/sh: can't create my_secret: Read-only file system\r\n/run/secrets # mount | grep secret\r\ntmpfs on /run/secrets/my_secret type tmpfs (ro,relatime)\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nI'd like the ability to either remove or overwrite the secret to make it unavailable after the container has been initialized. After the container exits and is recreated by swarm mode, the original secret should be available again for the container entrypoint to process.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n$ docker version\r\nClient:\r\n Version:           18.06.1-ce\r\n API version:       1.38\r\n Go version:        go1.10.3\r\n Git commit:        e68fc7a\r\n Built:             Tue Aug 21 17:23:18 2018\r\n OS/Arch:           linux/amd64\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          18.06.1-ce\r\n  API version:      1.38 (minimum version 1.12)\r\n  Go version:       go1.10.3\r\n  Git commit:       e68fc7a\r\n  Built:            Tue Aug 21 17:22:21 2018\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n$ docker info\r\nContainers: 13\r\n Running: 11\r\n Paused: 0\r\n Stopped: 2\r\nImages: 272\r\nServer Version: 18.06.1-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: q44zx0s2lvu1fdduk800e5ini\r\n Is Manager: true\r\n ClusterID: b4qh6x548wfo2sv9pubs33hk7\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 4\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 192.168.233.100\r\n Manager Addresses:\r\n  192.168.233.100:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 468a545b9edcd5932818eb9de8e72413e616e86e\r\nrunc version: 69663f0bd4b60df09991c08812a60108003fa340\r\ninit version: fec3683\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.0-8-amd64\r\nOperating System: Debian GNU/Linux 9 (stretch)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 5.739GiB\r\nName: bmitch-asusr556l\r\nID: 5OZQ:RKOD:MLMQ:3YC4:QOWX:SLWX:SYO2:XDJF:CG6C:43ER:4QQ4:DXOO\r\nDocker Root Dir: /home/var-docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: bmitch3020\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\n foo=bar\r\n env=laptop\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nThis is running on a physical Linux host."},{"labels":[null,null,null,"enhancement"],"text":"There are a HostConfig.DiskQuota option with description \"Disk limit (in bytes).\" on https://docs.docker.com/engine/api/v1.37/#operation/ContainerCreate.\r\n\r\nBut, unfortunately, I can't find any other information about what does it mean. What constrains of use does it have? Is it can be used on all platfroms/storage drivers? What would be if this limit will be exceeded?"},{"labels":[null,"enhancement"],"text":"This is a bit of the opposite request from #34151 and #34513 because currently we have to explicitly tag the early stages so that `docker image prune` doesn't clean them up and blow away build cache that should still be valid.\r\n\r\nThe current workaround is to run the following:\r\n```\r\ndocker build --target build -t myimage_build myimage\r\ndocker build -t myimage myimage\r\n```\r\n\r\nThis is inefficient because the build for the `build` stage has to happen twice. The second time it's all cached, but it means that the context has to be reloaded and this explicit tagging is required when it shouldn't be necessary because the image isn't dangling and is part of a tagged build."},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\ndownload-frozen-image-v2.sh does not contain a check to detect if go is in the PATH. It checks for curl and jq, it should do the same for go.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n1. Run the script on a machine where go is not installed, you will hit the following error:\r\n\r\n```\r\n./download-frozen-image-v2.sh /tmp/old-hello-world hello-world:latest\r\n./download-frozen-image-v2.sh: line 41: go: command not found\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nI expect the script to throw an error like it does for jq:\r\n\r\n```\r\n$ ./download-frozen-image-v2.sh /tmp/old-hello-world hello-world:latest\r\nerror: \"go\" not found!\r\n```"},{"labels":[null,null,"enhancement"],"text":"https://github.com/mvdan/sh seems useful for keeping indent style consistent across all shell scripts\r\n\r\n```console\r\n$ git rev-parse HEAD\r\n503b1a9b6f24488db6a67f7ba24258e4ff5ea2a7\r\n$ git grep --name-only -e '^#!' | xargs shfmt -l -w\r\n$ git diff --shortstat \r\n 55 files changed, 938 insertions(+), 867 deletions(-)\r\n```\r\n\r\n@thaJeztah @vdemeester WDYT?\r\n\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nThe `docker save` command will not save an image that contains only a `FROM scratch` and a `VOLUME` declaration. It fails with:\r\n\r\n    Error response from daemon: empty export - not implemented\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n1. Create a Dockerfile\r\n```Bash\r\ncat <<EOF > Dockerfile\r\nFROM scratch\r\nVOLUME /var/log\r\nEOF\r\n```\r\n\r\n2. Create an image from that Dockerfile\r\n```Bash\r\ndocker build -t emptyscratchvolume .\r\n```\r\n\r\n3. Try to save that image with `docker save`  into a .tar file\r\n```Bash\r\ndocker save -o emptyscratchvolume.tar emptyscratchvolume\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\nIt fails with:\r\n\r\n    Error response from daemon: empty export - not implemented\r\n\r\n**Describe the results you expected:**\r\n\r\nThe `docker save` command should produce a .tar file that will be compatible with `docker load`.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nThe `docker save` command will work as expected if the image contains at least one file, like the Dockerfile itself:\r\n```Bash\r\ncat <<EOF > Dockerfile\r\nFROM scratch\r\nCOPY Dockerfile .\r\nVOLUME /var/log\r\nEOF\r\n```\r\n\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:       18.03.0-ce\r\n API version:   1.37\r\n Go version:    go1.9.4\r\n Git commit:    0520e24\r\n Built: Wed Mar 21 23:10:06 2018\r\n OS/Arch:       linux/amd64\r\n Experimental:  false\r\n Orchestrator:  swarm\r\n\r\nServer:\r\n Engine:\r\n  Version:      18.03.0-ce\r\n  API version:  1.37 (minimum version 1.12)\r\n  Go version:   go1.9.4\r\n  Git commit:   0520e24\r\n  Built:        Wed Mar 21 23:08:35 2018\r\n  OS/Arch:      linux/amd64\r\n  Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 7\r\n Running: 3\r\n Paused: 0\r\n Stopped: 4\r\nImages: 197\r\nServer Version: 18.03.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: cfd04396dc68220d1cecbe686a6cc3aa5ce3667c\r\nrunc version: 4fc53a81fb7c994640722ac585fa9ca548971871\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.0-6-amd64\r\nOperating System: Debian GNU/Linux 9 (stretch)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 7.8GiB\r\nName: c3-development\r\nID: 5KME:NHKR:SAQH:EVTU:M5Y6:DKML:NU45:7USQ:2PAP:2DAC:ZR5I:QM2K\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"Currently VxLAN default port is 4789 in swarm cluster. This value is not configurable at present.\r\nThis proposal will allow user to configure VxLAN  UDP port number during swarm init.  This way if anyone blocks VxLAN default port 4789 for swarm cluster, by changing default port to different value we will be able to still send/receive VxLAN packets on new port.\r\n \r\n\r\nThe change will be in 4 components. CLI, Moby, SwarmKit and  Libnetwork.\r\nIANA procedures for each range in detail\r\nthe Well Known Ports, aka the System Ports, from 0-1023\r\nthe Registered Ports, aka the User Ports, from 1024-49151\r\nthe Dynamic Ports, aka the Private Ports, from 49152-65535\r\n\r\nSo we can allow range between 1024 to 49151\r\n\r\nCLI command syntax :\r\n swarm init --vxlan-udp-port <1024-49151>\r\nPS : swarm update will not have this option.  Only swarm init will have port config option.\r\n\r\ncc : @thaJeztah @mavenugo @fcrisciani   @mark-church\r\nPls provide your inputs. "},{"labels":[null,"enhancement"],"text":"Docker throws errors when it tries to bind A:B where A's and B's types do not match: they both have to be either files or dirs.\r\nWhen user starts a container with such a mount and A doesn't exist yet - docker creates it always as a dir, so if your B is a file - you'll get an error.\r\nThe only solution now is to do `touch A` beforehand, but I'd like docker to be able to do that for me via an option that I could specify somehow.\r\n\r\nPersonally I'd prefer relying on the presence/absence of a `/` at the end of the path, so that `A:B` would be a file:file mount and `A/:B/` would be a dir:dir mount."},{"labels":[null,null,"enhancement",null],"text":"I am working on an authorization plugin for Docker CE using the documented authorization API. I would like to relate the two daemon-to-plugin requests (AuthZReq and AuthZRes) that service the same originating client-to-daemon API call.  In many cases, except at least for certain Image operations, it is useful for authorizing a create operation and then intercepting the newly created object ID. While both authorization plugin phases are provided with the same information (i.e. RequestBody) there is not enough information in that alone to reliably recognize an AuthZReq event's corresponding AuthZRes event when several identical API calls are made rapidly. As with other examples of HTTP-based services, tracking a single user request through processing may be much simpler by introducing a practically unique user request ID accompanying the request's representation in each step. Such a thing might be emitted in each log line generated during the request's processing to facilitate forensics and analytics. In my own case, the request ID would be likewise be passed to both the request's AuthZReq and AuthZRes plugin calls, making obvious their relationship.\r\n\r\nIs this something which can be naturally implemented within the Docker daemon architecture? Would it be a reasonably simple matter to do so?\r\n\r\nCan this be done using a middleware component to add the client request ID as a request header or in the vars map? Although it seems like a more fundamental behavior than would be made optional in middleware."},{"labels":[null,"enhancement"],"text":"Hello everyone,\r\n\r\nwhile working on my system sourcing buildroot images as base for docker images and wanting to handle configuration files in a way that could be easily moved to different systems, I was looking into being able to use files as read only volumes.\r\n\r\nThe volume subsystem of docker only likes having block devices for the device parameter while `mount` is capable of handling files by automatically reserving a loop device.\r\nI was wanting to be able to use a squashfs image as volume that could be mounted as config location inside a container since it was the closest to a transportable `.tar` that I could think of.\r\n\r\nCreating the volume goes without issue :\r\n`docker volume create -d local -o type=squashfs -o device=$(pwd)/pia.squashfs`\r\n\r\nBut mounting it into the container would yield a `block device required` error.\r\n\r\nMount is capable of handling this itself : \r\n`mount -o loop pia.squashfs /mnt/pia`\r\n\r\nwhich is actually doing: \r\n`losetup -f pia.squashfs`\r\n`mount -t squashfs /dev/loopX /mnt/pia`\r\n\r\nSo doing the `losetup` beforehand and then using `/dev/loopX` as `device=` works for mounting it as volume.\r\n\r\nSo it would really be useful if one could tap into the features already exposed by the native mount command.\r\n\r\nMy suggestion would maybe be for the volume docker subsystem get closer to the native system mount capabilities or to allow the subsystem to use normal files for the `device=` parameter.\r\n\r\nRight now I can push around my docker-compose stuff alongside the neatly squashed configuration stuff, and use a script on the different hosts to set up the loop device for mounting. It would make it less tedious though to expose the full `mount` capabilities in the docker mount subsystem."},{"labels":["enhancement"],"text":"**Description**\r\n\r\n\r\nI have a persistent container that I need to add permanent environment variables to. For reasons related to my service I cannot remove this container and recreate with a new docker run. I am allowed however stop/restart the container.\r\n\r\nHow can I add permanent environment variables to my container without doing a new docker run command?\r\n\r\n\r\n**Steps to reproduce the issue:**\r\n1. Run a container \r\n2. Commands can be ran with one time environment variables, but permanent ones cannot be added\r\n\r\n\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nVersion: 17.03.2-ce\r\n```\r\n\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nContainer is running CoreOS running an etcd cluster\r\n"},{"labels":["enhancement",null],"text":"**Description**\r\n\r\nAnyone forking moby/moby and modifying a few lines of code can enable process isolation for Windows client. It is clearly unsupported and might have some drawbacks, but IMO, for development purposes, it would be really nice to enable it under the following conditions:\r\n- Make it experimental\r\n- When process isolation is used on windows 10 client, emit a warning \"this is an unsupported feature, for development only\"\r\n- Consider checking that \"developer mode\" is enabled on the machine\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nPeople want to develop in conditions as close as possible as their production environment. Using the same isolation mode on both developer machines, and the production / pre-prod environment makes much sense for them.\r\nMaking it an unambiguously \"unsupported feature\" would protect both Docker and Microsoft against having to support yet another kind of production workload.\r\n\r\n"},{"labels":[null,"enhancement"],"text":"It would be great if one could mount a docker image as a read only volume.\r\n\r\nLets say one want to build app-infra and app-deployment images.\r\n\r\napp-infra would consist of an infrastructure to run an app, while the app-deployment would be created as another image in CI/CD server.\r\n\r\nThen the app container would use the app-infra image mounting the app-deployment as a read only volume.\r\n\r\nWith that one could benefit from docker tags to tag app-deployment and use different deployments with the same infra image."},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nI deployed a service (an HTML page that shows the Hostname of the webserver), with 2 replicas.  \r\nEvery time the command \"curl localhost\" is executed it displays the name of one of the 2 containers that belong to the service. (e.g. 0921a6ddfd54, 55ed7f4d43f8) . \r\nIf I run \"docker pause 0921a6ddfd54\" the command \"curl localhost\" hangs and doesn't display anything until i execute \"docker unpause 0921a6ddfd54\". It seems that Swarm is not aware that the container's status is Pause and that it can't handle requests.\r\n**What you expected to happen**:\r\nSwarm reacts to \"docker pause <i>containerID</i>\" as it does to \"docker stop <i>containerID</i>\"\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create service on port 80 \r\n2. docker pause <i>containerID</i> (where <i>containerID</i> belongs to the service)\r\n3. No new container is created and therefore the service doesn't run properly\r\n\r\n**Describe the results you received:**\r\n\"curl localhost\" hangs \r\n\r\n**Describe the results you expected:**\r\nSimilar to \"docker stop\", \"docker pause\" should create new container for the service\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           18.06.0-ce\r\n API version:       1.38\r\n Go version:        go1.10.3\r\n Git commit:        0ffa825\r\n Built:             Wed Jul 18 19:11:02 2018\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          18.06.0-ce\r\n  API version:      1.38 (minimum version 1.12)\r\n  Go version:       go1.10.3\r\n  Git commit:       0ffa825\r\n  Built:            Wed Jul 18 19:09:05 2018\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 4\r\n Running: 1\r\n Paused: 0\r\n Stopped: 3\r\nImages: 2\r\nServer Version: 18.06.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: spl3yofkis4ltnu8cmaw16gho\r\n Is Manager: true\r\n ClusterID: r1a7wmbtcegdasiifaa2mh43f\r\n Managers: 1\r\n Nodes: 3\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 10\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 10.142.0.2\r\n Manager Addresses:\r\n  10.142.0.2:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: d64c661f1d51c48782c9cec8fda7604785f93587\r\nrunc version: 69663f0bd4b60df09991c08812a60108003fa340\r\ninit version: fec3683\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.15.0-1018-gcp\r\nOperating System: Ubuntu 16.04.4 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 3.607GiB\r\nName: swarm-manager\r\nID: KQ2V:2QNQ:3VHV:VJLU:V7R3:BRWV:ECZM:ZLO2:UAIN:J4PN:KNLO:QV37\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nIf one extracts a tar file with a version of tar that understands the gname/uname string attribute, if gname/uname exist in the group/user lookup, it will use that to set the gid/uid of the file instead of the gid/uid stored numerically in the tar file.  However, if one adds a tar file within a Dockerfile, it ignores gname/uname, and only uses the gid/uid.\r\n\r\nThis would make sense if the claim was that a Dockerfile context cannot know the name to id mapping within a container image, but this doesn't seem to be true.  the --chown argument to the same dockerfile directive can take a name and map it to an id if it's defined within the container image.\r\n\r\n**Steps to reproduce the issue:**\r\n1. create a tarball with a file owned by you  (note your uid, make sure its not the same uid as the first user that will be created in a debian system, i.e. 1000).  In my case \"spotter/1001\"\r\n\r\n```\r\ntar cf test.tar <some file>\r\n```\r\n\r\n2. create a simple dockerfile as such\r\n\r\n```\r\nFROM ubuntu:16.04\r\nRUN useradd spotter\r\nADD ./test.tar\r\n```\r\n\r\n3. see file ownership within a running container\r\n\r\n```\r\ndocker run -it <image name> ls -l /<file you tar'ed up>\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nexpect to see file owned by user spotter\r\n\r\n**Describe the results you received:**\r\n\r\ninstead see it owned by user with id 1001.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```Client:\r\n Version:      17.05.0-ce\r\n API version:  1.29\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:15:36 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.05.0-ce\r\n API version:  1.29 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:15:36 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 10\r\n Running: 2\r\n Paused: 0\r\n Stopped: 8\r\nImages: 598\r\nServer Version: 17.05.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 299\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 9048e5e50717ea4497b757314bad98ea3763c145\r\nrunc version: 9c2d8d184e5da67c95d601382adf14862e4f2228\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.15.0-24-generic\r\nOperating System: Ubuntu 18.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 15.4GiB\r\nName: x1\r\nID: 7E3V:YIUY:MIXX:JIMW:XDHE:ENCS:H7LY:EU7E:FMGZ:ZGYC:AFMF:2RPV\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: spotter\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 52.229.48.71:30002\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```"},{"labels":[null,null,"enhancement"],"text":"I suspect the following is a bug as IMHO `docker logs -f` should mimic the behavior of `tail -f`.\r\n\r\n## What is seen / buggy behavior:\r\n\r\n```\r\nterm_1# docker run -d busybox sh -c \"while true; do date; sleep 0.1; done\"\r\n\r\nterm_2# docker logs -f CONTAINER_ID\r\n\r\nterm_1# docker stop CONTAINER_ID\r\n```\r\n\r\nOnce container on term_1 stops, `docker logs -f` on term_2 exits.\r\n\r\n## What is expected to be seen (correct behavior):\r\n\r\n1. `docker logs -f` does not exit upon container stop.\r\n2. `docker logs -f` exits only upon container removal.\r\n\r\nReproduced on docker-ce 18.06 and 17.03 (most probably all the intermediate versions are also affected)."},{"labels":[null,"enhancement"],"text":"Once parameters support have been introduced into `FROM` instruction through `ARG`, I think the following will improve consistency and enable reducing the number of layers in our images.\r\n\r\nConsider the following Dockerfie:\r\n```\r\nARG CENTOS_VERSION=7.4.1708\r\nFROM centos:${CENTOS_VERSION}\r\nARG MY_TAG=\"my tag value\"\r\nLABEL arg_version=\"${CENTOS_VERSION}\" \\\r\n      my_tag=\"${MY_TAG}\"\r\n```\r\n\r\nYou'll get the following build output:\r\n```\r\n$ docker build --rm -t arg_issue -f Dockerfile .\r\nSending build context to Docker daemon  3.072kB\r\nStep 1/4 : ARG CENTOS_VERSION=7.4.1708\r\nStep 2/4 : FROM centos:${CENTOS_VERSION}\r\n7.4.1708: Pulling from library/centos\r\nDigest: sha256:de88676c62e619d07f081be9e518bb201c03fa554882aa7cf7ca72fe3ca7846d\r\nStatus: Downloaded newer image for centos:7.4.1708\r\n ---> d3949e34634c\r\nStep 3/4 : ARG MY_TAG=\"my tag value\"\r\n ---> Using cache\r\n ---> 57794f23d33d\r\nStep 4/4 : LABEL arg_version=\"${CENTOS_VERSION}\"       my_tag=\"${MY_TAG}\"\r\n ---> Using cache\r\n ---> cdc00d203730\r\nSuccessfully built cdc00d203730\r\nSuccessfully tagged arg_issue:latest\r\n```\r\n\r\nand image's labels:\r\n```\r\n$ docker inspect arg_issue | jq '.[].Config.Labels'\r\n{\r\n  \"arg_version\": \"\",\r\n  \"build-date\": \"20170911\",\r\n  \"license\": \"GPLv2\",\r\n  \"my_tag\": \"my tag value\",\r\n  \"name\": \"CentOS Base Image\",\r\n  \"vendor\": \"CentOS\"\r\n}\r\n```\r\n\r\nYou can see that the same `ARG` instruction have different behaviors:\r\n- `CENTOS_VERSION` will only be visible to `FROM` instruction but not to any following instruction\r\n- `CENTOS_VERSION` instruction will create no new layer in the resulting image\r\n- `MY_TAG` will be available for any following build container\r\n- `MY_TAG` instruction will imply the creation of a new layer and multiple of them can't be merged into one as `ENV` ( #35950 )\r\n\r\nThis behavior is problematic for the following reasons:\r\n- I can't record the value of `CENTOS_VERSION` in my image without duplicating the instruction declaration after `FROM`, introducing inconsistency risks and complexity\r\n- `ARG` being a build argument, I don't see a strong need to generate a layer for each of them (e.g.: we don't generate new layers for proxy configuration)\r\n\r\nWhat I propose is:\r\n- Support any number of `ARG` instructions to be declared before `FROM` and make them available to instructions following `FROM`\r\n- `ARG` instructions declared after `FROM` can remain supported for backward compatibility purposes but I'd encourage the new behavior.\r\n\r\nI think it would bring the following benefits:\r\n- I could reduce the number of layers by moving `ARG` before `FROM`.\r\n- It would establish what I think is a more intuitive difference between `ARG` and `ENV` which I see colleagues being confused about them.\r\n- I'm not so sure about this but I think those `ARG` values could be available to all stages of multi-stage build.\r\n\r\nDocker versions (Docker Toolbox in Win 8.1):\r\n```\r\n$ docker-machine ssh default docker -v\r\nDocker version 18.04.0-ce, build 3d479c0\r\n\r\n$ docker -v\r\nDocker version 18.06.0-ce, build 0ffa8257ec\r\n```"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nAs a user, I want to be able to set a field in `daemon.json` to turn on buildkit instead of turning it on by defaut from `dockerd --experimental`\r\n\r\ncc @tiborvass @andrewhsu \r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1.\r\n2.\r\n3.\r\n\r\n**Describe the results you received:**\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,null,"enhancement"],"text":"**Description**\r\n\r\nI have noticed that, while a service has a task that is running, that task has a status with `ContainerStatus.ExitCode: 0`. That is a misleading value. \r\n\r\nI believe `0` is being used here as a stand-in for `null` (or `nil`). But those values are distinct. A `null` value (or `nil`, or no value at all) should mean \"the container has not yet reported an exit value\". Any non-`null` value should mean \"the container *has* exited, and its process has reported this exit code\". `0` in particular is a special value because it is the only value indicating success.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a service with a command that will continue running for at least a few seconds.\r\n```\r\n$ docker service create --name service_name busybox:latest sleep 120\r\neeest5s97viw1qzoy8pvllbkm\r\noverall progress: 1 out of 1 tasks\r\n1/1: running   [==================================================>]\r\nverify: Service converged\r\n```\r\n2. Get the task status. I am not aware of a way to do this with the CLI, so I use the API.\r\n```\r\n$ curl -s --unix-socket /var/run/docker.sock http://localhost/tasks?filters%3D%5B%22service%22%3A%22service_name%22%5D | jq '.[].Status'\r\n{\r\n  \"Timestamp\": \"2018-08-01T15:41:54.007243253Z\",\r\n  \"State\": \"running\",\r\n  \"Message\": \"started\",\r\n  \"ContainerStatus\": {\r\n    \"ContainerID\": \"a14116edc1b8e8222efde70af7b7531bb71c702b813e5a8d81f848b8dfa1013c\",\r\n    \"PID\": 6947,\r\n    \"ExitCode\": 0\r\n  },\r\n  \"PortStatus\": {}\r\n}\r\n```\r\n\r\n**Describe the results you received:**\r\nThe task is in `\"State\": \"running\"` but the `\"ContainerStatus.ExitCode\"` value is `0`.\r\n\r\n**Describe the results you expected:**\r\nEither no `\"ExitCode\"` property within `\"ContainerStatus\"`, or one with a `null` value.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           18.06.0-ce\r\n API version:       1.38\r\n Go version:        go1.10.3\r\n Git commit:        0ffa825\r\n Built:             Wed Jul 18 19:11:02 2018\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          18.06.0-ce\r\n  API version:      1.38 (minimum version 1.12)\r\n  Go version:       go1.10.3\r\n  Git commit:       0ffa825\r\n  Built:            Wed Jul 18 19:09:05 2018\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n```"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nThere's no chance to log into the same registry using different credentials to access to different images within.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker login https://registry.example.com/team1` - provide user & password.\r\n2. `docker image pull registry.example.com/team1/image1` - OK.\r\n3. `docker login https://registry.example.com/team2`\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\nAuthenticating with existing credentials...\r\nLogin Succeeded\r\n```\r\n\r\nBut, of course, I won't be able to pull `registry.example.com/team2/image2`.\r\n\r\n**Describe the results you expected:**\r\n\r\nI expected to be able to provide different logins for different namespaces.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nThe use case I have at hand for this bug is deploying with a Gitlab registry. See their bug report in https://gitlab.com/gitlab-org/gitlab-ce/issues/47767.\r\n\r\nGitlab provides a different namespace per project, under the same registry server. It also allows you to define a deploy token per project. Thus, if you need to deploy 2 projects into the same docker server, it's not possible to use both tokens at the same time.\r\n\r\nHowever, it is easy to imagine that the same problem can happen on any other registry.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:       17.12.0-ce\r\n API version:   1.35\r\n Go version:    go1.9.2\r\n Git commit:    c97c6d6\r\n Built: Wed Dec 27 20:11:19 2017\r\n OS/Arch:       linux/amd64\r\n\r\nServer:\r\n Engine:\r\n  Version:      17.12.0-ce\r\n  API version:  1.35 (minimum version 1.12)\r\n  Go version:   go1.9.2\r\n  Git commit:   c97c6d6\r\n  Built:        Wed Dec 27 20:09:53 2017\r\n  OS/Arch:      linux/amd64\r\n  Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 51\r\n Running: 51\r\n Paused: 0\r\n Stopped: 0\r\nImages: 250\r\nServer Version: 17.12.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: journald\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 89623f28b87a6004d4b785663257362d1658a729\r\nrunc version: b2567b37d7b75eb4cf325b77297b140ea686ce8f\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.15.0-29-generic\r\nOperating System: Ubuntu 16.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 16\r\nTotal Memory: 62.91GiB\r\nName: ****\r\nID: CJB5:Q2UM:YQDG:NATW:Z6LW:HZW2:3ZHX:N2KF:TGGI:4ZPC:TMBX:6YJW\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\n os=Ubuntu 16.04.3 LTS\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nPhysical, although it doesn't matter for this issue AFAICS."},{"labels":[null,null,null,"enhancement"],"text":"FEATURE REQUEST\r\n\r\n**Description**\r\n\r\n--userns for docker service create and update would be useful.\r\n\r\nI'm trying to use userns-remap to secure my cluster containers and services transparently by default but I have a few services that need to mount files from the Docker host and I was hoping to use docker service create --userns=host ... for these few services but there doesn't appear to be a way to do this using the normal command line. I suppose I could deploy a stack but I'd rather not because it mangles the service name to be stack-service instead of just service.\r\n\r\nI was hoping to use --user=ID to explicitly run the service with a specific host user but this doesn't work because (I believe) the container's file system restricts access to only the root user.  I might be able to work around this by modifying the Dockerfiles by adding a USER and setting the container filesystem permissions, but this breaks my hope for transparency.\r\n\r\nSpecifically, I'm requesting support for:\r\n\r\n```\r\ndocker service create --userns=host ...\r\ndocker service update --userns=host ...\r\n```\r\n\r\nThis will make it possible to secure Docker service by default without needing to munge Dockerfiles for any exceptions.\r\n\r\nRelated issue: https://github.com/moby/moby/issues/25303\r\nRelated PR (`docker run`): https://github.com/moby/moby/pull/20111\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n  API version:  1.37 (minimum version 1.12)\r\n  Go version:   go1.9.5\r\n  Git commit:   9ee9f40\r\n  Built:        Thu Apr 26 07:15:30 2018\r\n  OS/Arch:      linux/amd64\r\n  Experimental: false\r\nrClient:: command not found\r\nroot@manager-0:~#  Version:      18.03.1-ce\r\nVersion:: command not found\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 7\r\n Running: 2\r\n Paused: 0\r\n Stopped: 5\r\nImages: 3\r\nServer Version: 18.03.1-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: fluentd\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local neon\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: sw6o4jpzdwri2dz5zgvq6mju0\r\n Is Manager: true\r\n ClusterID: gq87clficzj7iaxahgafwri5w\r\n Managers: 1\r\n Nodes: 4\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 10\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 10.0.0.30\r\n Manager Addresses:\r\n  10.0.0.30:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 773c489c9c1b21a6d78b5c538cd395416ec50f88\r\nrunc version: 4fc53a81fb7c994640722ac585fa9ca548971871\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\n userns\r\nKernel Version: 4.4.0-119-generic\r\nOperating System: Ubuntu 16.04.4 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 3.936GiB\r\nName: manager-0\r\nID: BIYB:LQF3:PO5J:Z2EN:CCNF:3ERB:I7JV:KZNF:OVL4:KXUQ:RKKW:JJ75\r\nDocker Root Dir: /mnt/docker/231072.231072\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nRegistry Mirrors:\r\n https://manager-0.neon-registry-cache.hive:5001/\r\n https://docker.io/\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nRunning on Windows 10/Pro on local Hyper-V Ubuntu 16.04 VMs."},{"labels":[null,"enhancement"],"text":"We closed #36282 saying in Swarm mode we don't want to support labelling volumes, they should be labelled correctly. But currently the only way to do this is to relabel the host paths directly by poking into `/var/lib/docker/volumes`. If you could create a volume with a specific label this would simplify this process."},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nWe are using docker with docker-compose for our services (GitLab, Wiki, etc).\r\nEvery service is using port from 10010 to 10050 (we jump by ten for every service) and it is sitting behind Nginx for reverse proxy. It worked nice, but yesterday I have added new service at port 10050 and after reboot I'm unable to start GitLab on 10010, as it keeps erroring on `Error starting userland proxy: listen tcp 0.0.0.0:10010: bind: address already in use`. Process using port 10010 is `docker-containerd`. I have moved GitLab to 10060 and it works currently. Why did docker-containerd started ocupying port 10010? Is it configurable?\r\n\r\n**Steps to reproduce the issue:**\r\nI'm unsure. I had five services (10010 and 10011, 10020, 10030, 10040, 10050). Rebooted machine.\r\n\r\n**Describe the results you received:**\r\nPort 10010 is used even though no container is using it.\r\n\r\n**Describe the results you expected:**\r\nPort 10010 is free and ready to bind.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           18.06.0-ce\r\n API version:       1.38\r\n Go version:        go1.10.3\r\n Git commit:        0ffa825\r\n Built:             Wed Jul 18 19:11:02 2018\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          18.06.0-ce\r\n  API version:      1.38 (minimum version 1.12)\r\n  Go version:       go1.10.3\r\n  Git commit:       0ffa825\r\n  Built:            Wed Jul 18 19:09:05 2018\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 16\r\n Running: 10\r\n Paused: 0\r\n Stopped: 6\r\nImages: 76\r\nServer Version: 18.06.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: d64c661f1d51c48782c9cec8fda7604785f93587\r\nrunc version: 69663f0bd4b60df09991c08812a60108003fa340\r\ninit version: fec3683\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-130-generic\r\nOperating System: Ubuntu 16.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 15.42GiB\r\nName: luna\r\nID: 6H36:M6K6:T2XY:PCPQ:23KJ:LCLS:TE3K:GLV3:6SLF:OWQ6:3IM4:S4AF\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n* physical server\r\n* Ubuntu 16.04.5 LTS\r\n\r\nI will happily provide any details needed."},{"labels":[null,null,"enhancement",null,null],"text":"Disabling ipv4_forwarding on linux (we do this for CIS L1 Compliance) and setting _dockerd_ to use NAT Hairpinning mode (dockerd --userland-proxy=false) we seem to still get a warning message to _stderr_ in various places.\r\n\r\n> WARNING: IPv4 forwarding is disabled\r\n\r\nand \r\n\r\n> WARNING: IPv4 forwarding is disabled. Networking will not work.\r\n\r\nSetting _--log-level error_ or _fatal_ also does not appear to suppress these messages either. \r\n\r\nI would expect that ipv4 forwarding being disabled would only warrant a warning in cases where _dockerd_ was attempting to run with userland-proxy enabled (the current default). With NAT Hairpinning enabled, it seems that no warning would be warranted, right?\r\n\r\nI understand that work is also being considered to make NAT Hairpinning the default configuration as well: https://github.com/moby/moby/issues/14856\r\n\r\n**Steps to reproduce the issue:**\r\n1. Disable ipv4 forward on the host `(echo \"net.ipv4.ip_forward = 0\" >> /etc/sysctl.d/60-cis-network-configuration.conf) && sysctl -p /etc/sysctl.d/60-*.conf`\r\n2. Disable userland-proxy by passing as an arg to dockerd (or as config in daemon.json)\r\n3. Spin up a simple container and observe the warning `(docker run -it --rm -p 80:80 nginx)` or `(docker --log-level \"fatal\" run -it --rm -p 80:80 nginx)`\r\n\r\n**Describe the results you received:**\r\n\r\nReceive one of two warnings:\r\n\r\n> WARNING: IPv4 forwarding is disabled\r\n> WARNING: IPv4 forwarding is disabled. Networking will not work.\r\n\r\n**Describe the results you expected:**\r\n\r\nWhen userland-proxy is _disabled_ and ipv4_forwarding is _disabled_, I expect _not_ to be warned, or if we still want to warn, that setting --log-level should suppress the warning.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nhttps://github.com/moby/moby/blob/5fc12449d830ae9005138fb3d3782728fa8d137a/daemon/daemon_unix.go#L599-L602\r\n\r\nis the warning code I've found so far that is probably related. Not sure how --log-level not being respected plays into this.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nDocker version 17.03.2-ce, build 7392c3b/17.03.2-ce\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 3\r\n Running: 3\r\n Paused: 0\r\n Stopped: 0\r\nImages: 3\r\nServer Version: 17.03.2-ce\r\nStorage Driver: devicemapper\r\n Pool Name: docker-docker--pool\r\n Pool Blocksize: 524.3 kB\r\n Base Device Size: 107.4 GB\r\n Backing Filesystem: ext4\r\n Data file:\r\n Metadata file:\r\n Data Space Used: 2.775 GB\r\n Data Space Total: 12.73 GB\r\n Data Space Available: 9.959 GB\r\n Metadata Space Used: 737.3 kB\r\n Metadata Space Total: 16.78 MB\r\n Metadata Space Available: 16.04 MB\r\n Thin Pool Minimum Free Space: 1.273 GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: true\r\n Deferred Deletion Enabled: true\r\n Deferred Deleted Device Count: 0\r\n Library Version: 1.02.135-RHEL7 (2016-11-16)\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 4ab9917febca54791c5f071a9d1f404867857fcc\r\nrunc version: 54296cf40ad8143b62dbcaa1d90e520a2136ddfe\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.43-17.39.amzn1.x86_64\r\nOperating System: Amazon Linux AMI 2017.03\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 993.5 MiB\r\nName: ip-10-31-71-89\r\nID: UBVV:KESJ:ICWE:FMMP:4RWY:S4JM:WYVU:EPCG:FGF5:6CHI:IWRY:BZEU\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: IPv4 forwarding is disabled\r\n\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nRunning on AWS Elastic Beanstalk 2.7.x.\r\n\r\nThis undesired message **can cause elastic beanstalk to fail deployments**, as some of the internal AWS scripts like run00.sh rely on clean output from docker run commands to function correctly. This is obviously a bug on their end. A good workaround would be simply setting --log-level though for now.\r\n"},{"labels":[null,"enhancement"],"text":"Given the following `Dockerfile`:\r\n\r\n```\r\nFROM alpine:3.5 AS stage1\r\nRUN echo \"stage1\"\r\n\r\nFROM alpine:3.5 AS stage2\r\nRUN echo \"stage2\"\r\n\r\nFROM alpine:3.5 AS stage3\r\nRUN echo \"stage3\"\r\n\r\nFROM alpine:3.5\r\nRUN echo \"test\"\r\n```\r\n\r\nrunning `docker build --pull` will try to pull `alpine:3.5` **four** times which seems unnecessary and slows down builds. Would it be possible to parse all parent images and only pull each unique one once?\r\n\r\nOutput with `--pull`:\r\n\r\n```\r\nSending build context to Docker daemon   2.56kB\r\n\r\nStep 1/8 : FROM alpine:3.5 AS stage1\r\n3.5: Pulling from library/alpine\r\nDigest: sha256:4d3ec631cdde98a03b91477b411a1fb42a9cadd8139c2e78029e44e199e58433\r\nStatus: Image is up to date for alpine:3.5\r\n ---> a2b04ae28915\r\nStep 2/8 : RUN echo \"stage1\"\r\n ---> Using cache\r\n ---> c6db214abf6a\r\nStep 3/8 : FROM alpine:3.5 AS stage2\r\n3.5: Pulling from library/alpine\r\nDigest: sha256:4d3ec631cdde98a03b91477b411a1fb42a9cadd8139c2e78029e44e199e58433\r\nStatus: Image is up to date for alpine:3.5\r\n ---> a2b04ae28915\r\nStep 4/8 : RUN echo \"stage2\"\r\n ---> Using cache\r\n ---> b892bd3bfdfe\r\nStep 5/8 : FROM alpine:3.5 AS stage3\r\n3.5: Pulling from library/alpine\r\nDigest: sha256:4d3ec631cdde98a03b91477b411a1fb42a9cadd8139c2e78029e44e199e58433\r\nStatus: Image is up to date for alpine:3.5\r\n ---> a2b04ae28915\r\nStep 6/8 : RUN echo \"stage2\"\r\n ---> Using cache\r\n ---> b892bd3bfdfe\r\nStep 7/8 : FROM alpine:3.5\r\n3.5: Pulling from library/alpine\r\nDigest: sha256:4d3ec631cdde98a03b91477b411a1fb42a9cadd8139c2e78029e44e199e58433\r\nStatus: Image is up to date for alpine:3.5\r\n ---> a2b04ae28915\r\nStep 8/8 : RUN echo \"test\"\r\n ---> Using cache\r\n ---> 626547251895\r\nSuccessfully built 626547251895\r\n\r\n0,08s user 0,01s system 0% cpu 13,279 total\r\n```\r\n\r\nOutput without `--pull`:\r\n```\r\nSending build context to Docker daemon  4.608kB\r\n\r\nStep 1/8 : FROM alpine:3.5 AS stage1\r\n ---> a2b04ae28915\r\nStep 2/8 : RUN echo \"stage1\"\r\n ---> Using cache\r\n ---> c6db214abf6a\r\nStep 3/8 : FROM alpine:3.5 AS stage2\r\n ---> a2b04ae28915\r\nStep 4/8 : RUN echo \"stage2\"\r\n ---> Using cache\r\n ---> b892bd3bfdfe\r\nStep 5/8 : FROM alpine:3.5 AS stage3\r\n ---> a2b04ae28915\r\nStep 6/8 : RUN echo \"stage2\"\r\n ---> Using cache\r\n ---> b892bd3bfdfe\r\nStep 7/8 : FROM alpine:3.5\r\n ---> a2b04ae28915\r\nStep 8/8 : RUN echo \"test\"\r\n ---> Using cache\r\n ---> 626547251895\r\nSuccessfully built 626547251895\r\n\r\n0,08s user 0,03s system 8% cpu 1,262 total\r\n```"},{"labels":[null,null,"enhancement"],"text":"This is really a feature request suggesting that it would be nice to have a --quiet/-q option to the docker push command."},{"labels":[null,"enhancement"],"text":"Swarm-scoped events contain `service` events, but not `task` ones. As `task` lifecycle can be different from a `service` (a container is killed, it will create a new task — without any change to the `service` spec, so no service event), we should also send events when there is events on `tasks`.\r\n\r\nThis is especially useful for tools like `traefik` (that reacts to service events and update their configuration depending on the service's tasks) — having a way to listen to task events too would ease the pain (and the `time.Sleep`).\r\n\r\nAlso, `task`s can be other element than container, like plugins or network, and some tools may want to react to those events.\r\n\r\ncc @thaJeztah "},{"labels":[null,"enhancement"],"text":"Similar to https://github.com/moby/moby/issues/35280, there's a need to skip files from the build stage. For example, `npm` allows installing all dependencies for building/testing and then just the runtime dependencies. This is done using [`devDependencies`](https://docs.npmjs.com/files/package.json#devdependencies) but there's currently not an easy way exclude the `node_modules` from the build so `npm` can be run again to exclude the `devDependencies` and take advantage of multi-stage builds.\r\n\r\nThis is requesting that an `--exclude` or `--ignoreDir` option be added to `COPY --from`.\r\n\r\nThis is also a [related request](https://github.com/moby/moby/issues/33923) but for some system it may require that different ignore options be used at the different stages."},{"labels":[null,"enhancement"],"text":"Hi there,\r\n\r\nI'm using the nvidia-docker 2.0 runtime to work with GPUs from within a container:\r\nhttps://github.com/NVIDIA/nvidia-docker\r\n\r\nUnfortunately, building images which rely on the runtime is tricky right now... The nvidia-docker repo actually documents this in their wiki:\r\n\r\n>Default runtime\r\n>\r\n>The default runtime used by the Docker® Engine is runc, our runtime can become the default one by configuring the docker daemon with --default-runtime=nvidia. Doing so will remove the need to add the --runtime=nvidia argument to docker run. It is also the only way to have GPU access during docker build. \r\n\r\nThe nvidia runtime not only provides GPU access, it overlays the libcuda file into the container, which is necessary for building things like Tensorflow. So it seems like there is a good case for making runtime-setting during builds easier. An alternative to changing the default runtime is to run a base container and then commit the container as an image after building the necessary libraries, but obviously that is a little unpleasant.\r\n\r\nIs it possible to set the runtime in a dockerfile, perhaps as the first line?\r\n```RUNTIME nvidia```"},{"labels":[null,"enhancement"],"text":"This is a feature request.\r\n\r\nI see image metrics exist for delete and history, but none for push/pull. Any reason not to add metrics for these operations? "},{"labels":[null,null,"enhancement"],"text":"https://github.com/moby/moby/blob/65bd038fc5e47ed37d2702cbdd6ce484d320380b/pkg/urlutil/urlutil.go#L13\r\n\r\ndiscussed in https://github.com/moby/buildkit/pull/416#discussion_r191599027\r\n\r\ncc @tonistiigi "},{"labels":[null,"enhancement",null,null],"text":"The largest limitation that I am facing using (non-LCOW) Docker for windows is that all files mounted to the moby vm via a volume are assigned UID 0 : GID 0 755 unix file permissions, which cannot be changed.\r\nSince many linux docker projects involve processes that need file permissions to be changed, these projects cannot be run on a windows host without severe modifications - examples are the standard apache server, nextcloud or gitlab which change file permissions to special users created in the container and fail if they are not able to do so.\r\n\r\nIn (https://github.com/docker/for-win/issues/2042), it was mentioned that while standard Docker for Windows uses CIFS mounts that are quite inflexible, the LCOW way of mounting host files into containers using 9p might have the potential to allow for much more flexible mapping. Is this correct, and is there any support for creating volume mounts with custom uid/gid/permission sets on the roadmap for LCOW development? \r\n\r\nThank you for any insight into this issue!\r\n"},{"labels":[null,"enhancement"],"text":"Hello all,\r\n\r\nCurrently docker does not exposes any internal data about any of its logdrivers and their internal state, there's also currently no interface for this (in case a logdriver wants to publish these numbers).\r\n\r\nThe idea is to have the ability to publish (don't know how yet, maybe on the output of docker inspect?) some numbers about the internal state of the logdriver, such as:\r\n\r\n* Is my internal buffer full?\r\n* Am I having any difficulty delivering my logs to the configured destination?\r\n* How many retries I already made?\r\n* Total buffer size, in bytes;\r\n* Buffer used space, in bytes;\r\n* other useful metrics\r\n\r\nWith numbers like these it would be possible to monitor the health of the log collecting for each container, and whenever some problems arise to be able to identify where the problem is quicker.\r\n\r\nWhat do you think about this possibility? The idea of this issue is to discuss about this and eventually get to a point where it's possible to implement.\r\n\r\nThanks,\r\n\r\n"},{"labels":[null,"enhancement"],"text":"This relates to:\r\n\r\n- https://github.com/moby/moby/issues/36759 \"Support \"--from\" for ADD in multi stage build\"\r\n- https://github.com/moby/moby/issues/37112 `COPY --from=foo bar.tgz ...`\r\n\r\n## Problem statement\r\n\r\nThe `COPY` (and `ADD`) Dockerfile instructions by default reset the ownership of files added to `0:0`.\r\n\r\nWhile this makes sense when copying files from a build-context (users/groups on the host in most situations won't match user/group in the container), in multi-stage builds this situation may be different.\r\n\r\nIn a multi-stage build, intermediate stages are meant to prepare content/artifacts for the final stage(s) they are copied to. This preparation can include: setting the correct ownership (and permissions) of files.\r\n\r\nBecause of the current behavior of `COPY`, those permissions are reset, and workarounds, such as `tar`-ing the files before `COPY`-ing, then extracting the tar in the final stage (which preserves permissions and ownership as set on the files before tar-ing) are not ideal.\r\n\r\n## Proposal\r\n\r\nI propose to preserve permissions and ownership of files/directories when `COPY`-ing between stages in a multi-stage build\r\n\r\n### Example\r\n\r\nBuilding this Dockerfile on a current version of Docker:\r\n\r\n```Dockerfile\r\nFROM busybox AS one\r\nRUN mkdir -p /foo/1-subdir \\\r\n && touch \\\r\n  /foo/4-five-six \\\r\n  /foo/7-eight-nine \\\r\n && chown -R 123:123 /foo/1-subdir \\\r\n && chown 456:456 /foo/4-five-six \\\r\n && chown 789:789 /foo/7-eight-nine \\\r\n && chmod -R 0600 /foo/1-subdir \\\r\n && chmod 0060 /foo/4-five-six \\\r\n && chmod 0006 /foo/7-eight-nine\r\nRUN echo \"In stage one\" \\\r\n && ls -l /foo/\r\n\r\n\r\nFROM busybox AS final\r\nCOPY --from=one /foo /bar\r\nRUN echo \"In final stage\" \\\r\n && ls -l /bar/\r\n```\r\n\r\nProduces:\r\n\r\n```\r\nIn stage one\r\ntotal 4\r\ndrw-------    2 123      123           4096 May 22 12:24 1-subdir\r\n----rw----    1 456      456              0 May 22 12:24 4-five-six\r\n-------rw-    1 789      789              0 May 22 12:24 7-eight-nine\r\n```\r\n\r\n```\r\nIn final stage\r\ntotal 4\r\ndrw-------    2 root     root          4096 May 22 12:24 1-subdir\r\n----rw----    1 root     root             0 May 22 12:24 4-five-six\r\n-------rw-    1 root     root             0 May 22 12:24 7-eight-nine\r\n```\r\n\r\n\r\nWith the proposed changes, the final stage would look like:\r\n\r\n\r\n```\r\nIn final stage\r\ntotal 4\r\ndrw-------    2 123      123           4096 May 22 12:24 1-subdir\r\n----rw----    1 456      456              0 May 22 12:24 4-five-six\r\n-------rw-    1 789      789              0 May 22 12:24 7-eight-nine\r\n```\r\n\r\n\r\n\r\n## Question / to be discussed\r\n\r\n`COPY --from` accepts both the name/number of a build-stage, as well as an image-reference:\r\n\r\n- Should we preserve ownership/permissions when copying from an _image_ as well? (`COPY --from myimage:latest`)\r\n- Should we add new options to make the `--from` less ambiguous, and only preserve ownership/permissions when copying from other _stages_ (i.e., add `--from-stage` and `--from-image` options)?\r\n\r\n\r\n\r\n"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nCurrently only directory volume mapping is supported for windows container, file a Feature Request issue to support file volume mapping for windows container\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. \r\n```\r\nF:\\>docker run -d -v c:\\k.zip:c:\\k.zip --name servercore microsoft/aspnet:4.7.2-windowsservercore-1803\r\ndocker: Error response from daemon: invalid bind mount spec \"c:\\\\k.zip:c:\\\\k.zip\": invalid volume specification: 'c:\\k.zip:c:\\k.zip': invalid mount config for type \"bind\": source path must be a directory.\r\nSee 'docker run --help'.\r\n```\r\n2.\r\n3.\r\n\r\n**Describe the results you received:**\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nF:\\>docker version\r\nClient:\r\n Version:       17.06.2-ee-10\r\n API version:   1.30\r\n Go version:    go1.8.7\r\n Git commit:    66261a0\r\n Built: Fri Apr 27 00:42:30 2018\r\n OS/Arch:       windows/amd64\r\n\r\nServer:\r\n Engine:\r\n  Version:      17.06.2-ee-10\r\n  API version:  1.30 (minimum version 1.24)\r\n  Go version:   go1.8.7\r\n  Git commit:   66261a0\r\n  Built:        Fri Apr 27 00:54:58 2018\r\n  OS/Arch:      windows/amd64\r\n  Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"It looks like `COPY` does not untar files when they're coming from a\r\nprevious stage.  It would be good if it would.\r\n\r\nUse case: I'm trying to minimize an ubuntu image by creating a tgz with\r\nsome parts left out in one stage, then using the tgz in a scratch stage.\r\nThis is to avoid the problem of using `COPY` which loses ownership and\r\nset[ug]id information.  The thing is that I cannot use `ADD` since it\r\ndoesn't have a `--from`; I cannot use `COPY` since it won't unpack the\r\ntgz; and I cannot copy and *then* unpack the tgz since there is no\r\n`/bin/sh` so a `RUN` fails too (complaining that there is no `/bin/sh`).\r\n"},{"labels":[null,null,"enhancement"],"text":"When running docker service update, the process waits for the updates to finish. However, when running docker stack deploy, it always runs in attached mode.\r\n\r\nWould it be possible to add an attached mode to docker stack deploy?\r\n\r\nThis will be usefull when the docker stack deploy command is initiated from a continuous deployment script. In this case, you typically want to fail the deployment if \"docker stack deploy\" failed to update the services."},{"labels":[null,null,"enhancement"],"text":"Hi there,\r\n\r\n**Description**\r\nas [already asked](https://github.com/docker/compose/pull/5314) for docker compose it would be nice to have that feature also for `docker stack deploy`.\r\nstop_grace_period is nice for consistently shutdown containers during rolling updates but this not work during swarm node maintenance (service, container and host shutdown).\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker stack deploy -c docker-compose.yml test\r\n\r\n```\r\nversion: '3.6'\r\n\r\nservices:\r\n  nginx:\r\n    stop_grace_period: 2m\r\n    image: nginx:alpine\r\n```\r\n\r\n2. \r\n```\r\ndocker inspect <container id> | grep StopTimeout\r\n```\r\n\r\n**Describe the results you received:**\r\nI expect to find 2m also for the StopTimeout\r\n\r\n**Describe the results you expected:**\r\nwith docker-compose i have this:\r\n```\r\n$ docker inspect <container id> | grep StopTimeout\r\n            \"StopTimeout\": 120\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      18.03.1-ce\r\n API version:  1.37\r\n Go version:   go1.9.5\r\n Git commit:   9ee9f40\r\n Built:        Thu Apr 26 07:17:20 2018\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n Orchestrator: swarm\r\n\r\nServer:\r\n Engine:\r\n  Version:      18.03.1-ce\r\n  API version:  1.37 (minimum version 1.12)\r\n  Go version:   go1.9.5\r\n  Git commit:   9ee9f40\r\n  Built:        Thu Apr 26 07:15:30 2018\r\n  OS/Arch:      linux/amd64\r\n  Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 10\r\n Running: 4\r\n Paused: 0\r\n Stopped: 6\r\nImages: 29\r\nServer Version: 18.03.1-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: y8jr5aturl6z8uv8jtdqezdfy\r\n Is Manager: true\r\n ClusterID: 5vw3klw72dzidcslo4a9ungai\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 10\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 172.16.111.5\r\n Manager Addresses:\r\n  172.16.111.5:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 773c489c9c1b21a6d78b5c538cd395416ec50f88\r\nrunc version: 4fc53a81fb7c994640722ac585fa9ca548971871\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.13.0-37-generic\r\nOperating System: Ubuntu 16.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 15.44GiB\r\nName: master\r\nID: 7ZB6:IG3J:2EXA:VEEA:KLNX:FGFI:43KQ:DERQ:XBVU:37P7:LRGN:SZ55\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\nThanks for any advice.\r\nJacopo"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Update to tini 0.18.0**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\ntini 0.18.0 includes a fix for https://github.com/krallin/tini/issues/100, which is quite important in terms of management, since it means you basically don't need to include an init anymore in most cases.\r\n\r\nIt'd be great if it is included in the next docker release.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:       18.04.0-ce\r\n API version:   1.37\r\n Go version:    go1.9.4\r\n Git commit:    3d479c0\r\n Built: Tue Apr 10 18:27:27 2018\r\n OS/Arch:       linux/amd64\r\n Experimental:  false\r\n Orchestrator:  swarm\r\n\r\nServer:\r\n Engine:\r\n  Version:      18.04.0-ce\r\n  API version:  1.37 (minimum version 1.12)\r\n  Go version:   go1.9.4\r\n  Git commit:   3d479c0\r\n  Built:        Tue Apr 10 18:31:15 2018\r\n  OS/Arch:      linux/amd64\r\n  Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 29\r\n Running: 6\r\n Paused: 0\r\n Stopped: 23\r\nImages: 407\r\nServer Version: 18.04.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: journald\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 773c489c9c1b21a6d78b5c538cd395416ec50f88\r\nrunc version: 4fc53a81fb7c994640722ac585fa9ca548971871\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\n selinux\r\nKernel Version: 4.15.17-300.fc27.x86_64\r\nOperating System: Fedora 27 (Workstation Edition)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 14.63GiB\r\nName: yajolap.yajodomain\r\nID: KUBN:F7JL:URX6:HO55:R3L2:SCUU:IWVY:EZ2O:F53G:WHTO:3G4D:R4YU\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nPhysical, fedora."},{"labels":["enhancement"],"text":"**Docker daemon should have a validate-config option**\r\n\r\n\r\nAutomated updates to docker's `daemon.json` (eg: puppet) are unable to parse the config file for validity before restarting the daemon.  This may result in the daemon failing to restart, causing an outage for any running containers.\r\n\r\n\r\neg:\r\n\r\n```\r\n# docker daemon --config myconfig.json --validate\r\nSyntax OK\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"When using multi-stage build, thew output does not clearly identify where one stage ends and another begin. I need to run the first stage image after the build is done to extract build related info (such a build cache) but findng the imgage ID is difficult because the output is just like any other step.\r\n\r\nHere is the output of one of my multi-stage edge\r\n\r\n```\r\nRemoving intermediate container 535f930c06dc\r\n ---> 3e491620080f\r\nStep 7/14 : FROM alpine:3.4\r\n```\r\n\r\nIt would be nice if instead it would say:\r\n\r\n```\r\nRemoving intermediate container 535f930c06dc\r\nSuccessfully built 3e491620080f (stage: 0)\r\nStep 7/14 : FROM alpine:3.4\r\n```\r\n\r\nor for named ones:\r\n\r\n```\r\nRemoving intermediate container 535f930c06dc\r\nSuccessfully built 3e491620080f (stage: build)\r\nStep 7/14 : FROM alpine:3.4\r\n```"},{"labels":[null,"enhancement"],"text":"Is possible to use node label in --log-opt tag ?\r\n\r\n**Steps to reproduce the issue:**\r\n```\r\ndocker node update --label-add foo=bar vplv3lt877jv0mi8zsqyb4db3\r\ndocker run --log-driver=fluentd --name nginx --log-opt tag=\"test.{{.Name}}.{{.Node.Spec.Labels.foo}}\"  nginx echo $(date '+%Y %b %d %H:%M') Hello\r\n```\r\n**Describe the results you received:**\r\ndocker: Error response from daemon: failed to initialize logging driver: template: log-tag:1:22: executing \"log-tag\" at <.Node.Spec.Labels.fo...>: can't evaluate field Node in type *logger.Info.\r\nERRO[0001] error waiting for container: context canceled\r\n\r\n**Describe the results you expected:**\r\nResolve log opt tag as test.nginx.bar\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nCurrently its not possible to specify multiple listen ips for a single port.\r\n\r\n**Steps to reproduce the issue:**\r\n1. create a `docker-compose.yaml` that contains something like this:\r\n```\r\nversion: '3.3'\r\nservices:\r\n  whoami: \r\n    image: emilevauge/whoami\r\n    ports:\r\n      - 127.0.0.1:3080:80\r\n      - 127.0.0.2:3080:80\r\n```\r\n2. deploy `docker stack deploy test --compose-file docker-compose.yaml`\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\nCreating network test_default\r\nCreating service test_whoami\r\nfailed to create service test_whoami: Error response from daemon: rpc error: code = InvalidArgument desc = EndpointSpec: duplicate published ports provided\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nWhen i run docker-compose it works flawlessly:\r\n\r\n```\r\n$ docker-compose up -d\r\nStarting test_whoami_1 ... done\r\n$ ss -lnput | grep 3080\r\ntcp   LISTEN  0       128                127.0.0.1:3080           0.0.0.0:*      users:((\"docker-proxy\",pid=6720,fd=4))                                         \r\ntcp   LISTEN  0       128                127.0.0.2:3080           0.0.0.0:*      users:((\"docker-proxy\",pid=6709,fd=4))             \r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nThis is useful to setup HA Setups of docker. \r\nWith `sysctl -w net.ipv4.ip_nonlocal_bind=1` you can manage the ip addresses with keepalived or corosync and let docker-swarm handle the containers.\r\n\r\nAlso its possbile to isolate services to a single network using this method, without relying on a firewall\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n> docker version\r\nClient:\r\n Version:\t18.03.0-ce\r\n API version:\t1.37\r\n Go version:\tgo1.9.4\r\n Git commit:\t0520e24\r\n Built:\tWed Mar 21 23:10:06 2018\r\n OS/Arch:\tlinux/amd64\r\n Experimental:\tfalse\r\n Orchestrator:\tswarm\r\n\r\nServer:\r\n Engine:\r\n  Version:\t18.03.0-ce\r\n  API version:\t1.37 (minimum version 1.12)\r\n  Go version:\tgo1.9.4\r\n  Git commit:\t0520e24\r\n  Built:\tWed Mar 21 23:08:35 2018\r\n  OS/Arch:\tlinux/amd64\r\n  Experimental:\tfalse\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 5\r\n Running: 3\r\n Paused: 0\r\n Stopped: 2\r\nImages: 38\r\nServer Version: 18.03.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: xxx\r\n Is Manager: true\r\n ClusterID: xxx\r\n Managers: 3\r\n Nodes: 3\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 172.20.2.101\r\n Manager Addresses:\r\n  172.20.2.101:2377\r\n  172.20.2.102:2377\r\n  172.20.2.103:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: cfd04396dc68220d1cecbe686a6cc3aa5ce3667c\r\nrunc version: 4fc53a81fb7c994640722ac585fa9ca548971871\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.0-6-amd64\r\nOperating System: Debian GNU/Linux 9 (stretch)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 3.862GiB\r\nName: swarm-01\r\nID: xxx\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"Docker's JSON file log rotation is currenly only volume-based (via the `max-size` and `max-file` options for `--log-opt`). However, for GDPR compliance we need time-based rotation, so that container logs are guaranteed to be deleted after e.g. 30 days. We could do the rotation ourselves, but unfortunately Docker does not support reopening log files (see #7333), which prevents this.\r\n\r\nWe use Docker via Kubernetes on GKE, and therefore have limited control over the Docker setup (so we would clearly need Google to update their node images as well)."},{"labels":[null,null,"enhancement",null],"text":"**Description**\r\nThe splunk logging driver does not currently have the ability to specify the `host` field sent with the logs and [instead grabs the hostname from the docker host](https://github.com/moby/moby/blob/f653485e57a36518085868dcb48b4d924a520877/daemon/logger/splunk/splunk.go#L152). This should be configurable with a `--log-opt`."},{"labels":[null,"enhancement"],"text":"The `%L` option of `awslogs-datetime-format` has incorrect documentation and inflexible behaviour. Trying to match a time format of `00:00:00,789` where `789` is the millisecond component, is not possible with `%L`.\r\n\r\nLooking at the driver, `%L` matches `\\.\\d{3}`:\r\nhttps://github.com/moby/moby/blob/785b3e32876e10d638d9b1bae00e3f340a2f47ec/daemon/logger/awslogs/cloudwatchlogs.go#L209\r\nThis means it's requires a leading `.` at the start of the pattern which is not documented (https://github.com/docker/docker.github.io/blame/3a08079a03256b1525c62bc40b192f20e4ab1b65/config/containers/logging/awslogs.md#L172). This doesn't match the `%f` behaviour, which matches `\\d{6}` without a leading dot. It also means for time patterns such as my example, the `%L` option cannot be used. Changing the pattern of `%L` to `?\\.\\d{3}` to make the leading dot optional (as is done for `:` in other patterns like `%a`) would be ideal without breaking current uses.\r\n\r\nIt would also be helpful to clarify in the documentation that `%L` is intentionally used and not in `strftime `standard. Currently the documentation says \"The following strftime codes are supported:\" (https://github.com/docker/docker.github.io/pull/6344/files#diff-dba43446f630f91349e3973091c3bb88R154), but `%L` is not in the `strftime` standard. It's slightly confusing.\r\n"},{"labels":["enhancement",null],"text":"While the linux version of Docker provides the possibility to forward USB devices / COM support in Docker containers, this feature is currently not supported in Docker for Windows, which poses a severe restriction on any application that is relying on external hardware.\r\n\r\nThe overlying Docker for Windows application seems to support forwarding (at least in the docker run syntax), but containers do not accept the connection:\r\n\r\n### Steps to reproduce the behavior\r\n\r\n  1. Switch to Docker for Windows\r\n  2. Locate an existing COM port from your device manager - here COM3 is used\r\n  3. run `docker run -it --device  //./COM3://./COM3 microsoft/windowsservercore powershell`\r\n  4. in powershell, confirm that COM ports are listed:\r\n       `PS> [System.IO.Ports.SerialPort]::getportnames()`\r\n         `--> COM3`\r\n   5. When trying to open the port, the resulting error indicates however that the port cannot be found:\r\n```\r\nPS> $port= new-Object System.IO.Ports.SerialPort COM3,9600,None,8,one\r\nPS> $port.open()\r\n```\r\n\r\n```\r\nException calling \"Open\" with \"0\" argument(s): \"The port 'COM3' does not exist.\"\r\nAt line:1 char:1\r\n+ $port.Open()\r\n+ ~~~~~~~~~~~~\r\n    + CategoryInfo          : NotSpecified: (:) [], MethodInvocationException\r\n    + FullyQualifiedErrorId : IOException\r\n\r\n```\r\n\r\n    Windows host: Windows Server 2016\r\n    Docker Version:\r\n    Client:\r\n    Version: 17.06.2-ce\r\n    API version: 1.30\r\n    Go version: go1.8.3\r\n    Git commit: cec0b72\r\n    Built: Tue Sep 5 19:57:19 2017\r\n    OS/Arch: windows/amd64\r\n\r\nI have described this request further in \r\nhttps://github.com/docker/for-win/issues/1139\r\nand was forwarded to this repository as the basic work for supporting this feature would need to be done on moby/the upstream docker on windows support.\r\n\r\n"},{"labels":[null,"enhancement"],"text":"We have applications which frequently generate log messages > 16k in length and the json-file logging driver truncates these messages. (modsecurity audit logs, application stack traces etc) \r\n\r\nDocker daemon has a 16K buffer for log messages. If a message length exceeds 16K, it should be split by the json file logger and merged at the endpoint.\r\n\r\nFor reference can functionality similar to this be implemented for the `json-file` logging driver? Some option that would add new fields that allows a consumer of the logs to re-correlate split/partial log messages that are > 16k in length overall. \r\n\r\nhttps://github.com/moby/moby/pull/35831\r\nhttps://github.com/elastic/beats/issues/6605\r\nhttps://github.com/moby/moby/pull/34888#issuecomment-378346970\r\n"},{"labels":[null,"enhancement"],"text":"When attempting to write a test for https://github.com/moby/moby/pull/36684, I realized the daemon implementation in the integration test suite is always using the main test daemon's containerd instance: https://github.com/moby/moby/blob/6be0f709830113966f295401327b027ec2f0bbca/integration-cli/daemon/daemon.go#L223\r\n\r\nThis prevents certain test scenarios from being possible."},{"labels":[null,null,"enhancement",null],"text":"I'm on latest Docker for Mac. It would be quite nice if `docker push` had a quiet mode, similar to how `curl` has `-q` to avoid showing the download-in-progress animations. I like the push output for working directly with `docker` but not for shell scripts."},{"labels":[null,"enhancement"],"text":"**Steps to reproduce the issue:**\r\n1. I have a Dockerfile `FROM node:alpine`. This base image defines `CMD [\"node\"]`.\r\n\r\n2. In my Dockerfile, I set my own custom entrypoint: `ENTRYPOINT [ \"/usr/local/bin/npm\", \"run\" \"debug\" ]`\r\n\r\n3. When building and running I get: `sh: run: unknown operand`.\r\n\r\n**Describe the results you received:**\r\n\r\nAfter much head-banging I noticed that my ENTRYPOINT should have a comma between the \"run\" and \"debug\" arguments. According to the explanation in #16262 this is owing to the fact that some non-JSON commands may start with `[` and thus anything that is not valid JSON is treated as a string and passed to `sh`.\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\nCan we re-visit this behaviour or at least improve the error messages around this? I'm sure I'm not the only developer who has lost time trying to debug subtle syntax issues in Dockerfiles thanks to this.\r\n\r\nHaving `/bin/sh` executed inside the container (and then the error message from `sh`) when neither `CMD` nor `ENTRYPOINT` uses `sh` is very confusing.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:       18.02.0-ce\r\n API version:   1.36\r\n Go version:    go1.9.4\r\n Git commit:    fc4de447b5\r\n Built: Tue Feb 13 15:28:01 2018\r\n OS/Arch:       linux/amd64\r\n Experimental:  false\r\n Orchestrator:  swarm\r\n\r\nServer:\r\n Engine:\r\n  Version:      18.02.0-ce\r\n  API version:  1.36 (minimum version 1.12)\r\n  Go version:   go1.9.4\r\n  Git commit:   fc4de447b5\r\n  Built:        Tue Feb 13 15:28:34 2018\r\n  OS/Arch:      linux/amd64\r\n  Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 3\r\n Running: 1\r\n Paused: 0\r\n Stopped: 2\r\nImages: 22\r\nServer Version: 18.02.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: false\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 9b55aab90508bd389d7654c4baf173a981477d55\r\nrunc version: 9f9c96235cc97674e935002fc3d78361b696a69e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.15.10-1-ARCH\r\nOperating System: Arch Linux\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 3.833GiB\r\nName: avm\r\nID: DSHK:R5UI:VORC:YLR3:WHFN:I4NW:F45A:APLO:QCXI:G22A:ZO4S:6FRO\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):** Arch Linux (up to date) running inside VMWare fusion on MacOS\r\n"},{"labels":["enhancement",null],"text":"When running Docker for Windows and Docker for macOS, we get these handy aliases like `docker.for.mac.localhost`, `docker.for.mac.internal`, `docker.for.win.localhost` and `docker.for.win.internal`.\r\n\r\nUnfortunately, when my projects are being worked on between macOS and Windows, someone has to go changing any of these host names for the specific platform they're on, despite the premise of the two being the same!  The fact that these two things that accomplish the exact same outcome are named differently is unhelpful.\r\n\r\nIs there any way we can get a new convention like `docker.internal` and `docker.localhost`? Would it also be possible to simply have them resolve `localhost` when used on Linux which obviously doesn't have the extra layer going on?\r\n\r\nThe name of the game here is that I need my containers to have a consistent way to reference not the VM, but the actual physical hardware they're running on. Bonus points if it's a consistent IP address so that I can configure my own DNS aliases!\r\n\r\nMoreover, would it be possible either with or without my suggested change to get all these magic hostnames documented somewhere? They seem to be rather un-googleable and largely defined by anecdotal knowledge rather than being clearly offered.\r\n"},{"labels":[null,null,null,"enhancement"],"text":"Follow up to https://github.com/moby/moby/pull/36538#discussion_r174090407\r\n\r\nThere are many tests that create objects (containers, networks, volumes, services) with a name. We should avoid using names (where possible), to prevent tests possibly influencing other tests due to resources not (yet) being cleaned up, or possibly if tests are run in parallel\r\n\r\n- Remove names from tests, and use object ID's (where possible)\r\n- For those tests that do need a name, make sure that a _unique_ name is used. (For example, use `t.Name()` to name an object after the name of the test, or use a randomly generated name)\r\n\r\nThis may be a lot of work to go through all the tests, but perhaps there are contributors that want to work on some of these\r\n\r\nIf you're interested to work in this, leave a comment below, and mention what tests you'll be working on (so that it doesn't conflict with other people working on the same tests 😅)\r\n\r\n"},{"labels":[null,null,null,"enhancement",null],"text":"Creating a tracking issue for https://github.com/moby/moby/pull/36538#discussion_r174087838\r\n\r\nWe should update `testenv.ProtectAll()` and `testenv.Clean()` to also cleanup services, so that integration-tests don't have to clean up services after running the tests.\r\n\r\nSimilar to the way containers, networks, and other resources are currently handled;\r\n\r\nhttps://github.com/moby/moby/blob/3e0299f58c580dd356163cce0d9ca2e686ff1d15/internal/test/environment/protect.go#L32-L43\r\n\r\nhttps://github.com/moby/moby/blob/3e0299f58c580dd356163cce0d9ca2e686ff1d15/internal/test/environment/clean.go#L25-L42\r\n\r\n\r\nlabelling with both \"beginner\" and \"intermediate\"; it's probably somewhere in between :smile:"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nIs it possible for docker deamon to not translate error messages returned by registry?\r\nFor all pull request 403 error, docker daemon always translate it into error message like following: \"pull access denied for somerepo/someproject/someimage, repository does not exist or may require 'docker login'\"\r\nWe want deamon to not translate the error message our registry service returned, so that we can customize our error message to provide clearer content.\r\n\r\nFYI for push request, I believe deamon does not translate any error messages.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Client make a pull request \r\n2. Our registry service returned some error message with error code 403\r\n\r\n**Describe the results you received:**\r\nThe client got the error \"pull access denied for somerepo/someproject/someimage, repository does not exist or may require 'docker login'\"\r\n\r\n**Describe the results you expected:**\r\nThe client got the error message our registry returned\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.09.0-ce\r\n API version:  1.32\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:42:09 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.09.0-ce\r\n API version:  1.32 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:40:48 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 3\r\n Running: 0\r\n Paused: 0\r\n Stopped: 3\r\nImages: 8\r\nServer Version: 17.09.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 06b9cb35161009dcb7123345749fef02f7cea8e0\r\nrunc version: 3f2f8b84a77f73d38244dd690525642a72156c64\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.0-6-amd64\r\nOperating System: Debian GNU/Linux buster/sid\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 12\r\nTotal Memory: 62.91GiB\r\nName: molian-glinux.kir.corp.google.com\r\nID: TCMM:JH7Y:22YX:YHRE:KB7P:EHRO:2VIY:YTMO:PJQP:QHEP:6VJH:QGOA\r\nDocker Root Dir: /usr/local/google/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 17\r\n Goroutines: 25\r\n System Time: 2018-03-12T10:45:11.541509347-07:00\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,null,null,"enhancement"],"text":"While triaging https://github.com/moby/moby/issues/36510#issuecomment-371121690, I noticed that the current error message is rather cryptic, and doesn't provide enough information for the user to resolve the problem:\r\n\r\n```\r\nError initializing network controller: list bridge addresses failed: no available network\r\n```\r\n\r\nWhen the daemon starts, it attempts to find if any of the standard private IP-ranges is available for docker to use (doesn't overlap with existing networks in your situation): `172.17-31.x.x/16`, `192.168.x.x/20` or `10.x.x.x/24`: https://github.com/moby/moby/blob/747c163a65365933c5d6d7f0740f2ac8e3775287/vendor/github.com/docker/libnetwork/ipamutils/utils.go#L10-L15\r\n\r\nWhen failing to select a range, only the error above is printed.\r\n\r\n\r\nI sugest we improve the error, and (if possible) call out:\r\n\r\n- each IP-range that is tried\r\n- a message explaining that Docker failed to automatically select an available IP-range\r\n- possibly referring to the documentation: https://docs.docker.com/v17.09/engine/userguide/networking/default_network/custom-docker0/ (note: in the latest rewrite of the docs, I think some of that information has gone lost /cc @gbarr01)\r\n- should we mention the \"VPN\" situation? (i.e. a VPN capturing all IP-ranges, causing detection to fail?)\r\n\r\n\r\nNote that the detection is done in libnetwork; generally \"libraries\" should not do logging, so that's possibly something to take into account when implementing.\r\n\r\n\r\nping @fcrisciani @selansen PTAL if you have input on this; I'm assigning this `exp/beginner` and `exp/intermediate` - possibly a contributor wants to work on this"},{"labels":[null,"enhancement"],"text":"Today I got the following error on a `docker deploy`:\r\n\r\n`failed to create service <name-which-includes-domain-name>: Error response from daemon: rpc error: code = InvalidArgument desc = name must be 63 characters or fewer`\r\n\r\nIs there a specific reason for the 63 char limitation? If not, can that constant be enlarged to for example 256? Still arbitrary I know, but 63 is rather short, if it includes a domain name, and something like `production`, and the name of the service itself."},{"labels":[null,null,null,null,null,null,null,"enhancement"],"text":"While reviewing https://github.com/moby/moby/pull/36438, we noticed that the format / properties of log-messages coming from storage-drivers are not consistent; https://github.com/moby/moby/pull/36438#issuecomment-369457421\r\n\r\n- devmapper uses `logrus.WithField(\"storage-driver\", \"devicemapper\")` in some messages.\r\n- aufs uses \r\n```go\r\nlogrus.WithFields(logrus.Fields{\r\n        \"module\": \"graphdriver\",\r\n        \"driver\": \"aufs\",\r\n})\r\n```\r\n\r\n(\"module\" + \"driver\" versus \"storage-driver\")\r\n\r\nIt would be nice to standardise on one for all storage-drivers.\r\n\r\n/cc @kolyshkin as discussed 👍 "},{"labels":["enhancement",null],"text":"**Description**\r\n\r\nWindows 10 and Server 2016 RS4 (upcoming update to Windows due 18/03) will bring support for Unix Sockets (address family AF_UNIX). If we could support them in Moby, this could bring better feature parity with Linux, and it would make it easier for users of WSL. There are 2 levels of support we can achieve:\r\n\r\n***Make docker daemon on Windows listen on a Unix socket - the easy stuff***\r\n\r\nThe same way as we do on linux (listening on `/var/run/docker.sock`), we could listen on `c:\\ProgramData\\Docker\\docker.sock` (or any file path passed trough the --HOST flag). This would allow WSL users to connect to the daemon without having to expose anything trough TCP. This would also make it easier for admins to secure access to the daemon (using ntfs rights).\r\n\r\nThis could be done easily entirely in the Moby code-base.\r\n\r\n***Make Unix socket files bind-mountable in containers - the not so easy one***\r\n\r\nThe goal here is to make it easy to bind mount a Unix Socket in a container (windows or lcow), the same way that we do on Linux. It would for exemple allow to bind mount Docker socket itself inside a container (windows or lcow), like many Linux containers do (and as we do ourself with Docker EE).\r\n\r\nThis might require some additional work on Windows HCS, but we should ask for confirmation from Microsoft.\r\n\r\n\r\n"},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nWhen inspecting a container, no information can be found about secrets or configs that are used by the container.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\nCreate a service using a secret and a config:\r\n\r\n```bash\r\necho \"bla\" | docker config create foo.conf -\r\necho \"bla\" | docker secret create secret.conf -\r\n\r\n\r\ndocker service create -d \\\r\n    --config source=foo.conf,target=/foo.conf,uid=123,gid=456 \\\r\n    --secret source=secret.conf,target=/secret.conf,uid=234,gid=567 \\\r\n    --replicas 1 \\\r\n    --name myservice \\\r\n    nginx:alpine\r\n```\r\n\r\nInspect the service:\r\n\r\n```bash\r\ndocker service inspect myservice\r\n```\r\n\r\n<details>\r\n\r\n```json\r\n[\r\n    {\r\n        \"ID\": \"ib7yble4rm5qbz3ijmk83cxeh\",\r\n        \"Version\": {\r\n            \"Index\": 776\r\n        },\r\n        \"CreatedAt\": \"2018-02-28T09:42:57.612406537Z\",\r\n        \"UpdatedAt\": \"2018-02-28T09:42:57.612406537Z\",\r\n        \"Spec\": {\r\n            \"Name\": \"myservice\",\r\n            \"Labels\": {},\r\n            \"TaskTemplate\": {\r\n                \"ContainerSpec\": {\r\n                    \"Image\": \"nginx:alpine@sha256:48947591194ac5a9dce1e110f9198a547debb21630f121081640b87d99ca8b11\",\r\n                    \"StopGracePeriod\": 10000000000,\r\n                    \"DNSConfig\": {},\r\n                    \"Secrets\": [\r\n                        {\r\n                            \"File\": {\r\n                                \"Name\": \"/secret.conf\",\r\n                                \"UID\": \"234\",\r\n                                \"GID\": \"567\",\r\n                                \"Mode\": 292\r\n                            },\r\n                            \"SecretID\": \"yv9vtn86nugxsbshu1ccumlod\",\r\n                            \"SecretName\": \"secret.conf\"\r\n                        }\r\n                    ],\r\n                    \"Configs\": [\r\n                        {\r\n                            \"File\": {\r\n                                \"Name\": \"/foo.conf\",\r\n                                \"UID\": \"123\",\r\n                                \"GID\": \"456\",\r\n                                \"Mode\": 292\r\n                            },\r\n                            \"ConfigID\": \"j915gbt419wv9022nr2w8pqdh\",\r\n                            \"ConfigName\": \"foo.conf\"\r\n                        }\r\n                    ],\r\n                    \"Isolation\": \"default\"\r\n                },\r\n                \"Resources\": {\r\n                    \"Limits\": {},\r\n                    \"Reservations\": {}\r\n                },\r\n                \"RestartPolicy\": {\r\n                    \"Condition\": \"any\",\r\n                    \"Delay\": 5000000000,\r\n                    \"MaxAttempts\": 0\r\n                },\r\n                \"Placement\": {\r\n                    \"Platforms\": [\r\n                        {\r\n                            \"Architecture\": \"amd64\",\r\n                            \"OS\": \"linux\"\r\n                        },\r\n                        {\r\n                            \"Architecture\": \"arm64\",\r\n                            \"OS\": \"linux\"\r\n                        },\r\n                        {\r\n                            \"Architecture\": \"386\",\r\n                            \"OS\": \"linux\"\r\n                        },\r\n                        {\r\n                            \"Architecture\": \"ppc64le\",\r\n                            \"OS\": \"linux\"\r\n                        },\r\n                        {\r\n                            \"Architecture\": \"s390x\",\r\n                            \"OS\": \"linux\"\r\n                        }\r\n                    ]\r\n                },\r\n                \"ForceUpdate\": 0,\r\n                \"Runtime\": \"container\"\r\n            },\r\n            \"Mode\": {\r\n                \"Replicated\": {\r\n                    \"Replicas\": 1\r\n                }\r\n            },\r\n            \"UpdateConfig\": {\r\n                \"Parallelism\": 1,\r\n                \"FailureAction\": \"pause\",\r\n                \"Monitor\": 5000000000,\r\n                \"MaxFailureRatio\": 0,\r\n                \"Order\": \"stop-first\"\r\n            },\r\n            \"RollbackConfig\": {\r\n                \"Parallelism\": 1,\r\n                \"FailureAction\": \"pause\",\r\n                \"Monitor\": 5000000000,\r\n                \"MaxFailureRatio\": 0,\r\n                \"Order\": \"stop-first\"\r\n            },\r\n            \"EndpointSpec\": {\r\n                \"Mode\": \"vip\"\r\n            }\r\n        },\r\n        \"Endpoint\": {\r\n            \"Spec\": {}\r\n        }\r\n    }\r\n]\r\n```\r\n\r\n</details>\r\n\r\nInspect a task for the service:\r\n\r\n```bash\r\ndocker inspect n94nynzgc3m5  \r\n```\r\n\r\n<details>\r\n\r\n```json\r\n[\r\n    {\r\n        \"ID\": \"n94nynzgc3m5w7vowovraow1w\",\r\n        \"Version\": {\r\n            \"Index\": 782\r\n        },\r\n        \"CreatedAt\": \"2018-02-28T09:42:57.614135729Z\",\r\n        \"UpdatedAt\": \"2018-02-28T09:42:58.266882287Z\",\r\n        \"Labels\": {},\r\n        \"Spec\": {\r\n            \"ContainerSpec\": {\r\n                \"Image\": \"nginx:alpine@sha256:48947591194ac5a9dce1e110f9198a547debb21630f121081640b87d99ca8b11\",\r\n                \"DNSConfig\": {},\r\n                \"Secrets\": [\r\n                    {\r\n                        \"File\": {\r\n                            \"Name\": \"/secret.conf\",\r\n                            \"UID\": \"234\",\r\n                            \"GID\": \"567\",\r\n                            \"Mode\": 292\r\n                        },\r\n                        \"SecretID\": \"yv9vtn86nugxsbshu1ccumlod\",\r\n                        \"SecretName\": \"secret.conf\"\r\n                    }\r\n                ],\r\n                \"Configs\": [\r\n                    {\r\n                        \"File\": {\r\n                            \"Name\": \"/foo.conf\",\r\n                            \"UID\": \"123\",\r\n                            \"GID\": \"456\",\r\n                            \"Mode\": 292\r\n                        },\r\n                        \"ConfigID\": \"j915gbt419wv9022nr2w8pqdh\",\r\n                        \"ConfigName\": \"foo.conf\"\r\n                    }\r\n                ],\r\n                \"Isolation\": \"default\"\r\n            },\r\n            \"Resources\": {\r\n                \"Limits\": {},\r\n                \"Reservations\": {}\r\n            },\r\n            \"Placement\": {\r\n                \"Platforms\": [\r\n                    {\r\n                        \"Architecture\": \"amd64\",\r\n                        \"OS\": \"linux\"\r\n                    },\r\n                    {\r\n                        \"Architecture\": \"arm64\",\r\n                        \"OS\": \"linux\"\r\n                    },\r\n                    {\r\n                        \"Architecture\": \"386\",\r\n                        \"OS\": \"linux\"\r\n                    },\r\n                    {\r\n                        \"Architecture\": \"ppc64le\",\r\n                        \"OS\": \"linux\"\r\n                    },\r\n                    {\r\n                        \"Architecture\": \"s390x\",\r\n                        \"OS\": \"linux\"\r\n                    }\r\n                ]\r\n            },\r\n            \"ForceUpdate\": 0\r\n        },\r\n        \"ServiceID\": \"ib7yble4rm5qbz3ijmk83cxeh\",\r\n        \"Slot\": 1,\r\n        \"NodeID\": \"oifk2p0hd4tvlb62uf76womx0\",\r\n        \"Status\": {\r\n            \"Timestamp\": \"2018-02-28T09:42:58.210561074Z\",\r\n            \"State\": \"running\",\r\n            \"Message\": \"started\",\r\n            \"ContainerStatus\": {\r\n                \"ContainerID\": \"f01771ae88eff92a8fbb8caaa2b29f2cbb81d6e415a42644789a9970f60ec290\",\r\n                \"PID\": 20669\r\n            },\r\n            \"PortStatus\": {}\r\n        },\r\n        \"DesiredState\": \"running\"\r\n    }\r\n]\r\n```\r\n\r\n</details>\r\n\r\nNotice that information about secrets and configs is present in both\r\n\r\n\r\nInspect a _container_ for the service:\r\n\r\n```bash\r\ndocker container inspect f01771ae88ef\r\n```\r\n\r\n<details>\r\n\r\n```json\r\n[\r\n    {\r\n        \"Id\": \"f01771ae88eff92a8fbb8caaa2b29f2cbb81d6e415a42644789a9970f60ec290\",\r\n        \"Created\": \"2018-02-28T09:42:57.777199629Z\",\r\n        \"Path\": \"nginx\",\r\n        \"Args\": [\r\n            \"-g\",\r\n            \"daemon off;\"\r\n        ],\r\n        \"State\": {\r\n            \"Status\": \"running\",\r\n            \"Running\": true,\r\n            \"Paused\": false,\r\n            \"Restarting\": false,\r\n            \"OOMKilled\": false,\r\n            \"Dead\": false,\r\n            \"Pid\": 20669,\r\n            \"ExitCode\": 0,\r\n            \"Error\": \"\",\r\n            \"StartedAt\": \"2018-02-28T09:42:58.198336062Z\",\r\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\r\n        },\r\n        \"Image\": \"sha256:537527661905acfd8c6954bbfcfc9b5fe3120e8af34949f68c972f6589635f3c\",\r\n        \"ResolvConfPath\": \"/var/lib/docker/containers/f01771ae88eff92a8fbb8caaa2b29f2cbb81d6e415a42644789a9970f60ec290/resolv.conf\",\r\n        \"HostnamePath\": \"/var/lib/docker/containers/f01771ae88eff92a8fbb8caaa2b29f2cbb81d6e415a42644789a9970f60ec290/hostname\",\r\n        \"HostsPath\": \"/var/lib/docker/containers/f01771ae88eff92a8fbb8caaa2b29f2cbb81d6e415a42644789a9970f60ec290/hosts\",\r\n        \"LogPath\": \"/var/lib/docker/containers/f01771ae88eff92a8fbb8caaa2b29f2cbb81d6e415a42644789a9970f60ec290/f01771ae88eff92a8fbb8caaa2b29f2cbb81d6e415a42644789a9970f60ec290-json.log\",\r\n        \"Name\": \"/myservice.1.n94nynzgc3m5w7vowovraow1w\",\r\n        \"RestartCount\": 0,\r\n        \"Driver\": \"overlay2\",\r\n        \"Platform\": \"linux\",\r\n        \"MountLabel\": \"\",\r\n        \"ProcessLabel\": \"\",\r\n        \"AppArmorProfile\": \"\",\r\n        \"ExecIDs\": null,\r\n        \"HostConfig\": {\r\n            \"Binds\": null,\r\n            \"ContainerIDFile\": \"\",\r\n            \"LogConfig\": {\r\n                \"Type\": \"json-file\",\r\n                \"Config\": {}\r\n            },\r\n            \"NetworkMode\": \"default\",\r\n            \"PortBindings\": {},\r\n            \"RestartPolicy\": {\r\n                \"Name\": \"\",\r\n                \"MaximumRetryCount\": 0\r\n            },\r\n            \"AutoRemove\": false,\r\n            \"VolumeDriver\": \"\",\r\n            \"VolumesFrom\": null,\r\n            \"CapAdd\": null,\r\n            \"CapDrop\": null,\r\n            \"Dns\": null,\r\n            \"DnsOptions\": null,\r\n            \"DnsSearch\": null,\r\n            \"ExtraHosts\": null,\r\n            \"GroupAdd\": null,\r\n            \"IpcMode\": \"shareable\",\r\n            \"Cgroup\": \"\",\r\n            \"Links\": null,\r\n            \"OomScoreAdj\": 0,\r\n            \"PidMode\": \"\",\r\n            \"Privileged\": false,\r\n            \"PublishAllPorts\": false,\r\n            \"ReadonlyRootfs\": false,\r\n            \"SecurityOpt\": null,\r\n            \"UTSMode\": \"\",\r\n            \"UsernsMode\": \"\",\r\n            \"ShmSize\": 67108864,\r\n            \"Runtime\": \"runc\",\r\n            \"ConsoleSize\": [\r\n                0,\r\n                0\r\n            ],\r\n            \"Isolation\": \"default\",\r\n            \"CpuShares\": 0,\r\n            \"Memory\": 0,\r\n            \"NanoCpus\": 0,\r\n            \"CgroupParent\": \"\",\r\n            \"BlkioWeight\": 0,\r\n            \"BlkioWeightDevice\": null,\r\n            \"BlkioDeviceReadBps\": null,\r\n            \"BlkioDeviceWriteBps\": null,\r\n            \"BlkioDeviceReadIOps\": null,\r\n            \"BlkioDeviceWriteIOps\": null,\r\n            \"CpuPeriod\": 0,\r\n            \"CpuQuota\": 0,\r\n            \"CpuRealtimePeriod\": 0,\r\n            \"CpuRealtimeRuntime\": 0,\r\n            \"CpusetCpus\": \"\",\r\n            \"CpusetMems\": \"\",\r\n            \"Devices\": null,\r\n            \"DeviceCgroupRules\": null,\r\n            \"DiskQuota\": 0,\r\n            \"KernelMemory\": 0,\r\n            \"MemoryReservation\": 0,\r\n            \"MemorySwap\": 0,\r\n            \"MemorySwappiness\": null,\r\n            \"OomKillDisable\": false,\r\n            \"PidsLimit\": 0,\r\n            \"Ulimits\": null,\r\n            \"CpuCount\": 0,\r\n            \"CpuPercent\": 0,\r\n            \"IOMaximumIOps\": 0,\r\n            \"IOMaximumBandwidth\": 0\r\n        },\r\n        \"GraphDriver\": {\r\n            \"Data\": {\r\n                \"LowerDir\": \"/var/lib/docker/overlay2/1adb6374d069331d8caa841d7fdf39a9e9f409633f7f3638e1bd8d1e9834bc0a-init/diff:/var/lib/docker/overlay2/426e6d9f256ab4cfcd4848c23df7b052b186926e1b45e69fe51fb54afe0529a4/diff:/var/lib/docker/overlay2/1a5442cb9ee0585a91f59d516d5d0d35a00c8b6bc020e2deb58abdc3c80666b0/diff:/var/lib/docker/overlay2/300bcce9d665ada94b27c8c0c673a5030a3a8415b423efb84d6a10aefe45098b/diff:/var/lib/docker/overlay2/094e25b69031f5b7a9d727823d7e2d55207a5a466023f101f70cc8622173c9e4/diff\",\r\n                \"MergedDir\": \"/var/lib/docker/overlay2/1adb6374d069331d8caa841d7fdf39a9e9f409633f7f3638e1bd8d1e9834bc0a/merged\",\r\n                \"UpperDir\": \"/var/lib/docker/overlay2/1adb6374d069331d8caa841d7fdf39a9e9f409633f7f3638e1bd8d1e9834bc0a/diff\",\r\n                \"WorkDir\": \"/var/lib/docker/overlay2/1adb6374d069331d8caa841d7fdf39a9e9f409633f7f3638e1bd8d1e9834bc0a/work\"\r\n            },\r\n            \"Name\": \"overlay2\"\r\n        },\r\n        \"Mounts\": [],\r\n        \"Config\": {\r\n            \"Hostname\": \"f01771ae88ef\",\r\n            \"Domainname\": \"\",\r\n            \"User\": \"\",\r\n            \"AttachStdin\": false,\r\n            \"AttachStdout\": false,\r\n            \"AttachStderr\": false,\r\n            \"ExposedPorts\": {\r\n                \"80/tcp\": {}\r\n            },\r\n            \"Tty\": false,\r\n            \"OpenStdin\": false,\r\n            \"StdinOnce\": false,\r\n            \"Env\": [\r\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\r\n                \"NGINX_VERSION=1.13.9\"\r\n            ],\r\n            \"Cmd\": [\r\n                \"nginx\",\r\n                \"-g\",\r\n                \"daemon off;\"\r\n            ],\r\n            \"ArgsEscaped\": true,\r\n            \"Image\": \"nginx:alpine@sha256:48947591194ac5a9dce1e110f9198a547debb21630f121081640b87d99ca8b11\",\r\n            \"Volumes\": null,\r\n            \"WorkingDir\": \"\",\r\n            \"Entrypoint\": null,\r\n            \"OnBuild\": null,\r\n            \"Labels\": {\r\n                \"com.docker.swarm.node.id\": \"oifk2p0hd4tvlb62uf76womx0\",\r\n                \"com.docker.swarm.service.id\": \"ib7yble4rm5qbz3ijmk83cxeh\",\r\n                \"com.docker.swarm.service.name\": \"myservice\",\r\n                \"com.docker.swarm.task\": \"\",\r\n                \"com.docker.swarm.task.id\": \"n94nynzgc3m5w7vowovraow1w\",\r\n                \"com.docker.swarm.task.name\": \"myservice.1.n94nynzgc3m5w7vowovraow1w\",\r\n                \"maintainer\": \"NGINX Docker Maintainers <docker-maint@nginx.com>\"\r\n            },\r\n            \"StopSignal\": \"SIGTERM\"\r\n        },\r\n        \"NetworkSettings\": {\r\n            \"Bridge\": \"\",\r\n            \"SandboxID\": \"11b90ebf9e0718a5719d9a7edac605f3391c91f4f8a1bb9a37aa1af4efd6db98\",\r\n            \"HairpinMode\": false,\r\n            \"LinkLocalIPv6Address\": \"\",\r\n            \"LinkLocalIPv6PrefixLen\": 0,\r\n            \"Ports\": {\r\n                \"80/tcp\": null\r\n            },\r\n            \"SandboxKey\": \"/var/run/docker/netns/11b90ebf9e07\",\r\n            \"SecondaryIPAddresses\": null,\r\n            \"SecondaryIPv6Addresses\": null,\r\n            \"EndpointID\": \"7e5b51ec1d41fe402b96b0902fee875c8dbbec68ff5d0b4b7e606d516185e9a5\",\r\n            \"Gateway\": \"172.17.0.1\",\r\n            \"GlobalIPv6Address\": \"\",\r\n            \"GlobalIPv6PrefixLen\": 0,\r\n            \"IPAddress\": \"172.17.0.9\",\r\n            \"IPPrefixLen\": 16,\r\n            \"IPv6Gateway\": \"\",\r\n            \"MacAddress\": \"02:42:ac:11:00:09\",\r\n            \"Networks\": {\r\n                \"bridge\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": null,\r\n                    \"NetworkID\": \"51b49f47f62e1fe796a2eb616085efe41c84551d036f4983c23e4f578a4af234\",\r\n                    \"EndpointID\": \"7e5b51ec1d41fe402b96b0902fee875c8dbbec68ff5d0b4b7e606d516185e9a5\",\r\n                    \"Gateway\": \"172.17.0.1\",\r\n                    \"IPAddress\": \"172.17.0.9\",\r\n                    \"IPPrefixLen\": 16,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"MacAddress\": \"02:42:ac:11:00:09\",\r\n                    \"DriverOpts\": null\r\n                }\r\n            }\r\n        }\r\n    }\r\n]\r\n```\r\n\r\n</details>\r\n\r\nNotice that no information about either secrets or configs can be found in the output\r\n\r\n**Describe the results you received:**\r\n\r\nNo information about secrets or configs in the output\r\n\r\n**Describe the results you expected:**\r\n\r\nSecrets and Configs being shown in (e.g.) the `Mounts` property\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:\t18.02.0-ce\r\n API version:\t1.36\r\n Go version:\tgo1.9.3\r\n Git commit:\tfc4de44\r\n Built:\tWed Feb  7 21:13:05 2018\r\n OS/Arch:\tdarwin/amd64\r\n Experimental:\ttrue\r\n Orchestrator:\tswarm\r\n\r\nServer:\r\n Engine:\r\n  Version:\t18.02.0-ce\r\n  API version:\t1.36 (minimum version 1.12)\r\n  Go version:\tgo1.9.3\r\n  Git commit:\tfc4de44\r\n  Built:\tWed Feb  7 21:20:15 2018\r\n  OS/Arch:\tlinux/amd64\r\n  Experimental:\ttrue\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\nnot relevant for this issue"},{"labels":["enhancement"],"text":"HI, I'm using devicemapper in our production environment, and I want to limit io rate for each container. but we can't known dm's DeviceName before create container, so I want update blkio limit config after created container, may it work?"},{"labels":[null,null,null,"enhancement"],"text":"The `moby/moby` CI is far from being optimized : \r\n- <del>one big Dockerfile that hits really bad when something changes in it</del> handled by multi-stage #35100 \r\n- there is at least 5 jobs on each PR, each of them doing a lot of steps in common (some of them are not really required to be repeated), and they all took at least 1h30 to 2h.\r\n- the integration tests are still quite flaky, and when they are, it's another 2h round of build, \"just\" to get green light !\r\n- the CI definition is not on the repository, and needs to be taken care for each job.\r\n- Only building the dev image (used for any `make` target) takes ~14min on the CI.\r\n\r\nThe idea of this issue is to track enhancement required for the moby ci to be better, and help us instead of scaring us each time we wanna do rebase or a small change. Here is a *brain-dump* list of work we should do to enable that.\r\n\r\n- [x] Use multi-stage build to lower *cache burn* when we update a dependency\r\n- [ ] Split target to use different stages (see https://github.com/moby/moby/issues/36415)\r\n- [ ] Migrate `integration-cli` to `integration` and make those tests as parallelizable as possible (see https://github.com/moby/moby/issues/32866)\r\n- [ ] Try publishing stage(s) build as image and use `--cache-from` to remove the need to build the base image for each and every CI run (on each jobs)\r\n- [ ] Switch to `Jenkinsfile` to be able to make the builds evolve directly from the source code (in a secure manner)\r\n  - [ ] Migrate current jobs into a `Jenkinsfile` as is\r\n  - [ ] Define a pipeline to optimize time spent for a run\r\n  - [ ] Split integration tests suite (integration-cli and integration) into several parallel run (probably on multiple node)\r\n  - [ ] Refactor `Docker Master` jobs to have \"one\" build job and X targeted test jobs (for different distribution/graphdriver/setup)\r\n  - [ ] Make sure we can restart steps independently from each other \r\n- [ ] Remove experimental build/runner : *experimental* is (for quite a long time now) a runtime option (build before). We should have a `experimental` test suite instead of a whole build/runner dedicated for that. wip in https://github.com/moby/moby/pull/36697 https://github.com/moby/moby/pull/36746 https://github.com/moby/moby/pull/36722\r\n- [ ] Explore having a bot/*merge queue* workflow.\r\n  Once the required LGTM are in place, a bot takes over to merge in a merge queue — rebuilds jobs against master, merge it if everything is good, or labels it and comments on it if something went wrong.\r\n  We could also have some build run only if asked from maintainers/curator, and have a lighter CI runs when PRs are in certain states (like `design-review`) …\r\n  See https://github.com/moby/moby/issues/35490 for that.\r\n- [ ] Measure the CI\r\n  - [ ] How much time each step takes (and their evolution in time)\r\n  - [ ] What are the slowest test\r\n  - [ ] What are the most flaky test (report each test failure to be able to track which tests need some love)\r\n  - [ ] Create a dashboard for maintainers with all those information\r\n"},{"labels":[null,null,null,null,"enhancement"],"text":"https://github.com/moby/moby/pull/35100 rewrote the Dockerfile to use multi-stage builds. Some review-comments were left.\r\n\r\nA minor enhancement/cleanup: see https://github.com/moby/moby/pull/35100#discussion_r170234813\r\n\r\nCan we standardise on an output path for artefacts? e.g. make all the build-stages output artefacts in (say) `/output/`, then we can simplify all these to\r\n\r\n```Dockerfile\r\nCOPY --from=foo /output/ /usr/local/bin/\r\nCOPY --from=bar /output/ /usr/local/bin/\r\nCOPY --from=baz /output/ /usr/local/bin/\r\nCOPY --from=bif /output/ /usr/local/bin/\r\n```\r\n"},{"labels":[null,null,null,"enhancement"],"text":"https://github.com/moby/moby/pull/35100 rewrote the dockerfile to use multi-stage  builds.\r\n\r\nhttps://github.com/moby/moby/pull/35100#discussion_r170151654 made a suggestion to split the last `COPY` step to a separate stage, so that no copy is performed when using `make BIND_DIR=. ....`.\r\n\r\nThis is part of the improvement that was worked on in https://github.com/moby/moby/pull/29298\r\n\r\n"},{"labels":[null,null,null,null,"enhancement"],"text":"https://github.com/moby/moby/pull/35100 rewrite the Dockerfile to use multi-stage builds\r\n\r\nTo make the Dockerfile more flexible (e.g. to test a different version of `containerd` or the Docker CLI), a suggestion was made to use build-args for various options; https://github.com/moby/moby/pull/35100#discussion_r170151654\r\n\r\nlabeling as \"beginner\", but some knowledge about the Dockerfile may come in handy :)"},{"labels":[null,null,null,null,"enhancement"],"text":"https://github.com/moby/moby/pull/35100 rewrote the Dockerfile to use multi-stage builds. We should:\r\n\r\n- Try to use a single Dockerfile instead of one per architecture (now that multi-arch is a thing)\r\n- If not possible (yet); sync the other architectures to use multi-stage as well\r\n\r\nLabeling this \"beginner\", but some knowledge about the Dockerfile may be handy :)"},{"labels":[null,"enhancement"],"text":"\r\n**Docker fails to read files with UTF8 BOM encoding**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Save TLS file with UTF8 BOM encoding (as opposed just UTF8)\r\n2. Try to start docker with TLS enabled\r\n3.\r\n\r\n**Describe the results you received:**\r\nFails to start\r\n\r\n**Describe the results you expected:**\r\nShall read files fine\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:\t17.12.0-ce\r\n API version:\t1.35\r\n Go version:\tgo1.9.2\r\n Git commit:\tc97c6d6\r\n Built:\tWed Dec 27 20:05:22 2017\r\n OS/Arch:\twindows/amd64\r\n\r\nServer:\r\n Engine:\r\n  Version:\t17.12.0-ce\r\n  API version:\t1.35 (minimum version 1.24)\r\n  Go version:\tgo1.9.2\r\n  Git commit:\tc97c6d6\r\n  Built:\tWed Dec 27 20:15:52 2017\r\n  OS/Arch:\twindows/amd64\r\n  Experimental:\ttrue\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 29\r\n Running: 0\r\n Paused: 0\r\n Stopped: 29\r\nImages: 43\r\nServer Version: 17.12.0-ce\r\nStorage Driver: windowsfilter\r\n Windows: \r\nLogging Driver: json-file\r\nPlugins:\r\n Volume: local\r\n Network: ics l2bridge l2tunnel nat null overlay transparent\r\n Log: awslogs etwlogs fluentd gelf json-file logentries splunk syslog\r\nSwarm: inactive\r\nDefault Isolation: hyperv\r\nKernel Version: 10.0 16299 (16299.15.amd64fre.rs3_release.170928-1534)\r\nOperating System: Windows 10 Enterprise\r\nOSType: windows\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 63.92GiB\r\nName: CND70637PKZBook\r\nID: 3BGZ:RYAG:C7JG:MGHC:KPPR:A667:FJD7:SFGW:JLQ3:T5SC:ZYI4:QPV5\r\nDocker Root Dir: D:\\images\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\n\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nWhen a `--mount` fails the source (or destination) path that failed should be printed to indicate which path failed.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker run` with invalid source path for `--mount`\r\n\r\n```\r\n$ docker run --mount type=bind,source=/nonexistant,destination=/srv IMAGE\r\ndocker: Error response from daemon: invalid mount config for type \"bind\": bind source path does not exist.\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:\t17.12.0-ce\r\n API version:\t1.35\r\n Go version:\tgo1.9.2\r\n Git commit:\tc97c6d6\r\n Built:\tWed Dec 27 20:11:19 2017\r\n OS/Arch:\tlinux/amd64\r\n\r\nServer:\r\n Engine:\r\n  Version:\t17.12.0-ce\r\n  API version:\t1.35 (minimum version 1.12)\r\n  Go version:\tgo1.9.2\r\n  Git commit:\tc97c6d6\r\n  Built:\tWed Dec 27 20:09:54 2017\r\n  OS/Arch:\tlinux/amd64\r\n  Experimental:\tfalse\r\n```"},{"labels":[null,"enhancement"],"text":"The `api/types` package should only contain the types used to serialize/deserialize requests and responses for the HTTP API. This is the case for most of the types in this package and sub packages.\r\n\r\nThere are two exceptions:\r\n* `api/types/client.go` contains types that are used by `APIClient` interface in the `client` package\r\n* `api/types/backend` contains types that are used by the backend interfaces between the API routers and the server implementation\r\n\r\nThese two categories of types are never used for serialization/deserialization.\r\n\r\nI propose we move these types to a more appropriate place:\r\n* `api/types/client.go` should move to `client/types`\r\n* `api/types/backend` could be split up and moved to `api/server/router/<component>` or moved to `api/server/backend/types`\r\n\r\ncc @tiborvass @vdemeester @cpuguy83 "},{"labels":[null,"enhancement"],"text":"At the moment, the following labels are available when inspecting a container/service that is part of stack:\r\n\r\nContainer example:\r\n```json\r\n\"Labels\": {\r\n  \"com.docker.stack.namespace\": \"stackname\",\r\n  \"com.docker.swarm.node.id\": \"rcv5hydqcwhiweojbospwgjcs\",\r\n  \"com.docker.swarm.service.id\": \"3kginyct1zchqjc0txdcfxj2a\",\r\n  \"com.docker.swarm.service.name\": \"stackname_servicename\",\r\n  \"com.docker.swarm.task\": \"\",\r\n  \"com.docker.swarm.task.id\": \"011gmklaus0mbjqappuw0bbkw\",\r\n  \"com.docker.swarm.task.name\": \"stackname_servicename.1.011gmklaus0mbjqappuw0bbkw\"\r\n}\r\n```\r\n\r\nService example:\r\n```json\r\n\"Labels\": {\r\n  \"com.docker.stack.image\": \"imagename\",\r\n  \"com.docker.stack.namespace\": \"stackname\"\r\n}\r\n```\r\n\r\nIt would also be nice to have information about the Swarm related to these services/containers, e.g. by adding the Swarm identifier in these labels:\r\n\r\n```json\r\n\"Labels\": {\r\n  \"com.docker.stack.namespace\": \"stackname\",\r\n  \"com.docker.swarm.id\": \"u908vqxibhrd118gdsggsbw6y\",\r\n  \"com.docker.swarm.node.id\": \"rcv5hydqcwhiweojbospwgjcs\",\r\n  \"com.docker.swarm.service.id\": \"3kginyct1zchqjc0txdcfxj2a\",\r\n  \"com.docker.swarm.service.name\": \"stackname_servicename\",\r\n  \"com.docker.swarm.task\": \"\",\r\n  \"com.docker.swarm.task.id\": \"011gmklaus0mbjqappuw0bbkw\",\r\n  \"com.docker.swarm.task.name\": \"stackname_servicename.1.011gmklaus0mbjqappuw0bbkw\"\r\n}\r\n```\r\n\r\n```json\r\n\"Labels\": {\r\n  \"com.docker.stack.image\": \"imagename\",\r\n  \"com.docker.stack.namespace\": \"stackname\",\r\n  \"com.docker.swarm.id\": \"u908vqxibhrd118gdsggsbw6y\"\r\n}\r\n```\r\n\r\nI think that this is probably related to the following code for containers:\r\nhttps://github.com/moby/moby/blob/master/daemon/cluster/executor/container/container.go#L227-L237\r\n\r\nNot sure about services though.\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nWhen pulling by digest, it seems like docker doesn't check to see if it already has that image locally before talking to the registry.\r\n\r\nAdding a check [here](https://github.com/moby/moby/blob/3a6f8cfd5108aa3fac145525b7cd2f4f0439702d/distribution/pull_v2.go#L340) could skip downloading the manifest entirely.\r\n\r\nWould you be open to a PR to fix this? Or is it intentional?\r\n\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n```\r\n$ time docker pull ubuntu\r\nUsing default tag: latest\r\nlatest: Pulling from library/ubuntu\r\nDigest: sha256:e27e9d7f7f28d67aa9e2d7540bdc2b33254b452ee8e60f388875e5b7d9b2b696\r\nStatus: Image is up to date for ubuntu:latest\r\n\r\nreal\t0m1.132s\r\nuser\t0m0.016s\r\nsys\t0m0.012s\r\n\r\n\r\n$ time docker pull ubuntu@sha256:e27e9d7f7f28d67aa9e2d7540bdc2b33254b452ee8e60f388875e5b7d9b2b696\r\nsha256:e27e9d7f7f28d67aa9e2d7540bdc2b33254b452ee8e60f388875e5b7d9b2b696: Pulling from library/ubuntu\r\nDigest: sha256:e27e9d7f7f28d67aa9e2d7540bdc2b33254b452ee8e60f388875e5b7d9b2b696\r\nStatus: Image is up to date for ubuntu@sha256:e27e9d7f7f28d67aa9e2d7540bdc2b33254b452ee8e60f388875e5b7d9b2b696\r\n\r\nreal\t0m1.150s\r\nuser\t0m0.008s\r\nsys\t0m0.016s\r\n```\r\n\r\n\r\n**Describe the results you received:**\r\n\r\nPulling ubuntu by tag and pulling it by digest both talk to the registry.\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\nThe digest pull should be effectively instant since we already have it.\r\n\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:\t17.12.0-ce\r\n API version:\t1.35\r\n Go version:\tgo1.9.2\r\n Git commit:\tc97c6d6\r\n Built:\tWed Dec 27 20:11:19 2017\r\n OS/Arch:\tlinux/amd64\r\n\r\nServer:\r\n Engine:\r\n  Version:\t17.12.0-ce\r\n  API version:\t1.35 (minimum version 1.12)\r\n  Go version:\tgo1.9.2\r\n  Git commit:\tc97c6d6\r\n  Built:\tWed Dec 27 20:09:54 2017\r\n  OS/Arch:\tlinux/amd64\r\n  Experimental:\tfalse\r\n```\r\n"},{"labels":["enhancement"],"text":"I'm semantically lost in the FHS\r\n\r\nOne thing is clear to me, and I understand it's intention is the adoption of `/run/` as emerging best practice. I like containers to use it for runtime components supplementing the stateless base executable.\r\n\r\nAs to date there is a tendency to use it for:\r\n\r\n1. init scripts\r\n2. mounting secrets `/run/secrets/`\r\n\r\nHowever, configs are just mounted to `/foo` by default. Default should enforce best practice...\r\n\r\nAnd I would argue, that usually you don't (still) have single executable binaries in your containers, but rather deal with a (linux) file system of some sort, therefore aligning best practice and mounting configs into `/run/configs/` would be a lot clearer to me."},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nWhen creating a tarball by hand for docker build, if the path is in parent directory, build cache is never invalidated.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a directory structure like this [parent.zip](https://github.com/moby/moby/files/1667460/parent.zip) : \r\n```\r\n$ tree\r\n.\r\n├── build.json\r\n└── child\r\n    ├── Dockerfile\r\n    └── Makefile\r\n$  cat build.json\r\nversion1\r\n$  cat child/Dockerfile\r\nFROM alpine\r\nCOPY build.json /\r\nRUN cat build.json\r\n$ cat child/Makefile\r\nall:\r\n\ttar cf - Dockerfile ../build.json | docker build -t child -\r\n``` \r\n2. Run `make`\r\n3. Change version in build.json\r\n4.  Run `make`. There is a cache hit.\r\n\r\n**Describe the results you received:**\r\n\r\nThe cache is used and the file not changed for the following steps\r\n\r\n**Describe the results you expected:**\r\n\r\nThe file should not be cached.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:\t17.12.0-ce\r\n API version:\t1.35\r\n Go version:\tgo1.9.2\r\n Git commit:\tc97c6d6\r\n Built:\tWed Dec 27 20:03:51 2017\r\n OS/Arch:\tdarwin/amd64\r\n\r\nServer:\r\n Engine:\r\n  Version:\t17.12.0-ce\r\n  API version:\t1.35 (minimum version 1.12)\r\n  Go version:\tgo1.9.2\r\n  Git commit:\tc97c6d6\r\n  Built:\tWed Dec 27 20:12:29 2017\r\n  OS/Arch:\tlinux/amd64\r\n  Experimental:\ttrue\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 5\r\n Running: 0\r\n Paused: 0\r\n Stopped: 5\r\nImages: 41\r\nServer Version: 17.12.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 89623f28b87a6004d4b785663257362d1658a729\r\nrunc version: b2567b37d7b75eb4cf325b77297b140ea686ce8f\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.60-linuxkit-aufs\r\nOperating System: Docker for Mac\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 1.952GiB\r\nName: linuxkit-025000000001\r\nID: K4IU:BYEJ:55LM:S3GN:MGG4:EWRW:BR2O:I5Z6:XB6K:WIMS:IMXT:STSK\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 21\r\n Goroutines: 40\r\n System Time: 2018-01-26T10:41:17.898620973Z\r\n EventsListeners: 2\r\nHTTP Proxy: docker.for.mac.http.internal:3128\r\nHTTPS Proxy: docker.for.mac.http.internal:3129\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nUsing Docker for Mac\r\n\r\n"},{"labels":[null,"enhancement"],"text":"You have probably seen this before, but just in case\r\n\r\nI run this:\r\n`$docker ps -a`\r\n\r\nand I see this:\r\n\r\n```bash\r\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                      NAMES\r\nef71b9269e92        cdt-prod            \"/bin/bash /home/new…\"   15 seconds ago      Up 34 seconds       0.0.0.0:3055->3055/tcp     cdt-prod-container\r\n282d1f0d0f5a        mongo:2.4           \"/entrypoint.sh mong…\"   20 seconds ago      Up 39 seconds       0.0.0.0:27017->27017/tcp   cdt-mongo\r\n```\r\n\r\nseems wrong - the status column has bigger numbers than created column?"},{"labels":[null,"enhancement"],"text":"We are seeing a lot of lost packets between 172.18.0.2 (on the docker_gwbrigde) and a memcached container. The 172.18.0.2 belongs to the ingress-sbox container.\r\n\r\nQuite a bit of traffic goes through 172.18.0.2, and right now we suspect that `net.ipv4.tcp_rmem` and `net.ipv4.tcp_wmem` are too low. On the host, the default max was about 6MB which is far to low for the traffic being shuffled through this server.\r\n\r\nI fail to see how we can change these settings for containers. From what I can read, Docker inherits the default settings when a container starts. That is, not the host settings, but the default settings. However, I'd really like to be proven wrong on this one.\r\n\r\nIs there any way we can adjust `net.ipv4.tcp_wmem` or `net.ipv4.tcp_rmem` for Docker containers?"},{"labels":[null,null,"enhancement",null,null],"text":"## Description\r\nWhen I try to run container with command: \r\n`docker run -it --device=/dev/sdc4:/dev/xvdc --privileged ubuntu /bin/bash`\r\n\r\nThere is supposed to have device /dev/xvdc inside the container, but I just saw device /dev/sdc4.\r\n\r\nWhen I removed the `--privileged` option,I  can find device `/dev/xvdc`  exists.\r\n\r\n## Steps to reproduce the issue\r\nRun container with `--device` and `--privileged` option\r\n\r\n## Describe the results you expected\r\nWhen run container with option `--device` and `--privileged` option,  can see the specified device inside the container\r\n\r\n*Docker version*\r\n`1.12.6`\r\n\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nRight now `docker pull -a` appears to serially pull every tag associated with the repo, with each tag blocking the downloading of the next.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n1. `docker pull -a reponame`\r\n\r\n**Describe the results you expected:**\r\n\r\nIdeally, docker could begin pulling the next layer of a different tag as previous layers on a different tag are being extracted.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:       17.12.0-ce\r\n API version:   1.35\r\n Go version:    go1.9.2\r\n Git commit:    c97c6d6\r\n Built: Wed Dec 27 20:10:36 2017\r\n OS/Arch:       linux/amd64\r\n\r\nServer:\r\n Engine:\r\n  Version:      17.12.0-ce\r\n  API version:  1.35 (minimum version 1.12)\r\n  Go version:   go1.9.2\r\n  Git commit:   c97c6d6\r\n  Built:        Wed Dec 27 20:09:12 2017\r\n  OS/Arch:      linux/amd64\r\n  Experimental: true\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 58\r\nServer Version: 17.12.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: false\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 89623f28b87a6004d4b785663257362d1658a729\r\nrunc version: b2567b37d7b75eb4cf325b77297b140ea686ce8f\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 4.4.0-31-generic\r\nOperating System: Ubuntu 14.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 7.78GiB\r\nName: ubuntu\r\nID: YHNS:UFJ3:OE6E:CKFX:N7LF:WXWN:QAXG:ITDY:EM72:K6VC:COBA:P27C\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n"},{"labels":["enhancement"],"text":"`docker sytem prune` can be quite useful to get rid of stale images and containers but it is not easy to see exactly what it is going to delete. It is possible that there is a needle in the haystack the user wants to keep around. So, it might be useful to provide a `--dry-run` option that would do nothing except to print what the list of containers/images/volumes would have been deleted if the dry-run option was not provided.\r\n\r\nIf we go ahead with this option, it would also make sense to add the same option to other similar commands like `docker container prune`, `docker image prune` etc.\r\n\r\nWhat do others think? I'd be happy to attempt a patch for this but would first like to get some feedback from others about the idea itself."},{"labels":[null,null,"enhancement"],"text":"This issue was brought up in https://github.com/docker/cli/issues/297; docker performs some validation steps for volumes to prevent a volume being defined for `/` (i.e., the whole container), but validation is incomplete, and misses some cases.\r\n\r\n\r\nFor example, all of the Dockerfiles below will build **without producing an error**;\r\n\r\n```Dockerfile\r\nFROM nginx\r\nVOLUME [\"/\"]\r\n```\r\n\r\n```Dockerfile\r\nFROM nginx\r\nVOLUME /\r\n```\r\n\r\n```Dockerfile\r\nFROM nginx\r\nVOLUME [\".\"]\r\n```\r\n\r\n```Dockerfile\r\nFROM nginx\r\nVOLUME .\r\n```\r\n\r\n\r\nBut starting a container from any of the above (as expected) won't work;\r\n\r\n```\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:296: starting container process caused \"process_linux.go:398: container init caused \\\"open /dev/ptmx: no such file or directory\\\"\": unknown.\r\n```\r\n\r\nSimilarly, at runtime, the following correctly produces a validation error;\r\n\r\n```bash\r\n$ docker run -it --rm -v myvolume:/ nginx \r\ndocker: Error response from daemon: invalid volume specification: 'myvolume:/': invalid mount config for type \"volume\": invalid specification: destination can't be '/'.\r\nSee 'docker run --help'.\r\n```\r\n\r\nBut only specifying the \"container path\" does _not_ perform the correct validation, thus results in the same \"cryptic\" error;\r\n\r\n```bash\r\n$ docker run -it --rm -v / nginx\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:296: starting container process caused \"process_linux.go:398: container init caused \\\"open /dev/ptmx: no such file or directory\\\"\": unknown.\r\n```\r\n\r\nInteresting bit is that using `.` (working directory) for a volume results in a _literal_ `.` as path for the volume; the path should likely be expanded to the actual working-directory (`/` in the examples above);\r\n\r\n\r\n```\r\n            \"Volumes\": {\r\n                \".\": {}\r\n            },\r\n            \"WorkingDir\": \"\",\r\n```\r\n\r\n\r\n/cc @dnephin @tonistiigi \r\n\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n\r\n\r\n**Description**\r\n\r\nFEATURE REQUEST: for certain situation, it'll be handy to be able to disable caching for a certain stage, instead of one global switch to disable caching as a whole.\r\n\r\n"},{"labels":["enhancement"],"text":"I created a .env file with the following contents\r\n\r\n```\r\nvariable=value\r\nvariable2=value2\r\nuser-a-name=test\r\n```\r\n\r\nObviously, the \"user-a-name\" variable name is not valid because it contains a dash / hypen / whatever you call it.\r\n\r\nThen I ran `docker run --env-file myfile.env my-container-image`. The user-a-name of course could not be set because the variable name is invalid.\r\n\r\nI want docker to output a warning on STDOUT/STDERR (whatever is the suitable stream) when I have chosen an invalid variable name in my .env file"},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nWhen [docker daemon certificate](https://docs.docker.com/engine/security/https/) is generated without the *public* server ip, the docker client should use `tls` option but can not use `tlsverify` option for a remote call (Error: *certificate is valid for x.x.x.x, not y.y.y.y* otherwise).\r\n\r\nCurrently docker (`17.09.1-ce`) can only provide this use case by using `--tls` [command line option](https://docs.docker.com/engine/reference/commandline/cli/).\r\n\r\nThe [Environment variables](https://docs.docker.com/engine/reference/commandline/cli/#environment-variables) can only active `tlsverify` (with `DOCKER_TLS_VERIFY`), not only `tls`.\r\n\r\nIn some case, the end users can not use docker command line option (e.g.: when docker included in third-part script or in some continuous integration plugin) ; so configuring behavior using only environment variables (generally possible) could be useful.\r\n\r\n**A `DOCKER_TLS` environment variable to active TLS (but non-verify), like `--tls` command line parameter, could be a great feature.**\r\n\r\n#22411 (*let DOCKER_TLS_VERIFY=0 disable TLS verification*) could help to understand this feature request.\r\n\r\n**Steps to reproduce the behavior:**\r\n```\r\n# Configure server endpoint (having not a really good certificate for 'verify' usage) and 'ca.pem', 'cert.pem', 'key.pem' (in cert directory)\r\n$ export DOCKER_HOST=tcp://10.1.2.3:2376\r\n$ export DOCKER_CERT_PATH=cert\r\n$ export DOCKER_TLS_VERIFY=anyValue\r\n\r\n$ docker images\r\nerror during connect: Get https://10.1.2.3:2376/v1.32/images/json: x509: certificate is valid for 192.168.4.5, 127.0.0.1, not 10.1.2.3\r\n```\r\n\r\n**Only way to work:**\r\n```\r\n$ export DOCKER_HOST=tcp://10.1.2.3:2376\r\n$ export DOCKER_CERT_PATH=cert\r\n$ docker --tls images\r\nREPOSITORY                                     TAG                 IMAGE ID            CREATED             SIZE\r\nxxx .. OK\r\n```\r\n\r\n**Feature request:**\r\n```\r\n$ export DOCKER_HOST=tcp://10.1.2.3:2376\r\n$ export DOCKER_CERT_PATH=cert\r\n$ export DOCKER_TLS=anyValue\r\n\r\n$ docker images\r\nREPOSITORY                                     TAG                 IMAGE ID            CREATED             SIZE\r\nxxx .. OK\r\n```\r\n\r\n\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.09.1-ce\r\n API version:  1.30 (downgraded from 1.32)\r\n Go version:   go1.8.3\r\n Git commit:   19e2cf6\r\n Built:        Thu Dec  7 22:24:23 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.06.2-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   cec0b72\r\n Built:        Tue Sep  5 19:59:11 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n\r\n\r\n**Additional environment details:**\r\n\r\nUbuntu 16.04 on OpenStack platform.\r\n```\r\nLinux ubuntu-test 4.4.0-103-generic #126-Ubuntu SMP Mon Dec 4 16:23:28 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n```"},{"labels":[null,"enhancement"],"text":"Wanted to get some thoughts on this idea before actually implementing anything.\r\n\r\nIn a CI environment, it's useful to use `--cache-from` to select a set of images to consider for caching (that are _pulled_ and not necessarily built on the box).\r\n\r\nFor usage with an internal registry, I might use something like this as a wrapper to `docker build`:\r\n\r\n```bash\r\ndocker build $(docker images -q -a 'my-registry.com/*' | xargs -n1 echo --cache-from) ...\r\n```\r\n\r\nThis effectively expands to:\r\n\r\n```bash\r\ndocker build --cache-from ... --cache-from ... --cache-from ... --cache-from ... ...\r\n```\r\n\r\nWhile this is workable, it's a bit low on the quality-of-life scale.\r\n\r\nI'm proposing an interface similar to this:\r\n\r\n```bash\r\ndocker build --cache-from='my-registry.com/*' ...\r\n```\r\n\r\nThis would consider all images tagged with `my-registry.com/*` as part of the trusted cache set (without having to manually enumerate them myself).\r\n\r\nThoughts?"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nDocker attach API do use `application/vnd.docker.raw-stream` content type, which has two implementations:\r\n- multiplexed stdout/stderr streams\r\n- raw tty output\r\n\r\nfrom API response there's no way (afaict) to determine which one is used\r\nI suggest a header is introduced to document which mode is being used\r\n\r\n**Steps to reproduce the issue:**\r\n1. ask a friend to create a container, setting tty= to a random value kept secret\r\n2. invoke containers/{id}/attach\r\n3. try to guess the raw.stream format in use\r\n\r\n**Describe the results you received:**\r\n\r\nhave to try parsing a multiplexed frame, then admit I was wrong.\r\n\r\n**Describe the results you expected:**\r\n\r\nsimple way to determine the parsing logic to use\r\n"},{"labels":[null,null,"enhancement"],"text":"As the Datadog agent needs to list and inspect containers, images and volumes for monitoring, we currently connect to the `/var/run/docker.sock` unix socket. Although the monitoring entrypoints are stable and expose enough data, exposing the docker socket to the agent comes with a security risk: an attacker could use it to stop containers, run malicious ones and even get `root` access on the host. Although we are currently working on getting all the information we need via the orchestrator, we want to support all orchestrators, including Swarm and vanilla Docker.\r\n\r\nWe have been investigating [the AuthZ system](https://docs.docker.com/engine/extend/plugins_authorization/), with Twislock's third-party [AuthZ plugin](https://github.com/twistlock/authz) for RBAC filtering. Unfortunately, as the unix listener does not currently have TLS enabled, all requests via the unix socket fail because no user is authenticated. This is unacceptable to most of our clients as either they rely on the unix socket for adminitration or they use an orchestrator that require access to the socket. This is why we investigated how to improve on the current situation. In this RFC I'd like to discuss two solutions we could contribute:\r\n\r\n## \"Easy\" solution: enable HTTPS on the unix socket\r\n\r\nWe could patch [the uds listener](https://github.com/docker/go-connections/blob/master/sockets/unix_socket.go) to wrap its traffic in TLS like [the tcp listener](https://github.com/docker/go-connections/blob/master/sockets/tcp_socket.go#L17) does.\r\n\r\nThis would re-enable the unix socket, as long as we configure the docker CLI and the orchestrators to provide a valid client certificate. This has two drawbacks:\r\n\r\n- if the user does not already have a PKI for their cluster, setting one up can be a significant hurdle\r\n- we'd need to test (and probably patch) all orchestrators to support HTTPS over `unix://` traffic\r\n\r\n## Preferred solution: authenticate using the `UCred` information\r\n\r\nWhen listening on a unix socket, a process can use the [`SO_PEERCRED`](http://man7.org/linux/man-pages/man7/socket.7.html) option, which makes the kernel transmit the client process' credentials. From this `UnixCredentials` struct, the server can get the PID, UID and GID of the process:\r\n\r\n```go\r\ntype Ucred struct {\r\n    Pid int32\r\n    Uid uint32\r\n    Gid uint32\r\n}\r\n```\r\n\r\nWhile on other unices the credentials are passed by the client and verified by the kernel, on Linux the credentials are injected by the kernel, hence very secure. Several server applications ([mysql](https://dev.mysql.com/doc/refman/5.5/en/socket-pluggable-authentication.html), [pgsql](https://www.postgresql.org/docs/9.1/static/auth-methods.html#AUTH-PEER) for example) implement this as an authentication method.\r\n\r\n[We already use](https://github.com/DataDog/datadog-agent/blob/master/pkg/dogstatsd/listeners/uds_linux.go) its datagram equivalent `SO_PASSCRED` to retrieve the `PID` of incoming statsd metric packets and tag metrics with the relevant container tags, so we know handling this in Go is pretty simple.\r\n\r\nIf we implement this, no change will be required on the client side, they will continue using HTTP over UDS on `/var/run/docker.sock`. The docker daemon will:\r\n\r\n- retrieve the peer UID on new connections\r\n- lookup `/etc/passwd` and fetch the unix username\r\n- pass `unix:username` (or `unix:UID` if no match is found) to the AuthZ system\r\n- the AuthZ will need no change and will lookup this user in their configuration. Prefixing the username would allow both this method and TLS client certs to coexist without collisions.\r\n\r\nWhitelisting `unix:root` will allow orchestrators and the docker CLI to function as normal, then we'd provide limited RBACs for our `dd-agent` user, that only allow read-only endpoints. This might also open new use cases, like providing limited `docker ps` / `docker stats` access to all local users.\r\n\r\nWe are willing to contribute the code necessary to make that viable, but we prefer to discuss with maintainers and contributors before starting the implementation.\r\n\r\nWhat do you think?"},{"labels":["enhancement"],"text":"**Description**\r\n\r\nAs a container terminate by itself (main PID 1 process complete) an `container die` event is reported with exit code. \r\nAs an exec process completes, docker inspect do report updated `ExecIDs` set but no event is logged. As a result, one only can monitor exec completion by polling. An `container exec_die` event would allow an asynchronous implementation.\r\n\r\nMy actual use case is to run build processes in an existing containers from a Jenkins plugin. I'd like to avoid polling this container for exec completion.\r\n\r\n**Steps to reproduce the issue:**\r\n1. run `docker event`\r\n2. start a container with `sleep 100`\r\n3. run docker exec ID echo hello\r\n\r\n**Describe the results you received:**\r\n\r\nlogged events:\r\n- container exec_create\r\n- container exec_start\r\n- container die\r\n\r\n**Describe the results you expected:**\r\n\r\nlogged events:\r\n- container exec_create\r\n- container exec_start\r\n- container exec_die\r\n- container die\r\n\r\n"},{"labels":[null,"enhancement",null],"text":"With the addition of .Node.Hostname to available templating variables #34686 it should be trivial to add more node information to the system labels of each contianer. At a minimum, adding the node hostname will simplify debugging certain system issues a lot.\r\n\r\nI don't mind submitting a PR for this, but would just like to get some feedback or possible objections."},{"labels":[null,"enhancement"],"text":"Docker caps log lines at 16K by default. When log lines exceed this limit, it results in partial log chunks. Such logs are hard to parse for tools and humans alike. Docker detects chunked up log lines and sets a 'Partial' flag in each log message. It is up-to the driver to interpret the flag and assemble the chunks.\r\n\r\nThere seems to be a good deal of request for seeing this feature implemented in the Splunk driver. This issue aims to track that effort."},{"labels":[null,"enhancement",null,null],"text":"**Description**\r\nDocker allows to delete image of stopped container, resulting in \"ghost\" behavior described below.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker create -p 80:80 --name nginx nginx`\r\n2. `docker rmi -f nginx:latest`\r\n3. `docker image inspect $(docker inspect nginx --format '{{.Image}}')`\r\n\r\n**Describe the results you received:**\r\n\r\nError: No such image: sha256:9e7424e5dbaeb9b28fea44d8c75b41ac6104989b49b2464b7cbbed16ceeccfc3\r\n\r\n**Describe the results you expected:**\r\n\r\nDocker should only remove named tag (nginx:latest), and the sha256 of image should still be available in list of images. Instead docker removes both tag and sha256 from images and shows not existing image tag in `docker inspect nginx` output, even after starting container.\r\n\r\nThe command `docker image inspect $(docker inspect nginx --format '{{.Image}}')` should still work even after force removing container (which is the case if I remove image of running container).\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nWeirdly enough, I'm able to start container with \"ghost\" image using `docker start nginx`. It works without issues.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.11.0-ce\r\n API version:  1.34\r\n Go version:   go1.8.4\r\n Git commit:   1caf76c\r\n Built:        Mon Nov 20 18:30:18 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      17.11.0-ce\r\n API version:  1.34 (minimum version 1.12)\r\n Go version:   go1.8.5\r\n Git commit:   1caf76c\r\n Built:        Mon Nov 20 18:39:28 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 0\r\n Paused: 0\r\n Stopped: 1\r\nImages: 7\r\nServer Version: 17.11.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: zmb8evqs5ndwat9857zf8lox5\r\n Is Manager: true\r\n ClusterID: qxqwop1ztssr90zr43ifjiaol\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 192.168.65.2\r\n Manager Addresses:\r\n  192.168.65.2:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 992280e8e265f491f7a624ab82f3e238be086e49\r\nrunc version: 0351df1c5a66838d0c392b4ac4cf9450de844e2d\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.60-linuxkit-aufs\r\nOperating System: Docker for Mac\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 1.952GiB\r\nName: linuxkit-025000000001\r\nID: EN2Y:NUG3:PVGZ:7IHR:S2AN:PUP6:2LDW:DVKL:SLWD:3HXM:PFGS:XUBC\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 37\r\n Goroutines: 152\r\n System Time: 2017-11-25T17:05:25.960150383Z\r\n EventsListeners: 2\r\nNo Proxy: *.local, 169.254/16\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```"},{"labels":[null,"enhancement"],"text":"Hi,\r\n\r\nIt would be useful (imho) if we could port a container port to a random host port, but only bound to a certain IP. For example, say I want to run the `tomcat` container and port its `8080` port to a random port in the host to the `127.0.0.1` IP:\r\n\r\n    docker run -p 127.0.0.1:8080 tomcat\r\n\r\nThis is different from the situation where you define the host port (which is possible):\r\n\r\n    docker run -p 127.0.0.1:80:8080 tomcat\r\n\r\nWhat do you think?\r\n\r\nThank you!"},{"labels":[null,null,"enhancement"],"text":"In a non privileged container, it can set suid for some app, as a result , nonroot in the host can use the app to elevate privileges.\r\n## reproduce\r\nrun a non privileged container:\r\n```\r\n[root@localhost seccomp]# docker run --rm -ti -v /bin/ls:/home/ls -w /home  busybox ash          \r\n/home # chmod 4777 ls\r\n/home # \r\n```\r\nanother terminal, nonroot can list the files in /root.\r\n```\r\n# In the host, not in the container.\r\n[hello@localhost ~]$ id\r\nuid=1000(hello) gid=1000(hello) groups=1000(hello) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023\r\n[hello@localhost ~]$ ls /root\r\nanaconda-ks.cfg  autofs-5.0.7-56.el7.src.rpm  dirty \r\n```\r\n\r\n## Why?\r\nA root user in a non privileged container is a really root user who just drop some capabilities and is restricted to use some system calls and others. Currently chmod syscall is not restricted, so it can set suid  for some app and nonroot can use it to  elevate privileges.\r\n\r\n## How can we deal with it ?\r\nWe can use seccomp to restrict the parameter of chmod and make chmod can change general file mode(such ass 0777)， but not suid(like 4000). In consideration of some user want to set suid in a container, so I add sys_admin capability to remove this restriction, just like clone.\r\nhttps://github.com/moby/moby/pull/35583 for details.\r\n\r\n## About the compatibility\r\nI f we restricte a  non privileged container to set suid and build a docker image, \"RUN set suid operation\" is not allowed, so I add sys_admin to docker build command to avoid break the build with \"RUN set suid operation\" .\r\n"},{"labels":["enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nCurrently, when I tried to `make install` the moby binaries after `make binary`, the installation path is hardcoded to `/usr/local/bin`, but in that path, I can't startup the docker daemon with `systemctl start docker` in my ubuntu 16.04 environment. I would like have a shortcut like this:\r\n`make install --prefix=/usr/bin`\r\nThen all the binaries will be copied into `/usr/bin/` path.\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1.\r\n`make binary`\r\n2.\r\n`make install`\r\n3.\r\n`systemctl stop docker`\r\n`systemctl start docker`\r\n\r\n**Describe the results you received:**\r\nThe new built docker daemon can not be stood up\r\n\r\n**Describe the results you expected:**\r\n`systemctl start docker` will spawn a new docker daemon with the latest built binaries.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n# docker version\r\nClient:\r\n Version:      unknown-version\r\n API version:  1.34\r\n Go version:   go1.8.3\r\n Git commit:   e4940cb\r\n Built:        Fri Oct 27 10:08:49 2017\r\n OS/Arch:      linux/arm64\r\n\r\nServer:\r\n Version:      17.06.0-dev\r\n API version:  1.34 (minimum version 1.12)\r\n Go version:   go1.8.4\r\n Git commit:   074b1fc\r\n Built:        Fri Oct 27 09:31:58 2017\r\n OS/Arch:      linux/arm64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 39\r\n Running: 3\r\n Paused: 0\r\n Stopped: 36\r\nImages: 994\r\nServer Version: 17.06.0-dev\r\nStorage Driver: overlay\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: v1.0.0-beta.2-53-g992280e8 (expected: 992280e8e265f491f7a624ab82f3e238be086e49)\r\nrunc version: 0351df1c5a66838d0c392b4ac4cf9450de844e2d\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.20\r\nOperating System: Ubuntu 16.04.2 LTS\r\nOSType: linux\r\nArchitecture: aarch64\r\nCPUs: 64\r\nTotal Memory: 125.6GiB\r\nName: entos-huawei-d05\r\nID: X4QW:BJJZ:3MVW:GKDT:7YHJ:NNTZ:GV24:JI3Y:CNIL:CXGQ:Y2FP:UJEM\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: arm64b\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nAll above operations are based on a physical AArch64 machine with ubuntu 16.04."},{"labels":[null,null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create two networks:\r\n\r\n        docker network create test\r\n        docker network create test2\r\n\r\n2. Try to run a container attached to both networks and output the IPs:\r\n\r\n        docker run -it --rm --network test --network test2 busybox ip a\r\n\r\n**Describe the results you received:**\r\n\r\nThere is only one network present (apart from the loopback one). Example output: \r\n\r\n```\r\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000\r\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\r\n    inet 127.0.0.1/8 scope host lo\r\n       valid_lft forever preferred_lft forever\r\n45343: eth0@if45344: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue \r\n    link/ether 02:42:ac:16:00:02 brd ff:ff:ff:ff:ff:ff\r\n    inet 172.22.0.2/16 scope global eth0\r\n       valid_lft forever preferred_lft forever\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nThere should be three interfaces (including loopback) including the two networks specified.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n`docker run` seems to only take into account _the last network_ specified at the commandline. \r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.10.0-ce\r\n API version:  1.33\r\n Go version:   go1.8.3\r\n Git commit:   f4ffd25\r\n Built:        Tue Oct 17 19:04:16 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.10.0-ce\r\n API version:  1.33 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   f4ffd25\r\n Built:        Tue Oct 17 19:02:56 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 5\r\n Running: 4\r\n Paused: 0\r\n Stopped: 1\r\nImages: 8\r\nServer Version: 17.10.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: novju2obbkwighigowx8slr76\r\n Is Manager: true\r\n ClusterID: 2dbj37zxgyf0iv9pk0ncozg6f\r\n Managers: 1\r\n Nodes: 3\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 192.158.50.11\r\n Manager Addresses:\r\n  192.158.50.11:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 06b9cb35161009dcb7123345749fef02f7cea8e0\r\nrunc version: 0351df1c5a66838d0c392b4ac4cf9450de844e2d\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-98-generic\r\nOperating System: Ubuntu 16.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 992.1MiB\r\nName: longhorn-01\r\nID: FS4U:GCWE:MPIQ:EFZU:Y43S:GCLM:PQX3:PYHR:5BIY:A4KA:Q5GU:TNVV\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nThis is on Ubuntu 16.04, but the same happens on Arch Linux as well. Tested on Docker swarm hosts only."},{"labels":[null,"enhancement"],"text":"**Description**\r\nTrying to ADD a tar.gz file with the new `--chown` flag. The flag was ignored.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Write a Docker file with an ADD command of an archive tar.gz file\r\n2. Build the image\r\n3. Run the image with `--entrypoint=/bin/sh`\r\n4. cd to the working directory, and ls -la\r\n\r\n**Describe the results you received:**\r\nThe tar.gz file was extracted, as expected. But file and directoryies the ownership was wrong\r\n\r\n**Describe the results you expected:**\r\nThe ownership should changed recursively, according to the `--chown` flag\r\n\r\n**Output of `docker version`:**\r\n```\r\nDocker version 17.09.0-ce, build afdb6d4\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 70\r\nServer Version: 17.09.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 118\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 06b9cb35161009dcb7123345749fef02f7cea8e0\r\nrunc version: 3f2f8b84a77f73d38244dd690525642a72156c64\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.49-moby\r\nOperating System: Alpine Linux v3.5\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 1.952GiB\r\nName: moby\r\nID: VPWK:YWD2:UCME:FA7F:RKXW:WBM6:MMCZ:RCEL:25WG:TBO3:U2L2:MWGV\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 19\r\n Goroutines: 31\r\n System Time: 2017-11-16T11:54:10.072089Z\r\n EventsListeners: 1\r\nNo Proxy: *.local, 169.254/16\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nRunning on MacOS 11"},{"labels":[null,"enhancement",null,null],"text":"Issue https://github.com/moby/moby/pull/34263 added `--chown` flag support for `ADD`/`COPY` commands in the Dockerfile for Linux/Unix (but now Windows), parsing the `/etc/passwd` and `/etc/group` files within the container.\r\n\r\nThis change adds `--chown` function to the `ADD` and `COPY` commands in Dockerfile for Windows.\r\nThe `--chown` flag for Windows takes an account name and translates the account into a SID, whether it be a well known SID or a User/Group within the container (in which case the container's SAM database is accessed to determine the relevant SID). The security descriptor of the target file/directory is modified and the owner set to the account specified in the command (provided it is valid), the discretionary  access control list is also modified to grant the specified owner read/write/execute access (since this more closely mirrors the behavior on Linux/Unix).\r\n\r\nThe following are some example usages:\r\n```\r\n--chown=ContainerUser\r\n--chown=someuser\r\n--chown=Administrator\r\n```\r\n\r\n\r\nISSUE #35026 requested a warning message for Windows, however, that is no longer necessary given that this change enables this scenario on Windows.\r\n"},{"labels":[null,"enhancement"],"text":"@alexellis, @tiborvass and I were recently discussing bots that help maintaining this project. I had some notes laying around (my \"bots wish-list\"), rephrased some of those a bit and dumped them here for inspiration, and possible implementation at some point :smile:\r\n\r\nI know some of these are provided already by bots \"in the wild\", so we should investigate if those are options.\r\n\r\n\r\n# Triage helper bots\r\n\r\n## lock-a-bot\r\n\r\nOld pull requests and issues frequently get commented on (\"I'm still seeing this\" on a 3 year old issue, or \"This new feature doesn't work on my 5 year old Docker\"). To prevent this, discussions that were closed should be automatically locked after some time.\r\n\r\n- After an issue / PR is closed, leave a comment mentioning that the discussion on the PR will be locked after 14 days(?)\r\n- The 14 days \"grace period\" allows for relevant comments to be added (which can be a simple \"thank you for implementing this!\", or a \"This PR broke the build!\"). After that, comments are generally just \"noise\", and not useful.\r\n- After 14 days, lock the discussion, leave a comment that if people run into issues with the PR, or have the same issue, they should open a new issue/ticket (because merged/closed PR's/tickets are not actively monitoried, and comments get overlooked easily)\r\n\r\n\r\n## re-re-re-rebase-bot\r\n\r\nGitHub checks mergeable state of pull requests, but only does so \"real time\" (when _viewing_ the pull request). With a lot of activity in the repository, every merged pull-request can lead to other pull requests needing a rebase. Unfortunately, it's not possible to discover that a rebase is needed, without manually opening each pull request and watch the \"mergeable stage\".\r\n\r\nThe `re-re-re-rebase-bot` automatically checks the mergeable state of each pull request, and:\r\n\r\n- Adds a label \"rebase needed\" if the pull request is no longer mergeable (this allows filtering pull requests by state)\r\n- Leaves a comment, asking the contributor to rebase the pull request (possibly limited to pull requests in \"status/2-code-review\", \"status/3-docs-review\", and \"status/4-merge\")\r\n\r\n\r\n## me-too-me-too-bot\r\n\r\nWe used to have a bot for this, but I don't think it's enabled anymore. People like to \"+1\", \"+1000\", `:+1` or \"me too\" issues. While it's understandable people may \"have the same issue\", or would kick in that they are interested in a proposed feature, those comments;\r\n\r\n- generate a lot of noise (over 3000 subscribers to the repository that receive an e-mail)\r\n- derail the discussion (long discussions on GitHub are hard to read)\r\n- don't help resolving issues (\"+1\" to what? the previous comment? the original issue? do you have more _information_ that would help _resolving_ the issue?)\r\n\r\nThe `me-too-me-too-bot`:\r\n\r\n- removes `+1` comments\r\n- collects the GitHub handles of those that left a `+1` comment, into a single comment, describing that the people listed left their \"+1\"\r\n- in that comment, describes the _preferred_ way to interact on issues (use the \"subscribe\" button to stay informed, use \"emoji's\" to express your support, and if you have more information that has not been included in the discussion, or have a use-case that explains why the feature is useful: leave a comment with that information)\r\n\r\n\r\n# Review helper bots\r\n\r\n## ready-to-take-the-next-step-bot\r\n\r\nThis project uses labels [to mark each stage in the review process](https://github.com/moby/moby/blob/master/project/REVIEWING.md#pull-request-reviewing-process). Going through those steps requires _removing_ the label for the previous stage, and _adding_ the label for the next. This can be simplified (with the added bonus that it allows setting those labels even from the mobile web-interface :+1:)\r\n\r\nThe `ready-to-take-the-next-step-bot` allows you to move to the next stage by leaving a comment, e.g.:\r\n\r\nCommenting \"bot: move this to code review\"  on a PR that's in \"design\" review would remove the `status/1-design-review` label, and _add_ the `status/2-code-review` label (remove previous stage, add next).\r\n\r\nAlso see the [`let-me-merge-that-for-you-bot`](#let-me-merge-that-for-you-bot) for moving a pull request to `status/4-merge`.\r\n\r\n\r\n## tell-us-a-story-bot\r\n\r\nSome pull requests have an exemplary description on GitHub, but turn out to have no commit message.\r\n\r\nCommit messages are important, because the commit-message is what ends up in source-control, not what's on GitHub. However, it's easy to overlook that a commit message is missing (or doesn't match the latest code changes - this may be more dificult to address though).\r\n\r\nThe `tell-us-a-story-bot` should check the size of the patch, and raise a red flag if the commit message is empty (or \"short\" compared to the patch size), for example:\r\n\r\n- add a label \"no commit message\" or \"short commit message\"\r\n- print the commit message as a comment (for easier reviewing)\r\n\r\n\r\n## what-the-branch-bot\r\n\r\nPull requests should generally be opened against the default (\"master\") branch. It's easy to miss that a pull-request was actually opened against a non-standard branch (as reviewer, you're focussed on the _diff_, not what the diff was opened against).\r\n\r\nThe `what-the-branch-bot` helps identifying pull request that were opened against a branch other than the default, by:\r\n\r\n- applying a label (\"non-default-branch\")\r\n- (if not present) adding a prefix to the PR's title (e.g.`[vX.Y.Z]` / `[branch name]` prefix for release branches)\r\n\r\n\r\n## squishy-squashy-bot\r\n\r\nPull requests are ideally _small_, and should only address a single change. Overall this means that most pull requests only have a single commit.\r\n\r\nWhile grouping changes into separate commits is encouraged, they are not intended to preserve history of the review process (\"Fix typo\", \"Address review comments\", \"Address more review comments\").\r\n\r\nReviewers may be focussed on the _changes_ made in a pull request, and overlook that the PR has multiple commits, or commits are not properly \"grouped\".\r\n\r\nThe `squishy-squashy-bot` helps reviewers by;\r\n\r\n- adding label if multiple-commits were found (\"multiple commits\"? \"check-if-squashing-is-needed\"?)\r\n- detection may be configurable (setting a threshhold for \"number of commits per file\", \"ratio LOC <--> number of commits\"?)\r\n\r\nWhen combining with the [`let-me-merge-that-for-you-bot`](#let-me-merge-that-for-you-bot), possibly:\r\n\r\n- When giving instructions to merge, ask confirmation if squashing may be needed (\"This pull request consists of 10 commits. Confirm if merging should proceed\")\r\n- Allow reviewers to do so \"in one go\" (\"Bot: merge when green, no squashing needed\")\r\n\r\n\r\n## did-you-miss-me-bot\r\n\r\nWhen reviewing a pull request, reviewers tend to focus on code changes. There are other changes that can be easy to miss (due to how GitHub's UI presents them). For example:\r\n\r\n- check if file-permissions changed, and leave a summary comment that describes which files changed (GitHub only shows a small badge, e.g. `0600 -> 0777`).\r\n- check if possibly unwanted files (`.tmp`, `.bak`, `Thumbs.db` etc) were added. This would be a great feature, as it doesn't require the `.gitignore` [to be cluttered with these](https://github.com/moby/moby/blob/4308e881cc00141a1dd187120d1c4e2106148282/.gitignore#L2-L3)\r\n- check if binary file(s) were added. Binary files are \"hidden\" in the diff view on GitHub, and easy to miss if there's a large diff\r\n  ![binary files are hidden](https://user-images.githubusercontent.com/1804568/29143248-baff2182-7d54-11e7-9030-c531a85ef66a.png)\r\n\r\n\r\n## does-this-make-my-repo-look-big-bot\r\n\r\nImages can tell a thousand words. Unfortunately, a single image can easily take up the _space_ of a _million_ words. Once merged, those bytes are in the repository _forever_, so it's good practice to check for this.\r\n\r\nThe `does-this-make-my-repo-look-big-bot`\r\n\r\n- checks if images that are included in a pull request can be (loslessly) optimized\r\n- if optimizing would [save a lot in size](https://github.com/jessfraz/blog/pull/5#issue-102414426) (configurable threshold in \"percentage\" or \"bytes\"), leaves a comment with the results\r\n- ideally, does the actual optimization, and provides links to the optimized versions of the images\r\n\r\n> **Note** wondering if this is a job for a bot (commenting, and providing optimized links may be), or for CI. Left this idea here anyway :smile:\r\n\r\n\r\n# Vendoring helper bots\r\n\r\n## let-me-diff-that-for-you-bot\r\n\r\nGitHub collapses all external dependencies in the diff viewer. While this is a great thing to allow you focus on the _local_ code-changes separate from those that were pulled in from upstreams, it also makes it impossible (or \"very difficult\") to verify upstream code-changes (which may be relevant).\r\n\r\nIn addition; even if the diff of external dependencies _would be visible_, without context, it's difficult to understand those changes.\r\n\r\nThe `let-me-diff-that-for-you-bot` would;\r\n\r\n- check the `vendor.conf` file for changes (assuming a Go project, but the same would apply to other dependency systems)\r\n- generate a github \"diff\" link for each changed dependency (see https://github.com/docker/cli/pull/679 for an example)\r\n\r\n\r\nThe diff link enables a reviewer to see all commits that are made in upstream dependencies, which is often more descriptive than just the code changes itself. If also allows to make notes about those changes for the changelog, if important issues were fixed, or the dependency brings changes in behavior.\r\n\r\n\r\n## inception-bot\r\n\r\nIt's standard practice to \"flatten\" (\"strip\") vendored dependencies; this means that only a _top level_ `vendor` directory is present in the repository, and every package (including vendored packages) use the same version of that dependency.\r\n\r\nWhen \"bumping\" a dependency, it's important to check if the \"upstream\" repository also expects other dependencies to be updated (e.g. bumping \"moby\" in the \"docker/cli\" repository may require the \"swarmkit\" dependency to be bumped as well; https://github.com/docker/cli/pull/679). Keeping dependencies \"in sync\" better guarantees that you're working with the same version of the dependency as the upstream was tested/verified against.\r\n\r\nThe `inception-bot` would collect all nested `vendor.conf` files, compare them with the top-level `vendor.conf`, _and_ with other nested `vendor.conf` files to generate a table with \"x-commits ahead/behind\" for each one:\r\n\r\n\r\npackage | [g/d/swarmkit](https://github.com/docker/swarmkit) | [g/o/runc](https://github.com/opencontainers/runc) | [g/c/containerd](https://github.com/containerd/containerd)\r\n--------|----------------------------------------------------|----------------------------------------------------|------------\r\n[golang.org/x/sys](https://godoc.org/golang.org/x/sys) | https://github.com/golang/sys/compare/07c182904dbd53199946ba614a412c61d3c548f5...95c6576299259db960f6c5b9b69ea52422860fce (50 commits ahead) | https://github.com/golang/sys/compare/07c182904dbd53199946ba614a412c61d3c548f5...47bdb838cce8879fca08207206a5dfde2f2538bc (9 commits behind) | -\r\n[github.com/some/package](#) | ... (5 ahead) | - | ... (10 ahead)\r\n\r\n\r\n\r\n# CI-helper-bots\r\n\r\nBots that help with CI (and merging):\r\n\r\n## let-me-merge-that-for-you-bot\r\n\r\nThe `let-me-merge-that-for-you-bot` merges the pull request after CI goes \"green\". This bot could also be used to allows grant reviewers permissions to merge pull requests, without having to give them write access to the repository.\r\n\r\nMerging can be triggered:\r\n\r\n- By leaving a comment (\"bot: merge on green\")\r\n- By applying the `\"status/4-merge\"` label\r\n\r\n\r\n## I-remember-the-days-you-were-still-green-bot\r\n\r\nSome pull requests have been open for a while, and although CI status shows a nice and shiny \"green\", that status may no longer be accurate (given that CI ran days, or even weeks before).\r\n\r\nWhile this problem could be resolved by automatically re-running CI on a schedule, running CI again may not always be nescessary (yet), for example if a pull-request is still in design review, or is a work-in-progress.\r\n\r\nThe `I-remember-the-days-you-were-still-green-bot`;\r\n\r\n- Checks if CI status is possibly outdated (number of days since, number of _commits_ since?)\r\n- Leaves a comment that describes this (\"I noticed that CI was last run XX days ago. Want me to run CI again?\")\r\n- The bot will automatically update the existing comment as long as CI hasn't run again (perhaps remove, and re-add the comment to make sure it's always the last comment)\r\n- Reviewers can:\r\n  - Leave a special comment to trigger CI again\r\n  - Leave a special comment to trigger CI and merge (`let-me-merge-that-for-you-bot`)\r\n  - Click generated links to do this (?)\r\n- Once CI has run (either through a comment, or other means, like directly restarting CI in Jenkins), the bot will remove the comment to unclutter the discussion.\r\n\r\nNote: ideally, GitHub would allow setting a \"stale\" threshold for checks, and block merge, but as far as I'm aware it only allows \"checks passed\" as a constraint.\r\n\r\n\r\n## what-went-wrong-bot\r\n\r\nCI sometimes fails (surprise!); sometimes related to the pull request (awesome, it caught a bug!), sometimes because of a bad (\"flaky\") test.\r\n\r\nDiscovering what went wrong is time-consuming; having to open the CI logs, scroll through thousands of lines of output (or searching for keywords).\r\n\r\nThe `what-went-wrong-bot` would:\r\n\r\n- Summarize CI failures (ideally, formatted with [`<summary>`](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/summary) / [`<details>`](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/details) for each test that failed)\r\n- Show a link to the source code of each failing test\r\n- If many tests failed, skip the per-test summary (as it's likely not informative)\r\n- Detect general \"Jenkins\" failures? (just a thought; Jenkins or a Jenkins plugin sometimes fail)\r\n- Super useful (but likely out of scope for a bit) would be to keep track of failing tests, and automatically mark them as \"flaky\" (or just statistics of failing tests)\r\n- Link to issues that keep track of \"flaky tests\" (\"This test is marked as 'flaky' in issue `<link>`\")\r\n- Allow marking tests as flaky (\"mark test X as flaky, or have a clickable link\")\r\n- Git blame links for failing tests? :innocent:\r\n\r\n\r\n# Other bots\r\n\r\n## something-something-bot\r\n\r\nAs mentioned in some of the descriptions above (just thinking) have the bot leave comments with actionable links (click the link to interact)\r\n\r\n## remind-me-bot\r\n\r\nLike slack! \"remind x to review this tomorrow\"\r\n\r\n\r\n- **A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n![solar-tortoise-toy-puzzle-funny-cute-little](https://user-images.githubusercontent.com/1804568/32779192-0ed66930-c93d-11e7-97e4-b34241742163.jpg)\r\n\r\nImage taken from (spam alert :joy:): [\"dhgate.com\"](https://www.dhgate.com/product/solar-tortoise-toy-puzzle-funny-cute-little/379733227.html#s1-10-1;disc|240989363)\r\n"},{"labels":[null,"enhancement"],"text":"Currently, Docker build suppresses progress output when running in a script, e.g. when triggered from Maven.\r\nThis is sane since otherwise the output would be garbled and weird.\r\n\r\nHowever, it would be nice to have at least SOME output.\r\n\r\nFor example, I several times per day build a docker image with a build context of about 2 gigabytes. When building manually, I get a progress report for the \"Sending build context...\" part, but when running through Maven (which will automatically supply relevant --build-args) I get no output at all until the context is sent.\r\n\r\nIt would be nice to have at least something, like an output every 5% or so. \r\nThe same goes for docker pull and any other progress report.\r\n\r\nThanks!"},{"labels":[null,null,"enhancement"],"text":"`/dev` doesn't have the `noexec` mount option: https://github.com/moby/moby/blob/2613c3d53cee46d8e0dc8e1aba83686f259985e4/oci/defaults.go#L81\r\n\r\nBeing a _system_ `tmpfs` with `RW` rights, it'd be better to have no execution rights there.\r\n\r\n"},{"labels":["enhancement",null],"text":"This is kind of related to #30623 (in that the reason I tried to do this was because I wanted to know how the prune filter worked.)\r\n\r\n**Description**\r\n\r\nThe documentation in https://docs.docker.com/engine/reference/commandline/images/#filtering describes the filters available for `docker image ls`\r\n\r\nThe documentation in https://docs.docker.com/engine/reference/commandline/image_prune/#filtering describes the filters available for `docker image prune`\r\n\r\nThey are not the same.\r\n\r\n**Steps to reproduce the issue:**\r\n```\r\n$ docker image ls -f before=foo\r\nError response from daemon: No such image: foo:latest\r\n$ docker image prune --filter before=foo \r\nWARNING! This will remove all dangling images.\r\nAre you sure you want to continue? [y/N] y\r\nError response from daemon: Invalid filter 'before'\r\n```\r\n\r\nThere is also a more minor bug; `docker image prune` should test if filters are valid before the y/n prompt.\r\n\r\n**Describe the results you received:** These two commands support different filters\r\n\r\n**Describe the results you expected:** They support the same filters\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n$ docker version\r\nClient:\r\n Version:      17.09.0-ce\r\n API version:  1.32\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:42:18 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.09.0-ce\r\n API version:  1.32 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:40:56 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n$ docker info\r\nContainers: 10\r\n Running: 0\r\n Paused: 0\r\n Stopped: 10\r\nImages: 61\r\nServer Version: 17.09.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 06b9cb35161009dcb7123345749fef02f7cea8e0\r\nrunc version: 3f2f8b84a77f73d38244dd690525642a72156c64\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-1038-aws\r\nOperating System: Ubuntu 16.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 14.69GiB\r\nName: ip-172-31-17-4\r\nID: J75D:NQJA:CKPS:7K4T:LZCP:65KV:HIBE:JEGS:2GWO:RB7Q:TE3L:YMBF\r\nDocker Root Dir: /data/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):** This is deployed on AWS\r\n"},{"labels":[null,"enhancement",null,null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nAfter updating to lcow (17.10.0-ce, build f4ffd25) I started to encounter errors when running several linux containers in regards to the ulimits being set to default 1024. I attempted to update my compose file but receive an error stating it is not supported on windows, also tried updates in the daemon.json file. At current I am not able to find a way to increase ulimits on windows.\r\n\r\n**Steps to reproduce the issue:**\r\n1. update a compose file for ulimits\r\n\r\n ulimits:\r\n      nofile:\r\n        soft: 32000\r\n        hard: 40000\r\n\r\nupdate deamon.json with: \r\n\r\n \"default-ulimits\": {\r\n\t\t\"nofile\": {\r\n\t\t\t\"Name\": \"nofile\",\r\n\t\t\t\"Hard\": 40000,\r\n\t\t\t\"Soft\": 30000\r\n\t\t}\r\n\t}\r\n\r\nI also tried with: \r\n\r\n\"default-ulimit\": {\r\n\t\t\"nofile\": {\r\n\t\t\t\"Name\": \"nofile\",\r\n\t\t\t\"Hard\": 40000,\r\n\t\t\t\"Soft\": 30000\r\n\t\t}\r\n\t}\r\n\r\n**Describe the results you received:**\r\n\r\nUsing compose.yml:\r\n\r\nInvalid option: Windows does not support Ulimits\r\n\r\nUsing defaults-ulimits:\r\n\r\nThe following directives don't match any configuration option: default-ulimits\r\n\r\nUsing defaults-ulimit:\r\n\r\nThe following directives don't match any configuration option: nofile\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nDocker version 17.10.0-ce, build f4ffd25\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 5\r\nServer Version: 17.10.0-ce\r\nStorage Driver: lcow (linux) windowsfilter (windows)\r\n LCOW:\r\nLogging Driver: json-file\r\nPlugins:\r\n Volume: local\r\n Network: ics l2bridge l2tunnel nat null overlay transparent\r\n Log: awslogs etwlogs fluentd json-file logentries splunk syslog\r\nSwarm: inactive\r\nDefault Isolation: hyperv\r\nKernel Version: 10.0 16299 (16299.15.amd64fre.rs3_release.170928-1534)\r\nOperating System: Windows 10 Pro\r\nOSType: windows\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 15.84GiB\r\nName: LAPTOP-82U8ONK8\r\nID: JUA6:R5B4:27H4:GSAO:PKWO:GME2:K24S:FNA2:LI6N:KFN5:6FJI:F2RV\r\nDocker Root Dir: C:\\ProgramData\\Docker\\lcow\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: -1\r\n Goroutines: 24\r\n System Time: 2017-10-25T13:52:51.4575925-05:00\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,null,"enhancement",null],"text":"Docker secrets being used in a docker-compose file should be able to handle the use of variables.\r\n\r\nThis would be really handy to be able to systematically update a secret in the next \"version\" of the container\r\n\r\n**Steps to reproduce the issue:**\r\n1. deploy a stack with docker secrets with variables\r\n**Describe the results you received:**\r\npostgres_passwd-${DEPOY_VERSION:-default} Additional property postgres_passwd-${DEPOY_VERSION:-default}   is not allowed\r\n\r\n**Describe the results you expected:**\r\ndeployment is successful \r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n```\r\n--- docker-compose.yml ---\r\nversion: '3.1'\r\nservices:\r\n  postgres:\r\n    image: \"postgres:10\"\r\n    environment:\r\n       POSTGRES_PASSWORD_FILE: /run/secrets/postgres_passwd-${DEPOY_VERSION:-default}\r\n    secrets:\r\n      - postgres_passwd-${DEPOY_VERSION:-default}\r\nsecrets:\r\n  postgres_passwd-${DEPOY_VERSION:-default}:\r\n    external: true\r\n\r\n```\r\n**Output of `docker version`:**\r\n```\r\nClient:\r\n Version:      17.09.0-ce\r\n API version:  1.32\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:40:09 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      17.09.0-ce\r\n API version:  1.32 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:45:38 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nDocker swarm"},{"labels":[null,null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nWhen attaching multiple macvlan nets on `docker service create` (one external, one internal), the internal is always chosen. There is no way to choose the external for the default route.\r\n\r\n**Steps to reproduce the issue:**\r\n1. create two macvlan swarm nets, separate nets\r\n2. create service using and attach both nets\r\n3. check route table and you will see its selects a route\r\n\r\n**Describe the results you received:**\r\n\r\ndefault behavior:\r\n# ifconfig -a\r\n\r\n```\r\neth0      Link encap:Ethernet  HWaddr 02:42:0A:F1:01:82\r\n          inet addr:10.241.1.130  Bcast:0.0.0.0  Mask:255.255.0.0\r\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\r\n          RX packets:52313 errors:0 dropped:0 overruns:0 frame:0\r\n          TX packets:50601 errors:0 dropped:0 overruns:0 carrier:0\r\n          collisions:0 txqueuelen:0\r\n          RX bytes:32512621 (31.0 MiB)  TX bytes:5352903 (5.1 MiB)\r\n\r\neth1      Link encap:Ethernet  HWaddr 02:42:43:E2:D2:21\r\n          inet addr:192.0.0.25  Bcast:0.0.0.0  Mask:255.255.254.0\r\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\r\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\r\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\r\n          collisions:0 txqueuelen:0\r\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\r\n```\r\n\r\n# netstat -rn\r\n```\r\nKernel IP routing table\r\nDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface\r\n0.0.0.0         10.241.0.1      0.0.0.0         UG        0 0          0 eth0\r\n10.241.0.0      0.0.0.0         255.255.0.0     U         0 0          0 eth0\r\n192.0.0.25    0.0.0.0         255.255.254.0   U         0 0          0 eth1\r\n```\r\n\r\n**Describe the results you expected:**\r\nI expected the above results. \r\n\r\nThe problem is that I want the following route table instead:\r\n\r\n# netstat -rn\r\n```\r\nKernel IP routing table\r\nDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface\r\n0.0.0.0      67.226.210.1    255.255.254.0   U         0 0          0 eth1\r\n10.241.0.0      0.0.0.0         255.255.0.0     U         0 0          0 eth0\r\n67.226.210.0    0.0.0.0         255.255.254.0   U         0 0          0 eth1\r\n```\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nRegardless of order, the RFC1918 ip is always chosen. This is an issue when you are trying to run a container with a public ip and an internal ip.\r\n\r\nHACK:\r\nThe only workaround for this, is to ditch the use of Swarm, since swarm doesn't support `--privileged`. I need `--privileged` in order to run a script when the containers starts (via `docker run`). The script will juggle the routes and we will end up with the desired state. So unless we can select a specific gateway (and add addtional routes) or get `--privileged` added to `docker service`, it's not easy running a public/private container with swarm + macvlan.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.06.2-ce\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   cec0b72\r\n Built:        Tue Sep  5 19:59:06 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.06.2-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   cec0b72\r\n Built:        Tue Sep  5 20:00:25 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 10\r\n Running: 3\r\n Paused: 0\r\n Stopped: 7\r\nImages: 16\r\nServer Version: 17.06.2-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: xfs\r\n Supports d_type: false\r\n Native Overlay Diff: true\r\nLogging Driver: gelf\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: r6exfwgg7p4837ytv7746sntz\r\n Is Manager: true\r\n ClusterID: ejo78ur1erlw6azupui3wy175\r\n Managers: 3\r\n Nodes: 3\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Root Rotation In Progress: false\r\n Node Address: 10.242.0.25\r\n Manager Addresses:\r\n  10.242.0.23:2377\r\n  10.242.0.24:2377\r\n  10.242.0.25:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 6e23458c129b551d5c9871e5174f6b1b7f6d1170\r\nrunc version: 810190ceaa507aa2727d7ae6f4790c76ec150bd2\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.12.10-1.el7.elrepo.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 12\r\nTotal Memory: 11.73GiB\r\nName: dockerhost-test-nb5h9zw3\r\nID: X75E:3H7J:23MA:L7C2:KXIY:IS7I:KI4H:2PK7:GVS2:AGRM:2A2P:L56F\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nThis is running under VMware, but results are the same on bare metal."},{"labels":[null,null,"enhancement"],"text":"Hi\r\n\r\nMaybe it has been asked before, but I haven't been able to find it.\r\n\r\nActually docker inspect on an exited container returns:\r\n```\r\n        ...\r\n        \"State\": {\r\n            \"Status\": \"exited\",\r\n            \"Running\": false,\r\n            \"Paused\": false,\r\n            \"Restarting\": false,\r\n            \"OOMKilled\": true,\r\n            \"Dead\": false,\r\n            \"Pid\": 0,\r\n            \"ExitCode\": 137,\r\n            \"Error\": \"\",\r\n            \"StartedAt\": \"2017-10-12T05:50:23.819164201Z\",\r\n            \"FinishedAt\": \"2017-10-12T06:45:33.520387397Z\"\r\n        },\r\n        ...\r\n```\r\n\r\nIt would be possible to preserve the PID after container exited, in order to easily match logs with atop/kernel OOM and so?\r\n\r\nThanks."},{"labels":[null,"enhancement"],"text":"We are running multiple VPC's with different IP addresses.\r\nOne of our swarm clusters resides in a VPC with 172.19.0.0/16 and communicates with another VPC\r\nthat uses the IP CIDR of 172.18.0.0/16\r\n\r\nOur swarm cluster auto generated docker_gwbridge overlapping the other VPC CIDR\r\n```\r\ninet 172.18.0.1/16 scope global docker_gwbridge \r\n```\r\nOr event stand alone docker0 that defaults to\r\n```\r\ninet 172.17.0.1/16 scope global docker0\r\n```\r\nThis caused severe connectivity issues on our cluster due to overlapping of the route published in AWS Routing table.\r\n\r\nI am suggesting a feature request to add a list of IP's to /etc/docker/daemon.json a list of restricted IP CIDR that the engine could not possibly understand by it self."},{"labels":[null,null,"enhancement"],"text":"# Issue\r\nWhen a swarm mode container fails and the service is scheduled on another node, the service will go to a `preparing` state while the image is downloaded. The service will be unavailable until the image is downloaded and container starts. For large images this can create significant downtime while the image is being downloaded. \r\n\r\n# Workaround\r\n- As a workaround, manually pulling images to each node that the service may failover to will alleviate the issue.\r\n\r\n# Potential Solution\r\n- Allow a swarm/stack service option to pull the image to N nodes during the service creation/update. This will allow nodes to be better prepared for a task in the event a service fails."},{"labels":[null,"enhancement"],"text":"https://github.com/moby/moby/blob/master/hack/dockerfile/install-binaries.sh\r\n\r\nIt would be good if the env vars in were externalise into something like a env file, but more importantly can we put things like the repo URLs into the env file so that people that are building docker can source that file to know which repositories to checkout e.g. the repo for some of these keeps changing from upstream (opencontainers) to the docker fork and back."},{"labels":[null,"enhancement"],"text":"Hey there,\r\n\r\nthe `ADD` and `COPY` command are, according to [this comment in builder/dockerfile/dispatchers.go](builder/dockerfile/dispatchers.go) the same – except for additional remote and tar handling in `ADD`.\r\n\r\nSince the introduction of multi-stage builds, this isn’t true anymore. Now `COPY` has one feature that `ADD` doesn’t have: Taking a file from another build stage/context.\r\n\r\nBecause I find the automatic tar handling of `ADD` to be rather convenient, I propose to make the `--from` flag available for `ADD` as well. I would also be willing to create a PR for it.\r\n\r\nWhat do you think?"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nCurrently, it is not possible to write a volume plugin that attaches/detaches physical hardware on a call to Mount and Unmount safely while supporting `docker cp` without potentially causing disk corruption. An example of this in the real world is when trying to write a volume driver to attach an EBS disk to an EC2 instance or a GCP disk to a GCP instance.\r\n\r\nThe issue roots from a volume plugin being in a separate process from Docker itself and not having access to enough information to know when to actually unmount. This becomes an issue, since `docker cp` can trigger a call to `Mount` and then `Unmount` while there are other outstanding Mount calls require the physical volume to still be mounted.\r\n\r\nThe following is valid given the current docker implementation as it was intended:\r\n\r\n1) Launch container on an EC2 instance with a EBS volume using a EBS volume plugin. `Mount` is called with an ID of 1\r\n2) Call `docker cp` on the same container. `Mount` is called with an ID of 2.\r\n3) `docker cp` ends and `Unmount` is called with an ID of 2.\r\n4) Container is stopped and `Unmount` is called with an ID of 1.\r\n\r\nThis requires the state to be stored in the volume driver itself to know that the volume should not be removed at step 3 above. With the current API as is, it is not possible to implement a volume driver that can safely maintain this state.\r\n\r\nFor example, say you are implementing a volume driver:\r\n\r\nFirst, you decide to put the volume plugin state in memory. The issue arises when a volume plugin is restarted but volumes Mount's with have not received all outstanding Unmounts yet. The volume plugin cannot currently recover knowledge of outstanding Mount IDs that haven't had Unmounts... the plugin does not know if Unmount actually should lead to a disk being disconnected / physically unmounted or not.\r\n\r\nYou decide to solve this by storing state to disk. When your volume plugin is restarted, you can rehydrate the state of all outstanding Mount IDs. This way when an unmount is issued, you know when you should actually de-provision a disk / physically unmount. The issue arises when Docker itself is restarted. Presumably, all outstanding Mount IDs are now invalid. Your Volume plugin doesn't know to invalidate these mounts and when a Mount followed by Unmount pair, which would have otherwise indicated a disk being physically unmounted -- the volume plugin still thinks there are outstanding Mount requests.\r\n\r\nTwo valid solutions to this:\r\n1. Docker should be modified to only call Mount when the underlying disk should be mounted and Unmount only when the underlying disk should be unmounted\r\n2. Every call to the Mount / Unmount API should contain information on all outstanding Mounts / Unmounts\r\n\r\nWith either of these solutions, the volume plugin would no longer need to store state related to docker.\r\n\r\nFurther discussion on this issue can be found in https://github.com/moby/moby/issues/34665"},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nWilling to spread some tasks over a swarm cluster, I would like to get the following behavior : when a new task is up to be scheduled, the chosen node is the one where the least amount of CPUs is reserved. So i use the `--reserve-cpu` parameter when I create my services but the spread strategy is defined to schedule the next task on the node where the lowest number of tasks are running, isn't it ? But it does not matter of the already reserved CPUs on a node while the number of available CPUs is greater than the number of CPUs requested by the next task. \r\nSo my request is to know if there is a way to make the scheduler choose the node where the least amount of CPUs is reserved instead of choosing the one where the lowest number of tasks are running ?\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a swarm cluster with 4 nodes containing 16 CPUs each one\r\n2. Create 3 services without reserving some resources\r\n```\r\ndocker service create --name nginx-01 --replicas 1 nginx\r\ndocker service create --name mydb --replicas 1 mongo:3.5.10\r\ndocker service create --name mybackend --replicas 1 mybackendImageName\r\n```\r\n3. Create 4 services by specifying a CPU reservation for each one\r\n```\r\ndocker service create --name myprocess --reserve-cpu 8 --replicas 1 myProcessImageName\r\ndocker service create --name myprocess2 --reserve-cpu 8 --replicas 1 myProcessImageName\r\ndocker service create --name myprocess3 --reserve-cpu 8 --replicas 1 myProcessImageName\r\ndocker service create --name myprocess4 --reserve-cpu 8 --replicas 1 myProcessImageName\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\n- On the Node 1, the replicas of **myprocess** and **myprocess2** are running\r\n- On the Node 2, the replicas of **nginx-01** and **myprocess3** are running\r\n- On the Node 3, the replicas of **mydb** and **myprocess4** are running\"\r\n- On the Node 4, the replica of **mybackend** is running\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\nFor example a spread of the replicas like this:\r\n- On the Node 1, the replica of **myprocess**\r\n- On the Node 2, the replicas of **nginx-01** and **myprocess2**\r\n- On the Node 3, the replicas of **mydb** and **myprocess3**\r\n- On the Node 4, the replica of **mybackend** and **myprocess4**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nDefinitely if I reserve 9 CPUs for each **myprocess** service, the replicas are spread as I expect because the scheduler does not allow to run 2 tasks requiring 9 CPUs each one on the same node.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.06.1-ce\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   874a737\r\n Built:        Thu Aug 17 22:51:12 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.06.1-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   874a737\r\n Built:        Thu Aug 17 22:50:04 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 31\r\nServer Version: 17.06.1-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: journald\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: cifs local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: c26xt8qyaf2ozap43dv83ov0b\r\n Is Manager: true\r\n ClusterID: u1phbzj4t37l87jeon4a3v72g\r\n Managers: 1\r\n Nodes: 4\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Root Rotation In Progress: false\r\n Node Address: 172.30.8.34\r\n Manager Addresses:\r\n  172.30.8.34:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 6e23458c129b551d5c9871e5174f6b1b7f6d1170\r\nrunc version: 810190ceaa507aa2727d7ae6f4790c76ec150bd2\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-87-generic\r\nOperating System: Ubuntu 16.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 16\r\nTotal Memory: 3.857GiB\r\nName: swarmmanager\r\nID: IGMG:DTNH:RZX6:SZOG:DT2X:TH4B:KVM4:ST4F:FNQU:YJDK:AYNH:MXBR\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 0.0.0.0/0\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nMy cluster nodes are some VMs running on KVM\r\n"},{"labels":[null,"enhancement",null,null],"text":"Hi there,\r\n\r\nWe have case when we not use hard coded replicas in service, but we have service that keep number of replicas by our own logic instead. With upgrade to 17.09.0-ce from 17.06.2-ce we found that our logic stop working instead design:\r\nBy documentation of task behavior, failed tasks should:\r\n\r\n[If the task fails the orchestrator removes the task and its container and then creates a new task to replace it according to the desired state specified by the service](https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#tasks-and-scheduling).\r\n\r\nIn docker:\r\n\r\n```\r\nServer:\r\n Version:      17.06.2-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   cec0b72\r\n Built:        Tue Sep  5 19:59:11 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n ```\r\n\r\nThe behavior as designed:\r\nStep to reproduce:\r\nCreate docker-compose.yml:\r\n\r\n```yml\r\nversion: \"3.3\"\r\nservices:\r\n  server:\r\n    image: nginx\r\n    ports:\r\n      - 80:80\r\n    deploy:\r\n      mode: replicated\r\n      replicas: 0\r\n      restart_policy:\r\n        condition: none\r\n```\r\n\r\nrun `docker swarm init && docker stack deploy -c docker-compose.yml test && docker && docker service ls`\r\n\r\noutput is:\r\n```\r\nID                  NAME                MODE                REPLICAS            IMAGE               PORTS\r\nyb6v98o31knb        test_server         replicated          0/0                 nginx:latest        *:80->80/tcp\r\n```\r\n\r\nthen `docker service scale test_server=5`\r\n\r\n`docker service ps test_server` return:\r\n\r\n```\r\nID                  NAME                IMAGE               NODE                      DESIRED STATE       CURRENT STATE           ERROR               PORTS\r\n m28sw9fjtdrd        test_server.1       nginx:latest        lev-hp-elitebook-850-g3   Running             Running 8 seconds ago\r\n wdgprv7a4qns        test_server.2       nginx:latest        lev-hp-elitebook-850-g3   Running             Running 8 seconds ago\r\n u5qbnm2178oa        test_server.3       nginx:latest        lev-hp-elitebook-850-g3   Running             Running 8 seconds ago\r\n ieydt40t9dlx        test_server.4       nginx:latest        lev-hp-elitebook-850-g3   Running             Running 7 seconds ago\r\n 0bstwmuh1400        test_server.5       nginx:latest        lev-hp-elitebook-850-g3   Running             Running 6 seconds ago\r\n ```\r\n\r\n Let's kill two containers by `docker rm $(docker kill [container id]`\r\n\r\n The tasks now in status `failed`:\r\n\r\n ```\r\n ID                  NAME                IMAGE               NODE                      DESIRED STATE       CURRENT STATE           ERROR                         PORTS\r\n  m28sw9fjtdrd        test_server.1       nginx:latest        lev-hp-elitebook-850-g3   Running             Running 2 minutes ago\r\n  wdgprv7a4qns        test_server.2       nginx:latest        lev-hp-elitebook-850-g3   Running             Running 2 minutes ago\r\n  u5qbnm2178oa        test_server.3       nginx:latest        lev-hp-elitebook-850-g3   Shutdown            Failed 5 seconds ago    \"task: non-zero exit (137)\"\r\n  ieydt40t9dlx        test_server.4       nginx:latest        lev-hp-elitebook-850-g3   Running             Running 2 minutes ago\r\n  0bstwmuh1400        test_server.5       nginx:latest        lev-hp-elitebook-850-g3   Shutdown            Failed 16 seconds ago   \"task: non-zero exit (137)\"\r\n  ```\r\n\r\nRestart tasks status by running: `docker service scale test_server=6`\r\n\r\nThen behavior as documented the failed tasks replaced by new one:\r\n\r\n```\r\nID                  NAME                IMAGE               NODE                      DESIRED STATE       CURRENT STATE                    ERROR                         PORTS\r\n m28sw9fjtdrd        test_server.1       nginx:latest        lev-hp-elitebook-850-g3   Running             Running 4 minutes ago\r\n wdgprv7a4qns        test_server.2       nginx:latest        lev-hp-elitebook-850-g3   Running             Running 4 minutes ago\r\n kogxnct8wrgj        test_server.3       nginx:latest        lev-hp-elitebook-850-g3   Running             Running less than a second ago\r\n u5qbnm2178oa         \\_ test_server.3   nginx:latest        lev-hp-elitebook-850-g3   Shutdown            Failed 2 minutes ago             \"task: non-zero exit (137)\"\r\n ieydt40t9dlx        test_server.4       nginx:latest        lev-hp-elitebook-850-g3   Running             Running 4 minutes ago\r\n 6sljck2m855c        test_server.5       nginx:latest        lev-hp-elitebook-850-g3   Running             Running 1 second ago\r\n 0bstwmuh1400         \\_ test_server.5   nginx:latest        lev-hp-elitebook-850-g3   Shutdown            Failed 2 minutes ago             \"task: non-zero exit (137)\"\r\n cydglz4o314b        test_server.6       nginx:latest        lev-hp-elitebook-850-g3   Running             Starting 2 seconds ago\r\n ```\r\n\r\n\r\nIn docker:\r\n\r\n```\r\nServer:\r\n Version:      17.09.0-ce\r\n API version:  1.32 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:40:56 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n ```\r\n\r\n After return on previouse steps tasks still in `failed` state:\r\n\r\n ```\r\n ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                ERROR                         PORTS\r\n  fbe6l06dwazv        test_server.1       nginx:latest        ubuntu-server       Running             Running about a minute ago\r\n  v9bxxa6ur3kp        test_server.2       nginx:latest        ubuntu-server       Running             Running about a minute ago\r\n  ox0852jr0kie        test_server.3       nginx:latest        ubuntu-server       Shutdown            Failed 17 seconds ago        \"task: non-zero exit (137)\"\r\n  dhxttorbjb5e        test_server.4       nginx:latest        ubuntu-server       Running             Running about a minute ago\r\n  cy30hiqppags        test_server.5       nginx:latest        ubuntu-server       Shutdown            Failed 28 seconds ago        \"task: non-zero exit (137)\"\r\n  rm1sxbojsdkf        test_server.6       nginx:latest        ubuntu-server       Running             Running 3 seconds ago\r\n ```\r\n\r\n Instead designed.\r\n\r\n If scale service to 4 and scale back to 6 then failed tasks is deleted.\r\n\r\n Thank you for your help..\r\n\r\n\r\n"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Deploy a stack from images which no longer (or never) existed on the designated registry\r\n\r\n**Describe the results you received:**\r\nThe Docker CLI command `stack deploy` gives no indication that the images do not exist, and leaves service tasks at 0/X started with no indication of underlying problem. The service logs do not explain the underlying issue either.\r\n\r\n**Describe the results you expected:**\r\n`docker stack deploy -c` should receive a message from the underlying Docker Engines that the images cannot be pulled, and thus the operation is blocked until the image exists\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nHere is output that shows the `docker service logs` have no pertinent output -\r\n```\r\n$ export DOCKER_HOST=tcp:IP:2375\r\n$ docker stack deploy --with-registry-auth -c FILE.yml\r\n$ docker service ls -q\r\nzht30tzql7xo\r\nd0rtfw5gcv57\r\nogu9irz557d6\r\nuuqexaegfibt\r\n$ docker service logs zh\r\n$ docker service logs d0\r\n$ docker service logs og\r\n$ docker service logs uu\r\n$\r\n```\r\n\r\n```\r\nID                  NAME                     MODE                REPLICAS            IMAGE                                                   PORTS\r\nzht30tzql7xo        redacted_test_service_a  replicated          0/3                 registry.MYDOMAIN:5000/redacted_test_service_a:347d034  *:8501->8500/tcp\r\nd0rtfw5gcv57        redacted_test_service_b  replicated          0/3                 registry.MYDOMAIN:5000/redacted_test_service_b:347d034  *:8198->8998/tcp,*:8199->8999/tcp\r\nogu9irz557d6        redacted_test_service_c  replicated          0/3                 registry.MYDOMAIN:5000/redacted_test_service_c:347d034  *:8298->8998/tcp,*:8299->8999/tcp\r\nuuqexaegfibt        redacted_test_service_d  replicated          0/0                 registry.MYDOMAIN:5000/redacted_test_service_d:347d034\r\n \r\n```\r\n\r\n**Output of `docker version`:**\r\n```\r\nClient:\r\n Version:      17.09.0-ce\r\n API version:  1.32\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:42:45 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.09.0-ce\r\n API version:  1.32 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d44a8\r\n Built:        Wed Sep 27 13:05:49 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n```\r\nContainers: 2\r\n Running: 1\r\n Paused: 0\r\n Stopped: 1\r\nImages: 4\r\nServer Version: 17.09.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: xfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: vlpem14ayl9gxmx5fmxq5bw0h\r\n Is Manager: true\r\n ClusterID: mtdknk21b5rd9msmw40xzeloa\r\n Managers: 3\r\n Nodes: 3\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 10.210.102.211\r\n Manager Addresses:\r\n  10.210.102.211:2377\r\n  10.210.102.212:2377\r\n  10.210.102.213:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 06b9cb35161009dcb7123345749fef02f7cea8e0\r\nrunc version: 3f2f8b84a77f73d38244dd690525642a72156c64\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.51-linuxkit\r\nOperating System: Alpine Linux v3.6 (containerized)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 7.787GiB\r\nName: linuxkittest1.MYDOMAIN.com\r\nID: D7YE:ZJPI:ICBY:KEIC:4OGX:Q4NZ:DPRN:SSK2:7YP3:X4E7:SJ6P:HXLT\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 55\r\n Goroutines: 212\r\n System Time: 2017-09-27T21:44:18.133551864Z\r\n EventsListeners: 1\r\nUsername: eyz77\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nRegistry Mirrors:\r\n http://registry.MYDOMAIN.com:5000/\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nI am connected to a remote docker-ce 17.09.0 dind on LinuxKit via `export DOCKER_HOST=tcp://IP:2375`. I suspect the implementation of Docker Engine is not the cause, however."},{"labels":[null,null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nWhen starting `dockerd` on a host that has no `/var/lib/docker/tmp` directory, a warning is printed:\r\n\r\n```\r\nWARN[2017-09-26T08:49:16.221459130Z] failed to rename /var/lib/docker/tmp for background deletion: rename /var/lib/docker/tmp /var/lib/docker/tmp-old: no such file or directory. Deleting synchronously\r\n```\r\n\r\nAlthough harmless, it would be nice to skip printing that warning, because it doesn't provide much useful information.\r\n\r\nThe code producing the warning is in these lines;\r\n\r\nhttps://github.com/moby/moby/blob/b6b85da6576b3e923325897aab9b12d85c014e1b/daemon/daemon.go#L1047-L1052\r\n\r\nAdding a check for `os.IsNotExist(err)` would probably do\r\n"},{"labels":[null,"enhancement"],"text":"When scaling service down with `docker service scale` a task is being killed after 1 second.\r\nIs it possible to make this timeout configurable, same as in `docker-compose scale --timeout`?\r\n"},{"labels":[null,null,"enhancement",null],"text":"If the base64url or json decoding of the X-Registry-Auth header fails, the error is swallowed and the credentials are taken as empty.\r\nhttps://github.com/moby/moby/blob/4bf8714fac11e95e835cf78eb15ba5a518c67c4b/api/server/router/image/image_routes.go#L98-L107\r\n\r\nThe code comments states:\r\n```\r\n\t// for a pull it is not an error if no auth was given\r\n\t// to increase compatibility with the existing api it is defaulting to be empty\r\n```\r\n\r\nBut this situation is not that no auth was given, but that it was badly encoded. If there is a backward compatibility issue, there should be a better way to tackle it than ignoring the header entirely on error."},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nWhile calling `docker volume prune`, the pruning of a volume doesn't create an event entry.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker events > events.log &` to listen docker events\r\n2. `docker volume create chazam` to create a test volume\r\n3. `docker volume prune` to delete volumes\r\n\r\n**Describe the results you received:**\r\nOn the events.log file you should, only, see the creation event:\r\n`2017-09-23T07:10:44.965370986Z volume create chazam (driver=local)`\r\n\r\n**Describe the results you expected:**\r\nI should expect to have an event entry on the volume deletion.\r\n\r\n**Problem localisation**\r\nThe problem is on the `daemon/prune.go` file. More precisely the `VolumesPrune` function, line `143`.\r\nWe should call `err = daemon.VolumeRm(v.Name(), false)` instead of the actual `err = daemon.volumes.Remove(v)`.\r\n\r\n**Output of `docker version`:**\r\n```\r\nClient:\r\n Version:      17.06.0-ce\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:15:15 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.06.0-dev\r\n API version:  1.32 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   c982ee8\r\n Built:        Sat Sep 23 07:09:43 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 0\r\nServer Version: 17.06.0-dev\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 06b9cb35161009dcb7123345749fef02f7cea8e0\r\nrunc version: 1c81e2a794c6e26a4c650142ae8893c47f619764\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.41-moby\r\nOperating System: Debian GNU/Linux 9 (stretch) (containerized)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 1.952GiB\r\nName: 977aaa405be0\r\nID: K6M2:ALHJ:AVJ5:XVW7:E4X5:W34D:35XQ:K6HK:YVOF:EA2N:U4BO:6QRT\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 17\r\n Goroutines: 28\r\n System Time: 2017-09-23T07:12:39.720242284Z\r\n EventsListeners: 1\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n"},{"labels":["enhancement",null],"text":"**Description**\r\n\r\nI cannot use `docker events --filter config=allen`to filter specific config events. And node, service, secret as well.\r\n\r\n**Steps to reproduce the issue:**\r\n1. using `docker events --filter config=allen` to only filter confg events named allen\r\n2. using `echo abc | docker secret create abc -`\r\n3. it still gets following results:\r\n```\r\n[root@localhost ~]# docker events --filter config=allen\r\n2017-09-21T18:01:36.175357979-04:00 secret create nsrrb915tbww8ymn5dfrsgcs9 (name=abc)\r\n```\r\n\r\n**Describe the results you received:**\r\n```\r\n[root@localhost ~]# docker events --filter config=allen\r\n2017-09-21T18:01:36.175357979-04:00 secret create nsrrb915tbww8ymn5dfrsgcs9 (name=abc)\r\n```\r\n\r\n**Describe the results you expected:**\r\nNone events but config events named allen.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n[root@localhost ~]# docker version\r\nClient:\r\n Version:      17.06.2-ce\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   cec0b72\r\n Built:        Tue Sep  5 19:59:06 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.06.2-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   cec0b72\r\n Built:        Tue Sep  5 20:00:25 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n[root@localhost ~]# docker info\r\nContainers: 3\r\n Running: 3\r\n Paused: 0\r\n Stopped: 0\r\nImages: 1\r\nServer Version: 17.06.2-ce\r\nStorage Driver: overlay\r\n Backing Filesystem: xfs\r\n Supports d_type: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: ow8mb55lw3hzo0c0zuret6g5e\r\n Is Manager: true\r\n ClusterID: pdilqojdnsie9qpvdx6no1txt\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Root Rotation In Progress: false\r\n Node Address: 192.168.1.103\r\n Manager Addresses:\r\n  192.168.1.103:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 6e23458c129b551d5c9871e5174f6b1b7f6d1170\r\nrunc version: 810190ceaa507aa2727d7ae6f4790c76ec150bd2\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.10.0-693.2.2.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1.796GiB\r\nName: localhost.localdomain\r\nID: CAAD:DJJS:IABW:AOX3:PKRV:F7QQ:VVIB:PQ6Y:63IF:7NXE:DOOZ:B6ZE\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"Currently, `docker info` lists out all plugins currently available, whether embedded drivers or plugin pulled to the environment in the newer format. \r\n\r\n```\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay store/weaveworks/net-plugin:2.0.1\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\n```\r\n\r\n*Problem statement*\r\nOutput of this command shows a Network plugin which has been pulled from Docker Store. However, it is not showing a logging plugin, similarly pulled from Docker Store, as available.\r\n\r\n*Solution*\r\nEnsure that logging plugins are visible in the environment. \r\n\r\nNote that this plugin from Sumo Logic was installed to validate this functionality -- \r\nhttps://store.docker.com/plugins/sumologic-logging-plugin"},{"labels":[null,"enhancement"],"text":"Currently, `docker info` lists out all plugins currently available, whether embedded drivers or plugin pulled to the environment in the newer format. \r\n\r\n```\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay store/weaveworks/net-plugin:2.0.1\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\n```\r\n\r\n*Problem statement*\r\nOutput of this command doesn't specify which plugin is actually configured and in use in the environment. This makes for a less targeted support/debugging understanding.\r\n\r\n*Solution*\r\nCreate additional output option of `docker info` which specifies the plugin in use. "},{"labels":["enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\nDocker on Windows Server 2016 requires the Hyper-V and Container roles to be installed. If the Container role is not installed the error message in the debug logs is a failure in a HNS (Host Networking Service) call and provides no clue that the Container role is missing.\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Install a WS2016 machine/VM\r\n2. Install the Hyper-V role (`Add-WindowsFeature -Name Containers`\r\n3. Install Docker (https://docs.microsoft.com/en-us/virtualization/windowscontainers/quick-start/quick-start-windows-server or https://docs.docker.com/engine/installation/windows/docker-ee/)\r\n\r\n**Describe the results you received:**\r\ndockerd starts and runs normally\r\n\r\n**Describe the results you expected:**\r\ndockerd fails to start with the following message in the debug log:\r\n`Error starting daemon: Error initializing network controller: hnsCall failed in Win32: Class not registered (0x800040154)`\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\nDoesn't run in this case, but I was using `17.10.0-ee-preview-1`, commit `139fcd3`\r\n\r\n**Output of `docker info`:**\r\n\r\nDoesn't run in this case\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nAzure VMs\r\nPer @dineshgovindasamy there is no check for the Container role being present"},{"labels":[null,"enhancement"],"text":"Follow-up of https://github.com/moby/moby/issues/34620\r\n\r\nAs mentioned in the original bug, [logging change introduced in 1.13](https://github.com/moby/moby/pull/22982) is not compatible with the existing tools for collecting logs. There are several scenarios when this change severely breaks logs collection. Example: if a message is a JSON object longer than 16KB and the logs collector is not prepared, parsing will be broken.\r\n\r\nThis feature is a breaking change that should have been made configurable in the first place. Right now 16KB is [_constant in the code_](https://github.com/nalind/docker/blob/master/daemon/logger/copier.go#L13) picked for better [_artificial benchmark results_](https://github.com/moby/moby/pull/22982#issuecomment-224042594). Moreover, this breaking change was described in the release notes as `Improve logging of long log lines` which tells _nothing_ about the change.\r\n\r\nThis functionality has to be configurable: it should be possible to change the limit and/or turn the feature off completely. Please also backport this to all affected releases.\r\n\r\n/cc @piosz @fgrzadkowski @thockin @igorpeshansky"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Run `docker run i3wm/travis-base`\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\n% docker run i3wm/travis-base\r\nUnable to find image 'i3wm/travis-base:latest' locally\r\ndocker: Error response from daemon: manifest for i3wm/travis-base:latest not found.\r\nSee 'docker run --help'.\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nAn error message indicating that there is no “latest” **tag** for i3wm/travis-base. Specifically, I never read about “manifest” in the Docker context before. Replacing “manifest for” with “tag” would already be a vast improvement, if we can’t be any more specific about the error.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.1\r\n API version:  1.26\r\n Go version:   go1.7.6\r\n Git commit:   092cba3\r\n Built:        Wed Jun  7 02:09:42 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.1\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.6\r\n Git commit:   092cba3\r\n Built:        Wed Jun  7 02:09:42 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 49\r\n Running: 1\r\n Paused: 0\r\n Stopped: 48\r\nImages: 52\r\nServer Version: 1.13.1\r\nStorage Driver: devicemapper\r\n Pool Name: docker-254:0-22831114-pool\r\n Pool Blocksize: 65.54 kB\r\n Base Device Size: 10.74 GB\r\n Backing Filesystem: ext4\r\n Data file: /dev/loop0\r\n Metadata file: /dev/loop1\r\n Data Space Used: 6.018 GB\r\n Data Space Total: 107.4 GB\r\n Data Space Available: 101.4 GB\r\n Metadata Space Used: 8.954 MB\r\n Metadata Space Total: 2.147 GB\r\n Metadata Space Available: 2.139 GB\r\n Thin Pool Minimum Free Space: 10.74 GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: false\r\n Deferred Deletion Enabled: false\r\n Deferred Deleted Device Count: 0\r\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\r\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\r\n Library Version: 1.02.142 (2017-07-20)\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version:  (expected: aa8187dbd3b7ad67d8e5e3a15115d3eef43a7ed1)\r\nrunc version: N/A (expected: 9df8b306d01f59d3a8029be411de015b7304dd8f)\r\ninit version: N/A (expected: 949e6facb77383876aeff8a6944dde66b3089574)\r\nKernel Version: 4.12.0-1-amd64\r\nOperating System: Debian GNU/Linux buster/sid\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 31.33 GiB\r\nName: midna\r\nID: 2BTC:X26J:T55C:4Z5N:ZYGL:622D:4WTG:I5YD:UMCS:VQZF:JGWQ:STD6\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: stapelberg\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement",null],"text":"**Description**\r\nWhen using larger image layers, gzip can perform fairly slowly in serial operation.  It's a quick fix to replace golang gzip with native parallel gzip (pigz/unpigz) to dramatically speed things up while still maintaining backwards compatibility with standard gzip.  I've done some tests in Moby and find it's well worth enabling pigz via environment variable USE_PIGZ=Y added to /etc/sysconfig/docker (EnvironmentFile in systemd unit).  A decompression comparison of performance from debug logs can be found below, showing about twice the performance for images > 1MB composed of more than one gzip block.  As compressed gzip blocks are variable length without length in header, decompression is fairly limited in performance as blocks are scanned for.\r\n\r\nLayer compression scales much better - almost linearly with CPU count up to memory and bus limitations (10x faster on my 12 core machine) but I haven't yet implemented it into CompressStream.\r\n\r\nThe following log excerpts are taken with -D (debug) and devicemapper thinpool as storage engine.  overlay2, etc don't seem to show untar time but do reflect similar results.\r\n_format:\r\ndocker command (compressed size, expanded size)\r\nStandard gzip runtime:\r\nUSE_PIGZ=Y runtime:_\r\n\r\ndocker pull fedora (75MB expands to 231MB)\r\ndockerd[22526]: time=\"2017-09-06T12:45:07.768143343+01:00\" level=debug msg=\"Untar time: 6.99320561s\"\r\ndockerd[17897]: time=\"2017-09-06T12:43:52.809042830+01:00\" level=debug msg=\"Untar time: 3.675086276s\"\r\n\r\ndocker pull shoop/oel7 (120MB expands to 352MB)\r\ndockerd[3836]: time=\"2017-09-07T15:11:01.935045814+01:00\" level=debug msg=\"Untar time: 9.368929673s\"\r\ndockerd[8738]: time=\"2017-09-07T15:13:48.979072649+01:00\" level=debug msg=\"Untar time: 4.869739837s\"\r\n\r\ndocker pull bash\r\n2017-09-06T12:53:33.857064636+01:00\" level=debug msg=\"Untar time: 0.056180032s\"\r\n2017-09-06T12:52:35.118040346+01:00\" level=debug msg=\"Untar time: 0.052013559s\"\r\n\r\n\r\n**Describe the results you currently experience:**\r\nThe container equivalent of coffee breaks while booting.\r\n\r\n**Describe the results you expected:**\r\nHalf the time spent waiting for images to be ready.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nIn my test code, I default back to normal gzip decompression for any error, such as no unpigz present or no exec permissions on it. This seems to work OK and add pigz/unpigz as soft depedencies.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n17.0.06.0-dev\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nPhysical dev box - HP Z600 2x6 Xeon.  All tests run from disk cache to eliminate I/O variable."},{"labels":["enhancement",null],"text":"Using docker-ce 17.06 `docker container ls` will not accept the same filter syntax as e.g. `docker container prune`.\r\n\r\n    docker container ls --filter \"label=foobar\"\r\n\r\nproduces the expected results whereas\r\n\r\n    docker container ls --filter \"label!=foobar\"\r\n\r\nyields \r\n\r\n    Error response from daemon: Invalid filter 'label!'\r\n\r\nThe very same negation syntax works with `docker container prune` so imho it should be working with `ls` also.\r\n\r\n    docker version\r\n    Client:\r\n     Version:      17.06.1-ce\r\n     API version:  1.30\r\n     Go version:   go1.8.3\r\n     Git commit:   874a737\r\n     Built:        Thu Aug 17 22:51:12 2017\r\n     OS/Arch:      linux/amd64\r\n    \r\n    Server:\r\n     Version:      17.06.1-ce\r\n     API version:  1.30 (minimum version 1.12)\r\n     Go version:   go1.8.3\r\n     Git commit:   874a737\r\n     Built:        Thu Aug 17 22:50:04 2017\r\n     OS/Arch:      linux/amd64\r\n     Experimental: false\r\n"},{"labels":[null,"enhancement"],"text":"Now the --mac-address option is only available for \"docker run\", not \"docker network connect\". So we can only set mac address for the first interface.\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nFor creating critical system services running on top of Docker it'd be nice to adjust the OOM score so that these services are not terminated by the OOM killer before other, less critical, userspace containers and service tasks are reaped.\r\n"},{"labels":["enhancement"],"text":"**Description**\r\nBefore I go about implementing anything, I'd like to get some folks opinion on how to implement project quota improvements. \r\n\r\nThe issues I have with project quotas today:\r\n\r\n1. Doesn't work on EXT4\r\n1. We leak quotas\r\n1. We can potentially collide with the project IDs used by others\r\n1. It's difficult to interrogate \r\n\r\nIn order to remediate this, I propose the following:\r\n- [ ] Ensure that the APIs we're using also work on EXT4, and not just XFS\r\n- [ ] Enable feature detection for quotas on both EXT4, and XFS, and try to enable them if the underying filesystem supports the feature\r\n- [ ] Allow the administrator to optionally specify a range of project IDs that Docker should use. \r\n- [ ] On deletion of a layer, we interrogate the project ID of the top level of the layer. If it has a non-0 number, and that number has a non-0 (infinite) limit, we delete the quota limits. This prevents the project quota code from needing persistence across reboots, or similar, outside of the R/W layers.\r\n- [ ] The overlayfs2 driver on Docker inspect exposes the project ID. I think that this is better than trying to expose the quota information directly.\r\n\r\nAgree? Disagree? \r\nCC: @cpuguy83 @dmcgowan \r\n"},{"labels":[null,"enhancement"],"text":"Nowadays it's possible to set project quotas if the underlying filesystem is XFS and uses `overlay2` as the graph driver (see https://github.com/moby/moby/blob/master/daemon/graphdriver/overlay2/overlay.go) but it only allows setting a limit on disk blocks.\r\n\r\nThat's very good but we could extend the functionality to also support `inodes` as it's just an extra field defined in `fs_disk_quota`:\r\n\r\n```c\r\ntypedef struct fs_disk_quota {\r\n// ...\r\n\t__u64\t\td_blk_hardlimit;/* absolute limit on disk blks */\r\n\t__u64\t\td_blk_softlimit;/* preferred limit on disk blks */\r\n\t__u64\t\td_ino_hardlimit;/* maximum # allocated inodes */\r\n\t__u64\t\td_ino_softlimit;/* preferred inode limit */\r\n// ...\r\n} fs_disk_quota_t;\r\n```\r\n\r\n(from https://github.com/torvalds/linux/blob/42ff72cf27027fa28dd79acabe01d9196f1480a7/include/uapi/linux/dqblk_xfs.h#L57-L60)\r\n\r\nMy proposal is to update `Quota` to include  `inodes`:\r\n\r\nhttps://github.com/moby/moby/blob/184cea5ff710abde25547749e5608b24a255ba09/daemon/graphdriver/quota/projectquota.go#L64-L66 \r\n\r\nIn `overlay.go` we could check for the option `overlay2.inodes`\r\n\r\nhttps://github.com/moby/moby/blob/184cea5ff710abde25547749e5608b24a255ba09/daemon/graphdriver/overlay2/overlay.go#L233-L240\r\n\r\nand then set `Quota.Inodes` properly.\r\n\r\nWith that it's a matter of setting the values in the struct:\r\n\r\nhttps://github.com/moby/moby/blob/184cea5ff710abde25547749e5608b24a255ba09/daemon/graphdriver/quota/projectquota.go#L180-L181\r\n\r\n(naturally that should come with some bound checking, etc).\r\n\r\nI'd be willing to implement the feature if desired.\r\n\r\nWdyt?\r\n\r\nThx!\r\n\r\n---\r\n\r\nUpdate: we also need to update the bitmask:\r\n\r\nhttps://github.com/moby/moby/blob/184cea5ff710abde25547749e5608b24a255ba09/daemon/graphdriver/quota/projectquota.go#L179\r\n\r\nso that it includes \r\n\r\n```c\r\n#define FS_DQ_ISOFT\t(1<<0)\r\n#define FS_DQ_IHARD\t(1<<1)\r\n```\r\n"},{"labels":[null,"enhancement",null],"text":"Migrated from https://github.com/docker/docker.github.io/issues/4300\r\n\r\nIf I want to add a line in in /etc/docker/daemon.json, then I have to change the old last line, too. Before:\r\n```\r\n{\r\n        \"graph\": \"/export/docker-data\",\r\n        \"storage-driver\": \"overlay\"\r\n}\r\n```\r\nAfter:\r\n```\r\n{\r\n        \"graph\": \"/export/docker-data\",\r\n        \"storage-driver\": \"overlay\",\r\n        \"fixed-cidr\": \"10.20.0.0/16\"\r\n}\r\n```\r\nSee the comma I had to insert just to make json happy? On a \"diff\" output I see 2 changed lines, even though only a single option has been added. This leads to merge conflicts with your favorite version control system.\r\n\r\nI'd love to write \r\n```\r\n{\r\n        \"graph\": \"/export/docker-data\",\r\n        \"storage-driver\": \"overlay\",\r\n}\r\n```\r\nright from the start.\r\n"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nThe Docker builder will complain that `COPY` needs two arguments even though two has been specified. This can be reproduced by using the `--from` multi-stage build flag along with a source folder.\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create this Dockerfile.\r\n```Dockerfile\r\nFROM alpine AS test\r\nFROM scratch\r\nCOPY --from=test .\r\n```\r\n2. Build the Dockerfile.\r\n```\r\n$ docker build .\r\n```\r\n\r\n**Describe the results you received:**\r\n```\r\n$ docker build .\r\nSending build context to Docker daemon  86.15MB\r\nStep 1/3 : FROM alpine AS test\r\nlatest: Pulling from library/alpine\r\n88286f41530e: Pull complete\r\nDigest: sha256:1072e499f3f655a032e88542330cf75b02e7bdf673278f701d7ba61629ee3ebe\r\nStatus: Downloaded newer image for alpine:latest\r\n ---> 7328f6f8b418\r\nStep 2/3 : FROM scratch\r\n --->\r\nStep 3/3 : COPY --from=test .\r\nCOPY requires at least two arguments\r\n```\r\n\r\n**Describe the results you expected:**\r\nPerhaps a less misleading error message given that I technically do indeed have two arguments to my `COPY` instruction.\r\n\r\n**Output of `docker version`:**\r\nTested in a PWD instance.\r\n```\r\n$ docker version\r\nClient:\r\n Version:      17.06.1-ce-rc1\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   77b4dce\r\n Built:        Fri Jul 14 07:32:23 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.06.1-ce-rc1\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   77b4dce\r\n Built:        Fri Jul 14 07:33:35 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n$ docker info\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 1\r\nServer Version: 17.06.1-ce-rc1\r\nStorage Driver: overlay2\r\n Backing Filesystem: xfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 6e23458c129b551d5c9871e5174f6b1b7f6d1170\r\nrunc version: 810190ceaa507aa2727d7ae6f4790c76ec150bd2\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-1031-aws\r\nOperating System: Alpine Linux v3.6 (containerized)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 60GiB\r\nName: node1\r\nID: XJLX:XIAU:L3DM:S62V:WEPY:562P:G6FW:XH54:QSON:SSL5:WVFC:PMQV\r\nDocker Root Dir: /graph\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 17\r\n Goroutines: 26\r\n System Time: 2017-08-22T19:12:54.4959394Z\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.1\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\nWARNING: bridge-nf-call-iptables is disabled\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\n```"},{"labels":[null,"enhancement"],"text":"Add optional init flag to `service create` for init feature since Docker v1.13\r\n\r\nThe PR in Swarmkit is docker/swarmkit#2350\r\n\r\nRelated to  moby/moby#25303 "},{"labels":[null,null,"enhancement",null],"text":"Hi !\r\n\r\nI'm writing a tool (in Python) to get the logs from any containers through the Docker Engine's HTTP API.\r\n\r\nAfter reading [the doc](https://docs.docker.com/engine/api/v1.30/), and trying to implement something, I founded that the protocol used seems to be different from the one for Attach, it seems to be :\r\n\r\n```{\\xFF}\\r\\n{\\xFF}{datas}```\r\n\r\nWhere the first hex number is the size of the message, and the second one seems to be (empirical deduction) the number of bytes that are supposed to be ignored before the start of the actual payload (so the log).\r\nAnd it seems to work, for some containers at least ...\r\n\r\n**EDIT** : The second seems to be the number of bytes that are supposed to be ignored IF the container has ```tty: false```, otherwise, it seems to be the start of the log **END of the edit**\r\n\r\nWith one strange deduction : if ```\\x07``` is found, then it overrides the second {\\xFF} size, and the message starts after this value...\r\n\r\nAm I missing a part of the documentation, or am I not using the HTTP API in a wrong way ?\r\n\r\nThanks for your time,\r\n\r\n**NB** : The protocol above seems to be specific for the logs endpoint, it's not working for the events' endpoint.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.05.0-ce\r\n API version:  1.29\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:10:54 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.05.0-ce\r\n API version:  1.29 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:10:54 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n"},{"labels":["enhancement"],"text":"At the moment the only way of detecting a containers name was auto-generated is working backwards from the name generator, this is somewhat fragile. \r\n\r\nI would like to be allowed to specify a \"prefix\" in the docker config that all auto-generated names have, that way I can very trivially tell programatically if a containers name was automatically generated or not. "},{"labels":[null,"enhancement"],"text":"Docker composes has the notion of project name (a prefix for the services). It may be set with env. var. $COMPOSE_PROJECT_NAME.\r\n\r\nIt would be good if we can use it with COPY --from inside Dockerfiles:\r\n\r\n```\r\nFROM ubuntu:xenial\r\n\r\nCOPY --from=${COMPOSE_PROJECT_NAME}_container /opt /opt\r\n```\r\n\r\nIt gives error:\r\n\r\n```\r\nERROR: Service '...' failed to build: invalid from flag value ${COMPOSE_PROJECT_NAME}_container: invalid reference format: repository name must be lowercase\r\n```"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nIs it possible to support multiple user namespace in the future? For example,\r\ndocker user create testuser\r\ndocker run -it --userns testuser ubuntu /bin/bash\r\n\r\nBecause linux quota command limit disk quota based on uid and gid. \r\nSo we can limit the disk usage of each user while multiple user namespace share one volume.\r\nBut now One docker deamon only support to create one user namespace.\r\n\r\nOf course, I have a successful attempt on  limiting volume usage for one user namespace.\r\n\r\n"},{"labels":[null,"enhancement"],"text":"I would ask that we consider ignoring instead of error on logging options that do not pertain to the default logger.\r\n\r\nThe reasoning for this is that from a devops perspective, a system admin can setup defaults in the `daemon.json` config for multiple different loggers, like Splunk, but leave the default logger as jsonfile so that `docker logs` work. \r\n\r\nThen at run-time if the logging driver is setup to `splunk` it can use the defaults provided in the `daemon.json` file without needing to know all the particulars of making the logger work, if it is not specified, `jsonfile` still works. \r\n"},{"labels":[null,"enhancement",null],"text":"I am looking for a way run a service on couple for labels with OR\r\nI am running swarm on AWS Auto scale, each autoscale group has its engine label and launch.\r\nso I have 2 Autoscale groups with 2 engine labels\r\n1. aws-instance=prod-spot-xl \r\n2. aws-instance=prod-xl\r\n\r\nBefore running sports I was doing something like this:\r\n```\r\nversion: \"3\"\r\nservices:\r\n  app:\r\n    image: nginx:latest\r\n    ports:\r\n      - \"86:80\"\r\n    deploy:\r\n      placement:\r\n        constraints:\r\n          - engine.labels.aws-instance == prod-xl\r\n      mode: global\r\n```\r\n\r\nAnd now I want to run the same service on the 2 labels with mode global for doing autoscale easy.\r\nBut, Constraints logic is with AND and not OR\r\n\r\nSo I want to do something like this:\r\n```\r\nversion: \"3\"\r\nservices:\r\n  app:\r\n    image: nginx:latest\r\n    ports:\r\n      - \"86:80\"\r\n    deploy:\r\n      placement:\r\n        constraints:\r\n          or:\r\n            - engine.labels.aws-instance == prod-xl \r\n            - engine.labels.aws-instance == prod-spot-xl\r\n      mode: global\r\n```\r\n\r\nAm I missing something or this feature is not possible?\r\n\r\nI am running latest swarm version:\r\n```\r\ndocker version\r\nClient:\r\n Version:      17.06.0-ce\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:23:31 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.06.0-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:19:04 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\nInfo:\r\n```\r\ndocker info\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 10\r\nServer Version: 17.06.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: aj52ywaiwn7lvd261cjr8bmq5\r\n Is Manager: true\r\n ClusterID: 8780czo41mcbjd71n0458aaiy\r\n Managers: 3\r\n Nodes: 31\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Root Rotation In Progress: false\r\n Node Address: 172.19.12.21\r\n Manager Addresses:\r\n  172.19.10.213:2377\r\n  172.19.11.175:2377\r\n  172.19.12.21:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: cfb82a876ecc11b5ca0977d1733adbe58599088a\r\nrunc version: 2d41c047c83e09a6d61d464906feb2a2f3c52aa4\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-89-generic\r\nOperating System: Ubuntu 16.04.2 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 15.67GiB\r\nName: swarm-manager03\r\nID: QF2R:TWS4:ECJZ:R5OZ:6SPH:QIIC:XQID:47Q4:2EOY:OAKP:SX6B:JNFJ\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```"},{"labels":[null,null,"enhancement"],"text":"Allow the uses of overlay2 in the 3.10.0-693.el7.x86_64 kernel now that Redhat officially supports it?\r\n\r\nhttps://github.com/moby/moby/blob/1009e6a40b295187e038b67e184e9c0384d95538/daemon/graphdriver/overlay2/overlay.go#L136-L141\r\n\r\n\r\n**Steps to reproduce the issue:**\r\n1. Install docker-ce 17.0.6 on rhel 7.4\r\n2. `echo -e \"{ \\\"storage-driver\\\": \\\"overlay2\\\" }\" > /etc/docker/daemon.json; systemctl restart docker`\r\n\r\n**Describe the results you received:**\r\nDocker should restart.\r\n\r\n**Describe the results you expected:**\r\nDocker doesn't support overlay2 on rhel 7.4.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n[root@rhel74 ~]# docker -v\r\nDocker version 17.06.0-ce\r\n\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nvmware fusion. also tested on aws. "},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nWe were having an issue with one of our applications that was testing things.  In order to isolate the problem they were running with a single replica of their application and a front end service that was\r\ncalling into that replica.  For one reason or another the application crashed/was restarted, at that point the front end service got the wrong application IP address.\r\n\r\nUpon further testing it appears that during the time that a single replica service is restarted the VIP that should represent that service either goes away or is no longer returned when queried.  Here is an\r\nexample of two simple containers.  I am doing an nslookup every second for the second service.  While I remove the only task in the second service nslookup starts returning erroneous results.\r\n<!--\r\n\r\n\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Deploy a stack and replicate it to two different worker nodes.\r\n2. The stack will have two containers: one that runs the service and another that runs nslookup in a while loop.\r\n3. Remove the container running the service.\r\n4. While the container is restarting, the VIP will no longer be available.\r\n\r\n**Describe the results you received:**\r\n```\r\nNon-authoritative answer:\r\nName:   httpd\r\nAddress: 10.0.0.3\r\n\r\nServer:         127.0.0.11\r\nAddress:        127.0.0.11#53\r\n\r\n** server can't find httpd: NXDOMAIN\r\n\r\nServer:         127.0.0.11\r\nAddress:        127.0.0.11#53\r\n\r\n** server can't find httpd: NXDOMAIN\r\n\r\nServer:         127.0.0.11\r\nAddress:        127.0.0.11#53\r\n\r\n** server can't find httpd: NXDOMAIN\r\n\r\nServer:         127.0.0.11\r\nAddress:        127.0.0.11#53\r\n\r\n** server can't find httpd: NXDOMAIN\r\n\r\nServer:         127.0.0.11\r\nAddress:        127.0.0.11#53\r\n\r\n** server can't find httpd: NXDOMAIN\r\n\r\nServer:         127.0.0.11\r\nAddress:        127.0.0.11#53\r\n\r\nNon-authoritative answer:\r\nName:   httpd\r\nAddress: 10.0.0.3\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nWe expect the VIP to continue working while the container is restarting.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.03.2-ee-5\r\n API version:  1.27\r\n Go version:   go1.7.5\r\n Git commit:   fa09039\r\n Built:        Thu Jul 20 00:18:48 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.2-ee-5\r\n API version:  1.27 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   fa09039\r\n Built:        Thu Jul 20 00:18:48 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 16\r\n Running: 11\r\n Paused: 0\r\n Stopped: 5\r\nImages: 28\r\nServer Version: 17.03.2-ee-5\r\nStorage Driver: devicemapper\r\n Pool Name: docker-253:0-69182140-pool\r\n Pool Blocksize: 65.54 kB\r\n Base Device Size: 10.74 GB\r\n Backing Filesystem: xfs\r\n Data file: /dev/loop0\r\n Metadata file: /dev/loop1\r\n Data Space Used: 1.989 GB\r\n Data Space Total: 107.4 GB\r\n Data Space Available: 13.65 GB\r\n Metadata Space Used: 3.465 MB\r\n Metadata Space Total: 2.147 GB\r\n Metadata Space Available: 2.144 GB\r\n Thin Pool Minimum Free Space: 10.74 GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: false\r\n Deferred Deletion Enabled: false\r\n Deferred Deleted Device Count: 0\r\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\r\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\r\n Library Version: 1.02.135-RHEL7 (2016-11-16)\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: w9tsy2wuqkftzpkahokgbrxuu\r\n Is Manager: true\r\n ClusterID: jgqknwi50p8h1qjxos0x7i9nk\r\n Managers: 1\r\n Nodes: 2\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  External CAs:\r\n    cfssl: https://192.168.1.6:12381/api/v1/cfssl/sign\r\n Node Address: 192.168.1.6\r\n Manager Addresses:\r\n  192.168.1.6:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 6c463891b1ad274d505ae3bb738e530d1df2b3c7\r\nrunc version: 54296cf40ad8143b62dbcaa1d90e520a2136ddfe\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.10.0-514.6.1.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 3.702 GiB\r\nName: mitaka\r\nID: PWUO:YFX4:O5IY:SRWH:X7XO:TZ6Q:GMEZ:4CWX:7UVW:5765:NGJ7:JG2C\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 112\r\n Goroutines: 212\r\n System Time: 2017-07-31T10:36:07.404498896-04:00\r\n EventsListeners: 2\r\nUsername: dockeradmin\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\nWhile building Docker images using Dockerfile, command output doesn't display the % of progress. It would be nice to have, so that user will know the actual build progress. Discussed this issue during the MTA workshop. \r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Use docker image build command to build any docker image. \r\n2.\r\n3.\r\n\r\n**Describe the results you received:**\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n17.03.2--ee.5\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**"},{"labels":[null,"enhancement"],"text":"I am running a service in swarm cluster with 2 replicas.\r\nThe worker nodes are in AWS Autoscaling group.\r\n\r\nWhen I terminate one of the worker nodes, swarm as expected restart the container on a new node, and keeps a record of the terminated node in service ps.\r\nThen when I update image of the service I get this output.\r\n\r\n```\r\nID                  NAME                                    IMAGE                                          NODE                        DESIRED STATE       CURRENT STATE           ERROR               PORTS\r\ne673fo9nxt8l        test_sample-service.1       sample-service:v7.147as   ip-10-30-2-23               Running             Running 2 hours ago\r\npyguz0w56ibh         \\_ test_sample-service.1   sample-service:v7.143     zl0l7nk66fpo895ufs8yg9rh7   Shutdown            Running 3 weeks ago\r\nnagqyws4tu3m        test_sample-service.2       sample-service:v7.147as   ip-10-30-3-69               Running             Running 2 hours ago\r\n9hgpug6vtx4l         \\_ test_sample-service.2   sample-service:v7.143     w3lblfx44vf94hab9v3mmqho6   Shutdown            Running 3 weeks ago\r\n```\r\n\r\nThe current state is not reflecting the real actual state of the container, That was actually terminated along with the AWS instance termination.\r\n\r\nIn addition the name changed to the node ID for some reason"},{"labels":[null,"enhancement",null],"text":"The responsible of the `client` package is to transform a go struct into an HTTP request, perform the HTTP request, and convert the HTTP response into a go struct + error.\r\n\r\n`client/service_update.go` and `client/service_create.go` have a bunch of application logic that does not belong in this package. We should remove it and move it into a separate package.\r\n\r\nIt looks like this logic was first introduced in #32388 and subsequently updated in #33239, and #33575.\r\n\r\nThis is related to #34242 and https://github.com/docker/cli/pull/386 \r\n\r\ncc @thaJeztah @nishanttotla @aaronlehmann "},{"labels":[null,"enhancement"],"text":"Hi, is there a way to bind a specific network interface for a particular overlay network? The reason for this is that DigitalOcean (as well as other cloud providers) offer private networking which allows you to communicate to other VMs within the same data center (region) without going over the public internet and being charged.\r\n\r\nIf I'm correct, the network interface that each overlay network created on a node goes over is specified when you join the swarm with `--advertise-addr`. Is it possible to somehow use this as a default, but in addition, when we create a new overlay network we can explicitly specify the network interface we want to use?\r\n\r\nie. `docker network create -d overlay --iface eth1 myoverlaynetwork` or `docker network create -d overlay --internal-ip PRIVATE_IP myoverlaynetwork`\r\n\r\nIn my opinion, this is not even an ideal approach to the problem because not every distribution names their network interfaces with ethX.\r\n\r\nUltimately, I want to define these overlay networks in a service stack config so I can `docker stack deploy` my services so that they go over the private network instead of the public internet.\r\n\r\nAlso, I have several services running in different regions/data centers, which communicate with each other through a global overlay network. The services that are running in the same region, however, should communicate through the private network interface which I am trying to do via another overlay network."},{"labels":[null,"enhancement"],"text":"Similar to other log drivers available with docker and dockerd, a group of us would like to contribute a logging driver for OMS that pumps messages from containers into OMS. We already have a good start.\r\n\r\nReading through _CONTRIBUTING.md_ we wanted to get this on the radar and have any discussions as needed prior to our initial PR. We want to contribute this into moby so its readily available and also because separate plugin modules are not yet supported on Windows (at least in the stable releases of Docker).\r\n\r\nCurrent log driver inputs are merely the workspace ID (aka customer ID) and shared key (either primary or secondary key). Similar to the splunk driver we are batching messages and are considering supporting options for tweaking batch sizes. The current driver also puts messages back in the queue to support `docker logs`."},{"labels":["enhancement",null],"text":"Currently the secrets files inside a Windows Docker container are restricted so only administrative and system accounts can access them. That limits the use for apps running as least-privilege accounts, because the app process can't access the secret.\r\n\r\nThis is an issue for ASP.NET apps in containers. The `IIS_IUSRS` group and application pool identities don't have access to secrets files. For web apps to read (say) connection strings from secrets, they need to run as `LocalSystem`, which is much higher privilege:\r\n\r\n```\r\nRUN Import-Module WebAdministration; `\r\n    Remove-Website -Name 'Default Web Site'; `\r\n    New-WebAppPool -Name 'ap-signup'; `\r\n    Set-ItemProperty IIS:\\AppPools\\ap-signup -Name managedRuntimeVersion -Value v4.0; `\r\n    Set-ItemProperty IIS:\\AppPools\\ap-signup -Name processModel.identityType -Value LocalSystem; `\r\n    New-Website -Name 'web-app' `\r\n                -Port 80 -PhysicalPath 'C:\\web-app' `\r\n                -ApplicationPool 'ap-signup'\r\n```\r\n(from [dockersamples/newsletter-signup](https://github.com/dockersamples/newsletter-signup) sample app).\r\n\r\nFor a given image, the SIDs for the built-in accounts are well-known, so we could explicitly grant access by SID when the secret is assigned to the service."},{"labels":["enhancement",null,null,null],"text":"On Windows 10 I tried to change the default storage size of containers by configuring daemon.json and adding an option \"storage-opts\": [\"size=50G\"]. This does not have any effect. \r\n\r\nWhen I execute `docker run microsoft/nanoserver powershell -c get-psdrive` I would expect to have a C drive with 50G instead it has default 20G. When I specify storage-opt for individual docker run it works. However this is not a solution for me because I need to have more disk space during a build process.\r\n\r\ndocker version:\r\nClient:\r\n Version:      17.06.0-ce\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:30:30 2017\r\n OS/Arch:      windows/amd64\r\n\r\nServer:\r\n Version:      17.06.0-ce\r\n API version:  1.30 (minimum version 1.24)\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 22:19:00 2017\r\n OS/Arch:      windows/amd64\r\n Experimental: false"},{"labels":[null,"enhancement"],"text":"This is a somewhat simplified scenario, but I would like to know how people deal with these kinds of problems.\r\n\r\nI have a docker swarm consisting of 3 managers and a service that \r\nwrites to a network volume. The service should therefore run only once, \r\nso there are no conflicts. I would like it to restart on a different \r\nhost if it's current host dies. The problem is that if the current host \r\nloses it's connection to the other hosts. The service continues running \r\non that host and is also restarted on one of the two remaining hosts \r\nbecause they still have the quorum. This results in 2 instances of the \r\nservice which could lead to conflicts.\r\n\r\n\r\n\r\n"},{"labels":[null,"enhancement",null,null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nThe `daemon` should not install a `plugin` if there is no `compatible OS/arch` version available. \r\n\r\n**Steps to reproduce the issue:**\r\n1. From `ppc64le` machine, run  docker plugin install `<org/plugin:linux_x86_64_tag>`\r\n\r\n**Describe the results you received:**\r\n```\r\nroot@sys-88148:/home/u0013538/metrics-plugin# docker version\r\nClient:\r\n Version:      17.06.0-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   8f0eeed28587\r\n Built:        Thu Jul  6 18:34:38 2017\r\n OS/Arch:      linux/ppc64le\r\n\r\nServer:\r\n Version:      17.06.0-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   8f0eeed28587\r\n Built:        Thu Jul  6 15:46:03 2017\r\n OS/Arch:      linux/ppc64le\r\n Experimental: false\r\nroot@sys-88148:/home/u0013538/plugin# docker plugin install index-stage.docker.io/rogaha/plugin:1.0.0.linux-x86_64-test\r\nPlugin \"index-stage.docker.io/rogaha/plugin:1.0.0.linux-x86_64-test\" is requesting the following privileges:\r\n - network: [host]\r\nDo you grant the above permissions? [y/N] Y\r\n1.0.0.linux-x86_64-test: Pulling from rogaha/plugin\r\n24d4990b8bbd: Download complete\r\nDigest: sha256:1196cdd8717c1337c422157fb7993e7053cab0730c70e07a9392d76f527d0c5c\r\nStatus: Downloaded newer image for index-stage.docker.io/rogaha/plugin:1.0.0.linux-x86_64-test\r\n\r\nError response from daemon: dial unix /run/docker/plugins/828bdef1269ad1555d236be26e51d32fe41cca1778a116f57a3d2b90565a8e93/metrics.sock: connect: no such file or directory\r\nroot@sys-88148:/home/u0013538/plugin#\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\n```\r\nroot@sys-88148:/home/u0013538/plugin# docker plugin install index-stage.docker.io/rogaha/plugin:1.0.0.linux-x86_64-test\r\nError: Plugin not available for your OS/arch.\r\n```\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.06.0-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   8f0eeed28587\r\n Built:        Thu Jul  6 18:34:38 2017\r\n OS/Arch:      linux/ppc64le\r\n\r\nServer:\r\n Version:      17.06.0-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   8f0eeed28587\r\n Built:        Thu Jul  6 15:46:03 2017\r\n OS/Arch:      linux/ppc64le\r\n Experimental: false\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"Hello,\r\n\r\nI find it useful to render my stack specification files from templates, therefore I deploy them with:\r\n\r\n`p2 -t stack.yml | docker stack deploy --with-registry-auth --prune --compose-file /dev/stdin stackname\r\n`\r\n\r\nWith the release of 17.06, I noticed that Docker subsequently looks for files in /dev/:\r\n\r\n`open /dev/config.monitoring: no such file or directory\r\n`\r\n\r\nNow, I am aware that this may be the right thing to do - look for relative paths from the compose file location - but I'd like a way to provide it as standard input.\r\n\r\nWould a PR to implement \"--compose-file -\" be accepted ?\r\n\r\nThanks"},{"labels":[null,null,null,"enhancement"],"text":"At the moment, the Dockerfile reference states the following:\r\n\r\n> A Dockerfile must start with a `FROM` instruction.\r\n\r\nHowever, this has since been changed because:\r\n\r\n> FROM instructions support variables that are declared by any ARG instructions that occur before the first FROM.\r\n\r\nThe `docs/reference/builder.md` file should be updated to reflect this change."},{"labels":[null,"enhancement"],"text":"current port mapping support this format: `host-ip:hostPort:containerPort/protocol` I am seeing a few things can be improved here:\r\n\r\n1. I tried an IPv6 address as the host-ip, seems not recognizable?\r\n2. the hostPort:containerPort support a port range, but when I do a port range, I see it creates a mapping for every individual port pair, this may be inefficient, as far as I know, iptables support a port range in a single rule; the docker-proxy mode may also be improved to support a port range in single process?\r\n3. I don't see this format support to specify a containerIP to connect, you may say the containerIP (`172.17.0.6` in this example) is determined by the runtime, but what I say is sometimes a third-party program may listen on localhost (`127.0.0.1`) only in the container, can this docker-proxy be improved to take a second parameter of which IP in the container to connect to?\r\n   for example, when I run chrome-browser inside the container, and enable its remote-debugging-port=9222, the chrome would listen on 127.0.0.1 only and since Google releases this chrome as a binary, I don't have an option to modify it and don't want to; here need a proxy program to listen on the host and forward packet into container but it create connection by another ip address (127.0.0.1) instead of the continerIP (172.17.0.6); so far I can run `socat` to create the proxy connection, but can this mode be integrated into docker-proxy?\r\n\r\nhttps://docs.docker.com/engine/reference/run/#expose-incoming-ports\r\n\r\n```console\r\n#### from 'ps f -e'\r\n\r\n 4617 ?        Sl     0:00  \\_ /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9230 -container-ip 172.17.0.6 -container-port 19230         \r\n 4625 ?        Sl     0:00  \\_ /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9229 -container-ip 172.17.0.6 -container-port 19229\r\n 4634 ?        Sl     0:00  \\_ /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9228 -container-ip 172.17.0.6 -container-port 19228\r\n 4643 ?        Sl     0:00  \\_ /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9227 -container-ip 172.17.0.6 -container-port 19227\r\n 4651 ?        Sl     0:00  \\_ /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9226 -container-ip 172.17.0.6 -container-port 19226\r\n 4659 ?        Sl     0:00  \\_ /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9225 -container-ip 172.17.0.6 -container-port 19225\r\n 4668 ?        Sl     0:00  \\_ /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9224 -container-ip 172.17.0.6 -container-port 19224\r\n 4676 ?        Sl     0:00  \\_ /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9223 -container-ip 172.17.0.6 -container-port 19223\r\n 4685 ?        Sl     0:00  \\_ /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9222 -container-ip 172.17.0.6 -container-port 19222\r\n```"},{"labels":[null,null,"enhancement"],"text":"Now that plugins on swarm are in (#33575), we need a follow-up to add support for private repos.\r\n\r\nIn the process of getting swarm plugins in, it was decided to not carry-on the existing hack for passing through registry auth from the client. In doing so, it means that plugins on swarm can only work with public images.\r\n\r\nAfter some discussion it was decided that using secrets to pass registry auth is the best way forward.\r\n\r\nCurrently swarm only distributes secrets by inspecting the `ContainerSpec` to determine what secrets are needed for a given task.\r\nSince the plugin spec is opaque to swarm, swarm cannot just inspect the plugin spec to look for secrets.\r\nSo we need to be able to have swarm distributes secrets for a task without requiring it to look into a given runtime's spec.\r\n\r\n---\r\nIdeally I think the we can just list references to secrets in the `TaskTemplate` directly... swarm will still need to look at the `ContainerSpec` for backwards compat, but for plugins it can just look at the task template.\r\n\r\nNote: This also solves the issue of giving a plugin access to a secret.\r\n\r\nWith the secret distributed, believe we can handle the registry auth bit in the plugin spec (by specifying which secret is for registry auth)."},{"labels":[null,null,"enhancement",null],"text":"**Description**\r\nSwarm: No key to tell if `--data-path-addr` was used\r\n\r\n**Steps to reproduce the issue:**\r\n1. Initialize a new swarm\r\n```\r\ndocker swarm init --advertise-addr=192.168.0.1 --data-path-addr=192.168.0.2\r\n```\r\n2. Inspect the node\r\n```\r\ndocker node inspect self\r\n```\r\n\r\n**Describe the results you received:**\r\nI got all node properties except the one set by `--data-path-addr`.\r\n\r\n**Describe the results you expected:**\r\nApart from `--advertise-addr`, `--data-path-addr` should have been present as well.\r\n\r\n```\r\n  },\r\n        \"Status\": {\r\n            \"State\": \"ready\",\r\n            \"Addr\": \"192.168.0.1\"\r\n        },\r\n        \"ManagerStatus\": {\r\n            \"Leader\": true,\r\n            \"Reachability\": \"reachable\",\r\n            \"Addr\": \"192.168.0.1:2377\"\r\n        }\r\n\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\ndocker version\r\nClient:\r\n Version:      17.06.0-ce\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:20:36 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.06.0-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:21:56 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\ndocker info\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 11\r\nServer Version: 17.06.0-ce\r\nStorage Driver: devicemapper\r\n Pool Name: docker-253:0-67374335-pool\r\n Pool Blocksize: 65.54kB\r\n Base Device Size: 10.74GB\r\n Backing Filesystem: xfs\r\n Data file: /dev/loop0\r\n Metadata file: /dev/loop1\r\n Data Space Used: 1.027GB\r\n Data Space Total: 107.4GB\r\n Data Space Available: 48.66GB\r\n Metadata Space Used: 1.704MB\r\n Metadata Space Total: 2.147GB\r\n Metadata Space Available: 2.146GB\r\n Thin Pool Minimum Free Space: 10.74GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: false\r\n Deferred Deletion Enabled: false\r\n Deferred Deleted Device Count: 0\r\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\r\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\r\n Library Version: 1.02.135-RHEL7 (2016-11-16)\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: m22q0ey5zf87ei88977dvlxyc\r\n Is Manager: true\r\n ClusterID: qrbptib70uqhs4gfhitb675ty\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Root Rotation In Progress: false\r\n Node Address: 192.168.0.1\r\n Manager Addresses:\r\n  192.168.0.1:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: cfb82a876ecc11b5ca0977d1733adbe58599088a\r\nrunc version: 2d41c047c83e09a6d61d464906feb2a2f3c52aa4\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.10.0-327.36.3.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 7.64GiB\r\nName: localhost.localdomain\r\nID: 4XCJ:T2FS:4JJA:FMEN:QAG7:NQX2:UKBS:NKEB:I5DR:IMX3:ULXK:FZAF\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: psg5\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: devicemapper: usage of loopback devices is strongly discouraged for production use.\r\n         Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\n\r\n```\r\n"},{"labels":[null,"enhancement",null],"text":"Client:\r\n Version:      17.06.0-ce\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:23:31 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.06.0-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:19:04 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n\r\nCurrently network_mode is not supported in docker stack deploy with docker-compose file.\r\n\r\n`docker stack deploy -c docker-compose.yml prod`\r\n\r\nWhat other options are available for mass deployment if all containers are using --network=host?\r\n\r\n--network=host is already supported in docker 17.06. Is there a reason it is not supported in docker-compose 3.3?\r\n\r\n\r\n"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nService container got recreated when using _\"docker service update\"_ even though the image used for _\"docker service update\"_ is the **same image** used during _\"docker service update --image\"_. By **same image**, I mean the two images shared **identical repo digest** but under **different tag**. For example: image nginx:latest and nginx:1.13 shared identical repo digest but with different tags.\r\n\r\nI wonder if it is a bug or by design? Is it possible to add a flag like \"--no-resolve-image\" but to only resolve the repo digest?\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker service create --name nginx nginx:latest\r\n2. docker service update --image nginx:1.13 nginx\r\n3. docker service ps nginx\r\n3. docker service inspect nginx\r\n\r\n**Describe the results you received:**\r\nthe nginx container has been recreated due to \"docker service update\"\r\n\r\n\r\n**Describe the results you expected:**\r\nthe nginx container shouldn't have been recreated since \"nginx:latest\" and \"nginx:1.13\" are two same images(same digest) only with different tags. \r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.06.0-ce\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:15:15 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.06.0-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:51:55 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 2\r\n Running: 1\r\n Paused: 0\r\n Stopped: 1\r\nImages: 1\r\nServer Version: 17.06.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /mnt/sda1/var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 7\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: hkeop8rz0yy1rvv7wdim0bg3q\r\n Is Manager: true\r\n ClusterID: pucz4cvrtbabkm1tckli698e8\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Root Rotation In Progress: false\r\n Node Address: 192.168.99.108\r\n Manager Addresses:\r\n  192.168.99.108:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: cfb82a876ecc11b5ca0977d1733adbe58599088a\r\nrunc version: 2d41c047c83e09a6d61d464906feb2a2f3c52aa4\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.74-boot2docker\r\nOperating System: Boot2Docker 17.06.0-ce (TCL 7.2); HEAD : 0672754 - Thu Jun 29 00:06:31 UTC 2017\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 995.8MiB\r\nName: 06\r\nID: 6RBH:UMRI:QQ6P:J53X:TOSJ:QZUT:6N4K:QCTA:ZV5Y:W2GE:SRAZ:HQMZ\r\nDocker Root Dir: /mnt/sda1/var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 39\r\n Goroutines: 151\r\n System Time: 2017-06-30T21:23:16.426357468Z\r\n EventsListeners: 1\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\n provider=virtualbox\r\nExperimental: false\r\nInsecure Registries:\r\n 10.10.1.181:5000\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"**Feature idea**\r\nSometimes you need to generate a variable to use in compose file. \r\n\r\n**Example**\r\nWhen deploying etcd, you need to pass a unique discovery address. The best way to do it is to generate it with command `curl -w \"\\n\" 'https://discovery.etcd.io/new?size=1'`. Right now, I do it by using environment variable in compose file:\r\n```\r\n  etcd:\r\n    image: elcolio/etcd:latest\r\n    deploy:\r\n      replicas: 1\r\n    command: -name etcd -discovery=${ETCD_DISCOVERY:-UNSET}\r\n```\r\nand deploy stack using command:\r\n```\r\nETCD_DISCOVERY=$(curl -w \"\\n\" 'https://discovery.etcd.io/new?size=1') \\\r\nbash -c 'docker stack deploy -c semaphore.yml semaphore'\r\n```\r\nI think this should have an simpler solution. For example if I could do \r\n```\r\n...\r\ncommand: -name etcd -discovery=$(curl -w \"\\n\" 'https://discovery.etcd.io/new?size=1')\r\n```\r\n"},{"labels":[null,null,"enhancement"],"text":"https://github.com/moby/moby/pull/32977 introduced a new size option for overlay2. The current option, `overlay2.size`, is not very clear that it applies a quota. If we add further options surrounding quotas, such as a project identifier, the grouping will be unclear. `overlay2.quota.size` would be better for future proofing.\r\n\r\nLet's not add another thing that requires a hunt through the code base to understand what it does."},{"labels":[null,"enhancement"],"text":"**Docker compose v3 depends-on does not affect creation order of services**\r\n\r\nMy compose file looks something like this:\r\n```\r\nversion: \"3\"\r\nservices:\r\n  semaphore:\r\n    image: semaphore\r\n    deploy:\r\n      replicas: 1\r\n    ports:\r\n      - \"8080:3000\"\r\n    networks:\r\n      - semaphore_network\r\n    depends_on:\r\n      - mysql\r\n      - etcd\r\n\r\n  etcd:\r\n    image: elcolio/etcd:latest\r\n    deploy:\r\n      replicas: 1\r\n    networks:\r\n      - semaphore_network\r\n    command: -name etcd -discovery=${ETCD_DISCOVERY:-UNSET}\r\n\r\n  mysql-galera:\r\n    image: perconalab/percona-xtradb-cluster:5.6\r\n    deploy:\r\n      replicas: 3\r\n    networks:\r\n      - semaphore_network\r\n    depends_on:\r\n      - etcd\r\n    environment:\r\n      - MYSQL_ROOT_PASSWORD=pass\r\n      - DISCOVERY_SERVICE=etcd:2379\r\n      - XTRABACKUP_PASSWORD=mypassword\r\n      - CLUSTER_NAME=galera\r\nnetworks:\r\n  semaphore_network:\r\n    driver: overlay\r\n```\r\nAs we can see, the launch order should be 1. etcd, 2. mysql, 3. semaphore. But when I do stack deploy, I get:\r\n```\r\ndocker stack deploy -c semaphore.yml semaphore\r\nCreating service semaphore_semaphore\r\nError response from daemon: network semaphore_semaphore_network not found\r\n```\r\n\r\nNow we can see 2 errors: \r\n1. semaphore_semaphore is being created first\r\n2. the network, that should be created before all services is not started yet...\r\n\r\n**Output of `docker version`:** \r\n```\r\nDocker version 17.05.0-ce, build 89658be\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n# docker info\r\nContainers: 11\r\n Running: 6\r\n Paused: 0\r\n Stopped: 5\r\nImages: 35\r\nServer Version: 17.05.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 139\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: it8bharqefr1k8xsmonscngsy\r\n Is Manager: true\r\n ClusterID: jbxeizda8htiwan1pth2c7kl5\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 81.2.254.225\r\n Manager Addresses:\r\n  81.2.254.225:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 9048e5e50717ea4497b757314bad98ea3763c145\r\nrunc version: 9c2d8d184e5da67c95d601382adf14862e4f2228\r\ninit version: 949e6fa\r\nKernel Version: 3.16.0-4-amd64\r\nOperating System: Debian GNU/Linux 8 (jessie)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1000MiB\r\nName: DominikFriml3\r\nID: W7MT:HWZR:4O2T:OAWW:OSQK:47FN:LXEK:INEG:EC5P:WIFO:2B34:BARB\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No memory limit support\r\nWARNING: No swap limit support\r\nWARNING: No kernel memory limit support\r\nWARNING: No oom kill disable support\r\nWARNING: No cpu cfs quota support\r\nWARNING: No cpu cfs period support\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"**docker stack deploy** with **cpuset** get error:\r\n\r\n```\r\ncpuset Additional property cpuset is not allowed\r\n```\r\n\r\nActually I globally searched [here](https://github.com/docker/compose/tree/master/compose/config), for `v3.*` there is totally no `cpuset` scheme definition there, only have 🙃\r\n\r\n```\r\n\"resource\": {\r\n  \"id\": \"#/definitions/resource\",\r\n  \"type\": \"object\",\r\n  \"properties\": {\r\n    \"cpus\": {\"type\": \"string\"},\r\n    \"memory\": {\"type\": \"string\"}\r\n  },\r\n  \"additionalProperties\": false\r\n},\r\n```\r\n\r\ndocker-compose.yml\r\n\r\n```\r\nversion: '3.2'\r\n\r\nnetworks:\r\n  default:\r\n    external:\r\n      name: gate_services_stack\r\n\r\nservices:\r\n\r\n  ms-auth-jwt:\r\n    image: ${REGISTRY}/auth-${SERVICE_TAG_SUFFIX}\r\n    environment:\r\n      SERVICE_NAME: auth-${SERVICE_TAG_SUFFIX}\r\n      MS_SERVICE_TAG: auth-${SERVICE_TAG_SUFFIX}\r\n    deploy:\r\n      mode: replicated\r\n      replicas: 2\r\n      resources:\r\n        limits:\r\n          cpuset: 0,1\r\n      restart_policy:\r\n        condition: any\r\n```\r\n\r\ndocker --version\r\n\r\n```\r\nDocker version 17.05.0-ce, build 89658be\r\n```\r\n\r\ndocker info\r\n\r\n```\r\nContainers: 2\r\n Running: 2\r\n Paused: 0\r\n Stopped: 0\r\nImages: 152\r\nServer Version: 17.06.0-ce-rc5\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: f4mk4ieawb4ogctjcry6xna1u\r\n Is Manager: true\r\n ClusterID: uhavxasxi7p5gnlvv75nekfjz\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 192.168.65.2\r\n Manager Addresses:\r\n  192.168.65.2:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: cfb82a876ecc11b5ca0977d1733adbe58599088a\r\nrunc version: 2d41c047c83e09a6d61d464906feb2a2f3c52aa4\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.31-moby\r\nOperating System: Alpine Linux v3.5\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 1.952GiB\r\nName: moby\r\nID: 4J46:TBGI:L5TE:CB4A:FM54:H3VH:N2OM:D4RO:5YI2:3WBI:A44A:63KI\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 53\r\n Goroutines: 170\r\n System Time: 2017-06-28T04:23:10.794513225Z\r\n EventsListeners: 3\r\nNo Proxy: *.local, 169.254/16\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n micro02.sgdev.vcube.com:7777\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```"},{"labels":[null,"enhancement"],"text":"Hello,\r\n\r\nI would have a feature request for docker which would be to have the possibility for the syslog logging driver to log to more than one syslog server for a docker container. Ideally I would run the container like that:\r\n\r\n```docker run --log-driver syslog --log-opt syslog-address=udp://1.2.3.4:514 --log-opt syslog-address=udp://5.6.7.8:514 alpine echo hello world```\r\n\r\nAs you can see I would be logging here to 2 different syslog servers at the same time using twice the `syslog-address` log option.\r\n\r\nCheers"},{"labels":[null,"enhancement"],"text":"Any plans to support `--from` for `ADD` as it is for `COPY`?\r\n\r\n```\r\nFROM <>\r\nADD --from=0 /<>.tar.gz /<>/<>\r\n```\r\n\r\nI could see this being a pretty powerful feature and so far I haven't been able to think of a downside to supporting it. If this ends up being something agreed on I'd be interesting in contributing this feature."},{"labels":[null,null,"enhancement"],"text":"Hello folks!\r\n\r\nI don't know if this is a bug per say but I consider we should improve this behaviour.\r\n\r\nI run Swarm 17.05CE within only one node.\r\n\r\nA week ago, I label my node like this:\r\n`docker node update --label-add DeployGRP=\"$THIS_NODE\" \"$THIS_NODE\"`\r\n\r\nToday I changed my mind and labelled it like this:\r\n`docker node update --label-add deploy_grp=\"$THIS_NODE\" \"$THIS_NODE\"`\r\n\r\nI re-deployed my containers using:\r\n`--constraint \"node.labels.DeployGRP==$THIS_NODE\"`\r\nand of course it didn't work. \r\n\r\nThen I checked was going on via:\r\n`docker service ps --no-trunc webapp`\r\n\r\nThe current state message was:\r\n`Pending 3 minutes ago`\r\n\r\nThe **issue here** is that, it didn't help me to _debug_ the problem. I think it would be fair to see a `Constraint mismatch` when a constraint is preventing the container to be deployed. \r\n\r\nMake sense?\r\nPascal | [@askpascalandy](https://twitter.com/askpascalandy)\r\n"},{"labels":[null,"enhancement"],"text":"Docker Version: Docker version 17.05.0-ce\r\nit can be the case that AWS CloudWatch Logs are un reachable.  In that case when you try to start a container it should simple throw an error and continue.  I'd much rather have loss of logs than having my service not start.\r\n\r\n steps\r\n1.  null route your connection to logs.us-west-2.amazonaws.com (it's hard to actually take the real AWS cloud watch logs offline)\r\n2. \r\n\r\n```\r\n docker -D run --log-driver=awslogs \\\r\n              --log-opt awslogs-region=us-west-2 \\\r\n              --log-opt awslogs-group=myLogGroup \\\r\n              --log-opt awslogs-create-group=true \\\r\n              busybox\r\n```\r\n\r\n```\r\nDEBU[0000] [hijack] End of stdout\r\ndocker: Error response from daemon: failed to initialize logging driver: RequestError: send request failed\r\ncaused by: Post https://logs.us-west-2.amazonaws.com/: dial tcp 127.0.10.10:443: getsockopt: connection refused.\r\nERRO[0000] error getting events from daemon: net/http: request canceled\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"**Motivation:**\r\nCurrently an application that would like to integrate with Docker by calling it's APIs has to either rely on docker listening on the default socket or be (manually) configured with the socket in case it differs from the default. \r\n\r\nSuch an application can try detecting the socket by parsing a running Docker process command line parameters and might need to parse the Docker configuration file. However, this duplicates Docker's logic inside the application and it's very brittle. Inspecting Docker's open file descriptors (e.g. through /proc) is not optimal either as file descriptors other than the API socket/s are held open. \r\n\r\nExposing the API sockets may also facilitate Docker CLI such that it will be able to communicate with a Docker host that is not listening on the default socket (changes to Docker CLI will go into a separate issue).\r\n\r\n**Implementation:**\r\nThe idea is to publish the socket info into a file, say similar to publishing the Docker PID through the docker.pid file.\r\n"},{"labels":[null,"enhancement"],"text":"A `docker-compose.yml` allows for variable substitution including `COMPOSE_PROJECT_NAME`.\r\n\r\nWhen launching a stack it would be useful for the stack name to be exposed as an environment variable for the same purpose. We use this for example to help attach to the correct network(s).\r\n\r\nWorkaround (example): `jamesg@svc1:~/test-hub$ COMPOSE_PROJECT_NAME=test-hub docker stack deploy -c docker-compose.yml test-hub`\r\n\r\nNotice the obvious redundancy!\r\n\r\nSee also #26876"},{"labels":[null,"enhancement",null],"text":"**That is no possible to start container with memory-swap/memory-swappiness parameters using docker stack deploy**\r\n\r\n#test.yml file\r\n```\r\nversion: \"3\"\r\nservices:\r\n\r\n  ubuntu:\r\n    image: ubuntu\r\n    deploy:\r\n        restart_policy:\r\n            condition: none\r\n        resources:\r\n            limits:\r\n                memory: 768M\r\n                memory-swap: 1M\r\n```\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker stack deploy -c test.yml test`\r\n\r\n**Result**\r\nmemory-swap Additional property memory-swap is not allowed\r\n\r\n**Describe the results you expected:**\r\ncontainer is up and running\r\n\r\n**Output of `docker version`:**\r\nClient:\r\n Version:      17.05.0-ce\r\n API version:  1.29\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:06:25 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.05.0-ce\r\n API version:  1.29 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:06:25 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n\r\n\r\n**Output of `docker info`:**\r\nContainers: 1\r\n Running: 1\r\n Paused: 0\r\n Stopped: 0\r\nImages: 33\r\nServer Version: 17.05.0-ce\r\nStorage Driver: overlay\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: ****\r\n Is Manager: true\r\n ClusterID: ****\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: ****\r\n Manager Addresses: ****\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 9048e5e50717ea4497b757314bad98ea3763c145\r\nrunc version: 9c2d8d184e5da67c95d601382adf14862e4f2228\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.10.0-514.16.1.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 9.601GiB\r\nName: ***\r\nID: 2FXY:XP3H:UWFL:3R7U:CF5H:THXJ:XC5K:SZTS:U4LV:AY7D:3JNV:FOJF\r\nDocker Root Dir: /docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nHttp Proxy: ****\r\nHttps Proxy: ****\r\nNo Proxy: ****\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries: ****\r\nLive Restore Enabled: false"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\nAllow setting mac-address when using network connect. Currently it is possible to do so when running a container, but when more than one network is needed, network connect is the only way.\r\n\r\nIt seems it is supported in Docker Engine API, at least fron 1.25 (https://docs.docker.com/engine/api/v1.25/#operation/NetworkConnect), but it's not included in the CLI\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n```\r\n1. docker create --name my_container ubuntu:16.04 /bin/bash -c \"cat /sys/class/net/eth0/address\" \r\n2. docker network connect --mac-address=\"92:20:de:b0:6b:62\" my_net my_container\r\n```\r\n\r\n**Describe the results you received:**\r\n```\r\nunknown flag: --mac-address\r\nSee 'docker network connect --help'.\r\n```\r\n\r\n**Describe the results you expected:**\r\n```\r\n# docker start --ai my_container\r\n> 92:20:de:b0:6b:62\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.03.1-ce\r\n API version:  1.27\r\n Go version:   go1.7.5\r\n Git commit:   c6d412e\r\n Built:        Mon Mar 27 17:10:36 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.1-ce\r\n API version:  1.27 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   c6d412e\r\n Built:        Mon Mar 27 17:10:36 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n"},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nDeployed container **hostname does not honor template expected value**, as described in: https://docs.docker.com/engine/reference/commandline/service_create/#create-services-using-templates\r\n\r\n**Steps to reproduce the issue:**\r\n1.Create a stack.yaml\r\n```\r\nversion: '3'\r\nservices:\r\n  nginx:\r\n    deploy:\r\n      replicas: 1\r\n      restart_policy:\r\n        condition: on-failure\r\n        delay: 5s\r\n        max_attempts: 1\r\n    hostname: '{{.Service.Name}}'\r\n    image: 'nginx:1.13.0'\r\n    ports:\r\n      - 80:80\r\n    volumes:\r\n      - '/etc/timezone:/etc/timezone:ro'\r\n      - '/etc/localtime:/etc/localtime:ro'\r\n```\r\n2. Deploy the stack\r\n> docker stack deploy --compose-file stack.yaml test\r\n\r\n3. Browse the running tasks\r\n```\r\n> docker ps -a\r\nCONTAINER ID        IMAGE                               COMMAND                  CREATED             STATUS              PORTS                                                      NAMES\r\n627d6809d5b0        nginx:1.13.0                        \"nginx -g 'daemon ...\"   29 seconds ago      Up 28 seconds       80/tcp   \r\n```\r\n\r\n4. Print the hostname or login to the container to see the hostname\r\n\r\n> docker exec -it 627d6809d5b0 hostname\r\ntest_nginx\r\n\r\n> docker exec -it 627d6809d5b0 bash\r\nroot@test_nginx:/#\r\n\r\n**Describe the results you received:**\r\n\r\nContainer is named **test_nginx**\r\n\r\n**Describe the results you expected:**\r\n\r\nContainer should be named **nginx**\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nThis might be problematic to https://github.com/docker/compose/issues/4475 -> https://github.com/docker/compose/issues/229\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.05.0-ce\r\n API version:  1.29\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:10:54 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.05.0-ce\r\n API version:  1.29 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:10:54 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 1\r\n Paused: 0\r\n Stopped: 0\r\nImages: 12\r\nServer Version: 17.05.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 44\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: o6ax5ex63j24xb670wrkdw9la\r\n Is Manager: true\r\n ClusterID: wa2ceofyt5ofwb5bdpqo17lml\r\n Managers: 3\r\n Nodes: 3\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 1.2.3.1\r\n Manager Addresses:\r\n  1.2.3.1:2377\r\n  1.2.3.2:2377\r\n  1.2.3.3:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 9048e5e50717ea4497b757314bad98ea3763c145\r\nrunc version: 9c2d8d184e5da67c95d601382adf14862e4f2228\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-78-generic\r\nOperating System: Ubuntu 16.04.2 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 3.851GiB\r\nName: host\r\nID: QDIZ:HAH6:OAJS:DUI6:K2TI:TH6W:JVKH:PKRD:X66K:HQK3:265E:IDNS\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n\r\n```\r\n\r\nCC: @stevvooe @thaJeztah "},{"labels":[null,"enhancement",null],"text":"Hi,\r\n\r\nWe have small team that develops .NET applications. Often have to install windows docker containers. \r\nBecause the size of microsoft/windowsservercore image more than 8GB we decided to install local cache registry to save time to download. \r\nUnfortunately the registry does not cache foreign layers or layers which contains urls parameter in manifest file. (btw, its no problem with any other images)\r\n\r\nExample manifest (microsoft/nanoserver/10.0.14393.953):\r\n\r\n```{\r\n\"schemaVersion\": 2,\r\n\"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\r\n\"config\": {\r\n\"mediaType\": \"application/vnd.docker.container.image.v1+json\",\r\n\"size\": 582,\r\n\"digest\": \"sha256:18a0d32a4b98e8a3e1ab7eb33b2be75b4826cbf43754961db08101b52bce0840\"\r\n},\r\n\"layers\": [{\r\n\"mediaType\": \"application/vnd.docker.image.rootfs.foreign.diff.tar.gzip\",\r\n\"size\": 252691002,\r\n\"digest\": \"sha256:bce2fbc256ea437a87dadac2f69aabd25bed4f56255549090056c1131fad0277\",\r\n\"urls\": [\r\n\"https://go.microsoft.com/fwlink/?linkid=837858\"\r\n]\r\n},\r\n{\r\n\"mediaType\": \"application/vnd.docker.image.rootfs.foreign.diff.tar.gzip\",\r\n\"size\": 114925341,\r\n\"digest\": \"sha256:58f68fa0ceda734a980c12dedf782342f892e218bba3c74ded58bfabed652ba1\",\r\n\"urls\": [\r\n\"https://go.microsoft.com/fwlink/?linkid=844835\"\r\n]\r\n}\r\n]\r\n}\r\n```\r\n\r\nIs this expected behavior and if so, are there any plans to support caching these items for windows? Is any other way to create cache for this type on images?\r\n\r\nBest Regards."},{"labels":[null,"enhancement"],"text":"When create a container, the namespaced kernel parameters such as `/proc/sys/net/ipv4/tcp_keepalive_time` are initialized to the kernel default values. Using `--sysctl` option of `docker run`, they can be overridden. However they cannot be overridden in swarm mode because `--sysctl` option is not supported in `docker service` (https://github.com/moby/moby/issues/25209). The only way to override them in swarm mode is to modify the kernel. \r\n\r\nI think it would be helpful if the docker daemon can have default values of namespaced kernel parameters which can be set when the daemon starts. Then the default values can be used to initialize the namespaced kernel parameters of a newly created container to those values."},{"labels":[null,"enhancement"],"text":"It would be good to be able to specify a set of labels to attach to all resource creation API calls from a single Docker client.\r\n\r\nThe cleanest option I can picture is specifying the default labels in the `~/.docker` config files in some acceptable format.\r\n\r\nMain motive is to allow for selective resource GC operations later, filtered by the label attached.\r\n\r\nFeedback/extra questions welcome to form an accurate understanding of the requirements :)"},{"labels":[null,"enhancement"],"text":"**Description**\r\nWe are currently importing the `pkg/term` package and we're having import path casing issues due to `sirupsen/logrus`. It appears that everything in this project is using the package name `Sirupsen/logrus` instead of `sirupsen/logrus` as is suggested by [the readme](https://github.com/sirupsen/logrus/blob/master/README.md):\r\n\r\n> **Seeing weird case-sensitive problems?** Unfortunately, the author failed to realize the consequences of renaming to lower-case. Due to the Go package environment, this caused issues. Regretfully, there's no turning back now. Everything using logrus will need to use the lower-case: github.com/sirupsen/logrus. Any package that isn't, should be changed.\r\n\r\n**Steps to Reproduce**:\r\n1. import/vendor `github.com/moby/moby/pkg/term`\r\n1. import/vendor `github.com/sirupsen/logrus`\r\n1. call both in your project\r\n1. run `go fmt`\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\ncan't load package: package code.cloudfoundry.org/cli/vendor/github.com/Sirupsen/logrus/examples/hook: case-insensitive import collision: \"code.cloudfoundry.org/cli/vendor/github.com/Sirupsen/logrus\" and \"code.cloudfoundry.org/cli/vendor/github.com/sirupsen/logrus\"\r\n```\r\n\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nThis appeared on our OSX CI environment where the file system was case insensitive. Probably would have happened on our windows CI environment too, but we fixed it on OSX first.\r\n\r\n## Status\r\n\r\n- [x] Initial import rename : #34272 @dmcgowan \r\n- [ ] Update `vendor.conf` with upstream instead of private forks\r\n  - [x] https://github.com/docker/libnetwork\r\n  - [ ] https://github.com/containerd/containerd\r\n  - [ ] https://github.com/docker/swarmkit\r\n  - [ ] https://github.com/opencontainers/runc\r\n  - [x] https://github.com/Microsoft/opengcs\r\n  - [ ] https://github.com/moby/buildkit"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\n*tl;dr - Is there any way I can pipe input to the* `docker build` *command?*\r\n\r\n(I apologize if this functionality already exists, for I cannot find it)\r\n\r\nMy example is a bit trivial, but essentially whenever I am developing a library which has a dockerfile, I like to remove the installation of the library itself from the dockerfile, leaving just the dependencies. Then I append my \"development environment\" code to the bottom of the dockerfile, rebuild it, and then link the library directory so I can rapidly edit, build, and test it within the container.\r\n\r\nIn stead of manually adjusting this each time, I think it would be nice to have a single command that could set up my development container. It seems like it would be a small modification to the `docker build` command (if this isn't already feasible) to add this.\r\n\r\n**Steps to ~~reproduce the issue~~ demonstrate the feature**\r\n\r\nEssentially, what I am trying to do:\r\n\r\n``` bash\r\n$ ls\r\nsrc\r\ntest\r\nDockerfile\r\nREADME\r\n$ docker build --stream $(append_dev_environment.sh Dockerfile 5) .\r\n... #(builds)\r\n```\r\n\r\nIn my case, `append_dev_environment.sh` would be:\r\n``` bash\r\n#!/bin/bash\r\n# print out the current Dockerfile,\r\n# removing the installation steps (last 5 lines here)\r\ncat $1 | head -n -$2\r\n\r\n# prints out my standalone development docker from github, \r\n# without the FROM statement (I gamble they will have similar base images)\r\nwget -q .../Dockerfile -O - | tail -n +2 \r\n```\r\n\r\nAs I mentioned above, this example is trivial and I can simply save a new dockerfile and build with the -f flag. But I could see this becoming useful as an automation tool for more complex Dockerfiles, instead of having to dump to a new file and build off of that."},{"labels":[null,"enhancement",null],"text":"Hello all,\r\n\r\nThis issue because I am facing a problem that cannot be solved using the current features in Docker Edge. I am trying to use the new multi-stage build capabilities of Docker Edge to build complex software and I want a fine-grained way to extract files from that build.\r\n\r\nFor tracing purposes, I need to know exactly what is the output of my build. So supposing I want to extract a few headers files that I get from the build, this is not a satisfying solution:\r\n`COPY --from=foo /usr/include /usr/include`\r\n\r\nbecause I want to be able to do:\r\n```\r\nCOPY --from=foo /usr/include/header_1.h /usr/include/header_1.h\r\n...\r\nCOPY --from=foo /usr/include/header_n.h /usr/include/header_n.h\r\n```\r\n\r\nOf course the problem of this solution is twofold: it will create unnecessary layers, and it makes the build fails when Docker considers that the Dockerfile has too many statements.\r\n\r\nIt would be nice to have a way to perform a copy that is sematically equivalent to:\r\n```\r\nCOPY src1 dest1 \\\r\n        src2 dest2 \\\r\n        ...\r\n        srcn destn\r\n```\r\n\r\nIn my case, precising the destination is even unnecessary because I want to replicate the directory structure created by previous build stages. Maybe an instruction tuple like `EXPORT` / `IMPORT` would be more convenient for these use cases."},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nI'm trying to use variables in my compose file.\r\nAccording to https://docs.docker.com/compose/compose-file/#variable-substitution\r\n\r\nI can do something like this:\r\n\r\n```\r\nservices:\r\n  someservice:\r\n    volumes:\r\n      - \"${VOLUME_NAME}:/mnt/volume\"\r\n```\r\n\r\nI assumed that I also can use variable substitution in volume name in the volumes section:\r\n\r\n```\r\n---\r\nversion: 3.1\r\n\r\nvolumes:\r\n  \"${VOLUME_NAME}\":\r\n    driver: cloudstor:aws\r\n\r\nservices:\r\n  someservice:\r\n    ...\r\n    volumes:\r\n      - \"${VOLUME_NAME}:/mnt/volume\"\r\n```\r\n\r\nBut it doesn't work.\r\nI get error:\r\n\r\n```\r\n${VOLUME_NAME} Additional property ${VOLUME_NAME} is not allowed\r\n```\r\n\r\nIs it a bug or a feature ?\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a compose yaml file with content\r\n\r\n```\r\n---\r\nversion: \"3.1\"\r\n\r\nvolumes:\r\n  \"${VOLUME_NAME}\":\r\n    driver: cloudstor:aws\r\n\r\nservices:\r\n  metadata:\r\n    image: hello-world\r\n    entrypoint: /bin/true\r\n    volumes:\r\n      - \"${VOLUME_NAME}:/mnt/volume\"\r\n```\r\n\r\n2. Deploy the stack \r\n\r\n```\r\nVOLUME_NAME=volume1 docker stack deploy --compose-file compose.yml someservice\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\n${VOLUME_NAME} Additional property ${VOLUME_NAME} is not allowed\r\n```\r\n\r\n**Describe the results you expected:**\r\nThe yaml/compose key has been substituted by the env variable and stack has been deployed.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.04.0-ce\r\n API version:  1.28\r\n Go version:   go1.7.5\r\n Git commit:   4845c56\r\n Built:        Tue Apr  4 00:37:25 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.04.0-ce\r\n API version:  1.28 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   4845c56\r\n Built:        Tue Apr  4 00:37:25 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 6\r\n Running: 6\r\n Paused: 0\r\n Stopped: 0\r\nImages: 18\r\nServer Version: 17.04.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: awslogs\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: =-=-=-=-= cleaned from the output =-=-=-=-\r\n Is Manager: true\r\n ClusterID: =-=-=-=-= cleaned from the output =-=-=-=-\r\n Managers: 3\r\n Nodes: 6\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: =-=-=-=-= cleaned from the output =-=-=-=-\r\n Manager Addresses:\r\n  =-=-=-=-= cleaned from the output =-=-=-=-\r\n  =-=-=-=-= cleaned from the output =-=-=-=-\r\n  =-=-=-=-= cleaned from the output =-=-=-=-\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary:\r\ncontainerd version: 422e31ce907fd9c3833a38d7b8fdd023e5a76e73\r\nrunc version: 9c2d8d184e5da67c95d601382adf14862e4f2228\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.19-moby\r\nOperating System: Alpine Linux v3.5\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 15.65GiB\r\nName: =-=-=-=-= cleaned from the output =-=-=-=-\r\nID: =-=-=-=-= cleaned from the output =-=-=-=-\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 176\r\n Goroutines: 298\r\n System Time: 2017-06-06T13:23:51.776737874Z\r\n EventsListeners: 2\r\nUsername: =-=-=-=-= cleaned from the output =-=-=-=-\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\n os=linux\r\n region==-=-=-=-= cleaned from the output =-=-=-=-\r\n availability_zone==-=-=-=-= cleaned from the output =-=-=-=-\r\n instance_type=t2.xlarge\r\n node_type=manager\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"When an image pull fails as part of running a service container, the task's status field only shows \"No such image\". It doesn't contain any of the context of how and why the pull failed - this is only available in daemon logs. The error message from the pull should also be put inside the task's status field."},{"labels":[null,"enhancement"],"text":"Now that the websocket endpoint is returning binary frames, it should include an header to differentiate the different streams, similarly to what is done for the /attach endpoint when tty is disabled.\r\nAs frames are atomic and contain a whole log line, a single byte header would probably be sufficient (ie no need to get the frame length, it is already known)\r\n\r\nUnless Im missing something, there is currently no way to differentiate stderr from stdout."},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nThe GELF Log driver does not currently support the TCP transport. In some cases, TCP would be more appropriate. \r\n\r\nIt is currently not supported directly by go-gelf, but there is currently a pull request (https://github.com/Graylog2/go-gelf/pull/9) that adds TCP support. \r\n\r\nI am currently working on a pull request that will enable the TCP transport once the go-gelf PR is merged.\r\n"},{"labels":[null,"enhancement"],"text":"## Background on logging\r\n\r\nA long long time ago, docker only supported one method of logging for containers. This is now affectionately known as `jsonfilelog`. There was no support for logging to external services (be it remote endpoints or a local out of process service like journald).\r\n\r\nThe json logger itself also had many performance related issues: excessive memory allocations, slow (reflection-based) json marshalling, and leaking resources. It also did not support log rotation, which means that every container's log file would just grow unbounded and no (good) way to rotate it without data loss. \r\n\r\nBecause of this we introduced the ability to disable logging as well as the concept of logging drivers and slowly started adding new loggers for external services (syslog, journald, cloudwatch, etc).\r\n\r\nThe performance issues have been partially dealt with (see the hack that is pkg/jsonlog) and of course resource leaking dealt with. Log rotation support added (but not enabled by default). \r\n\r\nAt this point most everything is great with logging... unless you need to run `docker logs`, since only jsonfilelog and journald support reading logs. For this reason many people just use the built-in json logger and consume the logs API to ingest into another service.\r\n\r\n\r\n## Proposal\r\n\r\n### Dual-logging\r\nAlways have containers log to a local logger that supports reading if/when the user selected log driver does not support reads in addition to the selected logger. This will allow users to have a consistent experience regardless of the logging driver selected.\r\n\r\n**This is not allowing users to specify two(or multiple) log drivers, but rather to log locally in addition to the user specified log driver**\r\n\r\nAn issue with this...the json logger still has some performance issues... json marshalling is slow and still makes several allocations (I think around 3) per log entry, and can potentially use up loads of disk-space unexpectedly.\r\n\r\n### Replace jsonfilelog\r\n\r\nFor this reason, I also propose replacing the jsonfilelog driver with a new driver: `local`.\r\n`jsonfilelog` is tied to json. We've tweaked the hell out of logging to json for container logs but it's still not ideal (for performance reasons) and really awful/fragile code.\r\n\r\nThe `local` logger would, like jsonfilelog, log to a local file, but would not be tied to any particular encoding format. This is possible by encoding the version/format into the top of each log file. This can be a binary encoded version, with the version corresponding to the version of the driver's encoding format (this could encapsulate both changes to the underlying encoding itself as well as major changes to log data). Doing this on each file allows Docker to support mixed versions for a single container.\r\n\r\nVersion 1 of this format would be encoded as protobuf following the same format as used for logging plugins: https://github.com/moby/moby/tree/master/api/types/plugins/logdriver.\r\n\r\nThis encoding format does not require any allocations and requires far less CPU than json.\r\nAnother plus is the protobuf format uses something like 1/3-1/2 less space (in my tests) on disk compared to the same output in json.\r\n\r\nThe new driver should also have sensible defaults for rotation (i.e. rotate by default) and rotated files should also be compressed (by default) to further save space on disk.\r\nWhen a container is only logged locally the sensible thing to do is to not ever remove log data (unless specified), but when logging to external services it should be ok to clean out older files.\r\n\r\n### Make `local` the default\r\n\r\nIdeally this new `local` log driver would become the default logger since it should be better in absolutely every way than `jsonfilelog`. `jsonfilelog` should then be deprecated, and eventually removed.\r\n\r\nSome problems here may be with users who are directly accessing container log files, expecting a particular format, place on disk, etc... this sucks... but ultimately on-disk state is not part of our API and people should not be messing around in there.... if they are, they should expect to be easily broken between versions.\r\n\r\n - [x] Implement new local logger - #37092\r\n- [x] Implement dual logging"},{"labels":[null,"enhancement"],"text":"This is a feature request.\r\n\r\nCurrently, health checks work and are great to ensure continuous, uninterrupted service, even when rolling updates.\r\n\r\nHowever, I might want to specify a rather long health check interval, let's say 5 minutes, because my health check checks for a few different things and is a bit time- and resource-consuming.\r\n\r\nThe [documentation](https://docs.docker.com/engine/reference/builder/#healthcheck) states:\r\n\r\n> The health check will first run interval seconds after the container is started, and then again interval seconds after each previous check completes.\r\n\r\nThis doesn't work well with continuous delivery, since I'll have to wait 5 minutes before the first container with my new software version is available. This is a long time to wait. And I don't really want to end up with containers running different versions of software for a long period time.\r\n\r\nWhy couldn't we just run the first health check right after startup, without waiting for `interval` delay?"},{"labels":[null,null,"enhancement"],"text":"Feature request: Make service network alias support templating.\r\n\r\nhttps://github.com/moby/moby/pull/24973 tries to do something similar automatically but is blocked on https://github.com/docker/swarmkit/issues/1242... One workaround idea that has received several 👍 from other community members is to make network aliases support templating (e.g.: https://github.com/moby/moby/issues/24787#issuecomment-271253245, https://github.com/moby/moby/issues/24787#issuecomment-275071381, https://github.com/moby/moby/issues/24787#issuecomment-276114764). I couldn't find an issue devoted to that specific request so I figured I'd file this one."},{"labels":[null,"enhancement"],"text":"Are there any policies (algorithms) to choose tasks to be terminated when scale down a service using `docker service scale` command? It would be nice to be able to know (or even determine by users) the termination order of the tasks when scale down stateful services like elasticsearch. More specifically, we can reallocate shards from tasks to be ended to another tasks before scaling down the elasticsearch cluster if we can know or determine the termination order.\r\n"},{"labels":["enhancement",null],"text":"**Description** in `moby/daemon/cluster/executor/container/controller.go` the func ` matchevent(event events.Message) ` matches events using the following condition : \r\n\r\n``` go \r\n\tif event.Actor.Attributes[\"name\"] != r.adapter.container.name() {\r\n\t\treturn false\r\n\t}\r\n```\r\nwe should also filter using ID matching"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nCurrently the `engine ID` shown in `docker info` is the `fingerprint ID` used to `sign` the deprecated `manifest schema v1`. The key signature and finger on these manifests are not checked or used any longer for security, deprecated by notary. So, given this context, we could: \r\n\r\n1) Generate ephemeral `fingerprint` keys to push the `v1` manifests\r\n2) Replace the current `engine ID` with an `UUID` generated once\r\n\r\n**Suggested format**\r\n\r\n**Location**: `<docker_storage_directory>/engine_uuid`\r\n\r\n```\r\n/var/lib/docker # cat engine_uuid\r\n0df9fdce-f163-41f0-92bb-d0df29dd79bc\r\n/var/lib/docker #\r\n```\r\n\r\n/cc @dmp42 @friism "},{"labels":[null,null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nI'm unable to join a node to a freshly initialized swarm.  Attempting to use the worker join-token from the swarm leader on a prospective worker yields this:\r\n```\r\n[bd-test321-vm2 /]$ docker swarm join --token SWMTKN-1-2jvcr4vjnl1hsbageqgrmb62q19x3t1w6j61el1814uc2hxnjg-7bc9rf7kcsctlz1y19o3b20d4 10.0.0.4:2377\r\nError response from daemon: rpc error: code = 14 desc = grpc: the connection is unavailable\r\n```\r\nI checked the logs in */var/log/messages*:\r\n```\r\nMay 22 18:02:28 bd-test321-vm2 dockerd: time=\"2017-05-22T18:02:28.618905383Z\" level=error msg=\"failed to retrieve remote root CA certificate\" error=\"rpc error: code = 14 desc = grpc: the connection is unavailable\" module=node\r\n```\r\nI also confirmed the clocks are not suffering from skew: the time is identical on both servers.\r\n\r\nTo confirm that this is not a network issue involving _network security groups_ or equivalent, I spun up an *nginx* container on port 2378 on the swarm leader node, and attempted to connect.\r\n\r\nOutput of `sudo netstat -plnt`:\r\n```\r\nActive Internet connections (only servers)\r\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\r\ntcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN      1/systemd\r\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      5106/sshd\r\ntcp6       0      0 :::111                  :::*                    LISTEN      1/systemd\r\ntcp6       0      0 :::22                   :::*                    LISTEN      5106/sshd\r\ntcp6       0      0 :::2377                 :::*                    LISTEN      9643/dockerd\r\ntcp6       0      0 :::2378                 :::*                    LISTEN      37816/docker-proxy\r\ntcp6       0      0 :::7946                 :::*                    LISTEN      9643/dockerd`\r\n```\r\n\r\nThis shows *dockerd* listening on port 2377 (swarm), and *docker-proxy* listening on 2378 (nginx):\r\n```\r\nroot     37816  0.0  0.0 108124  3640 ?        Sl   May20   0:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 2378 -container-ip 172.17.0.3 -container-port 80\r\n```\r\n\r\nSure enough, I can connect to *nginx* from the other node:\r\n```\r\n[bd-test321-vm2 /]$ telnet 10.0.0.4 2378\r\nTrying 10.0.0.4...\r\nConnected to 10.0.0.4.\r\nEscape character is '^]'.\r\nGET /\r\n<!DOCTYPE html>\r\n<html>\r\n<head>\r\n<title>Welcome to nginx!</title>\r\n... <snip> ...\r\nConnection closed by foreign host.\r\n```\r\n\r\nTrying to connect to the swarm port (2377) produces an odd error:\r\n```\r\n[bd-test321-vm2 /]$ telnet 10.0.0.4 2377\r\nTrying 10.0.0.4...\r\ntelnet: connect to address 10.0.0.4: No route to host\r\n```\r\nIt seems that the \"no route to host\" is an artifact of running in Azure.  Basically if Azure isn't aware of a service listening on that port, it reports that error message.\r\n\r\nThinking this might be a special issue with that port, I removed the swarm leader from the swarm (`docker swarm leave --force`), and spun up another *nginx* instance on the leader node, this time on port 2377.\r\n```\r\n[bd-test321-vm1 ~]$ docker run -d -p2377:80 nginx\r\n04536e1741da1c71476107aa7e7da62f6582d109402be60c4fdb8301319ffd5a\r\n[bd-test321-vm1 ~]$ sudo netstat -plnt | grep 2377\r\ntcp6       0      0 :::2377                 :::*                    LISTEN      43674/docker-proxy\r\n```\r\nNote: this time it's *docker-proxy* listening...\r\n\r\nNow *telnet* can connect to port 2377:\r\n```\r\n[bd-test321-vm2 var]$ telnet 10.0.0.4 2377\r\nTrying 10.0.0.4...\r\nConnected to 10.0.0.4.\r\nEscape character is '^]'.\r\nGET /\r\n<!DOCTYPE html>\r\n<html>\r\n<head>\r\n<title>Welcome to nginx!</title>\r\n... <snip> ...\r\nConnection closed by foreign host.\r\n```\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Provision 2 Azure VMs running RHEL (see [(gist)](https://gist.github.com/briantd/8fd64131ccd386064ee526ea7d358ffe#file-provision_rhel_on_azure-sh))\r\n2. Install docker following RHEL instructions (see [(gist)](https://gist.github.com/briantd/8fd64131ccd386064ee526ea7d358ffe#file-abbrv_rhel_install_docker-sh))\r\n3. Start swarm on node 1, save join-key output...\r\n```\r\n[bd-test321-vm1 ~]$ docker swarm init --advertise-addr eth0\r\nSwarm initialized: current node (rijmtkfct2el98i9n8jmla1cd) is now a manager.\r\n\r\nTo add a worker to this swarm, run the following command:\r\n\r\n    docker swarm join \\\r\n    --token SWMTKN-1-2y5oceutioq1qbeipo1b59js86y8dzzqzvy2lc03o67teosy9c-b42fs2laexqy8op56zwjniedu \\\r\n    10.0.0.4:2377\r\n\r\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\r\n```\r\n4. Attempt to join from the second node\r\n```\r\n[bd-test321-vm2 var]$ docker swarm join \\\r\n>     --token SWMTKN-1-2y5oceutioq1qbeipo1b59js86y8dzzqzvy2lc03o67teosy9c-b42fs2laexqy8op56zwjniedu \\\r\n>     10.0.0.4:2377\r\nError response from daemon: rpc error: code = 14 desc = grpc: the connection is unavailable\r\n```\r\n\r\n**Describe the results you received:**\r\n> Error response from daemon: rpc error: code = 14 desc = grpc: the connection is unavailable\r\n\r\n**Describe the results you expected:**\r\nNode successfully joins\r\n\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nI've tried using both 10.0.0.0/16 and 172.17.0.?  ip schemes for the underlying VMs -- that didn't make a difference.\r\n\r\n**Output of `docker version`:**\r\n```\r\n$ docker version\r\nClient:\r\n Version:      17.03.1-ee-3\r\n API version:  1.27\r\n Go version:   go1.7.5\r\n Git commit:   3fcee33\r\n Built:        Thu Mar 30 20:03:25 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.1-ee-3\r\n API version:  1.27 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   3fcee33\r\n Built:        Thu Mar 30 20:03:25 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n$ docker info\r\nContainers: 5\r\n Running: 1\r\n Paused: 0\r\n Stopped: 4\r\nImages: 2\r\nServer Version: 17.03.1-ee-3\r\nStorage Driver: overlay\r\n Backing Filesystem: xfs\r\n Supports d_type: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: rijmtkfct2el98i9n8jmla1cd\r\n Is Manager: true\r\n ClusterID: zl4tvp5lngium1l33fo9z7110\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 10.0.0.4\r\n Manager Addresses:\r\n  10.0.0.4:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 4ab9917febca54791c5f071a9d1f404867857fcc\r\nrunc version: 54296cf40ad8143b62dbcaa1d90e520a2136ddfe\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.10.0-514.16.1.el7.x86_64\r\nOperating System: Red Hat Enterprise Linux Server 7.3 (Maipo)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 6.805 GiB\r\nName: bd-test321-vm1\r\nID: MUUP:UOJ4:7KRK:BG5I:UEZG:4ABA:CRNE:CWI3:IATW:3VV7:3PZG:FX2F\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nAzure"},{"labels":[null,"enhancement",null],"text":"In a maintainers meeting last Thursday, there was a discussion how to move forward with Build secrets.\r\n\r\nThe previous PR https://github.com/moby/moby/pull/30637 is closed atm but we need to make sure that the issue is still tracked. The issue was closed because of design issues(listed below) and possible changes/features coming with #32507. This issue is mainly for keeping the secrets discussion from blocking #32507.\r\n\r\n\r\n### Open questions:\r\n\r\n#### Secret sources:\r\n\r\nIn #30637 secrets are sent from the client with the context tar. There were concerns if these should be loaded from swarm secrets instead. The use cases seem quite different but it does feel weird to have 2 secrets implementations. Also, the current \"build-secrets\" use cases are not as secure as swarm ones.\r\n\r\n#### Sending secrets:\r\n\r\nIn #30637 secrets are injected into context tar on the client side and extracted on daemon before being used. With #32677 this could be done independently from context.\r\n\r\n#### Dockerfile UI:\r\n\r\nIn #30637 user specifies target path for the secrets in cli command with `--build-secret` flag. All data exposed like this will become available in `/run/secrets` for every `RUN` operation. Normally it is the image/dockerfile author who knows where specific secrets were expected. `SECRET` Dockerfile command was one of the options considered. #32507 lets image author specify the mount path. With https://github.com/docker/swarmkit/pull/2118 regular swarm secrets do not need to be in `/run/secrets` either. #32507 allows exposing mounts to a specific command that needs a secret, not to everything at once.\r\n\r\n#### Build cache:\r\n\r\n#30637 ignores build cache, #32507 uses it. It is probably more correct to ignore cache to avoid chosen plaintext attacks so #32507 would need to allow that.\r\n\r\n#### Other solutions:\r\n\r\nMost examples that show build secrets use it for SSH keys. There are other ways for exposing this specific feature. #32677 allows ssh forwarding(poc https://github.com/tonistiigi/docker/commit/a175773938b73937b417c7d322961d53ceab359e). By exposing git sources as build stages we could use any auth(ssh, oauth) for cloning git repos.\r\n\r\n@ehazlett @thaJeztah @dnephin @cpuguy83 @diogomonica "},{"labels":[null,null,"enhancement",null],"text":"**Description**\r\n\r\nWhen I set the `--health-start-period` in a service, I cannot see the set value using `docker service inspect --pretty`, \r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker service update foo --health-start-period 2m`\r\n2. `docker service inspect --pretty foo | grep -i health`\r\n\r\n**Describe the results you received:**\r\n\r\nNo results while looking for an \"Health Start Period value\"\r\n\r\n**Describe the results you expected:**\r\n\r\nI think this is an important value that should appear in the prettified output too by default. I'm aware that the whole JSON inspect output carries indeed this info under\r\n```\r\n                \"ContainerSpec\": {\r\n                    \"Healthcheck\": {\r\n                        \"StartPeriod\": 120000000000\r\n                    },\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.05.0-ce\r\n API version:  1.29\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:06:25 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.05.0-ce\r\n API version:  1.29 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:06:25 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nIt would be very useful to be able to specify docker volume alias in Dockefile\r\ni.e. \r\n\r\n...\r\nVOLUME pg-data:/var/pg/data\r\nVOLUME pg-log:${PG_LOG}\r\n..\r\n\r\nSo when creating container,service it would not be necessary to actually know path inside container where application stores data. \r\n\r\n**Example of usage:**\r\n\r\ndocker run -v /path/on/host:pg-data\r\n\r\n\r\n**Output of docker version:**\r\n\r\nDocker version 17.05.0-ce\r\n"},{"labels":[null,"enhancement"],"text":"It would be great if there would be a simple way to configure Docker so that hostname would be set to `<name>.<custom network>`. This means that resolving `<name>.<custom network>` should work (it already does, even when not setting container's hostname to be the same as `<name>`). And also running `hostname -f` should return `<name>.<custom network>`. `hostname` only should return `<name>`.\r\n\r\nNot sure what should happen if there are more than one network attached to the container. :-("},{"labels":[null,null,"enhancement"],"text":"**Description**\r\n\r\n`docker stack rm` is a destructive operation that doesn't ask for confirmation. This is error prone.\r\nThis command should ask for confirmation and, perhaps, include a `--force` flag for those who do not want to see the confirmation prompt (ex. a script).\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nDocker version 17.05.0-ce-rc1, build 2878a85\r\n```"},{"labels":[null,null,"enhancement"],"text":"**Description**\r\n\r\nCurrently , the Docker prune commands does not take `reference` into account when filtering.  I have tested with the volume and image subcommands, there may be others.\r\n\r\nOther subcommands work with the [reference filter](https://docs.docker.com/engine/reference/commandline/images/#filtering), it would be nice to have this functionality for prune commands.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Run a prune command\r\n\r\n`docker pull alpine`\r\n`docker image prune --filter reference=alpine`\r\n`docker image ls`\r\n\r\n**Describe the results you received:**\r\n\r\nThe filter isn't picking up the reference specified.\r\n\r\n**Describe the results you expected:**\r\n\r\nExpect the prune command to remove only the alpine image, but it doesn't remove any.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nI opened this issue as a bug originally (https://github.com/moby/moby/issues/33142) but turns out this is known behavior so I am wondering if this can be added as a feature.\r\n"},{"labels":[null,null,"enhancement",null],"text":"**Description**\r\n\r\nHey,\r\nThere should be a flag (that IMHO suppose to enabled by default) for:\r\ndocker node update --availability drain <node-id> command which wait until swarm services are completely rebalanced in the cluster.\r\n\r\nThis could be very useful for automating docker engine updates with the newer versioning and releasing mechanism in community edition.\r\n\r\n**Steps to reproduce the issue:**\r\n1. issue the command ` docker node update --availability drain <node-id>  `\r\n2. missing wait-for-finish flag\r\n\r\n**Describe the results you received:**\r\nDidn't wait to finish. \r\n\r\n**Describe the results you expected:**\r\nShould wait for containers rebalance to finish. \r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.03.1-ce\r\n API version:  1.27\r\n Go version:   go1.7.5\r\n Git commit:   c6d412e\r\n Built:        Mon Mar 27 17:05:44 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.1-ce\r\n API version:  1.27 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   c6d412e\r\n Built:        Mon Mar 27 17:05:44 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 32\r\n Running: 14\r\n Paused: 0\r\n Stopped: 18\r\nImages: 81\r\nServer Version: 17.03.1-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: xfs\r\n Supports d_type: false\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: bzoj0oymrx6p2sozsvk9n11pw\r\n Is Manager: true\r\n ClusterID: cmn00hnruhl72sem8jn77bahf\r\n Managers: 3\r\n Nodes: 5\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 10.1.11.246\r\n Manager Addresses:\r\n  <internal-ip1>:2377\r\n  <internal-ip2>:2377\r\n  <internal-ip3>:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 4ab9917febca54791c5f071a9d1f404867857fcc\r\nrunc version: 54296cf40ad8143b62dbcaa1d90e520a2136ddfe\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.56-1.el7.elrepo.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 15.67 GiB\r\nName: <internal-ip-dns>\r\nID: MU6E:LUBS:BF4O:D3YG:EYHJ:LTA2:MUVP:GAXH:6FBG:ALNM:RTWK:VZU4\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nAWS, CentOS-7"},{"labels":[null,null,"enhancement",null],"text":"Some error messages returned by the daemon / API are referring to CLI flags not being valid, for example;\r\n\r\n- [runconfig/hostconfig_windows.go#L34](https://github.com/moby/moby/blob/4af3389d43d93f50d9b4fa217de148ec45abf8cb/runconfig/hostconfig_windows.go#L34)\r\n- [hostconfig_windows.go#L49](https://github.com/moby/moby/blob/4af3389d43d93f50d9b4fa217de148ec45abf8cb/runconfig/hostconfig_windows.go#L49)\r\n\r\nGiven that these are returned by the API, not the CLI, and we cannot assume that the _docker_ CLI is used as a client (it can be any (API) client), we should make these messages more generic and have them refer to the API-option that is not supported, not the CLI flag."},{"labels":[null,null,null,"enhancement"],"text":"# Summary\r\n\r\nDefine a cleaner separation between the daemon and image management logic. Image management today is composed of the layer store, image store, reference store, and distribution code. We want\r\na single interface to be able to make the image backend swappable (for containerd) and more\r\ncomposable (for Linux containers on Windows). This interface should attempt to be higher\r\nlevel and smaller than the existing interfaces, but not too high level as to require duplicating\r\nsimilar image logic in the backends.\r\n\r\n# Plan\r\n\r\n## Phase 1: Refactor image related packages under `image/manager`\r\n  - move `layer` to `image/manager/layer`\r\n  - move `image/{fs,store}*.go` to `image/manager/imagestore`\r\n  - move `daemon/graphdriver` to `image/manager/layer/graphdriver`\r\n  - move `distribution` to `image/manager/distribution`\r\n  - move `reference` to `image/manager/reference`\r\n\r\n## Phase 2: Define new interface under `image/manager`\r\n  - Define higher level interface for operating with stores and distribution\r\n  - Move higher level image logic from daemon to new docker image manager package (name tbd)\r\n  - Create backend to handle multiple platforms for Linux containers on Windows\r\n\r\n## Phase 3: Containerd image manager\r\n  - Experimental support for using containerd image backend in place of existing backend\r\n\r\n### Notes:\r\n- Work in progress to make daemon support multiple image stores for Linux containers on Windows\r\n- Containerd does not share any code with the current `distribution` package\r\n- Current daemon to image backend storage interface notes https://gist.github.com/dmcgowan/047423448531f069e5cca3e7a45409e2\r\n"},{"labels":[null,"enhancement"],"text":"Currently docker does not support binding to any other address than 127.0.0.1. As we know Loopback interface gives the whole range of addresses 127.0.0.0 to 127.255.255.255.\r\n\r\nThis might be especially relevant for Mac OS X. The following documentation link:\r\n\r\nhttps://docs.docker.com/docker-for-mac/networking/#per-container-ip-addressing-is-not-possible\r\n\r\nsuggests to bind a dedicate IP to the loopback interface. But why not using right from the scratch different loopback addresses?\r\n\r\nUsers are going to deal with well knows tools like: `/etc/hosts` file, e.g. one can easily just append to `/etc/hosts` a few lines stating something like:\r\n\r\n```\r\n127.0.0.2  unstable-server.local\r\n127.0.0.3  beta-server.local\r\n...\r\n```\r\n\r\nand simply run docker like:\r\n\r\n```\r\n$ docker run -ti --rm -p 127.0.0.2:80:80 -v some-dev-files:/opt/some/path --hostname unstable-server.local my-test-box\r\n```\r\n\r\nand finally the user can just use hostname path like\r\n\r\n```\r\n$ curl unstable-server.local/some/server/path/under/test\r\n```\r\nwithout delving into complex routing scenarios on how to access a container from host machine and how to export ports so that they are not exposed to outside world or having the headache that the desired port is already being used by some other container.\r\n\r\nI created this issue after asking the simplified version of that issue in #1403.\r\n\r\n----\r\nCurrently this  feature does not work by either issuing an error:\r\n\r\n```\r\ndocker: Error response from daemon: driver failed programming external connectivity on endpoint pensive_golick (5e7c1cb54c108d8786ca32943aa7b60fae29c6b6922bc0a5f7bf0793913cfeb7): Error starting userland proxy: Bind for 127.0.0.77:80: unexpected error (Failure EADDRNOTAVAIL).\r\n```\r\n\r\nor\r\n\r\n```\r\ndocker: Error response from daemon: driver failed programming external connectivity on endpoint quirky_mestorf (9c63f1e09bc79ab029b29cc629cae1c6abe246b8f9a6211bd6b2aa3c7c36f98f): Error starting userland proxy: listen tcp 127.0.0.77:9030: bind: cannot assign requested address.\r\n```\r\n\r\nI assume both errors depend more on port which docker tries to bind to (like well defined port or not)."},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nI would like to be able to see the plaintext contents of the AppArmor profile \"docker-default\", for debugging/audit purposes.\r\n\r\nThis bug/feature-request is related to #24786 (fix implemented in #26518), and to a lesser extent #26823, #25935 (claimed to be fixed in #27083, however in my 17.04 install 'docker-inspect' still doesn't show the new behavior)\r\n\r\nPrior to Docker 1.13, it stored the AppArmor Profile in /etc/apparmor.d/docker-default (which was overwritten when Docker started, so users couldn't modify it.  Docker devs added the --security-opt to let users specify a profile.  After v1.13, Docker now generates docker-default in tmpfs, uses apparmor_parser to load it into kernel, _then deletes the file_.  All of the AppArmor utils (aa-* on Ubuntu) expect a file parameter, and /sys/kernel/security/apparmor/policy/profiles/* only has cached binaries.\r\n\r\nIn discussion of #24786, @cyphar mentions this being opaque, and I agree.\r\n\r\n**Steps to reproduce the issue:**\r\n1.  `docker run` a container (without --privileged)\r\n2. `aa-status` to confirm \"docker-default\" profile is being enforced\r\n3. try to find what \"docker-default\" profile actually is, and the Policy/Policies being enforced\r\n\r\n**Describe the results you received:**\r\nfailed to find information in step 3.\r\n\r\n**Describe the results you expected:**\r\nexpected to find exactly what is being enforced.  (without reading the source code and running a Go template parser)\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nThis came up as I was trying to debug a Docker container (Rails application based on phusion/passenger-ruby) because kern.log/syslog had excessive AppArmor DENIED ptrace for a 'ps', which according to Docker documentation should be allowed in the default profile. \r\n\r\nI apologize in advance, if there is already some way to get this info. (I'm new to Docker and AppArmor, but old to Linux/Solaris/AIX/BSD/etc)\r\n\r\nPossible solution/suggestions:\r\n- restore `/etc/apparmor.d/default-docker`, and only change it during install time (dpkg and rpm can handle this, or check differences as part of your auto-setup script).  If the user/admin changes that file, then load their modified file instead of overwriting it.  (If someone has /etc/ mounted read-only, they should be expected to remount rw during install/update of software.)\r\n- instead of 'tmpfs', store the docker-default file somewhere under /var/run/docker/ (and don't delete it while the containers are running).  Then `docker inspect` and `aa-status` can reference the path to the profile, which can easily be seen by those who care.\r\n- I could submit a feature request to the AppArmor folks requesting a config-file directive for their parser, such that it always caches the plaintext versions of all profiles.\r\n\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.04.0-ce\r\n API version:  1.28\r\n Go version:   go1.7.5\r\n Git commit:   4845c56\r\n Built:        Mon Apr  3 18:07:42 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.04.0-ce\r\n API version:  1.28 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   4845c56\r\n Built:        Mon Apr  3 18:07:42 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 3\r\n Running: 3\r\n Paused: 0\r\n Stopped: 0\r\nImages: 47\r\nServer Version: 17.04.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 71\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary:\r\ncontainerd version: 422e31ce907fd9c3833a38d7b8fdd023e5a76e73\r\nrunc version: 9c2d8d184e5da67c95d601382adf14862e4f2228\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-75-generic\r\nOperating System: Ubuntu 16.04.2 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 3.859GiB\r\nName: [redacted]\r\nID: M2RC:GOLE:6JNO:CRMB:XEO7:FBBN:PNYC:BEUQ:PZUH:L7FN:SZI6:4JPR\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nHttp Proxy: [redacted]\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nVM/Instance on OpenStack (13.1)"},{"labels":[null,null,null,"enhancement"],"text":"I'd like to request that the \"network_mode\" from the docker-compose file be supported. I know that there's already been some discussion around this from those who'd like to use it for network_mode:host capabilities, but personally I'm more interested in the `network_mode:\"service:<svc name>`\".\r\n\r\nI'd also like to see this supported in the docker service create command or at least have the `--network` option be able to include the same semantics as the network_mode does in the compose file.\r\n\r\nThe use cases I see for this are:\r\n\r\n* launching containers that share the same network stack so that applications can communicate with each other over a loopback interface.  This would allow lower-latency communications between the containers and also obviate service discovery requirements for such containers.\r\n* launching a container which contains a network toolset so that I can do some network diagnostics on processes that run in containers derived from \"scratch\" (such as containers running go programs).\r\n\r\nThe first use case would definitely benefit from reviving container affinity constraints from the old swarm, but that could be worked around for now using the supported placement constraints from v3 compose format (or by making the network_mode:service<svcname> itself be a deployment constraint).\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nWe strive to provide a very user friendly experience to consumers of docker volume plugins. Some work needs to be done when docker users move from a docker legacy plugin to a docker managed volume plugin.\r\n \r\n**Steps to reproduce the issue:**\r\n1. Create a legacy volume plugin and call is \"myplugin\"\r\n2. Create a volume \"testvolume\" using this driver. The command to use is : \r\n```\r\ndocker volume create --driver=myplugin testvolume\r\n```\r\n3. Now install the new managed plugin, give it the same alias as the legacy plugin name\r\n```\r\ndocker plugin install myplugin:1.0 --alias myplugin\r\n```\r\n4. Kill the old plugin, and restart docker.\r\n5. List the volume.\r\n```\r\ndocker volume ls\r\n```\r\n6. The output now shows that the driver for this volume is \"myplugin:latest\"\r\n\r\nThis is a usability issue. It would be nice if the managed plugin name was exactly the same as the alias name (\"myplugin\") in this example, so that docker users can use \"myplugin\" as the driver name in their docker commands irrespective of the type of plugin (legacy/managed) in use.\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nDuring plugin development, it is expedient to use \"docker plugin create\". System testing and deployment however happens with \"docker plugin install\". In both cases, we would like to use the same friendly name for the plugin. The friendly name can be provided to \"docker plugin install\" via the --alias parameter, but there is no such way to assign a local name in \"docker plugin create\". We should have a \"--alias\" to docker plugin create (or alternatively, have something in config.json to provide a local name for the plugin).\r\n\r\n"},{"labels":[null,null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nWhen service is created, logging options don't work\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create service without `--log-driver`\r\n        \r\n       docker service create --name my-service --network local \\\r\n       --mount type=bind,src=/home/romach/logs,dst=/root/logs \\\r\n       --log-opt max-size=1m --log-opt max-file=2 image-local:1.0;\r\n\r\n**Describe the results you received:**\r\n- logs are greater than 1 Mb and in one single file\r\n\r\n**Describe the results you expected:**\r\n- there two rotated logs less than 1 Mb each\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nWhen I run container:\r\n\r\n```\r\ndocker run -it -v /home/romach/logs:/root/logs --name my-service \\\r\n--log-opt max-size=1m --log-opt max-file=2 image-local:1.0;\r\n```\r\nor create service with `--log-driver` option:\r\n\r\n```\r\ndocker service create --name my-service --network local \\\r\n--mount type=bind,src=/home/romach/logs,dst=/root/logs \\\r\n--log-driver json-file --log-opt max-size=1m --log-opt max-file=2 image-local:1.0;\r\n```\r\nlogging works properly.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nDocker version 17.03.1-ce, build c6d412e\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 9\r\n Running: 1\r\n Paused: 0\r\n Stopped: 8\r\nImages: 226\r\nServer Version: 17.03.1-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 204\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 4ab9917febca54791c5f071a9d1f404867857fcc\r\nrunc version: 54296cf40ad8143b62dbcaa1d90e520a2136ddfe\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.8.0-49-generic\r\nOperating System: Ubuntu 16.04.2 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 15.58 GiB\r\nName: romach\r\nID: 4PP6:SW6I:JYMQ:OHGR:4I4N:XQ7Y:3HVL:YXF3:S3YY:24FM:SNQ4:VOCZ\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: romach007\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":["enhancement",null],"text":"**Feature Request**\r\n\r\nI am finding the support for [creating services using templates](https://github.com/moby/moby/blob/master/docs/reference/commandline/service_create.md#create-services-using-templates) useful for our deployment scenarios. Thank you!\r\n\r\nI'd like to be able to be able to use Docker's standard [format command and log output functions](https://docs.docker.com/engine/admin/formatting/), specifically `json`, to pass environment variables. I'm currently doing a very explicit:\r\n\r\n```\r\nSWARM_SERVICE={\"service\": { \"ID\":\"{{.Service.ID}}\", \"Name\": \"{{.Service.Name}}\", \"Labels\": \"{{js .Service.Labels}}\" }}\r\n```\r\n\r\nWith support for the standard formatting functions, this would be nicely concise as:\r\n\r\n```\r\nSWARM_SERVICE={\"service\": {{json .Service}} }\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"**Feature Request**\r\n\r\nI am finding the support for [create services using templates](https://github.com/moby/moby/blob/master/docs/reference/commandline/service_create.md#create-services-using-templates) useful for our deployment scenarios.\r\n\r\nI'd like to be able to pass an environment variable with the unique Swarm Cluster ID (e.g. `docker info --format {{.Swarm.Cluster.ID}}`) in the same way I can pass in `{{.Service.Name}}` with v1.13.  \r\n\r\nWe have many different disposable swarms — per integration test run, per preprod endpoint, and per customer installation. We'd like services within the swarm to be able to connect to central points (logging, metrics, updating, cross-region sync) and be able to self-identify with the swarm.  We are currently hacking around this feature using `docker secerts`, but the information is of the same kind  as the `{{Service}}`,  `{{Node}}`, and `{{Task}}` already available in this new feature.\r\n\r\nThanks!\r\n-Shane"},{"labels":[null,null,"enhancement"],"text":"**Description**\r\nCurrently it is not possible to deploy a service with `docker stack deploy` that ends up running containers with a read-only filesystem.\r\n\r\nWe should add support for it.\r\n"},{"labels":[null,null,"enhancement"],"text":"The command `docker service ls --filter name=foo` will return both the services `foo` and `foobar`, and there seems to be no built-in option to change this mode. It makes sense when filtering on an id to make it match on whichever part of the id you use, but it would be handy if filters would be able to maybe use regex, like so: `docker service ls --filter name=^foo$`\r\n\r\nIf this might be considered a feature request, let's consider it for all other filterable output too.\r\n\r\n---\r\n\r\ndocker version\r\n\r\n`Docker version 17.04.0-ce, build 4845c56`\r\n\r\ndocker info\r\n\r\n```\r\nContainers: 6\r\n Running: 5\r\n Paused: 0\r\n Stopped: 1\r\nImages: 17\r\nServer Version: 17.04.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 151\r\n Dirperm1 Supported: false\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: gvd49po3fnmib337ya4exlxg6\r\n Is Manager: true\r\n ClusterID: w6lq1qvmj0e9v6i1laz3uhfgh\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 192.168.100.100\r\n Manager Addresses:\r\n  192.168.100.100:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: \r\ncontainerd version: 422e31ce907fd9c3833a38d7b8fdd023e5a76e73\r\nrunc version: 9c2d8d184e5da67c95d601382adf14862e4f2228\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 3.13.0-107-generic\r\nOperating System: Ubuntu 14.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 1.955GiB\r\nName: vagrant-ubuntu-trusty-64\r\nID: 2IBB:D2LL:IOK5:UGC6:WHZG:XQ73:SLMQ:4IUZ:5IJJ:5E5R:U2UE:4ABS\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n"},{"labels":[null,"enhancement"],"text":"At the moment, there are three options for restart condition: `none`, `on-failure`, `any`.\r\n\r\nIf I want to make sure that my task executes just once (regardless of the exit code), I would choose `none` for that. But if while it's running the underlying node fails, the task won't be rescheduled. The proposal is to add a new condition, i.e. `on-node-failure` (or similar) that reschedules the task only if it's shutdown because of external causes to the task (like a node failure).\r\n\r\ncc @aluzzardi @dongluochen @aaronlehmann "},{"labels":[null,"enhancement"],"text":"A couple of metrics we have are not following Prometheus naming conventions.\r\n\r\nSince they're still experimental, we can fix those:\r\n```\r\n$ curl http://localhost:3000/metrics | promtool check-metrics\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100  155k  100  155k    0     0  9472k      0 --:--:-- --:--:-- --:--:-- 9692k\r\nengine_daemon_events_subscribers_total: non-counter metrics should not have \"_total\" suffix\r\nhttp_request_duration_microseconds: use base unit \"seconds\" instead of \"microseconds\"\r\n```\r\n\r\n/cc @crosbymichael  @juliusv @thaJeztah "},{"labels":[null,"enhancement"],"text":"Prior to docker 1.13 containers could emit large log lines and docker passed the lines along intact to the given log-driver destination (fluentd, journald, json-file, etc).   I'm sure some upper bound existed but in my quick testing on 1.12.6 I was able to run a container which printed to STDOUT a 65K JSON object that got logged in tact as 1 line.   In my shop for better or worse log lines of 16k - 64k are being written by applications.     \r\n\r\nAs of docker 1.13 log lines are limited to 16k per log line.   Lines longer than this are split into multiple log entries which makes it difficult to view specific events in a log aggregation tool.   If people are writing log messages in a format like JSON then those log entries are not able to be parsed by down stream log aggregation tools because the JSON is now invalid as single objects got split into multiple log lines.\r\n\r\nI would like to be able to override this (https://github.com/moby/moby/blob/master/daemon/logger/copier.go#L13 ) constant which I believe is setting the upper bound on the log size.\r\n\r\nA log-driver option like --log-opt max-log-line=64k would allow users to choose if they want to emulate the behavior in previous versions of docker by setting a larger max than the default of 16k.\r\n"},{"labels":[null,"enhancement"],"text":"To detach from a running container, you can issue the key sequence `strg-p`, `strg-q`. This is the default detach key sequence, it can be adjusted.\r\n\r\nIf Docker detects the first keystroke of the detach sequence, it waits for the next keystroke. If the next keystroke is the next detach key, the container gets detached. If not, both keystrokes are passed on to the container. Between both keystrokes, an arbitrary amount of time may pass.\r\n\r\nI have a usecase where this causes confusion: Bash uses `ctrl-p` to navigate back in command history (`p`revious). This is a nice alternative to the cursor keys because you don't have to move your hand.\r\n\r\nIf you use `ctrl-p` to move through command history, the command line is only updated on every second keypress.\r\n\r\n### To reproduce\r\nStart bash in a container:\r\n```bash\r\ndocker run --rm -it ubuntu:14.04 bash\r\n```\r\nIssue a command so that there is an entry in the command history. Here, `ls` is used.\r\n```bash\r\n$ ls\r\nbin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\r\n$\r\n```\r\nType `strg-p` and watch the command line. It does not change. Expected behavior: `ls` should appear.\r\n```bash\r\n$\r\n```\r\nPress `Return`. This is the keystroke Docker was waiting for in order to decide whether to detach or pass `ctrl-p`. As it is not `ctrl-q`, bash finally updates the current line and executes the command.\r\n```bash\r\n$ ls\r\nbin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\r\n$\r\n```\r\nYou could also have typed the `cursor-left` or `cursor-right` key to trigger an update of the current line.\r\n\r\nThe overall result is very confusing:\r\n- The command displayed on the command line is not what gets executed when you press `return` (dangerous!)\r\n- Cycling through the history, you only see every second command.\r\n\r\n### Proposal\r\nThere should be a timeout: After the first key of the detach sequence is detected, Docker CLI will give the second detach key a special meaning only if it is pressed within one second. After one second, the first detach keystroke will be forwarded to the container.\r\n\r\nIn the use case above, this will feel like a lagging UI."},{"labels":[null,null,"enhancement"],"text":"Hi all,\r\nI am not sure how swarm's internal load balancer actually determines, to which node/container every call will be routed.\r\nImaging the following constellation:\r\n\r\n2 worker nodes, on every node is exactly the same setup, like a webserver service (each node 1 container), and a read-only mysql slave (each node 1 container).\r\n\r\nNow you want the webserver-container ONLY talk to the mysql container service on the same node and NEVER talk to the other node.\r\n\r\nI can do for example on every node: \r\n```\r\n# docker service create --constraint \"node.hostname == $(hostname)\" --name dbslave mysql-server\r\n ```\r\nbut that does not prevent the webserver container running on the same node randomly connect to the mysql service container on another node.\r\nHow do I do that?\r\nThx :+1: "},{"labels":[null,"enhancement"],"text":"when running a container with healthcheck then docker ps shows the state of the healthcheck\r\n\r\nwhen running in swarm a service with healthcheck there is no place to show the state of the healthcheck per container in the service nor the service healthcheck itself.\r\n\r\nAlso, if swarm decided to restart due to healthcheck error it should be in the Error tab\r\n\r\n```\r\ndocker service ps service_name\r\n```\r\n\r\nwill show\r\n```\r\nID            NAME                      IMAGE        NODE    DESIRED STATE  CURRENT STATE           ERROR  PORTS\r\ns8vtyvdqpn6s  service_name.1      image:latest  swarm2  Running        Running (Healthy) 18 hours ago\r\nf1jj05dah50z  service_name.2      image:latest  swarm1  Running        Restarting (Unhealthy) healthcheck error/ healthcheck timeout \r\n```"},{"labels":[null,null,"enhancement",null],"text":"Is it hard to distinguish global options between command specific options?\r\n\r\nOne might have the problem of logging into a private registry.\r\n\r\nYou choose to pick the debug (-D) option from the docker help screen and combine it with the docker login command. This doesn't gives you much help. And this leads one into thinking is this a global option supported by this command ?  Is it supported by all or some command ? \r\n\r\nAnother question is in which order does global options get put in ? The docker help screen gives us just:\r\n> Usage:\tdocker COMMAND \r\n\r\n-----\r\n\r\nThe help screen for a sub-command of another CLI product (azure cli) gives us this kind of help text:\r\n\r\n<pre>\r\n>\r\naz disk list --help\r\n\r\nCommand\r\n    az disk list: List managed disks.\r\n\r\nArguments\r\n    --resource-group -g: Name of resource group. You can configure the default group using 'az\r\n                         configure --defaults group=<name>'.\r\n\r\nGlobal Arguments\r\n    --debug            : Increase logging verbosity to show all debug logs.\r\n    --help -h          : Show this help message and exit.\r\n    --output -o        : Output format.  Allowed values: json, jsonc, table, tsv.  Default: json.\r\n    --query            : JMESPath query string. See http://jmespath.org/ for more information and\r\n                         examples.\r\n    --verbose          : Increase logging verbosity. Use --debug for full debug logs.\r\n</pre>\r\n\r\nHere it looks like the disk list sub command displays its supported arguments plus the supported global arguments.\r\n\r\n--\r\ndocker -v\r\nDocker version 17.03.1-ce, build c6d412e\r\n\r\n\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nCurrently, the Splunk Logging Driver sends events to Splunk's `services/collector/event/1.0` endpoint. While this endpoint is great for ingesting well formed structured JSON logs, it doesn't allow for traditional plaintext events to be \"transformed\" while being ingested. Splunk transformations typically run regexes for index-time field extractions, scrub the data, etc.\r\n\r\nLuckily, Splunk now supports the `services/collector/raw/1.0` endpoint which runs the incoming data through the typing pipeline that can run these index-time transformations on the incoming data.\r\n\r\nI don't suppose Splunk had the raw endpoint when the logging driver first came out, but now that it does, it would be helpful to support it via an optional argument. Something along the lines of  `--log-opt splunk-endpoint=event|raw` could parametrize the [underlying](https://github.com/moby/moby/blob/v1.13.1/daemon/logger/splunk/splunk.go#L569) URL construction.\r\n\r\nI'll be happy to put up a pull request if this proposal sounds sensible!\r\n\r\nTagging Denis @outcoldman & Glenn @glennblock here since they worked on the original logging driver.\r\n\r\n**Additional Information**\r\n\r\n[Splunk docs for the /raw endpoint\r\n](http://docs.splunk.com/Documentation/Splunk/latest/RESTREF/RESTinput#services.2Fcollector.2Fraw)\r\n[Splunk docs for the /event endpoint\r\n](http://docs.splunk.com/Documentation/Splunk/latest/RESTREF/RESTinput#services.2Fcollector)"},{"labels":[null,"enhancement"],"text":"`docker volume inspect` should include the date created, just like when inspecting an image, container, or network.\r\n"},{"labels":[null,null,"enhancement"],"text":"#30261 added `--update-order` and `--rollback-order` flags. We should add equivalents to the compose file schema.\r\n\r\ncc @dnephin"},{"labels":[null,null,"enhancement"],"text":"#31108 made it possible to control rollback parameters separately from update parameters. It added a set of `--rollback-*` options mirroring the `--update-*` options. These parameters should also be supported in compose files.\r\n\r\nThis PR also added a new failure action called \"rollback\". This value should be supported for the `failure_action` key in compose files if it isn't already.\r\n\r\ncc @dnephin"},{"labels":[null,null,"enhancement"],"text":"#30725 added a `--placement-pref` CLI option, but I don't believe it's supported within compose files yet.\r\n\r\ncc @dnephin"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nIt would be nice if `docker push` used `--log-level` to determine whether to output the (very verbose) messages like below. It can output this progress info by default (`info`), but should not when using `warn` or above.\r\n\r\n```\r\n$ docker -l error push registry.example.com:5000/example:tag\r\n15f253cdd958: Waiting \r\nbbb77c9cc130: Preparing \r\n503f277d757e: Pushed \r\nd6be9329a886: Layer already exists \r\n84c7bf1708b1: Layer already exists \r\ndca7429c9762: Layer already exists \r\ntag: digest: sha256:537cfb201eb21eee9011960af59cb7e7a42105544fb628bc90f3d5ae7f29c33c size: 16391\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nOne \"completed\" log message for each image pushed when at `warn` or above. \r\n\r\n**Output of `docker version`:**\r\nLatest client and server: 17.03.1-ce\r\n"},{"labels":[null,"enhancement"],"text":"Hi,\r\n\r\nIt'd be really great to have the additional `{{.Node.Hostname}}` placeholder for when creating services with templates.\r\n\r\nConsider the following usecase;\r\n\r\nI create the following mongo service to configure a mongo replicaset\r\n```\r\ndocker service create \\\r\n--name mongo \\\r\n--replicas 3 \\\r\n--network mongo \\\r\n--mount type=volume,src=\"{{.Service.Name}}-{{.Task.Slot}}\",dst=/data/,volume-driver=ebs \\\r\nmongo:3.4.3 bash -c \"mongod --replSet zipgo-rs0 --journal\"\r\n```\r\n\r\nI'm using a template to create a new AWS EBS volume of name `{{.Service.Name}}-{{.Task.Slot}}`  for each task, the service is global meaning that a single task will be scheduled to each node. `{{.Task.Slot}}` gives a truncated version of the node id in swarm, which is clever (replicated services will have an index as the slot eg, 1,2,3 etc).\r\n\r\nHowever if I'm running a 3 container replicaset in production and one of the nodes goes offline, and say a new one is created and is automatically joined to the swarm, a new task will be scheduled there to that node and a new volume will be created (new node, new swarmid). Herein lies the problem. A 100Gi volume that has data is leaked and a new blank 100Gi volume is created and has to start replicating data from the other nodes.\r\n\r\nYou could pass a `--snapshot-id` in the `--mount` options but that'll still be used to create a new volume. You'll still have a leaked 100Gi volume full of production data that has been leaked.\r\n\r\nSolution:\r\nAn easy way to solve this would be if it was possible to pass the node's hostname as a placeholder. They are predictable and can be set manually, they'll be different for each node/task and will persist when the swarm id of a node changes. To clarify on that, if you `docker swarm leave` and then you rejoin, or your node crashes and you have to rejoin it to the cluster, you can still bring it up with the previous node's hostname and that way when a task is scheduled there it will find that the `{{.Service.Name}}-{{.Node.Hostname}}` volume already exists. \r\nNo volumes will be leaked, no lags, or heavy iops to replicate hundreds of Gb of data to a new volume, unless you're adding new nodes intentionally."},{"labels":[null,null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n\r\nLong format for publish (or publish-add) using port ranges returns an error.\r\nThe long format is mentioned here as a way to publish ports in host mode: https://docs.docker.com/engine/swarm/services/#publish-a-services-ports-directly-on-the-swarm-node\r\n\r\nThis works fine for single ports, but it results in an error for port ranges.\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Start up a service on a swarm\r\n2. Publish a port range using the long format for publish:\r\n```docker service update --publish-add \"mode=host,target=8000-8100,published=8000-8100\" SERVICE```\r\n\r\n**Describe the results you received:**\r\n\r\nThe following error is being returned:\r\n```invalid argument \"mode=host,target=8000-8100,published=8000-8100\" for --publish-add: strconv.ParseUint: parsing \"8000-8100\": invalid syntax```\r\n\r\n\r\n**Describe the results you expected:**\r\nThe port range should be published in mode=host\r\n\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.03.0-ce\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   3a232c8\r\n Built:        Tue Feb 28 08:01:32 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.0-ce\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   3a232c8\r\n Built:        Tue Feb 28 08:01:32 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n"},{"labels":[null,"enhancement",null],"text":"## Proposal\r\n\r\nBeginning from 17.05, Docker supports \"multi-stage\" Dockerfile.\r\n\r\nMy proposal is to convert such Dockerfile to DAG internally, and execute it in parallel.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/9248427/24951034/00b9326c-1fae-11e7-8ade-c9956e3f0693.png)\r\n\r\nNo change to file format nor UX\r\n\r\nPOC is available: https://github.com/docker/docker/pull/32402\r\n\r\n## Tasks\r\n\r\n### Implement generic DAG utility\r\n\r\nI made a small generic DAG package: https://github.com/AkihiroSuda/go-dag\r\n\r\n```go\r\nimport (\r\n    \"github.com/AkihiroSuda/go-dag\"\r\n    \"github.com/AkihiroSuda/go-dag/scheduler\"\r\n)\r\n\r\ng := &dag.Graph{\r\n\tNodes: []dag.Node{0, 1, 2},\r\n\tEdges: []dag.Edge{\r\n\t\t{Depender: 2, Dependee: 0},\r\n\t\t{Depender: 2, Dependee: 1},\r\n\t},\r\n}\r\nconcurrency := 0\r\nscheduler.Execute(g, concurrency, func(n dag.Node) { buildStage(n) })\r\n```\r\n\r\nIf this design is correct, I think we can vendor this package (or just copy it to `github.com/docker/docker/pkg/dag`)\r\n\r\n### Determine Dockerfile DAG granularity\r\n\r\nAlternatively, we could use fine-grained DAG like this, but I'm -1 ATM, because it is likely to cause implementation issue\r\n\r\n![image](https://cloud.githubusercontent.com/assets/9248427/24951076/27fc4f12-1fae-11e7-88e8-0b63845e86fd.png)\r\n\r\n\r\n### Refactor `builder` pkg to create DAG\r\n\r\nMy previous POC (https://github.com/docker/docker/pull/32402) was implemented in weird way:\r\n\r\n- `builder/dockerfile/parser`(unchanged): parses `Dockerfile` text and returns parsed tree structure\r\n- `builder/dockerfile/parallel`: re-parses output from `builder/dockerfile/parser`, and creates DAG\r\n\r\nRe-parsing output from  `builder/dockerfile/parser` is likely to cause implementation issue; actually I forgot to implement support for `ARG`.\r\n\r\nRather, we should refactor `builder/dockerfile/parser` to emit DAG directly.\r\n\r\n### Investigate other usecases of DAG\r\n\r\nhttps://github.com/docker/docker/pull/32402#issuecomment-292254850\r\n\r\n> The DAG based execution engine should be the new core of the builder, not only provide concurrent stage execution but concurrent build jobs, more efficient processing and cache reuse, cache export/import for DAG branches etc. In the same time, this core should be separated from the frontend(Dockerfile) logic to provide more options for declaring the build definition and also provide more extension points for others to reuse this solver.\r\n\r\n### Investigate why `docker build` is slow\r\n\r\nStorage driver seems being bottleneck, but haven't looked into this deeper\r\n\r\nhttps://github.com/docker/docker/issues/9656\r\nhttps://github.com/docker/docker/pull/32402#issuecomment-292313684\r\nhttps://github.com/docker/docker/pull/32402#issuecomment-292888210\r\n\r\n\r\n\r\n\r\ncc @tonistiigi @dnephin @aaronlehmann @vdemeester @cpuguy83 @simonferquel @thaJeztah\r\n\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n\r\nIf your `.dockerignore` looks like this\r\n```\r\n.*/\r\n```\r\nyou probably want to exclude all hidden folders (but not hidden files). Using Docker `17.03.1-ce`, both, hidden files and hidden folders are ignored. I'd love to see that feature in a future release.\r\n"},{"labels":[null,null,null,"enhancement",null],"text":"**Description**\r\n\r\nUnknown fields in `daemon.json` are ignored by `dockerd`. At the least, I would expect a warning to be logged about use of unknown fields. The combination of ignored unknown fields and command line flags that differ from configuration fields is very user unfriendly. In the case of #32528, `\"default-ulimit\": {}` (cf. `--default-ulimit`) is accepted but `\"default-ulimits\": {}` is rejected which is very confusing. Additionally, the behavior of #32528 (`the following directives don't match any configuration option: default-ulimits`) implies that unknown fields should halt daemon start.\r\n\r\nI've been told that unknown fields are ignored as this reflects the behavior of the golang json unmarshaller. This behavior seems unsuitable for a configuration file format with a fixed specification. Perhaps a different JSON parsing solution needs to be used?\r\n\r\nPossible solutions include:\r\n\r\n1. Show a warning when unknown fields are used.\r\n2. Error when unknown fields are used. This is a user-friendly way to catch typos.\r\n3. Introduce a command-line option to enable/disable strict field checking (in the case where a user/other software abuses this behavior and stores its own configuration in `daemon.json`).\r\n4. Add an alias for every field that differs from its command-line argument equivalent so that the command-line argument name is also accepted. This is the principle of least surprise.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Put `{ \"jjj\": {}, \"default-ulimit\": {} }` in `daemon.json`\r\n2. Start `dockerd`\r\n\r\n**Describe the results you received:**\r\n\r\nNo failure occurred nor message logged that unknown fields were used in configuration.\r\n\r\n**Describe the results you expected:**\r\n\r\n`dockerd` should not start and the log file should contain an explanation.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nN/A\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.04.0-ce\r\n API version:  1.28\r\n Go version:   go1.7.5\r\n Git commit:   4845c56\r\n Built:        Wed Apr  5 06:06:36 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      17.04.0-ce\r\n API version:  1.28 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   4845c56\r\n Built:        Tue Apr  4 00:37:25 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 0\r\nServer Version: 17.04.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary:\r\ncontainerd version: 422e31ce907fd9c3833a38d7b8fdd023e5a76e73\r\nrunc version: 9c2d8d184e5da67c95d601382adf14862e4f2228\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.19-moby\r\nOperating System: Alpine Linux v3.5\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 1.952GiB\r\nName: moby\r\nID: LBRL:MJIS:5WS3:NVTP:MYHG:DD54:XVPJ:CQN6:TWVY:XL4Y:WPDX:PKAD\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 16\r\n Goroutines: 26\r\n System Time: 2017-04-11T15:17:48.302951361Z\r\n EventsListeners: 1\r\nNo Proxy: *.local, 169.254/16\r\nUsername: dsheets\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nN/A\r\n"},{"labels":[null,null,"enhancement"],"text":"This is a feature request for listing init PID and namespace info easily, eg, by adding a -v (for verbose) flag to docker ps. Here's a simple prototype in bash to show what I mean:\r\n\r\n```\r\n# ./dockerpsv.sh \r\nCONTAINER      NAME                    PID PATH             CGROUP     IPC        MNT        NET        PID        USER       UTS       \r\nhost           bgregg-xenial-bpf-i-      1 systemd          4026531835 4026531839 4026531840 4026532197 4026531836 4026531837 4026531838\r\n02a7cf65f82e   agitated_perlman       4560 bash             4026531835 4026532381 4026532379 4026532384 4026532382 4026531837 4026532380\r\n3b2350c3cf2b   tender_almeida        25068 bash             4026531835 4026532442 4026532440 4026532445 4026532443 4026531837 4026532441\r\n```\r\n\r\nIn my prototype (not shown above), I color namespaces red that match the host, to make it easy to see which namespaces are shared with the host.\r\n\r\nSource: https://gist.github.com/brendangregg/1abcfeef9155ac526197f6f0abdd86bf"},{"labels":[null,"enhancement"],"text":"From scratch\r\nsome url .... rootfs.squashfile  / \r\n(at tar.gz , tar.xz , etc unpacks)  however no dice on squashfiles, on dockerhub;  for the moment requires conversion. \r\nthen dropbox or etc, then dockerhub.  \r\n\r\nunsquashfs -f -d *.squashfs | tar -cvpJf backup.tar.xz --exclude=~/rootfs.tar.xz --one-file-system ~/ \r\nNot sure it'd work but point made.  normally I dump to/tmp/isoname/  and repack , \r\n\r\n unpack a squashfile to / insted of the bother of converting. \r\nhowever I'll suffer the conversion .. for now...   drop to dropbox /dockerhub  , just would be a plus \r\n\r\n**Option for Squashfs files to be treated like tarballs would be a plus.** \r\n\r\nother future state possible ? \r\nFrom ISOFILE \r\nISOURL=\"\" \r\nsquashfs=\"my.squashfile  @ > / \r\n#### Unpack squashfs from iso to root discard rest. \r\nalso might be a plus. for rebaking iso's .. custom respins etc. \r\n\r\n"},{"labels":[null,null,"enhancement"],"text":"I've looked through tickets, code, google, but do not see any procedure for calling dockerd through the latest API and running a similar command as:\r\n```\r\ndocker stack deploy -c docker-compose-stack.yml foo\r\n```\r\n\r\nI'd like to be able to build my own service running in a container then call the hosts docker service (if appropriate) to deploy into swarm mode. I'd like to do this without something hacky like SSHing back into the host and running a shell. That would unnecessary require additional pem and account configuration on the host.\r\n\r\nIf you know a work around or a best practice, please advise. Else this should be added as a feature to the API."},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n```docker update``` does not seem to support pids-limit. At the moment, my particular use case is working around https://github.com/docker/docker/issues/32442. Using ```docker update``` after restoring a container seems like the perfect workaround. While this works great for some of the cgroup limits I set, pids-limit is not supported. I imagine there could be other use cases as well.\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker run --name test -d --rm ubuntu:14.04 /bin/bash -c 'i=0; while true; do echo $i; i=$(expr $i + 1); sleep 5; done'\r\n2. docker update --pids-limit=1000 test\r\n\r\n**Describe the results you received:**\r\nunknown flag: --pids-limit\r\nSee 'docker update --help'.\r\n\r\n**Describe the results you expected:**\r\nShould be able to update pids-limit of running container.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.03.0-ce\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   3a232c8\r\n Built:        Tue Feb 28 07:57:58 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.0-ce\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   3a232c8\r\n Built:        Tue Feb 28 07:57:58 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 4\r\nServer Version: 17.03.0-ce\r\nStorage Driver: overlay\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 977c511eda0925a723debdc94d09459af49d082a\r\nrunc version: a01dafd48bc1c7cc12bdb01206f9fea7dd6feb70\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 4.10.5-041005-generic\r\nOperating System: Ubuntu 14.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 14.94 GiB\r\nName: ip-10-97-3-17\r\nID: G3FR:5WAN:LTDA:O5EK:HAOA:3RJG:JKU5:HBDD:TTG5:GASD:PLMB:CFIK\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"Trying to install docker on a fresh install of Ubuntu 17.04 fails:\r\n\r\n```\r\n# curl -sSL https://get.docker.com/ | sh\r\napparmor is enabled in the kernel and apparmor utils were already installed\r\n+ apt-key add -\r\n+ echo -----BEGIN PGP PUBLIC KEY BLOCK-----\r\nVersion: GnuPG v1\r\n\r\nmQINBFWln24BEADrBl5p99uKh8+rpvqJ48u4eTtjeXAWbslJotmC/CakbNSqOb9o\r\nddfzRvGVeJVERt/Q/mlvEqgnyTQy+e6oEYN2Y2kqXceUhXagThnqCoxcEJ3+KM4R\r\nmYdoe/BJ/J/6rHOjq7Omk24z2qB3RU1uAv57iY5VGw5p45uZB4C4pNNsBJXoCvPn\r\nTGAs/7IrekFZDDgVraPx/hdiwopQ8NltSfZCyu/jPpWFK28TR8yfVlzYFwibj5WK\r\ndHM7ZTqlA1tHIG+agyPf3Rae0jPMsHR6q+arXVwMccyOi+ULU0z8mHUJ3iEMIrpT\r\nX+80KaN/ZjibfsBOCjcfiJSB/acn4nxQQgNZigna32velafhQivsNREFeJpzENiG\r\nHOoyC6qVeOgKrRiKxzymj0FIMLru/iFF5pSWcBQB7PYlt8J0G80lAcPr6VCiN+4c\r\nNKv03SdvA69dCOj79PuO9IIvQsJXsSq96HB+TeEmmL+xSdpGtGdCJHHM1fDeCqkZ\r\nhT+RtBGQL2SEdWjxbF43oQopocT8cHvyX6Zaltn0svoGs+wX3Z/H6/8P5anog43U\r\n65c0A+64Jj00rNDr8j31izhtQMRo892kGeQAaaxg4Pz6HnS7hRC+cOMHUU4HA7iM\r\nzHrouAdYeTZeZEQOA7SxtCME9ZnGwe2grxPXh/U/80WJGkzLFNcTKdv+rwARAQAB\r\ntDdEb2NrZXIgUmVsZWFzZSBUb29sIChyZWxlYXNlZG9ja2VyKSA8ZG9ja2VyQGRv\r\nY2tlci5jb20+iQIcBBABCgAGBQJWw7vdAAoJEFyzYeVS+w0QHysP/i37m4SyoOCV\r\ncnybl18vzwBEcp4VCRbXvHvOXty1gccVIV8/aJqNKgBV97lY3vrpOyiIeB8ETQeg\r\nsrxFE7t/Gz0rsLObqfLEHdmn5iBJRkhLfCpzjeOnyB3Z0IJB6UogO/msQVYe5CXJ\r\nl6uwr0AmoiCBLrVlDAktxVh9RWch0l0KZRX2FpHu8h+uM0/zySqIidlYfLa3y5oH\r\nscU+nGU1i6ImwDTD3ysZC5jp9aVfvUmcESyAb4vvdcAHR+bXhA/RW8QHeeMFliWw\r\n7Z2jYHyuHmDnWG2yUrnCqAJTrWV+OfKRIzzJFBs4e88ru5h2ZIXdRepw/+COYj34\r\nLyzxR2cxr2u/xvxwXCkSMe7F4KZAphD+1ws61FhnUMi/PERMYfTFuvPrCkq4gyBj\r\nt3fFpZ2NR/fKW87QOeVcn1ivXl9id3MMs9KXJsg7QasT7mCsee2VIFsxrkFQ2jNp\r\nD+JAERRn9Fj4ArHL5TbwkkFbZZvSi6fr5h2GbCAXIGhIXKnjjorPY/YDX6X8AaHO\r\nW1zblWy/CFr6VFl963jrjJgag0G6tNtBZLrclZgWhOQpeZZ5Lbvz2ZA5CqRrfAVc\r\nwPNW1fObFIRtqV6vuVluFOPCMAAnOnqR02w9t17iVQjO3oVN0mbQi9vjuExXh1Yo\r\nScVetiO6LSmlQfVEVRTqHLMgXyR/EMo7iQIcBBABCgAGBQJXSWBlAAoJEFyzYeVS\r\n+w0QeH0QAI6btAfYwYPuAjfRUy9qlnPhZ+xt1rnwsUzsbmo8K3XTNh+l/R08nu0d\r\nsczw30Q1wju28fh1N8ay223+69f0+yICaXqR18AbGgFGKX7vo0gfEVaxdItUN3eH\r\nNydGFzmeOKbAlrxIMECnSTG/TkFVYO9Ntlv9vSN2BupmTagTRErxLZKnVsWRzp+X\r\nelwlgU5BCZ6U6Ze8+bIc6F1bZstf17X8i6XNV/rOCLx2yP0hn1osoljoLPpW8nzk\r\nwvqYsYbCA28lMt1aqe0UWvRCqR0zxlKn17NZQqjbxcajEMCajoQ01MshmO5GWePV\r\niv2abCZ/iaC5zKqVT3deMJHLq7lum6qhA41E9gJH9QoqT+qgadheeFfoC1QP7cke\r\n+tXmYg2R39p3l5Hmm+JQbP4f9V5mpWExvHGCSbcatr35tnakIJZugq2ogzsm1djC\r\nSz9222RXl9OoFqsm1bNzA78+/cOt5N2cyhU0bM2T/zgh42YbDD+JDU/HSmxUIpU+\r\nwrGvZGM2FU/up0DRxOC4U1fL6HHlj8liNJWfEg3vhougOh66gGF9ik5j4eIlNoz6\r\nlst+gmvlZQ9/9hRDeoG+AbhZeIlQ4CCw+Y1j/+fUxIzKHPVK+aFJd+oJVNvbojJW\r\n/SgDdSMtFwqOvXyYcHl30Ws0gZUeDyAmNGZeJ3kFklnApDmeKK+OiQIiBBABCgAM\r\nBQJXe5zTBYMHhh+AAAoJEDG4FaMBBnSp7YMQAJqrXoBonZAq07B6qUaT3aBCgnY4\r\nJshbXmFb/XrrS75f7YJDPx2fJJdqrbYDIHHgOjzxvp3ngPpOpJzI5sYmkaugeoCO\r\n/KHu/+39XqgTB7fguzapRfbvuWp+qzPcHSdb9opnagfzKAze3DQnnLiwCPlsyvGp\r\nzC4KzXgV2ze/4raaOye1kK7O0cHyapmn/q/TR3S8YapyXq5VpLThwJAw1SRDu0Yx\r\neXIAQiIfaSxT79EktoioW2CSV8/djt+gBjXnKYJJA8P1zzX7GNt/Rc2YG0Ot4v6t\r\nBW16xqFTg+n5JzbeK5cZ1jbIXXfCcaZJyiM2MzYGhSJ9+EV7JYF05OAIWE4SGTRj\r\nXMquQ2oMLSwMCPQHm+FCD9PXQ0tHYx6tKT34wksdmoWsdejl/n3NS+178mG1WI/l\r\nN079h3im2gRwOykMou/QWs3vGw/xDoOYHPV2gJ7To9BLVnVK/hROgdFLZFeyRScN\r\nzwKm57HmYMFA74tX601OiHhk1ymP2UUc25oDWpLXlfcRULJJlo/KfZZF3pmKwIq3\r\nCilGayFUi1NNwuavG76EcAVtVFUVFFIITwkhkuRbBHIytzEHYosFgD5/acK0Pauq\r\nJnwrwKv0nWq3aK7nKiALAD+iZvPNjFZau3/APqLEmvmRnAElmugcHsWREFxMMjMM\r\nVgYFiYKUAJO8u46eiQI4BBMBAgAiBQJVpZ9uAhsvBgsJCAcDAgYVCAIJCgsEFgID\r\nAQIeAQIXgAAKCRD3YiFXLFJgnbRfEAC9Uai7Rv20QIDlDogRzd+Vebg4ahyoUdj0\r\nCH+nAk40RIoq6G26u1e+sdgjpCa8jF6vrx+smpgd1HeJdmpahUX0XN3X9f9qU9oj\r\n9A4I1WDalRWJh+tP5WNv2ySy6AwcP9QnjuBMRTnTK27pk1sEMg9oJHK5p+ts8hlS\r\nC4SluyMKH5NMVy9c+A9yqq9NF6M6d6/ehKfBFFLG9BX+XLBATvf1ZemGVHQusCQe\r\nbTGv0C0V9yqtdPdRWVIEhHxyNHATaVYOafTj/EF0lDxLl6zDT6trRV5n9F1VCEh4\r\nAal8L5MxVPcIZVO7NHT2EkQgn8CvWjV3oKl2GopZF8V4XdJRl90U/WDv/6cmfI08\r\nGkzDYBHhS8ULWRFwGKobsSTyIvnbk4NtKdnTGyTJCQ8+6i52s+C54PiNgfj2ieNn\r\n6oOR7d+bNCcG1CdOYY+ZXVOcsjl73UYvtJrO0Rl/NpYERkZ5d/tzw4jZ6FCXgggA\r\n/Zxcjk6Y1ZvIm8Mt8wLRFH9Nww+FVsCtaCXJLP8DlJLASMD9rl5QS9Ku3u7ZNrr5\r\nHWXPHXITX660jglyshch6CWeiUATqjIAzkEQom/kEnOrvJAtkypRJ59vYQOedZ1s\r\nFVELMXg2UCkD/FwojfnVtjzYaTCeGwFQeqzHmM241iuOmBYPeyTY5veF49aBJA1g\r\nEJOQTvBR8Q==\r\n=Yhur\r\n-----END PGP PUBLIC KEY BLOCK-----\r\n\r\nOK\r\n+ sh -c mkdir -p /etc/apt/sources.list.d\r\n+ dpkg --print-architecture\r\n+ sh -c echo deb \\[arch=amd64\\] https://apt.dockerproject.org/repo ubuntu-zesty main > /etc/apt/sources.list.d/docker.list\r\n+ sh -c sleep 3; apt-get update; apt-get install -y -q docker-engine\r\nIgn:1 http://dl.google.com/linux/chrome/deb stable InRelease\r\nHit:2 http://dl.google.com/linux/chrome/deb stable Release                                  \r\nHit:4 http://security.ubuntu.com/ubuntu zesty-security InRelease                            \r\nGet:5 http://us.archive.ubuntu.com/ubuntu zesty InRelease [243 kB]                \r\nHit:6 http://us.archive.ubuntu.com/ubuntu zesty-updates InRelease\r\nIgn:7 https://apt.dockerproject.org/repo ubuntu-zesty InRelease\r\nHit:8 http://us.archive.ubuntu.com/ubuntu zesty-backports InRelease\r\nIgn:9 https://apt.dockerproject.org/repo ubuntu-zesty Release\r\nIgn:10 https://apt.dockerproject.org/repo ubuntu-zesty/main all Packages\r\nIgn:11 https://apt.dockerproject.org/repo ubuntu-zesty/main amd64 Packages\r\nIgn:12 https://apt.dockerproject.org/repo ubuntu-zesty/main Translation-en\r\nIgn:13 https://apt.dockerproject.org/repo ubuntu-zesty/main Translation-en_US\r\nIgn:14 https://apt.dockerproject.org/repo ubuntu-zesty/main all DEP-11 Metadata\r\nIgn:15 https://apt.dockerproject.org/repo ubuntu-zesty/main amd64 DEP-11 Metadata\r\nIgn:16 https://apt.dockerproject.org/repo ubuntu-zesty/main DEP-11 64x64 Icons\r\nIgn:10 https://apt.dockerproject.org/repo ubuntu-zesty/main all Packages\r\nIgn:11 https://apt.dockerproject.org/repo ubuntu-zesty/main amd64 Packages\r\nIgn:12 https://apt.dockerproject.org/repo ubuntu-zesty/main Translation-en\r\nIgn:13 https://apt.dockerproject.org/repo ubuntu-zesty/main Translation-en_US\r\nIgn:14 https://apt.dockerproject.org/repo ubuntu-zesty/main all DEP-11 Metadata\r\nIgn:15 https://apt.dockerproject.org/repo ubuntu-zesty/main amd64 DEP-11 Metadata\r\nIgn:16 https://apt.dockerproject.org/repo ubuntu-zesty/main DEP-11 64x64 Icons\r\nIgn:10 https://apt.dockerproject.org/repo ubuntu-zesty/main all Packages\r\nIgn:11 https://apt.dockerproject.org/repo ubuntu-zesty/main amd64 Packages\r\nIgn:12 https://apt.dockerproject.org/repo ubuntu-zesty/main Translation-en\r\nIgn:13 https://apt.dockerproject.org/repo ubuntu-zesty/main Translation-en_US\r\nIgn:14 https://apt.dockerproject.org/repo ubuntu-zesty/main all DEP-11 Metadata\r\nIgn:15 https://apt.dockerproject.org/repo ubuntu-zesty/main amd64 DEP-11 Metadata\r\nIgn:16 https://apt.dockerproject.org/repo ubuntu-zesty/main DEP-11 64x64 Icons\r\nIgn:10 https://apt.dockerproject.org/repo ubuntu-zesty/main all Packages\r\nIgn:11 https://apt.dockerproject.org/repo ubuntu-zesty/main amd64 Packages\r\nIgn:12 https://apt.dockerproject.org/repo ubuntu-zesty/main Translation-en\r\nIgn:13 https://apt.dockerproject.org/repo ubuntu-zesty/main Translation-en_US\r\nIgn:14 https://apt.dockerproject.org/repo ubuntu-zesty/main all DEP-11 Metadata\r\nIgn:15 https://apt.dockerproject.org/repo ubuntu-zesty/main amd64 DEP-11 Metadata\r\nIgn:16 https://apt.dockerproject.org/repo ubuntu-zesty/main DEP-11 64x64 Icons\r\nIgn:10 https://apt.dockerproject.org/repo ubuntu-zesty/main all Packages\r\nIgn:11 https://apt.dockerproject.org/repo ubuntu-zesty/main amd64 Packages\r\nIgn:12 https://apt.dockerproject.org/repo ubuntu-zesty/main Translation-en\r\nIgn:13 https://apt.dockerproject.org/repo ubuntu-zesty/main Translation-en_US\r\nIgn:14 https://apt.dockerproject.org/repo ubuntu-zesty/main all DEP-11 Metadata\r\nIgn:15 https://apt.dockerproject.org/repo ubuntu-zesty/main amd64 DEP-11 Metadata\r\nIgn:16 https://apt.dockerproject.org/repo ubuntu-zesty/main DEP-11 64x64 Icons                                                                             \r\nIgn:10 https://apt.dockerproject.org/repo ubuntu-zesty/main all Packages                                                                                   \r\nErr:11 https://apt.dockerproject.org/repo ubuntu-zesty/main amd64 Packages                                                                                 \r\n  403  Forbidden\r\nIgn:12 https://apt.dockerproject.org/repo ubuntu-zesty/main Translation-en                                                                                 \r\nIgn:13 https://apt.dockerproject.org/repo ubuntu-zesty/main Translation-en_US                                                                              \r\nIgn:14 https://apt.dockerproject.org/repo ubuntu-zesty/main all DEP-11 Metadata                                                                            \r\nIgn:15 https://apt.dockerproject.org/repo ubuntu-zesty/main amd64 DEP-11 Metadata                                                                          \r\nIgn:16 https://apt.dockerproject.org/repo ubuntu-zesty/main DEP-11 64x64 Icons                                                                             \r\nFetched 243 kB in 6s (35.6 kB/s)                                                                                                                           \r\nReading package lists... Done\r\nW: The repository 'https://apt.dockerproject.org/repo ubuntu-zesty Release' does not have a Release file.\r\nN: Data from such a repository can't be authenticated and is therefore potentially dangerous to use.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nE: Failed to fetch https://apt.dockerproject.org/repo/dists/ubuntu-zesty/main/binary-amd64/Packages  403  Forbidden\r\nE: Some index files failed to download. They have been ignored, or old ones used instead.\r\nReading package lists...\r\nBuilding dependency tree...\r\nReading state information...\r\nPackage docker-engine is not available, but is referred to by another package.\r\nThis may mean that the package is missing, has been obsoleted, or\r\nis only available from another source\r\n\r\nE: Package 'docker-engine' has no installation candidate\r\n```"},{"labels":[null,null,"enhancement"],"text":"Now that `--network-add`, `--network-rm` is being worked on, I think it would be good to add support for _services_ to `docker network connect`, `docker network disconnect`.\r\n\r\nI'm not sure what's the best way to implement this; doing these actions would probably mean \"updating the service spec\", in which case it could be a client-side only change. Alternatively, [`/networks/{id}/connect` and `/networks/{id}/disconnect` API endpoints](https://docs.docker.com/engine/api/v1.27/#operation/NetworkCreate) need an update to allow specifying a _service_ in the body (current body looks like this);\r\n\r\n```json\r\n{\r\n  \"Container\": \"3613f73ba0e4\",\r\n  \"EndpointConfig\": {\r\n    \"IPAMConfig\": {}\r\n  }\r\n}\r\n```\r\n\r\nrelates to https://github.com/docker/docker/pull/32062, https://github.com/docker/docker/issues/28247\r\n\r\n/cc @aaronlehmann @mavenugo "},{"labels":[null,null,"enhancement",null],"text":"There are several scripts in the [hack folder](https://github.com/docker/docker/tree/master/hack). It definitely deserves a good README 😄 "},{"labels":[null,null,"enhancement"],"text":"In https://github.com/docker/docker/pull/31144 we added support for `-d` and concluded that it should **eventually** default to `false`, however in order not to break the behavior we temporarily defaulted to `-d=true` and added a warning.\r\n\r\nThis issue is to keep track of:\r\n- Making `-d=false` the default\r\n- Removing the warning\r\n\r\nIt should be marked `P1` and milestoned with the proper release.\r\n\r\nAre we thinking `17.09`?\r\n\r\n@thaJeztah What's your opinion? Once we figure this out, would you mind creating the appropriate milestone and setting it to this issue?\r\n\r\nI think in the future in order to properly handle deprecation, we should probably have milestones for the next 12 months or so and create P1 issues in advance so we never forget at release time.\r\n\r\n/cc @aaronlehmann @vieux "},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nInstalling Docker on Linode via the `curl https://get.docker.com/ | sh` method [warns about missing AUFS support](https://github.com/docker/docker/blob/9f17b97820912001a1bf7fd827a65b0298b4dd1f/hack/install.sh#L428-L449). However, the Linode-supplied kernel *does* have the `overlay` module, and thus Docker's `overlay2` storage driver is used by default.\r\n\r\nAs `overlay2` seems to be the successor to AUFS, perhaps this warning should be suppressed when it's available?\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create new Ubuntu 16.04 LTS or Debian 8 Stable VPS on Linode\r\n2. As root, run `wget -nv -O - https://get.docker.com/ | sh`\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\nWarning: current kernel is not supported by the linux-image-extra-virtual\r\n package.  We have no AUFS support.  Consider installing the packages\r\n linux-image-virtual kernel and linux-image-extra-virtual for AUFS support.\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nInstallation without warnings."},{"labels":[null,null,"enhancement",null,null],"text":"**Description**\r\n\r\nI foundt that SwarmKit in docker does not validate health check parameters in a service create request.\r\n\r\n**Steps to reproduce the issue:**\r\n1. use api to create a service, in the service spec, we add parameter of HealthCheck.\r\n2. in parameter, we add an invalid value, like `timeout -1s, retries -1`, valid timeout should be larger than 1s, and valid retires should be a positive integer.\r\n3. send the request to create this service\r\n\r\nrequest like the following pic:\r\n![wechatimg8](https://cloud.githubusercontent.com/assets/9465626/24639949/eb42ecb8-1925-11e7-8154-6f05a47ad069.jpeg)\r\n\r\n\r\n\r\n**Describe the results you received:**\r\nService created OK\r\n\r\n**Describe the results you expected:**\r\nparameter invalid\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nroot@ubuntu:~# docker version\r\nClient:\r\n Version:      17.03.0-ce\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 10:57:47 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.0-ce\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 10:57:47 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nroot@ubuntu:~# docker info\r\nContainers: 3\r\n Running: 3\r\n Paused: 0\r\n Stopped: 0\r\nImages: 116\r\nServer Version: 17.03.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 148\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: vfb7vr7wp1dirmop8fbvmmgh7\r\n Is Manager: true\r\n ClusterID: mzzbi2s44h4vesargizzywryz\r\n Managers: 1\r\n Nodes: 2\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 192.168.59.103\r\n Manager Addresses:\r\n  192.168.59.103:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 977c511eda0925a723debdc94d09459af49d082a\r\nrunc version: a01dafd48bc1c7cc12bdb01206f9fea7dd6feb70\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 3.19.0-25-generic\r\nOperating System: Ubuntu 14.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1.954 GiB\r\nName: ubuntu\r\nID: FXOY:JCOY:HKDI:VO5U:FYDM:UEXV:YIFN:AISM:NR6U:VMW5:V4MZ:RQWF\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 52\r\n Goroutines: 154\r\n System Time: 2017-03-27T14:28:38.061668428+08:00\r\n EventsListeners: 3\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,null,"enhancement"],"text":"## Problem statement\r\nAs a `developer and sysadmin`, I would like to run `containers` that are treated as `system containers` and therefore they should filtered from `docker ps`. It would be interesting for use-cases such as: \r\n 1) `analytics/monitoring` containers\r\n 2) `logging` containers\r\n 3) `service discovery` containers\r\n\r\n## Proposed solution\r\nAdd a new flag `--system` to the `docker ps / docker service ps / docker ps` commands. Such containers should be filtered by default, but you can list them if `explicitly` asked for (e.g. `docker ps --system -a`). \r\n\r\n### Benefits\r\n- Prevent users from deleting `important` containers\r\n- Facilitate container management\r\n- Ability to easily distinguish `system containers` from normal user containers \r\n\r\n### Example\r\n```\r\ndocker run -i -t --system --name sysdig --privileged -v /var/run/docker.sock:/host/var/run/docker.sock -v /dev:/host/dev -v /proc:/host/proc:ro -v /boot:/host/boot:ro -v /lib/modules:/host/lib/modules:ro -v /usr:/host/usr:ro  sysdig/sysdig\r\n```\r\n\r\n/cc @anusha-ragunathan @cpuguy83 "},{"labels":[null,"enhancement"],"text":"The prompt for `docker system prune` says `[y/N]` which seems like it's\r\nsaying \"`N` is the default\", but if I just hit enter, nothing happens.\r\nYes, I then guessed that it's waiting for some non-whitespace character\r\nand if it's not \"y\" then it aborts.\r\n\r\nThis is unlike the usual way such prompts are implemented, where hitting\r\nenter is the same as entering the default (\"n\" in this case).\r\n"},{"labels":[null,"enhancement",null,null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nCannot supply value for `replicas` from environment variables during `docker stack deploy`, as it complains the environment variable is not an integer (and no documented way to force an integer).\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create compose version 3.1 file with a service with a deploy & replica section, where the value for replicas is using a variable, with or without a default (see provided sample config).\r\n2. Run a stack deploy with a provided environment variable, or use the default value for the variable in the compose file.\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\n$ REPLICAS=1 docker stack deploy -c test-compose.yml test\r\nservices.redis.deploy.replicas must be a integer\r\n# or with the default value:\r\n$ docker stack deploy -c test-compose.yml test\r\nservices.redis.deploy.replicas must be a integer\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nTo accept that the fact that REPLICAS environment variable is a string, but verify and allow the value if it can be treated as an integer. Same for variables that have a default value.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nMy test-compose.yml from above.\r\n\r\n```\r\nversion: '3.1'\r\n\r\nservices:\r\n  redis:\r\n    image: redis\r\n    deploy:\r\n      replicas: ${REPLICAS:-1}\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.03.0-ce\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 10:40:59 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      17.03.0-ce\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   3a232c8\r\n Built:        Tue Feb 28 07:52:04 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 79\r\nServer Version: 17.03.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 134\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: wrkh6nb5hoelqjjy0qkx62xrj\r\n Is Manager: true\r\n ClusterID: q5a1qw2bcj8cn7gmwsdgp19i9\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 192.168.65.2\r\n Manager Addresses:\r\n  192.168.65.2:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 977c511eda0925a723debdc94d09459af49d082a\r\nrunc version: a01dafd48bc1c7cc12bdb01206f9fea7dd6feb70\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.12-moby\r\nOperating System: Alpine Linux v3.5\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 1.952 GiB\r\nName: moby\r\nID: YKZ3:M2MK:CKR6:ZC65:RDXM:22SB:WFXC:32T2:AD55:QQXF:C4FR:43FO\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 32\r\n Goroutines: 130\r\n System Time: 2017-03-29T20:18:03.330922883Z\r\n EventsListeners: 1\r\nNo Proxy: *.local, 169.254/16\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nRunning Docker for Mac stable, OS X  10.11.6"},{"labels":[null,null,"enhancement"],"text":"platform: docker toobox for windows 17.0.03\r\n\r\nWhen running a container with a mount from a host drive, the docker host folder contents overwrites the container's folder contents entirely. There is no overlay of data as detailed in the docs.\r\n\r\nmy ubuntu container command: \r\n\r\n> docker run -d -P --name MYCONTAINER -v /c/Users/drupal8_www:/var/www/html MY:IMAGE\r\n\r\nby stopping and starting this i can clearly see and repeat the effect describe above.\r\n\r\nthanks"},{"labels":[null,null,"enhancement",null,null],"text":"**Description**\r\n\r\n\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Gather lot of garbage\r\n2. run `docker system prune -a`\r\n\r\n**Describe the results you received:**\r\n\r\nNo output\r\n\r\n**Describe the results you expected:**\r\n\r\nOutput showing the progress.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.04.0-dev\r\n API version:  1.27\r\n Go version:   go1.7.5\r\n Git commit:   08bbd43\r\n Built:        Tue Mar 14 17:10:25 2017\r\n OS/Arch:      windows/amd64\r\n\r\nServer:\r\n Version:      17.04.0-dev\r\n API version:  1.27 (minimum version 1.24)\r\n Go version:   go1.7.5\r\n Git commit:   08bbd43\r\n Built:        Tue Mar 14 17:10:25 2017\r\n OS/Arch:      windows/amd64\r\n Experimental: false\r\n```"},{"labels":[null,null,"enhancement"],"text":"When deploying a stack with a single service from a compose file, a network is still created for that stack (default_[stack name]) even though the network is unnecessary.\r\n\r\n`docker stack deploy` should notice that the compose file contains only one service definition and avoid creating the default network for the stack."},{"labels":[null,"enhancement",null],"text":"## Proposal: Filesystem integrity check\r\n**Experimental & opt-in flag required**\r\n\r\nCreate a build option which records a digest of an image's read-only filesystem. It would be stored in the image manifest. At container run-time, if a filesystem digest is present in the manifest, the filesystem is verified against the digest before container execution. If verification fails, an error would be thrown and container execution stopped.\r\n\r\nThe filesystem digest must be stable, computing the same value on different systems (though on the same platform). The current consideration is [continuity](https://github.com/stevvooe/continuity). The digest would be included in the image manifest, as an optional field of `RootFS`.\r\n\r\nThis proposal only pertains to filesystem integrity. Other portions of the data or distribution pipeline are out-of-scope.\r\n\r\n## Background\r\nDocker images have some mechanisms for ensuring data integrity - that image data at run-time is the same as data at build-time. Docker notary can supply signed digests for an image manifest. The image manifest is downloaded as _image@digest_ and the digest verified at download.\r\n\r\nImage manifests contain a `RootFS` JSON object with a sequence of digests of layers. Layers are tarballs that are downloaded, uncompressed, and metadata for tar-split persisted. The layer digest is checked at download. If all layers were downloaded (and digests verified) for new containers and restarts, the container filesystem should be authentic and match data from build-time. However, layer data is cached locally and the original tarball discarded, so digest checks at download do not guarantee current data integrity.\r\n\r\nA property of (read-only) layers is the original tar stream can be re-created, and optionally checked against the expected digest. However, verifying the tar stream is not the same as the container filesystem. There may be additional files or files masked by whiteouts, which do not appear in the tar stream.\r\n\r\n## Alternate approaches\r\n### dm-verity\r\n[dm-verity](https://gitlab.com/cryptsetup/cryptsetup/wikis/DMVerity) is an excellent way to ensure integrity of a read-only block device. It is not well suited for container images. A container filesystem is composed of stacked layers of decompressed tarballs. The result is not block-equivalent to a build system, but logically equivalent as files. Additionally, loopback devices are necessary for each dm-verify mount. There are resource limits on the number of loopback devices which would limit the number of containers or layers, depending on the implementation.\r\n\r\n### Downloading tarball every time or persisting original tarball\r\nIf the original, digest verified data was available, it would be possible to re-created a filesystem from verified data. However, the additional storage space, network availability, bandwidth, and latency necessary would be onerous.\r\n\r\n### Verifying re-created tar stream of layers\r\nAs discussed in [background](#background), tar stream data can be re-created from layers. This initially seems promising for ensuring the integrity of file data. However, the tar stream is not exactly the same as the filesystem data. When recreating the tar stream, only select files are inspected. Any additional files would not be inspected. Files masked by whiteouts may appear in the tar stream, but not in the filesystem. The is also a breaking of abstraction by comparing layer data to the filesystem. The filesystem is prepared by the configured graphdriver. Graphdrivers have very little high-level data and the layer store has no information on how layers are overlayed by the graphdriver.\r\n\r\n### Computing digest on first install\r\nBecause layer data is verified at download, it is possible to compute a digest of the filesystem at first use. For later uses of the filesystem, the digest from first use can be calculated again and compared. With this approach, the computed digest is stored so only root on the host OS may modify the digest file. This is the same level of security of the cached layer data itself. Additionally, there is no way to audit the computed digest against a known good value."},{"labels":[null,"enhancement"],"text":"Basically https://github.com/docker/docker/blob/master/api/server/httputils/errors.go#L52 has gone on far too long.\r\n\r\nTo replace this, we've tried a couple of different approaches which were pretty disastrous:\r\n\r\n- errcode, which while there's nothing wrong with the errcode package, we used it horribly wrong and for the wrong reasons.\r\n- HTTP errors package - https://github.com/docker/docker/blob/master/api/errors/errors.go#L13\r\n\r\nerrcode was a neat concept but in practice was misused and made the entire codebase just became absolutely horrible to work on where as before the horribleness was contained to just the API layer.\r\n\r\nThis lead to the http errors package currently in use for certain cases. This also seems like an \"ok\" thing, but in practice is also not really used correctly and generally more of a shortcut to correct error handling. We end up with low-level bits worried about HTTP status codes.\r\n\r\nWe need to standardize on a way to handle errors in the API. This potentially needs to be regardless of how the request came in (HTTP, GRPC, or even a direct function call...e.g. cluster->daemon).\r\n\r\nThe goal of good error handling is that the caller knows, or at least knows how to find out, the class of error that occurred so that the error can be dealt with appropriately. This is true of function calls and it is true across the \"remote\" API barrier.\r\n\r\n### Goals\r\n\r\n- API backends (e.g. `daemon`) should ideally not be concerned with HTTP.\r\n- API clients should be able to read a returned error and be able to know *what* an error is.\r\n- Reduce the cognitive overhead in trying to determine how to return an error\r\ni. Example, what status code? (400, 409, 422, 412)\r\nii. How do I get the API to produce that status code I expect?\r\niii. How would non-remote API callers handle my error?\r\n- Ensure status codes don't change because of either a change to the error text, or hopefully even a change to the low-level error type.\r\n\r\nThe way I see it there are really two ways to deal with this (in no particular order):\r\n\r\n### Error class defined in common API package\r\n\r\nThis means in the API package we define several error classes like so (psuedo-code):\r\n\r\n```go\r\npackage api\r\n\r\ntype Error interface {\r\n    error\r\n    NotFound() bool\r\n    // other fn's for conflict, invalid arg, etc\r\n}\r\n\r\ntype apiError struct { \r\n    err error\r\n    notFound bool\r\n    conflict bool\r\n    invalidArg bool\r\n}\r\n\r\nfunc (e *apiError) Error() string { return e.err.Error() }\r\nfunc (e *apiError) NotFound() bool { return e.notFound }\r\n\r\nfunc NewNotFoundError(err error) Error {\r\n    return &apiError{err: err, notFound: true}\r\n}\r\n```\r\n\r\nFrom here the HTTP layer can determine both the error message and the class of error (not found) and the status code to return.\r\n\r\nThis approach is similar to the current approach we have except it's moving the http handling to the http layer and non-http code can just deal with this error type.\r\n\r\nAnother slight alternative but generally the same thing to this would be to use the well defined grpc error codes instead of defining our own. This could fit in well since part of the daemon is already using grpc anyway.\r\n\r\n### Define error handling interfaces on backends\r\n\r\nIn this method, backends would be required to define a set of functions for error handling:\r\n\r\nExample:\r\n\r\n```go\r\npackage api\r\n\r\ntype ErrorInspector interface {\r\n    IsNotFound(error) bool\r\n    IsConflict(error) bool\r\n    IsInvalidArgument(error) bool\r\n}\r\n```\r\n\r\n```go\r\npackage container // container router\r\n\r\nimport \"github.com/docker/docker/api\"\r\n\r\ntype ContainerBackend interface {\r\n    CreateContainer(...) error\r\n    DeleteContainer(...) error\r\n    api.ErrorInspector\r\n}\r\n\r\nfunc(r *containerRouter) getContainer(w http.ResponseWriter, req *http.Request) error {\r\n    err := r.backend.DeleteContainer(...)\r\n    if err != nil {\r\n        if r.backend.IsNotFound(err) {\r\n           // make an HTTP 404\r\n       }\r\n       // make an HTTP 500 probably\r\n    }\r\n\r\n    // normal\r\n}\r\n```\r\n\r\nWriting out the above it seems a little risky since it requires the router to know the failure-modes of the backend. Maybe could be cleaned up a bit.\r\n\r\n\r\nInterested in your thoughts, other alternatives, etc.\r\n\r\nping @docker/core-engine-maintainers "},{"labels":[null,"enhancement"],"text":"(This came up when I tried to construct dockerfile programmatically.)\r\n\r\nIt looks like the parsing of `ENV` lines is a rough hack.  Specifically, it seems that:\r\n\r\n* If the line ends, no error is thrown if a variable is mid-value: `ENV VAR=\"1234` results in `VAR` being set to `1234`.\r\n\r\n* Backslashes are implemented as half-quote-chars, applicable only for double quotes and dollar signs?  What I see is that\r\n  - `ENV VAR=\"12\\\"34\"` works as expected,\r\n  - `ENV VAR=\"12\\34\"` has a backslash between the `2` and the `3`,\r\n  - `ENV VAR=\"12\\\\34\"` has two backslashes.\r\n\r\n* As a result, you cannot use `ENV` to have a backslash before a `$variable` or at the end of the string.  For dockerfile-generation tools, the implication is usually more severe, they should just avoid backslashes completely (or become very complicated).\r\n\r\n* In addition, it looks like there is no way to have a newline in the string value?\r\n\r\nIs there a way to get some consistent quoting scheme?  (I expected some json-like syntax, but looks like there isn't one?)\r\n\r\nGiven all of these, it would be nice if the parsing got cleaned up to make backslashes be proper escape characters similarly to how they're treated in (double-quoted) shell strings, which would imply throwing an error for `\\x` with an unknown `x`, which in turn would make it possible to parse `\\n` as a newline.\r\n\r\nBut I'm sure that you'd worry about backward compatibility so the minimum that could be done with this is clarify these limitations in the reference page.\r\n"},{"labels":[null,null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nSwarm mode, ```docker service create --mount type=tmpfs```,  how to remove noexec flag?\r\n\r\n**Steps to reproduce the issue:**\r\n1.  docker service create --mount type=tmpfs,destination=/data,tmpfs-size=100M -p:22 rastasheep/ubuntu-sshd:14.04\r\n2. ssh root@localhost -p 30000\r\n3. mount | grep /data\r\n\r\n**Describe the results you received:**\r\ntmpfs on /data type tmpfs (rw,nosuid,nodev,noexec,relatime,size=102400k)\r\n\r\n**Describe the results you expected:**\r\nI want to remove noexec flag, how? like, ```docker run -it --rm --tmpfs /data:exec bash```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.03.0-ce\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 10:53:04 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.0-ce\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 10:53:04 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 1\r\n Paused: 0\r\n Stopped: 0\r\nImages: 7\r\nServer Version: 17.03.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 72\r\n Dirperm1 Supported: false\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: 8t1tc9olkkkvto0m9ap20yg4q\r\n Is Manager: true\r\n ClusterID: mjf13s8g3luwoo579kbtkxvau\r\n Managers: 1\r\n Nodes: 2\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 172.25.20.22\r\n Manager Addresses:\r\n  172.25.20.22:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 977c511eda0925a723debdc94d09459af49d082a\r\nrunc version: a01dafd48bc1c7cc12bdb01206f9fea7dd6feb70\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 3.13.0-98-generic\r\nOperating System: Ubuntu precise (12.04.5 LTS)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 993.5 MiB\r\nName: docker0\r\nID: HPTL:DPNP:QNJ6:6K5R:NFNT:CMFE:RWWP:TWAW:5DAO:LZXV:DEKY:5VKZ\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 44\r\n Goroutines: 136\r\n System Time: 2017-03-27T10:03:52.81516827Z\r\n EventsListeners: 1\r\nUsername: huozic\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.): **\r\nVirtualBox 5.1.16 r113841\r\nubuntu 12.04 3.13.0-98-generic\r\n"},{"labels":[null,"enhancement",null,null],"text":"**Description**\r\n\r\nI'm using Kontena and OSSEC so this issue became very visible due to OSSEC not recognizing these log entries and Kontena creating a log of them, but the gist is that I propose the loglevel of these entries be reduced to \"warning\" or lower since they are harmless but may be abundant in some cases.\r\n\r\nExample:\r\n\r\n```\r\nMar 23 10:41:14 admin1 dockerd[15092]: time=\"2017-03-23T10:41:14.722146447-05:00\" level=error msg=\"Handler for GET /v1.26/containers/foobar/json returned error: No such container: foobar\"\r\nMar 23 10:41:14 admin1 dockerd[15092]: time=\"2017-03-23T10:41:14.722848795-05:00\" level=error msg=\"Handler for GET /v1.26/images/foobar/json returned error: No such image: foobar\"\r\nMar 23 10:41:14 admin1 dockerd[15092]: time=\"2017-03-23T10:41:14.724098031-05:00\" level=error msg=\"Handler for GET /v1.26/networks/foobar returned error: network foobar not found\"\r\nMar 23 10:41:14 admin1 dockerd[15092]: time=\"2017-03-23T10:41:14.724706840-05:00\" level=error msg=\"Handler for GET /v1.26/volumes/foobar returned error: get foobar: no such volume\"\r\nMar 23 10:41:14 admin1 dockerd[15092]: time=\"2017-03-23T10:41:14.745225668-05:00\" level=error msg=\"Handler for GET /v1.26/plugins/foobar/json returned error: plugin \\\"foobar\\\" not found\"\r\n```\r\n\r\n**Steps to reproduce the issue:**\r\n1. Run `docker inspect foobar` (assuming you don't have a container named foobar)\r\n2. Inspect syslogs (e.g. `journalctl --since \"2 minutes ago\" -u docker.service`)\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nOSSEC is commonly used IDS and alerts unrecognized syslog entries. Kontena and probably other scheduling systems may query non-existent containers frequently. This results in lots of error alerts for errors that really aren't important. If they are important to debug an issue then docker loglevel should be increased. The error alerting can be suppressed but these entries still are just wasting space and bandwidth considering lots of distributed systems transmit syslogs to central logging system.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.03.0-ce\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 11:02:43 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.0-ce\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 11:02:43 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```"},{"labels":[null,"enhancement"],"text":"I'm running a machine on which multiple docker containers are running and where new images are build at the same time. To prevent the intermediate containers created during the build from using too much memory (so the other running containers are not deprived from memory), I've limited the maximum memory the build container is allowed to use using `docker build -f dockerfile --pull --memory=\"512m\" -t test .`\r\n\r\nI've noticed that this does help restricting memory usage, but occasionally can lead to the oom killer being triggered:\r\n\r\n```\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.482993] grunt invoked oom-killer: gfp_mask=0x24000c0, order=0, oom_score_adj=0\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.482999] grunt cpuset=112dcd167f8491026f719a58b3b87d69e63b3ec666146f21270da8307ed38810 mems_allowed=0\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483008] CPU: 1 PID: 28564 Comm: grunt Not tainted 4.4.0-67-generic #88-Ubuntu\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483011] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.9.1-0-gb3ef39f-prebuilt.qemu-project.org 04/01/2014\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483019]  0000000000000286 00000000445df54c ffff8800749c7c68 ffffffff813f86d3\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483024]  ffff8800749c7d48 ffff88000a3c5940 ffff8800749c7cd8 ffffffff8120b24e\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483029]  ffff88013fd16dc0 ffff8800749c7ca8 ffffffff811922fb ffff880002e1d940\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483033] Call Trace:\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483047]  [<ffffffff813f86d3>] dump_stack+0x63/0x90\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483053]  [<ffffffff8120b24e>] dump_header+0x5a/0x1c5\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483058]  [<ffffffff811922fb>] ? find_lock_task_mm+0x3b/0x80\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483062]  [<ffffffff811928c2>] oom_kill_process+0x202/0x3c0\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483067]  [<ffffffff811feec4>] ? mem_cgroup_iter+0x204/0x390\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483072]  [<ffffffff81200f23>] mem_cgroup_out_of_memory+0x2b3/0x300\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483077]  [<ffffffff81201cf8>] mem_cgroup_oom_synchronize+0x338/0x350\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483081]  [<ffffffff811fcdd0>] ? kzalloc_node.constprop.49+0x20/0x20\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483085]  [<ffffffff81192f74>] pagefault_out_of_memory+0x44/0xc0\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483090]  [<ffffffff8106b2b2>] mm_fault_error+0x82/0x160\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483094]  [<ffffffff8106b768>] __do_page_fault+0x3d8/0x400\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483097]  [<ffffffff8106b7f7>] trace_do_page_fault+0x37/0xe0\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483102]  [<ffffffff81063ef9>] do_async_page_fault+0x19/0x70\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483106]  [<ffffffff8183e828>] async_page_fault+0x28/0x30\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483153] Task in /docker/112dcd167f8491026f719a58b3b87d69e63b3ec666146f21270da8307ed38810 killed as a result of limit of /docker/112dcd167f8491026f719a58b3b87d69e63b3ec666146f21270da8307ed38810\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483161] memory: usage 524184kB, limit 524288kB, failcnt 249307\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483164] memory+swap: usage 1048576kB, limit 1048576kB, failcnt 6738\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483166] kmem: usage 6296kB, limit 9007199254740988kB, failcnt 0\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483168] Memory cgroup stats for /docker/112dcd167f8491026f719a58b3b87d69e63b3ec666146f21270da8307ed38810: cache:128KB rss:517760KB rss_huge:0KB mapped_file:44KB dirty:0KB writeback:0KB swap:524392KB inactive_anon:259084KB active_anon:258676KB inactive_file:64KB active_file:64KB unevictable:0KB\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483187] [ pid ]   uid  tgid total_vm      rss nr_ptes nr_pmds swapents oom_score_adj name\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483431] [28211]     0 28211     1084      171       8       3       25             0 sh\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483436] [28253]     0 28253   226297     2986      64      28     6657             0 grunt\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483440] [28504]     0 28504   233514     4339      77      41     9373             0 grunt\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483443] [28564]     0 28564   504064   131255     954     769   114997             0 grunt\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483450] Memory cgroup out of memory: Kill process 28564 (grunt) score 945 or sacrifice child\r\nMar 22 17:49:28 37-97-165-17 kernel: [97823.483573] Killed process 28564 (grunt) total-vm:2016256kB, anon-rss:512408kB, file-rss:12612kB\r\n```\r\n\r\nWould it be nice to disable the oom-killer for build containers, just as this is an option with `docker run`?"},{"labels":[null,"enhancement"],"text":"**Description**\r\ndocker stack deploy is additive, in that ... if I have uniquely named services within 2 compose files, after this command ...\r\n\r\n`docker stack deploy --compose-file service_1.yml my-stack`\r\n\r\n... I will have one service, in one stack... \r\n```\r\nNAME       SERVICES\r\nmy-stack  1\r\n```\r\nand if I then run a different compose file, with a different service name...\r\n\r\n`docker stack deploy --compose-file service_2.yml my-stack`\r\n\r\nI will still have the one stack, but 2 services.\r\n\r\n```\r\nNAME       SERVICES\r\nmy-stack  2\r\n```\r\n\r\nNow, this is a really useful pattern, as you get some of the benefits of the stack (the namespacing, the idempotent deploy cmd), but you can define each service separately, and update them separately etc.\r\n\r\nBut the reverse doesn't work, i.e. I can't do ...\r\n\r\n`docker stack remove --compose-file service_1.yml my-stack`\r\n\r\n.. and just remove the services in the given compose file back off the stack (leaving those added with the _2.yml file).  The above just yields an error, as there is no \"--compose-file\" flag for remove.\r\n\r\nI can individually remove the services in the compose file, by name, and get the same result, but then I can't take advantage of all the service/nw names being defined already in the compose file.\r\n\r\n_The above was based on how I observed it to work in 17.04.0-ce-rc1_"},{"labels":[null,"enhancement"],"text":"The apt.dockerproject.org and yum.dockerproject.org repositories have index pages, allowing people to navigate the repositories, and easily find the RPM's / deb packages.\r\n\r\ndownload.docker.com does not have index pages, which leads to a lot of confusion (e.g. https://github.com/docker/docker/issues/31940), because trying to browse the repository shows 403 access denied and an ugly XML error message.\r\n\r\nIt also makes it difficult to find out which packages are available, and which architectures are supported (also see https://github.com/docker/docker.github.io/issues/2085)\r\n\r\nping @andrewhsu @dmp42 PTAL"},{"labels":[null,null,"enhancement"],"text":"I encountered this message in the log:\r\n\r\n```\r\nCommand \"daemon\" is deprecated, and will be removed in Docker 1.16. Please run `dockerd` directly\r\n```\r\n\r\nI think this needs to be updated to refer to a different version number."},{"labels":[null,"enhancement"],"text":"Docker commands have lots of flags (specifically `docker run`), most of which will not ever be used by a majority of Docker users.\r\n\r\nHere is `docker run --help` now:\r\n```\r\n      --add-host list                         Add a custom host-to-IP mapping (host:ip) (default [])\r\n  -a, --attach list                           Attach to STDIN, STDOUT or STDERR (default [])\r\n      --blkio-weight uint16                   Block IO (relative weight), between 10 and 1000, or 0 to disable (default 0)\r\n      --blkio-weight-device weighted-device   Block IO weight (relative device weight) (default [])\r\n      --cap-add list                          Add Linux capabilities (default [])\r\n      --cap-drop list                         Drop Linux capabilities (default [])\r\n      --cgroup-parent string                  Optional parent cgroup for the container\r\n      --cidfile string                        Write the container ID to the file\r\n      --cpu-count int                         CPU count (Windows only)\r\n      --cpu-percent int                       CPU percent (Windows only)\r\n      --cpu-period int                        Limit CPU CFS (Completely Fair Scheduler) period\r\n      --cpu-quota int                         Limit CPU CFS (Completely Fair Scheduler) quota\r\n      --cpu-rt-period int                     Limit CPU real-time period in microseconds\r\n      --cpu-rt-runtime int                    Limit CPU real-time runtime in microseconds\r\n  -c, --cpu-shares int                        CPU shares (relative weight)\r\n      --cpus decimal                          Number of CPUs (default 0.000)\r\n      --cpuset-cpus string                    CPUs in which to allow execution (0-3, 0,1)\r\n      --cpuset-mems string                    MEMs in which to allow execution (0-3, 0,1)\r\n      --credentialspec string                 Credential spec for managed service account (Windows only)\r\n  -d, --detach                                Run container in background and print container ID\r\n      --detach-keys string                    Override the key sequence for detaching a container\r\n      --device list                           Add a host device to the container (default [])\r\n      --device-read-bps throttled-device      Limit read rate (bytes per second) from a device (default [])\r\n      --device-read-iops throttled-device     Limit read rate (IO per second) from a device (default [])\r\n      --device-write-bps throttled-device     Limit write rate (bytes per second) to a device (default [])\r\n      --device-write-iops throttled-device    Limit write rate (IO per second) to a device (default [])\r\n      --disable-content-trust                 Skip image verification (default true)\r\n      --dns list                              Set custom DNS servers (default [])\r\n      --dns-option list                       Set DNS options (default [])\r\n      --dns-search list                       Set custom DNS search domains (default [])\r\n      --entrypoint string                     Overwrite the default ENTRYPOINT of the image\r\n  -e, --env list                              Set environment variables (default [])\r\n      --env-file list                         Read in a file of environment variables (default [])\r\n      --expose list                           Expose a port or a range of ports (default [])\r\n      --group-add list                        Add additional groups to join (default [])\r\n      --health-cmd string                     Command to run to check health\r\n      --health-interval duration              Time between running the check (ns|us|ms|s|m|h) (default 0s)\r\n      --health-retries int                    Consecutive failures needed to report unhealthy\r\n      --health-timeout duration               Maximum time to allow one check to run (ns|us|ms|s|m|h) (default 0s)\r\n      --help                                  Print usage\r\n  -h, --hostname string                       Container host name\r\n      --init                                  Run an init inside the container that forwards signals and reaps processes\r\n      --init-path string                      Path to the docker-init binary\r\n  -i, --interactive                           Keep STDIN open even if not attached\r\n      --io-maxbandwidth string                Maximum IO bandwidth limit for the system drive (Windows only)\r\n      --io-maxiops uint                       Maximum IOps limit for the system drive (Windows only)\r\n      --ip string                             Container IPv4 address (e.g. 172.30.100.104)\r\n      --ip6 string                            Container IPv6 address (e.g. 2001:db8::33)\r\n      --ipc string                            IPC namespace to use\r\n      --isolation string                      Container isolation technology\r\n      --kernel-memory string                  Kernel memory limit\r\n  -l, --label list                            Set meta data on a container (default [])\r\n      --label-file list                       Read in a line delimited file of labels (default [])\r\n      --link list                             Add link to another container (default [])\r\n      --link-local-ip list                    Container IPv4/IPv6 link-local addresses (default [])\r\n      --log-driver string                     Logging driver for the container\r\n      --log-opt list                          Log driver options (default [])\r\n      --mac-address string                    Container MAC address (e.g. 92:d0:c6:0a:29:33)\r\n  -m, --memory string                         Memory limit\r\n      --memory-reservation string             Memory soft limit\r\n      --memory-swap string                    Swap limit equal to memory plus swap: '-1' to enable unlimited swap\r\n      --memory-swappiness int                 Tune container memory swappiness (0 to 100) (default -1)\r\n      --name string                           Assign a name to the container\r\n      --network string                        Connect a container to a network (default \"default\")\r\n      --network-alias list                    Add network-scoped alias for the container (default [])\r\n      --no-healthcheck                        Disable any container-specified HEALTHCHECK\r\n      --oom-kill-disable                      Disable OOM Killer\r\n      --oom-score-adj int                     Tune host's OOM preferences (-1000 to 1000)\r\n      --pid string                            PID namespace to use\r\n      --pids-limit int                        Tune container pids limit (set -1 for unlimited)\r\n      --privileged                            Give extended privileges to this container\r\n  -p, --publish list                          Publish a container's port(s) to the host (default [])\r\n  -P, --publish-all                           Publish all exposed ports to random ports\r\n      --read-only                             Mount the container's root filesystem as read only\r\n      --restart string                        Restart policy to apply when a container exits (default \"no\")\r\n      --rm                                    Automatically remove the container when it exits\r\n      --runtime string                        Runtime to use for this container\r\n      --security-opt list                     Security Options (default [])\r\n      --shm-size string                       Size of /dev/shm, default value is 64MB\r\n      --sig-proxy                             Proxy received signals to the process (default true)\r\n      --stop-signal string                    Signal to stop a container, SIGTERM by default (default \"SIGTERM\")\r\n      --stop-timeout int                      Timeout (in seconds) to stop a container\r\n      --storage-opt list                      Storage driver options for the container (default [])\r\n      --sysctl map                            Sysctl options (default map[])\r\n      --tmpfs list                            Mount a tmpfs directory (default [])\r\n  -t, --tty                                   Allocate a pseudo-TTY\r\n      --ulimit ulimit                         Ulimit options (default [])\r\n  -u, --user string                           Username or UID (format: <name|uid>[:<group|gid>])\r\n      --userns string                         User namespace to use\r\n      --uts string                            UTS namespace to use\r\n  -v, --volume list                           Bind mount a volume (default [])\r\n      --volume-driver string                  Optional volume driver for the container\r\n      --volumes-from list                     Mount volumes from the specified container(s) (default [])\r\n  -w, --workdir string                        Working directory inside the container\r\n```\r\n\r\nProbably most users need no more than this:\r\n```\r\n      --cpus decimal                          Number of CPUs (default 0.000)\r\n  -d, --detach                                Run container in background and print container ID\r\n      --device list                           Add a host device to the container (default [])\r\n      --dns list                              Set custom DNS servers (default [])\r\n  -e, --env list                              Set environment variables (default [])\r\n      --help                                  Print usage\r\n  -h, --hostname string                       Container host name\r\n      --init                                  Run an init inside the container that forwards signals and reaps processes\r\n  -i, --interactive                           Keep STDIN open even if not attached\r\n  -l, --label list                            Set meta data on a container (default [])\r\n      --link list                             Add link to another container (default [])\r\n  -m, --memory string                         Memory limit\r\n      --name string                           Assign a name to the container\r\n      --network string                        Connect a container to a network (default \"default\")\r\n      --network-alias list                    Add network-scoped alias for the container (default [])\r\n      --no-healthcheck                        Disable any container-specified HEALTHCHECK\r\n      --pids-limit int                        Tune container pids limit (set -1 for unlimited)\r\n      --privileged                            Give extended privileges to this container\r\n  -p, --publish list                          Publish a container's port(s) to the host (default [])\r\n       --read-only                             Mount the container's root filesystem as read only\r\n      --restart string                        Restart policy to apply when a container exits (default \"no\")\r\n      --rm                                    Automatically remove the container when it exits\r\n      --tmpfs list                            Mount a tmpfs directory (default [])\r\n  -t, --tty                                   Allocate a pseudo-TTY\r\n  -u, --user string                           Username or UID (format: <name|uid>[:<group|gid>])\r\n   -v, --volume list                           Bind mount a volume (default [])\r\n   -w, --workdir string                        Working directory inside the container\r\n```\r\n\r\nEven with the short list, the help output probably isn't all that helpful.\r\n\r\nWith the plethora of things there are to tweak on `docker run` it may be helpful to simply hide advanced flags by default (e.g. all the cgroup related stuff other than `cpus` and `memory`).\r\n\r\nAlternatively flags could be broken down into sections.\r\nWIth `--help` we could show the common flags and a set of sections.\r\nSo something like:\r\n\r\n```\r\n$ docker --help\r\nMost commonly used flags:\r\n\r\n    <short flag list>\r\n\r\nHelp Sections: \r\n\r\n    Resource Management:   See `docker --help=resources`\r\n    Networking:            See `docker --help=network`\r\n    Health monitoring:     See `docker --help=health`\r\n    Process configuration: See `docker --help=process\r\n    Storage:               See `docker --help=storage`\r\n\r\nFull command line reference:\r\n\r\n    https://docs.docker.com/engine/reference/run/\r\n\r\nOr see the manpages:\r\n\r\n    $ man docker-run\r\n```\r\n"},{"labels":[null,null,null,"enhancement"],"text":"> **Note:** this design is already partly being discussed on https://github.com/docker/docker/pull/27638, but creating this issue for tracking, and wider discussion.\r\n\r\n`docker run` and `docker service create` currently don't support adding a container or service to multiple networks. The reason for this is that there is a number of options that can be set _per network_ (for example, network-scoped aliases through `--network-alias`, a fixed IP-address on a network through `--ip` / `--ip6`).\r\n\r\nThe current situation severely limits networking options for services, as it's currently not possible to;\r\n\r\n- Attach/detach services to a network\r\n- Modify networking configuration for a service (`docker service update`) after a service is created\r\n- Set network-aliases for services through the CLI (the API supports this, as does `docker stack deploy`)\r\n\r\nThe advanced \"csv\" syntax (as is used for `docker service create --mount`) solves this limitation by grouping options in a single flag, allowing options to be set _per mount_. The same can be done for networks.\r\n\r\n### Proposed change\r\n\r\nI want to propose adding the same syntax to `--network` and solve this limitation. The advanced syntax should support all network-related options that can be set currently on `docker run` and `docker network connect`, for example;\r\n\r\n\r\n```bash\r\ndocker service create\r\n  --network name=my-network,alias=foo,alias=foo.bar.com,ip=172.30.100.104,ip6=2001:db8::33\r\n```\r\n\r\nSuggested options;\r\n\r\nName | Type | Corresponds with | Description\r\n--- | --- | --- | ---\r\n`name` | `string` | `--network` | Name of the network to attach to\r\n`alias` | `string[]` | `--network-alias` | Network-scoped alias for the container / service\r\n`ip` | `string` | `--ip` | IPv4 address for the container / service\r\n`ip6` | `string` | `--ip6` | IPv6 address for the container / service\r\n`link` | `string` | `--link` | Container/service-scoped alias for another container/service (t.b.d.)\r\n`link-local-ip`| `string[]` | `--link-local-ip` | Link-local address(es) for the container\r\n\r\n\r\nIn addition, advanced networking options (as are proposed in https://github.com/docker/docker/pull/27638) can be passed, for example;\r\n\r\n- `ipam-opt` (options for the IPAM plugin/driver)\r\n- `opt` (driver-specific options for the network)\r\n\r\nPossibly this would also allow options, such as bandwidth limitation per-network for a container/service (see https://github.com/docker/docker/pull/20794, https://github.com/docker/docker/issues/27809, https://github.com/docker/docker/issues/26767, https://github.com/docker/docker/pull/27846)\r\n\r\n\r\n### To be discussed - per-task/container options\r\n\r\nFor _services_, some options can be ambiguous, for example, an _alias_, or IP-address can apply to a service as a _whole_, or _per task_ / container. Some thoughts if we want options to be applied to individual tasks;\r\n\r\n- provide separate options for attributes that apply to individual containers/tasks (e.g. `container-ip`, `container-alias`)\r\n- support templating (`container-alias={{.Service}}.{{.Slot}}.foobar`)\r\n- support numerators in templating? (`container-ip=172.30.100.x`) - haven't given this one much thought, perhaps that's an IPAM option.\r\n\r\n### To be discussed - auto-creation\r\n\r\nDo we want the `--network` flag to _create_ networks if they don't exist, or keep the current behavior, and require the network to be created up front?\r\n\r\nPersonally, I think requiring a network to exist is a cleaner approach (better separation of concerns).\r\n\r\n\r\n### Related issues;\r\n\r\n-  [epic] add more options to `service create` / `service update` (https://github.com/docker/docker/issues/25303)\r\n- [UX] Guidelines for short/long syntax flags (https://github.com/docker/docker/issues/28527)\r\n- Adding static IP option (--ip and --ip6) for Docker Services (https://github.com/docker/docker/issues/29816, https://github.com/docker/docker/issues/24170)\r\n- `--network-alias` option missing on docker service create https://github.com/docker/docker/issues/24787\r\n- multiple networks on `docker run` (https://github.com/docker/docker/issues/17750, https://github.com/docker/docker/issues/17289, https://github.com/docker/docker/pull/17796)\r\n- Add \"--network-opt\" option as a way to pass arbitrary options to network drivers (https://github.com/docker/docker/pull/27638)\r\n\r\nping @docker/core-engine-maintainers @docker/core-libnetwork-maintainers "},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nWe can configure namespaced kernel parameters while using docker run,for example:\r\n```\r\ndocker run --sysctl kernel.msgmax=12345 someimage\r\n```\r\nis there any way to modify kernel parameters when using docker service create ?\r\n\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nDocker version 1.13.0, build 49bf474\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 6\r\n Running: 3\r\n Paused: 0\r\n Stopped: 3\r\nImages: 10\r\nServer Version: 1.13.0\r\nStorage Driver: overlay\r\n Backing Filesystem: xfs\r\n Supports d_type: false\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local local-persist\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: 7wnwxhcm35b2l5neeg8gq3hnd\r\n Is Manager: false\r\n Node Address: 172.16.2.138\r\n Manager Addresses:\r\n  172.16.2.136:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.10.0-514.6.1.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 3.514 GiB\r\nName: node-172162138\r\nID: WEM6:UIPX:ABJW:W7UY:RCLW:HUZD:CHVE:KARL:5FCX:TSWA:KCZS:AWJO\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: bridge-nf-call-iptables is disabled\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\nLabels:\r\n id=172162138\r\nExperimental: false\r\nInsecure Registries:\r\n registry.hello.com:5000\r\n 127.0.0.0/8\r\nRegistry Mirrors:\r\n http://registry.hello.com:5555\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nphysical\r\n"},{"labels":[null,null,null,"enhancement"],"text":"The Container Storage Interface (CSI) is a proposed new industry standard for cluster-wide volume plugins.  This is a joint proposal from a group of us who work on Docker, Kubernetes, Mesosphere and Cloud Foundry.  CSI is currently in the early draft stage, and we are seeking feedback from the community.\r\n\r\nDraft spec here: [https://docs.google.com/document/d/1JMNVNP-ZHz8cGlnqckOnpJmHF-DNY7IYP-Di7iuVhQI/edit](https://docs.google.com/document/d/1JMNVNP-ZHz8cGlnqckOnpJmHF-DNY7IYP-Di7iuVhQI/edit)\r\n\r\nThe goal of this standard is to have a single, cluster-level volumes plugin API that is shared by all orchestrators.  So, for example, conformant storage plugins written for Docker would run unmodified in Kubernetes (and vice-versa).\r\n\r\nThe purpose of this issue is to ask the opinion of the Docker community on this proposed standard.  Questions include:\r\n* Would/should Docker Engine adopt this as the cluster-aware evolution of our [Docker Volumes API](https://docs.docker.com/engine/extend/plugins_volume/)?\r\n* Should Docker suggest changes to the current draft before making that decision?\r\n* More broadly, does the Docker community see value in adopting an industry standard volumes API?  Or is it preferable for Docker to have its own independent volumes API?\r\n\r\n/cc @docker/core-swarmkit-maintainers @docker/core-engine-maintainers"},{"labels":[null,null,"enhancement"],"text":"Dear all,\r\nmy docker swarm mode cluster keeps ignoring \"ipv4_address:\" definitions set in version-3 compose files. I try to get a [consul](https://hub.docker.com/_/consul/) cluster running on my swarm mode cluster. Consul needs to advertise an ip address, and you also need to define an ip address for the \"consul workers\" to join (\"-retry-join=<root agent ip>\"). With docker ignoring the fixed ipv4 addresses I set in the compose definition, you end up with consul containers advertising ip addresses they do not own. This will never lead to a working consul cluster.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Docker Swarm running in Swarm Mode, 3 manager-worker nodes and 2 worker-only nodes\r\n2. Deploy the following v3-yml-file:\r\n```\r\nversion: \"3\"\r\n\r\nservices:\r\n\r\n  px-consul-dsm01:\r\n    image: consul:0.7.5\r\n    command: [agent, -server, -bootstrap-expect=3, -advertise=10.0.111.3, -client=0.0.0.0]\r\n    networks:\r\n      px-consul-net:\r\n        ipv4_address: 10.0.111.3\r\n    volumes:\r\n      - consul-px-tst:/consul/data\r\n\r\n  px-consul-dsm02:\r\n    image: consul:0.7.5\r\n    command: [agent, -server, -bootstrap-expect=3, -advertise=10.0.111.4, -client=0.0.0.0]\r\n    networks:\r\n      px-consul-net:\r\n        ipv4_address: 10.0.111.4\r\n    volumes:\r\n      - consul-px-tst:/consul/data\r\n\r\n  px-consul-dsm03:\r\n    image: consul:0.7.5\r\n    command: [agent, -server, -bootstrap-expect=3, -advertise=10.0.111.5, -client=0.0.0.0]\r\n    networks:\r\n      px-consul-net:\r\n        ipv4_address: 10.0.111.5\r\n    volumes:\r\n      - consul-px-tst:/consul/data\r\n\r\nnetworks:\r\n  px-consul-net:\r\n    driver: overlay\r\n    ipam:\r\n      driver: default\r\n      config:\r\n        - subnet: 10.0.111.0/26\r\n\r\nvolumes:\r\n  consul-px-tst:\r\n```\r\n3.\r\n\r\n**Describe the results you received:**\r\n\r\n- The network `px-consul-net` is being created as expected, it comes up with the defined address range.\r\n- The 3 consul containers are created and start up as well.\r\n- Unfortunately they come up with random ipv4 addresses from the range defined in the compose file. - Not with the addresses I set in the \"services\" section.\r\n\r\n- As the result, I get f.e. one consul container with the IP 10.0.111.7, but advertising the ip 10.0.111.3; a second consul container with the IP 10.0.111.5, advertising the IP 10.0.111.5 (accidentally the right one) and a 3rd consul container with the IP 10.0.111.3, advertising the IP 10.0.111.4. This will never work.\r\n\r\n- Any `-retry-join=<root agent ip>` statement in the service definition file will never work.\r\n\r\n**Describe the results you expected:**\r\nI expected each of the 3 consul containers coming up with the ipv4 address I set in the service definition file, for example:\r\n```\r\nnetworks:\r\n      px-consul-net:\r\n        ipv4_address: 10.0.111.5\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nHappens each and every time I tried. Upgrading from docker engine v1.13.1 to the latest available did not help. I also installed all patches for the host system (`apt-get update` and `apt-get upgrade` followed by a system reboot). Did not help. Behaviour did not change.\r\n\r\n**Output of `docker version`:**\r\n```\r\nDocker version 17.03.0-ce, build 60ccb22\r\n```\r\nI already tried this with docker engine 1.13.1. - Behaviour regarding the randomness of the `ipv4_address:` statement was the same.\r\n\r\n**Output of `docker info`:**\r\n```\r\nContainers: 1\r\n Running: 1\r\n Paused: 0\r\n Stopped: 0\r\nImages: 1\r\nServer Version: 17.03.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 7\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: hyk1yifcdibctr5pnytdg8hb7\r\n Is Manager: true\r\n ClusterID: k2dzk1kjcby74hrs2vshbtq2u\r\n Managers: 3\r\n Nodes: 5\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 10.123.123.37\r\n Manager Addresses:\r\n  10.123.123.37:2377\r\n  10.123.123.38:2377\r\n  10.123.123.39:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 977c511eda0925a723debdc94d09459af49d082a\r\nrunc version: a01dafd48bc1c7cc12bdb01206f9fea7dd6feb70\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-66-generic\r\nOperating System: Ubuntu 16.04.2 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 13.69 GiB\r\nName: t-azeurn-dsm01\r\nID: SOIK:RX3Q:AILR:KD4C:SFK2:X75S:27R3:W3VG:UV5D:MC6A:AP4W:3K62\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: ajunglsy\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nAll nodes of this swarm are Azure VMs, size DS3_v2, running Ubuntu Linux 16.04.2 LTS:\r\nOutput of `uname -a`:\r\n```\r\nLinux t-azeurn-dsm01 4.4.0-66-generic #87-Ubuntu SMP Fri Mar 3 15:29:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\nOutput of `lsb_release -a`:\r\n```\r\nroot@t-azeurn-dsm01:~/containerconfigs/consul-px# lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 16.04.2 LTS\r\nRelease:        16.04\r\nCodename:       xenial\r\n```\r\n\r\nPlease investigate. This is really annoying. Thank you very much!\r\n[console_output_consul.txt](https://github.com/docker/docker/files/845402/console_output_consul.txt)\r\n[docker-stack-deploy.txt](https://github.com/docker/docker/files/845417/docker-stack-deploy.txt)\r\n\r\n\r\n"},{"labels":[null,null,null,null,"enhancement"],"text":"The markdown API docs are missing any description for the `NetworkingConfig` object that is included in the config to create a container. This object is included in the example JSON that gets POSTed to `/containers/create` in the 1.22, 1.23, and 1.24 docs, but is not mentioned in the following \"JSON Parameters\" section. In particular, its child object `EndpointsConfig` is not fully defined.\r\n\r\nThis issue only affects the older markdown API docs. The objects mentioned above are defined in more detail in `api/swagger.yaml`. For the most part, the various places in the swagger description of the API that accept or produce information about networks refer to `#/definitions/EndpointSettings`.\r\n\r\n"},{"labels":[null,"enhancement"],"text":"---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe official Docker repository does not support IPV6.\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\n**Description**\r\n\r\nWe use an _deb Proxy_, as dowload.docker.com uses https all traffic is simply forwarded. \r\nThe _deb Proxy_ is reached via IPV6 and so tries to forward to https://download.docker.com via IPV6.\r\nThe request fails as there is [no IPv6](https://ip6.nl/#!download.docker.com)\r\n\r\n**Steps to reproduce the issue:**\r\n\r\nUse IPV6 _deb Proxy_, forwards https traffic\r\n\r\n**Describe the results you received:**\r\n\r\nUpdating Docker packages fails\r\n"},{"labels":[null,"enhancement"],"text":"Docker allows to tag and push only one image. \r\nFor limiting the interactions between the server and the repository it's possible to add the possibility to push an image with multiple tag in one time ?\r\n\r\n"},{"labels":[null,"enhancement"],"text":"Hi I am able to successfully login but unable to push my local image to Docker Hub.\r\n\r\n#-> docker login\r\nLogin with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.\r\nUsername: vivekkrdubeyhub\r\nPassword:\r\nLogin Succeeded\r\n#-> docker  push vivekkrdubeyhub/myrepo\r\nThe push refers to a repository [docker.io/vivekkrdubeyhub/myrepo]\r\n3602cd2b738c: Preparing\r\nad12c47d819e: Preparing\r\n074e78f994f8: Preparing\r\nc15c995bb6bf: Preparing\r\n00343924ca78: Preparing\r\n56827159aa8b: Waiting\r\n440e02c3dcde: Waiting\r\n29660d0e5bb2: Waiting\r\n85782553e37a: Waiting\r\n745f5be9952c: Waiting\r\ndenied: requested access to the resource is denied\r\n\r\nI have tried logging out, re-login, create new Docker Hub user but still nothing helps.\r\n\r\nMy config.json looks like\r\n\r\n{\r\n        \"auths\": {\r\n                \"https://index.docker.io/v1/\": {\r\n                        \"auth\": \"Some Text Here\"\r\n                }\r\n}\r\n\r\nPlease help !!"},{"labels":[null,"enhancement"],"text":"**Description**\r\nConsider not deleting manually named volumes by default when running `docker system prune`.\r\nI now there is potentially a label filter coming soon but I am not sure that is enough.\r\nA lot of people are using external named volumes with docker-compose and deleting those by default seems very risky to me.\r\n\r\n**Describe the results you expected:**\r\nBy default only delete images without a manually given name.\r\nAdd a flag to delete all volumes, just like there is for images.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nDocker version 17.03.0-ce, build 60ccb22\r\ndocker-compose version 1.11.2, build dfed245\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 3\r\n Running: 3\r\n Paused: 0\r\n Stopped: 0\r\nImages: 78\r\nServer Version: 17.03.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 159\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 977c511eda0925a723debdc94d09459af49d082a\r\nrunc version: a01dafd48bc1c7cc12bdb01206f9fea7dd6feb70\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 3.19.0-74-generic\r\nOperating System: Ubuntu 14.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 3.737 GiB\r\nName: orchestra\r\nID: L6ZS:PQC7:KLDK:TJZP:3LIL:K3JF:B3TZ:7WJO:VNFK:REUC:PXVT:X7FE\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```"},{"labels":[null,"enhancement"],"text":"1. **Have a way to bundle multiple COPY and ADD commands just like it works for RUN command**\r\ne.g \r\n\r\n```\r\nADD file1.txt /files/file.txt && \\\r\n        folder1/files2.txt /files/file2.txt && \\\r\n        file3.txt /files/file3/file.txt\r\n\r\n```\r\nsame for copy\r\n\r\n```\r\nCOPY file1.txt /files/file.txt && \\\r\n        folder1/files2.txt /files/file2.txt && \\\r\n        file3.txt /files/file3/file.txt\r\n\r\n```\r\n\r\n\r\n2. **Multiple ARG and ENV commands as well**\r\ne.g\r\n```\r\nARG DB_HOST1, DB_HOST2, DB_USER1, DB_USER2\r\nENV DB_HOST1 dbhost1.example.com, DB_HOST2 dbhost2.example.com, DB_USER1 user1, DB_USER2 user2\r\n```\r\n\r\n\r\n3. **Allow multiple use of --build-arg at build time as well**\r\n\r\n`docker build --build-arg DB_HOST1=dbhost1.example.com, DB_HOST2=dbhost2.example.com, DB_USER1=user1, DB_USER2=user2 ...`\r\nThanks!!!"},{"labels":[null,"enhancement"],"text":"Reason for the enhancement: Logentries has powerful query operations when using JSON. For applications written with that capabilities in mind wrapping the JSON formatted log into a de-serialized go struct makes life a bit harder.\r\n\r\nProposal for the enhancement: add logentries-line-only=true and pass then msg.Line only"},{"labels":[null,null,"enhancement"],"text":"hello,\r\nI am using docker swarm and I would like to have the ability to access to service name into dockerfile\r\n\r\nToday using an enviroment variable but it is very dirty\r\n\r\nexample:\r\n\r\n`docker service create  --name myCoolService -e SERVICE_NAME=myCoolService ......\r\n`\r\n\r\nthanks"},{"labels":[null,null,null,"enhancement",null,null],"text":"**Description**\r\n\r\nWhen outputting to stdout and stderr in quick succession without a TTY, log/output lines are consistently returned in the wrong order.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\nI started by reproducing with `perl`, but realized I could do so with just `awk`, which simplifies the testing and makes the explicit flushing more obvious (and makes it easier to embed in a test later, if we can figure out what's causing it and fix it):\r\n\r\n```console\r\n$ docker run --rm debian:jessie awk '\r\n    BEGIN {\r\n        print \"1 (out)\" > \"/dev/stdout\";\r\n        fflush(\"/dev/stdout\");\r\n\r\n        print \"2 (err)\" > \"/dev/stderr\";\r\n        fflush(\"/dev/stderr\");\r\n\r\n        print \"3 (out)\" > \"/dev/stdout\";\r\n        fflush(\"/dev/stdout\");\r\n\r\n        print \"4 (err)\" > \"/dev/stderr\";\r\n        fflush(\"/dev/stderr\");\r\n\r\n        exit;\r\n    }'\r\n```\r\n\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\n1 (out)\r\n3 (out)\r\n2 (err)\r\n4 (err)\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\n```\r\n1 (out)\r\n2 (err)\r\n3 (out)\r\n4 (err)\r\n```\r\n\r\n(which is exactly what I get if I add `--tty`, but then Docker's only getting a single stream of all output rather than separate `stdout` and `stderr`, and I can't specify `--tty` on `RUN` in a `Dockerfile`, which is the place I see this issue in real life and it causes actual pain by lumping errors for a long `RUN` line together at the beginning or end without all the normal output which is the context for what the error means)\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nThis is definitely similar to https://github.com/docker/docker/issues/26986 (possibly the same), but I have good reason to believe it's not due to buffering outside Docker (which is why that issue was closed), and appears instead to be due to Docker itself doing something odd (either doing its own buffering, or not storing timestamps with enough granularity in order to accurately recreate the proper line ordering).  This is the reason why my example `awk` uses `fflush` explicitly (to make sure there's no output buffering on the `awk`/container side).\r\n\r\n**Output of `docker version`:**\r\n\r\n```console\r\n$ docker version\r\nClient:\r\n Version:      17.03.0-ce\r\n API version:  1.26\r\n Go version:   go1.7.4\r\n Git commit:   60ccb22\r\n Built:        Mon Mar  6 16:06:31 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.0-ce\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.4\r\n Git commit:   60ccb22\r\n Built:        Mon Mar  6 16:06:31 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n```console\r\n$ docker version\r\nClient:\r\n Version:      17.03.0-ce\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   60ccb2265\r\n Built:        Thu Feb 23 10:58:26 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.0-ce\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   60ccb2265\r\n Built:        Thu Feb 23 10:58:26 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```console\r\n$ docker info\r\nContainers: 20\r\n Running: 18\r\n Paused: 0\r\n Stopped: 2\r\nImages: 4048\r\nServer Version: 17.03.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 3426\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: 5nqmhj66diohvhvp91dsuvex9\r\n Is Manager: true\r\n ClusterID: b2ruotz7bsmx8congfwdtiff2\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 10.255.136.50\r\n Manager Addresses:\r\n  10.255.136.50:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: v0.2.5 (expected: 977c511eda0925a723debdc94d09459af49d082a)\r\nrunc version: c91b5be (expected: a01dafd48bc1c7cc12bdb01206f9fea7dd6feb70)\r\ninit version: N/A (expected: 949e6facb77383876aeff8a6944dde66b3089574)\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.52-gentoo\r\nOperating System: Gentoo/Linux\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 31.4 GiB\r\nName: nameless\r\nID: XTOG:WDQV:MC46:UH5F:CQXH:VTNK:CDR6:7BEE:JDRJ:4U2D:LUQM:EYN2\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: tianon\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n```console\r\n$ docker info\r\nContainers: 9\r\n Running: 9\r\n Paused: 0\r\n Stopped: 0\r\nImages: 137\r\nServer Version: 17.03.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 977c511eda0925a723debdc94d09459af49d082a\r\nrunc version: a01dafd48bc1c7cc12bdb01206f9fea7dd6feb70\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.0-1-amd64\r\nOperating System: Debian GNU/Linux 9 (stretch)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 31.45 GiB\r\nName: viper\r\nID: 3GRQ:ZZAL:3Y3M:LKO5:AYFL:EO5G:UE2G:7MWM:G7C7:ZM46:WI5L:A6GZ\r\nDocker Root Dir: /mnt/docker/debian-var-lib-docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: tianon\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nSometimes the output is even more exciting:\r\n```\r\n2 (err)1 (out)\r\n\r\n3 (out)\r\n4 (err)\r\n```"},{"labels":[null,"enhancement",null],"text":"problems and steps to reproduce:\r\n1. I can create service with duplicate aliases like\r\n`{Target: \"qmx102g5pm9qyytv3dntkbgj3\", Aliases: [\"2048\", \"dao2048\", \"dao2048\", \"dao-2048\"]}\r\n`\r\n2. In docker compose, duplicate aliases are not supported.\r\n```\r\nversion: '3'\r\nservices:\r\n  abc:\r\n    image:nginx:latest\r\n    ports:\r\n      - 80\r\n    networks:\r\n      test:\r\n        aliases:\r\n        - abcd\r\n        - abcd\r\n```\r\nthe stack deploy log\r\n```\r\nroot@dce177:~/nginx# docker stack deploy abc --compose-file docker-compose.yml\r\nservices.abc.networks.test.aliases array items must be unique\r\n```\r\n\r\n3. When I run deploy stack with compose, the network aliases are same as before and are only de-duplicated. If not, I will get error like ''.\r\n`{Target: \"qmx102g5pm9qyytv3dntkbgj3\", Aliases: [\"2048\", \"dao2048\", \"dao-2048\"]}`\r\n`\"rpc error: code = 2 desc = changing network in service is not supported\"`\r\n\r\n4. When I test the update service API, the network aliases order must be as before. If not, I will get error like ''.\r\n`\"rpc error: code = 2 desc = changing network in service is not supported\"`\r\n`{Target: \"qmx102g5pm9qyytv3dntkbgj3\", Aliases: [\"2048\", \"dao2048\", \"dao-2048\", \"dao2048\",]}`\r\n\r\nHow to fix this? I'm not sure and below are my suggestions\r\n- current service update logic uses reflect.DeepEqual() and we may change it to compare each aliases lists.\r\n- we may use unique set instead of array[] for aliases. \r\n- just de-dup aliases list when creating or updating service and compare the ordered list with reflect.DeepEqual().\r\n\r\n\r\n```\r\nroot@dce177:~# docker version\r\nClient:\r\n Version:      1.13.1\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 06:50:14 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.1\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 06:50:14 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\nThanks.\r\n\r\n\r\n"},{"labels":[null,"enhancement"],"text":"As shown on in the official doc: https://docs.docker.com/compose/compose-file/#args you can pass arguments from docker-compose to the build process.\r\nBut, the Dockerfile must start with directive FROM, and an argument is available only after calling ARG your-arg-name.\r\nSo one cannot provide base version through build argument.\r\n\r\nCould it be possible to make this working? So we can call something like this: FROM ubuntu:$UBUNTU_VERSION?"},{"labels":[null,null,"enhancement"],"text":"The feature would enable dynamically creating a unique volume for each instance (container) of a replicated service.  This would be useful for network attached block storage, which usually can only be attached to a single server for writes.\r\n\r\nCurrently, if you want replication for a service using networked block storage, the configuration has to be copy/pasted N times and the built in load balancing is lost.... As far as I can figure... Is there a better way to do this currently?\r\n\r\nHere is a stripped down Swarm Compose file to illustrate the kind of service I would like to be able to increase replication on:\r\n\r\n```yaml\r\nversion: '3.1'\r\n# docker stack deploy --compose-file=ec2-compose.yml mystack\r\n\r\nservices:\r\n my_service:\r\n  image: org/image\r\n  volumes:\r\n   - my_volume:/dir\r\n  deploy:\r\n    mode: replicated\r\n    replicas: 1\r\n    placement:\r\n      constraints:\r\n        - node.labels.aws.az==us-east-1e\r\n  command: ['cmd']\r\n  \r\nvolumes:\r\n  my_volume:\r\n    driver: rexray/ebs\r\n    driver_opts:\r\n      size: 100\r\n      volumeType: gp2\r\n      fsType: ext4\r\n      availabilityZone: us-east-1e\r\n```\r\n\r\nI believe to accomplish this, currently, you have to set up a stack like so:\r\n\r\n```yaml\r\nversion: '3.1'\r\n\r\nservices:\r\n  my_service_loadbalancer:\r\n    # Configure to load balance across my_service_1, my_service_2, my_service_3\r\n    image: something like haproxy or nginx\r\n    deploy:\r\n      mode: global\r\n    \r\n  my_service_1:\r\n    image: org/image\r\n    volumes:\r\n      - my_volume_1:/dir\r\n    deploy:\r\n      mode: replicated\r\n      replicas: 1\r\n      placement:\r\n        constraints:\r\n          - node.labels.aws.az==us-east-1b\r\n    command: ['cmd']\r\n    \r\n  my_service_2:\r\n    image: org/image\r\n    volumes:\r\n      - my_volume_2:/dir\r\n    deploy:\r\n      mode: replicated\r\n      replicas: 1\r\n      placement:\r\n        constraints:\r\n          - node.labels.aws.az==us-east-1c\r\n    command: ['cmd']\r\n    \r\n  my_service_3:\r\n    image: org/image\r\n    volumes:\r\n      - my_volume_3:/dir\r\n    deploy:\r\n      mode: replicated\r\n      replicas: 1\r\n      placement:\r\n        constraints:\r\n          - node.labels.aws.az==us-east-1e\r\n    command: ['cmd']\r\n  \r\nvolumes:\r\n  my_volume_1:\r\n    driver: rexray/ebs\r\n    driver_opts:\r\n      size: 100\r\n      volumeType: gp2\r\n      fsType: ext4\r\n      availabilityZone: us-east-1b\r\n  my_volume_2:\r\n    driver: rexray/ebs\r\n    driver_opts:\r\n      size: 100\r\n      volumeType: gp2\r\n      fsType: ext4\r\n      availabilityZone: us-east-1c\r\n  my_volume_3:\r\n    driver: rexray/ebs\r\n    driver_opts:\r\n      size: 100\r\n      volumeType: gp2\r\n      fsType: ext4\r\n      availabilityZone: us-east-1e\r\n```\r\n\r\nPerhaps an even better solution, is to allow adding a group of services to the internal swarm loadbalancing to prevent the need to configure and run a global haproxy or nginx service.  The ideal setup is to have a service available in each zone for a given region, with persistent storage.\r\n\r\nI originally opened this request under the rexray project, [codedellemc/rexray/issues/7650](https://github.com/codedellemc/rexray/issues/765), but then realized it was out of scope at the volume plugin level."},{"labels":[null,null,"enhancement"],"text":"Hi \r\n\r\nI want to create a stackfile for all my services.\r\nI have not been able to find how to define  network-aliases for my services.\r\nIs the parameter '--network-alias' is missing for services ? \r\n\r\nRegards\r\n"},{"labels":["enhancement",null],"text":"@darrenstahlmsft \r\n\r\nAs discussed - if you can find a way to not hold the client lock for 5 minutes during a terminate call. The relevant threads (on v1.13.1) were\r\n\r\n```\r\ngoroutine 447145 [select]:\r\ngithub.com/docker/docker/vendor/github.com/Microsoft/hcsshim.waitForNotification(0x591, 0x1, 0xc046d21cd8, 0x0, 0x0)\r\n\t/go/src/github.com/docker/docker/vendor/github.com/Microsoft/hcsshim/waithelper.go:36 +0x4d2\r\ngithub.com/docker/docker/vendor/github.com/Microsoft/hcsshim.(*container).WaitTimeout(0xc044a66c80, 0x45d964b800, 0xc046c66301, 0xc045e6fd28)\r\n\t/go/src/github.com/docker/docker/vendor/github.com/Microsoft/hcsshim/container.go:305 +0x18b\r\ngithub.com/docker/docker/libcontainerd.(*container).terminate(0xc04649c000, 0x44, 0xc045e6fd70)\r\n\t/go/src/github.com/docker/docker/libcontainerd/container_windows.go:300 +0xa3\r\ngithub.com/docker/docker/libcontainerd.(*container).shutdown(0xc04649c000, 0x29, 0xc045d33e68)\r\n\t/go/src/github.com/docker/docker/libcontainerd/container_windows.go:284 +0x18e\r\ngithub.com/docker/docker/libcontainerd.(*container).waitExit(0xc04649c000, 0xc04649c000, 0x1, 0xc04380e480, 0xc0469fa5f0)\r\n\t/go/src/github.com/docker/docker/libcontainerd/container_windows.go:231 +0x307\r\ncreated by github.com/docker/docker/libcontainerd.(*container).start\r\n\t/go/src/github.com/docker/docker/libcontainerd/container_windows.go:147 +0xdde\r\n```\r\n\r\nand \r\n\r\n```\r\ngoroutine 481993 [semacquire, 5 minutes]:\r\nsync.runtime_Semacquire(0xc046f808a4)\r\n\t/usr/local/go/src/runtime/sema.go:47 +0x37\r\nsync.(*Mutex).Lock(0xc046f808a0)\r\n\t/usr/local/go/src/sync/mutex.go:85 +0xd7\r\ngithub.com/docker/docker/pkg/locker.(*lockCtr).Lock(0xc046f808a0)\r\n\t/go/src/github.com/docker/docker/pkg/locker/locker.go:56 +0x34\r\ngithub.com/docker/docker/pkg/locker.(*Locker).Lock(0xc042643310, 0xc047822700, 0x40)\r\n\t/go/src/github.com/docker/docker/pkg/locker/locker.go:91 +0xb2\r\ngithub.com/docker/docker/libcontainerd.(*client).lock(0xc042757dc0, 0xc047822700, 0x40)\r\n\t/go/src/github.com/docker/docker/libcontainerd/client.go:19 +0x4a\r\ngithub.com/docker/docker/libcontainerd.(*client).Signal(0xc042757dc0, 0xc047822700, 0x40, 0x9, 0x0, 0x0)\r\n\t/go/src/github.com/docker/docker/libcontainerd/client_windows.go:381 +0x97\r\ngithub.com/docker/docker/daemon.(*Daemon).kill(0xc0423bb600, 0xc0490f7520, 0x9, 0x2, 0x2)\r\n\t/go/src/github.com/docker/docker/daemon/kill.go:163 +0x62\r\ngithub.com/docker/docker/daemon.(*Daemon).killWithSignal(0xc0423bb600, 0xc0490f7520, 0x9, 0x0, 0x0)\r\n\t/go/src/github.com/docker/docker/daemon/kill.go:94 +0x298\r\ngithub.com/docker/docker/daemon.(*Daemon).killPossiblyDeadProcess(0xc0423bb600, 0xc0490f7520, 0x9, 0xc0420062d0, 0x16f0731)\r\n\t/go/src/github.com/docker/docker/daemon/kill.go:153 +0x56\r\ngithub.com/docker/docker/daemon.(*Daemon).Kill(0xc0423bb600, 0xc0490f7520, 0xc0473fd3f8, 0x3)\r\n\t/go/src/github.com/docker/docker/daemon/kill.go:119 +0x68\r\ngithub.com/docker/docker/daemon.(*Daemon).containerStop(0xc0423bb600, 0xc0490f7520, 0xa, 0xc0490f7520, 0x0)\r\n\t/go/src/github.com/docker/docker/daemon/stop.go:75 +0x2b2\r\ngithub.com/docker/docker/daemon.(*Daemon).ContainerStop(0xc0423bb600, 0xc045892327, 0x40, 0xc046f5a9f0, 0xc04766a758, 0xc048cdab40)\r\n\t/go/src/github.com/docker/docker/daemon/stop.go:32 +0xd4\r\ngithub.com/docker/docker/api/server/router/container.(*containerRouter).postContainersStop(0xc043e41f00, 0x1ebffc0, 0xc048cdacc0, 0x1ebc840, 0xc0456ac680, 0xc0481881e0, 0xc048cdab70, 0x168f349, 0x5)\r\n\t/go/src/github.com/docker/docker/api/server/router/container/container_routes.go:181 +0xed\r\ngithub.com/docker/docker/api/server/router/container.(*containerRouter).(github.com/docker/docker/api/server/router/container.postContainersStop)-fm(0x1ebffc0, 0xc048cdacc0, 0x1ebc840, 0xc0456ac680, 0xc0481881e0, 0xc048cdab70, 0x41908f, 0x30)\r\n\t/go/src/github.com/docker/docker/api/server/router/container/container.go:61 +0x70\r\ngithub.com/docker/docker/api/server/middleware.ExperimentalMiddleware.WrapHandler.func1(0x1ebffc0, 0xc048cdacc0, 0x1ebc840, 0xc0456ac680, 0xc0481881e0, 0xc048cdab70, 0x17, 0x40)\r\n\t/go/src/github.com/docker/docker/api/server/middleware/experimental.go:27 +0xdf\r\ngithub.com/docker/docker/api/server/middleware.VersionMiddleware.WrapHandler.func1(0x1ebffc0, 0xc048cdac60, 0x1ebc840, 0xc0456ac680, 0xc0481881e0, 0xc048cdab70, 0x418858, 0xc045cd8d60)\r\n\t/go/src/github.com/docker/docker/api/server/middleware/version.go:47 +0x5d1\r\ngithub.com/docker/docker/pkg/authorization.(*Middleware).WrapHandler.func1(0x1ebffc0, 0xc048cdac60, 0x1ebc840, 0xc0456ac680, 0xc0481881e0, 0xc048cdab70, 0xc045cd8d60, 0xc0473fd998)\r\n\t/go/src/github.com/docker/docker/pkg/authorization/middleware.go:43 +0x846\r\ngithub.com/docker/docker/api/server/middleware.DebugRequestMiddleware.func1(0x1ebffc0, 0xc048cdac60, 0x1ebc840, 0xc0456ac680, 0xc0481881e0, 0xc048cdab70, 0xc0473fdae0, 0x413fc7)\r\n\t/go/src/github.com/docker/docker/api/server/middleware/debug.go:53 +0x5c2\r\ngithub.com/docker/docker/api/server.(*Server).makeHTTPHandler.func1(0x1ebc840, 0xc0456ac680, 0xc0481881e0)\r\n\t/go/src/github.com/docker/docker/api/server/server.go:139 +0x23a\r\nnet/http.HandlerFunc.ServeHTTP(0xc0436fabe0, 0x1ebc840, 0xc0456ac680, 0xc0481881e0)\r\n\t/usr/local/go/src/net/http/server.go:1726 +0x4b\r\ngithub.com/docker/docker/vendor/github.com/gorilla/mux.(*Router).ServeHTTP(0xc043e58370, 0x1ebc840, 0xc0456ac680, 0xc0481881e0)\r\n\t/go/src/github.com/docker/docker/vendor/github.com/gorilla/mux/mux.go:103 +0x25c\r\ngithub.com/docker/docker/api/server.(*routerSwapper).ServeHTTP(0xc0430e5700, 0x1ebc840, 0xc0456ac680, 0xc0481881e0)\r\n\t/go/src/github.com/docker/docker/api/server/router_swapper.go:29 +0x77\r\nnet/http.serverHandler.ServeHTTP(0xc04219e200, 0x1ebc840, 0xc0456ac680, 0xc0481881e0)\r\n\t/usr/local/go/src/net/http/server.go:2202 +0x84\r\nnet/http.(*conn).serve(0xc0467c8000, 0x1ebe3c0, 0xc047d420c0)\r\n\t/usr/local/go/src/net/http/server.go:1579 +0x4be\r\ncreated by net/http.(*Server).Serve\r\n\t/usr/local/go/src/net/http/server.go:2293 +0x454\r\n\r\n```"},{"labels":[null,"enhancement",null,null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\nAfter a fresh installation of docker and pulling the container I cannot copy any file into the container\r\nI get the following:\r\n```\r\ndocker run -it microsoft/dotnet-framwork cmd\r\ndocker cp D:\\x\\1.txt <container_name>:c:\\1.txt\r\n\r\nError response from daemon: hcsshim::ActivateLayer failed in Win32: \r\nThe process cannot access the file because it is being used by another process. (0x20) id=d1ad2f812e3b7a37da79538611d7fb4496b0a89369b2ba430e08a28d73f1ab50 flavour=1\r\n```\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.03.0-ce\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 10:40:59 2017\r\n OS/Arch:      windows/amd64\r\n\r\nServer:\r\n Version:      17.03.0-ce\r\n API version:  1.26 (minimum version 1.24)\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 10:40:59 2017\r\n OS/Arch:      windows/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 5\r\n Running: 1\r\n Paused: 0\r\n Stopped: 4\r\nImages: 2\r\nServer Version: 17.03.0-ce\r\nStorage Driver: windowsfilter\r\n Windows:\r\nLogging Driver: json-file\r\nPlugins:\r\n Volume: local\r\n Network: l2bridge l2tunnel nat null overlay transparent\r\nSwarm: inactive\r\nDefault Isolation: hyperv\r\nKernel Version: 10.0 14393 (14393.206.amd64fre.rs1_release.160915-0644)\r\nOperating System: Windows 10 Pro\r\nOSType: windows\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 31.91 GiB\r\nName: wk-ofir\r\nID: PKF6:LKFA:ROVD:X2LP:I2NL:HJZF:NSGN:BLOM:JDBH:RXA7:F3HQ:B3TK\r\nDocker Root Dir: C:\\ProgramData\\Docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: -1\r\n Goroutines: 33\r\n System Time: 2017-03-07T15:19:34.6735797+02:00\r\n EventsListeners: 1\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n\r\n"},{"labels":[null,"enhancement"],"text":"Currently, the global orchestrator creates a task for all nodes that meet the constraints, but only tasks for nodes that pass all filters (such as the plugin filter) will progress to the Running state. With respect to plugin filters, this is confusing, since tasks that depend on a plugin are expected to be scheduled only if the plugin is available on a node, as opposed to be created and then failing to progress to `Running`.\r\n\r\nIdeally, it would be good if the global orchestrator only creates tasks for nodes with the appropriate plugins."},{"labels":[null,"enhancement",null],"text":"Hi, I want to change my docker-compose files to version 3, so I tried it with an example:\r\n\r\n```\r\nversion: '3'\r\n\r\nservices:\r\n\r\n  bitbucket:\r\n    image: atlassian/bitbucket-server:4.14\r\n    hostname: bitbucket\r\n    ports:\r\n      - \"7990:7990\"\r\n```\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker stack deploy --compose-file .\\docker-stack.yml name\r\n2. docker images\r\n\r\n**Describe the results you received:**\r\n```\r\nREPOSITORY                   TAG                 IMAGE ID            CREATED             SIZE\r\natlassian/bitbucket-server   <none>              38116e66106c        5 days ago          659 MB\r\n```\r\n**Describe the results you expected:**\r\n```\r\nREPOSITORY                   TAG                 IMAGE ID            CREATED             SIZE\r\natlassian/bitbucket-server     4.14            38116e66106c        5 days ago          659 MB\r\n```\r\n\r\n**Output of `docker version`:**\r\n```\r\nClient:\r\n Version:      17.03.0-ce\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   60ccb22\r\n Built:        Thu Feb 23 10:40:59 2017\r\n OS/Arch:      windows/amd64\r\n\r\nServer:\r\n Version:      17.03.0-ce\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   3a232c8\r\n Built:        Tue Feb 28 07:52:04 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1                                                       \r\n Running: 1                                                         \r\n Paused: 0                                                          \r\n Stopped: 0                                                         \r\nImages: 1                                                           \r\nServer Version: 17.03.0-ce                                          \r\nStorage Driver: aufs                                                \r\n Root Dir: /var/lib/docker/aufs                                     \r\n Backing Filesystem: extfs                                          \r\n Dirs: 13                                                           \r\n Dirperm1 Supported: true                                           \r\nLogging Driver: json-file                                           \r\nCgroup Driver: cgroupfs                                             \r\nPlugins:                                                            \r\n Volume: local                                                      \r\n Network: bridge host ipvlan macvlan null overlay                   \r\nSwarm: active                                                       \r\n NodeID: 8mltynfgboqwe40jt77glbm73                                  \r\n Is Manager: true                                                   \r\n ClusterID: 30qn69c470mipl2w85q09xpuj                               \r\n Managers: 1                                                        \r\n Nodes: 1                                                           \r\n Orchestration:                                                     \r\n  Task History Retention Limit: 5                                   \r\n Raft:                                                              \r\n  Snapshot Interval: 10000                                          \r\n  Number of Old Snapshots to Retain: 0                              \r\n  Heartbeat Tick: 1                                                 \r\n  Election Tick: 3                                                  \r\n Dispatcher:                                                        \r\n  Heartbeat Period: 5 seconds                                       \r\n CA Configuration:                                                  \r\n  Expiry Duration: 3 months                                         \r\n Node Address: 192.168.65.2                                         \r\n Manager Addresses:                                                 \r\n  192.168.65.2:2377                                                 \r\nRuntimes: runc                                                      \r\nDefault Runtime: runc                                               \r\nInit Binary: docker-init                                            \r\ncontainerd version: 977c511eda0925a723debdc94d09459af49d082a        \r\nrunc version: a01dafd48bc1c7cc12bdb01206f9fea7dd6feb70              \r\ninit version: 949e6fa                                               \r\nSecurity Options:                                                   \r\n seccomp                                                            \r\n  Profile: default                                                  \r\nKernel Version: 4.9.12-moby                                         \r\nOperating System: Alpine Linux v3.5                                 \r\nOSType: linux                                                       \r\nArchitecture: x86_64                                                \r\nCPUs: 2                                                             \r\nTotal Memory: 1.934 GiB                                             \r\nName: moby                                                          \r\nID: F37C:NYPX:LXGB:EEKS:TFHO:Y5BG:SACC:XP76:C4PB:CEGN:BGMU:3D5R     \r\nDocker Root Dir: /var/lib/docker                                    \r\nDebug Mode (client): false                                          \r\nDebug Mode (server): true                                           \r\n File Descriptors: 48                                               \r\n Goroutines: 151                                                    \r\n System Time: 2017-03-06T13:15:56.2977373Z                          \r\n EventsListeners: 1                                                 \r\nRegistry: https://index.docker.io/v1/                               \r\nExperimental: true                                                  \r\nInsecure Registries:                                                \r\n 127.0.0.0/8                                                        \r\nLive Restore Enabled: false                                         \r\n```"},{"labels":[null,"enhancement"],"text":"Would be great for users to see the available tags of the particular image searched via `docker search` command. \r\n\r\nSomewhat may be like `docker search tags ubuntu` should display all the tags of the ubuntu image present in docker hub. \r\n\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nWhen creating services you can specify an amount of memory to reserve, to prevent, for example, large containers all being scheduled to the same node. When services are created which exceed the capacity of the swarm, the are not scheduled.\r\n\r\nThis process however is opaque, mainly because the available resources on a node is not exposed. Internally, the scheduler has a field per node called AvailableResources which is kept up to date.\r\n\r\nAs a side-note, the constraint enforcer counts any tasks between state Assigned and Running and so excluded failed tasks. But it seems sometimes failed tasks are included in the resource calculation, though I can't seem to reproduce it on demand. Exposing AvailableResources would help immensely here.\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nI have noticed on upstart based system Ubuntu 14.04, Docker containers are not gracefully stopped, if the Docker engine is configured with `live-restore` option. This causes problems for containers that need to be properly shutdown.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Enable [live-restore](https://docs.docker.com/engine/admin/live-restore/#enable-the-live-restore-option)\r\n2. `sudo reboot` \r\n\r\n**Describe the results you received:**\r\nNo entries in docker's upstart log (**/var/log/update/docker.log**) attempting to stop any containers. Upstart's `/etc/init.d/sendsigs` kills any remaining processes, with SIGKILL, as a last resort, which ends up killing all containers. \r\n\r\n**Describe the results you expected:**\r\nEven with `live-restore` enabled, a host halt/shutdown should still trigger the docker's init to stop containers.\r\n\r\nI feel a solution to this, would be to update the docker's upstart config (**/etc/init/docker.conf**) to:\r\n1. Change `stop on runlevel [!2345]` to `stop on starting rc RUNLEVEL=[06]`\r\n2. Create a post-stop script directive\r\n   1. Checks if the runlevel is either 0 (halt) or 6 (reboot)\r\n   2. Gracefully kill all running cotainers with `docker-runc kill <container`\r\n   3. Set a timer, maybe 30 seconds, and wait for either the timer to expire or all containers to enter stop state (**docker-runc list**).\r\n\r\nI have the above working and can submit a PR, but wanted to run it by others first.\r\n\r\nNot sure if this also affects non-upstart based systems.\r\n"},{"labels":[null,null,"enhancement"],"text":"With v3 syntax and deploy, container names aren't allowed anymore and docker stats report identical values for container and name.\r\nI think that the latter should be truncated to display unique values. This:\r\n```CONTAINER                                            NAME                                                 CPU %               MEM USAGE / LIMIT   BLOCK I/O               NET I/O\r\nselenium_chrome.1.b8jdwzntnikmr0xelgjpt63ki          selenium_chrome.1.b8jdwzntnikmr0xelgjpt63ki                              0.06%               109.2 MiB / 200 MiB     3.68 MB / 0 B       1.18 MB / 914 kB\r\nselenium_chrome-debug.1.b1vnedg2u81dne61dnbjkvgym    selenium_chrome-debug.1.b1vnedg2u81dne61dnbjkvgym                        0.64%               148 MiB / 200 MiB       10.9 MB / 233 kB    752 kB / 576 kB\r\n```\r\ncould become:\r\n```\r\nCONTAINER                                            NAME                                                 CPU %               MEM USAGE / LIMIT   BLOCK I/O               NET I/O\r\nselenium_chrome.1.b8jdwzntnikmr0xelgjpt63ki          selenium_chrome                              0.06%               109.2 MiB / 200 MiB     3.68 MB / 0 B       1.18 MB / 914 kB\r\nselenium_chrome-debug.1.b1vnedg2u81dne61dnbjkvgym    selenium_chrome-debug                        0.64%               148 MiB / 200 MiB       10.9 MB / 233 kB    752 kB / 576 kB\r\n```\r\nThis would enable users to get meaningful values and limit display length."},{"labels":[null,"enhancement"],"text":"Couldn't find any issues related to this.\r\n\r\nImplementing docker stack deploy support for v2 as well as using the [docker/libcompose](https://github.com/docker/libcompose) library would be *grand*. \r\n\r\nMeaning we would only need to install one binary (docker) instead of docker + docker-compose to the host system.\r\n\r\nOf course, this means we would have to implement https://github.com/aanand/compose-file 's parser into libcompose and then import libcompose into the Docker binary. \r\n\r\n"},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nWhen redeploying a compose file, services that have been removed from the file, should be removed from the swarm.\r\n\r\n**Steps to reproduce the issue:**\r\nCreate a test stack:\r\n\r\n```\r\nversion: \"3\"\r\nservices:\r\n\r\n nginx:\r\n    image: nginx\r\n tomcat:\r\n    image: tomcat\r\n\r\n```\r\n\r\nDeploy this: `docker stack deploy -c test.yaml test`\r\n\r\nRemove a service from the file and deploy again.\r\n\r\n\r\n**Describe the results you received:**\r\nThe removed service is still running.\r\n\r\n**Describe the results you expected:**\r\nThe service to be removed from the swarm.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.1\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 08:47:51 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.1\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 08:47:51 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nswarm-manager000000:~/azure-docker-swarm-deployment$ docker info\r\nContainers: 7\r\n Running: 7\r\n Paused: 0\r\n Stopped: 0\r\nImages: 9\r\nServer Version: 1.13.1\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: syslog\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: 13lscm8n4vvpzti4hdr34kz0z\r\n Is Manager: true\r\n ClusterID: yompvfav9w4vgxoedkhxh6dxt\r\n Managers: 3\r\n Nodes: 4\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 10.240.4.5\r\n Manager Addresses:\r\n  10.240.4.5:2377\r\n  10.240.4.6:2377\r\n  10.240.4.7:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: aa8187dbd3b7ad67d8e5e3a15115d3eef43a7ed1\r\nrunc version: 9df8b306d01f59d3a8029be411de015b7304dd8f\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.8-moby\r\nOperating System: Alpine Linux v3.5\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1.635 GiB\r\nName: swarm-manager000000\r\nID: HQW5:OOIC:YSVQ:HXAE:IGCA:PLEQ:TSJL:DKRQ:6LCH:7MJI:6W4S:2FVX\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 88\r\n Goroutines: 148\r\n System Time: 2017-03-02T14:42:18.354583422Z\r\n EventsListeners: 2\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nDeployed with docker-for-azure\r\n"},{"labels":[null,"enhancement"],"text":"Can we export more of the docker/cli/compose/loader functions, to allow partial element decoding.\r\n\r\nReasoning:\r\n\r\nwe may not like the compose format that much, but it is an industry standard.  Now there are various docker provided libraries for unmarshalling the format, but the new compose library in the docker cli should likely become a standard.\r\n\r\nCurrently the only exported method is really the `Load()` method, which unmarshalls an entire compose file.  This is done because in legacy files, there was so much interconnection between the services, and the networks, that it was impossible to consider a single service or network on it's own.  Those reasons are no longer a problem now that the v3 files do away with the keys that required such interconnection (links, volumes_from.)\r\nOr can we separate that loading code to it's own library (which would allow easy future deprecation anyway.)\r\n\r\nCan we export any of the following functions, to give other software core decoding functionality?  \r\n\r\n```\r\nloadServices()\r\nloadService()\r\nloadNetworks()\r\nloadVolumes()\r\nloadSecrets()\r\n```\r\n\r\nIf these methods were exported, then other projects could integrate the industry format into their own yml configurations, using the core code and not having to rewrite that code, or hack their formats to match the requirements for the Load() method.\r\n\r\nThe value gained would be easy reuse of the compose loader, to provide structs that can then be used to directly run client operations by mimicking the CLI."},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\ndocker volume inspect doesn't show connected containers.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker volume -d local local-100\r\n2. docker run -d -v local-100:/opt/ nginx:latest\r\n3. docker volume inspect local-100\r\n\r\n**Describe the results you received:**\r\nmissing attached containers.\r\n[\r\n    {\r\n        \"Driver\": \"local\",\r\n        \"Labels\": {},\r\n        \"Mountpoint\": \"/var/lib/docker/volumes/local-100/_data\",\r\n        \"Name\": \"local-100\",\r\n        \"Options\": {},\r\n        \"Scope\": \"local\"\r\n    }\r\n]\r\n\r\n**Describe the results you expected:**\r\n[\r\n    {\r\n        \"Driver\": \"local\",\r\n        \"Labels\": {},\r\n        \"Mountpoint\": \"/var/lib/docker/volumes/local-100/_data\",\r\n        \"Name\": \"local-100\",\r\n        \"Containers\": []\r\n        \"Options\": {},\r\n        \"Scope\": \"local\"\r\n    }\r\n]\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.1\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 06:42:29 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.1\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 06:42:29 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 6\r\n Running: 5\r\n Paused: 0\r\n Stopped: 1\r\nImages: 62\r\nServer Version: 1.13.1\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 315\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: aa8187dbd3b7ad67d8e5e3a15115d3eef43a7ed1\r\nrunc version: 9df8b306d01f59d3a8029be411de015b7304dd8f\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 4.4.0-51-generic\r\nOperating System: Ubuntu 14.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1.945 GiB\r\nName: dchq-gak\r\nID: 2MMO:IKKV:QTKW:SS3E:E5MY:7IIV:PBAV:PWDE:BMV5:FMG5:PG44:AQ2S\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: hypergrid\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"Several improvements can be done on the integration test side of `docker/docker`. This issue is there to list them and track the work on it.\r\n\r\n- [ ] Remove `utils_test.go`, latest `runCommandWithOutput` appearance\r\n- [ ] Remove `docker_utils_test.go`\r\n  - [x] Move cleanup function to `environment` package #30759 @vdemeester\r\n  - [x] Extract `fakeContext`, `buildImage`, `inspect`, `dockerCmd`, … methods in packages @vdemeester\r\n- [x] Use API for environment instead of `cli`\r\n- [x] <del>Make all test suite manage their own deamon. This should allow to use always the same method (instead of currently either `dockerCmd` or `s.d.Cmd`) and this would fix #17129</del> \r\n      Using `cli.Docker(...)` instead of `dockerCmd` and `cli.Docker(..., cli.Daemon(s.d))` instead of `s.d.Cmd`.\r\n- [x] Replace `request` deprecated function (`SockRequest`, `SockRequestRaw`) usage to the new ones (`Get`, `Post`, `Put`, `Do`, …). @vdemeester — most of those could also use the `client`'s package\r\n- [x] Freeze `test-integration-cli` and create `test-integration` that only does `api` tests. `cli` test are `docker` related tests and should be on another repository (owned by docker). @vdemeester #33344\r\n- [x] <del>Extract suites in their own packages\r\n  - [x] start with the API suite that is different enough from the other\r\n  - [x] Create an `ExperimentalTestSuite` and remove the current experimental build (experimental is not a different binary anymore, we just need a suite where the deamon is with `--experimental`). @vdemeester\r\n  - [x] Update this to insert other potential suites</del>\r\n- [x] Maintain integration test suite (probably should be an external issue in the future)\r\n  - [x] Make `test-integration` more self-contained (limit the number of external service dependency like the hub)\r\n  - <del>[x] `docker/docker-e2e` integration @vdemeester</del>\r\n- [x] List and remove integration tests that could be unit test. One example is some cli-only tests. These should be removed once #31217 is taken care of. I'll also list the one I think could be replaced by unit-test in another issue.\r\n\r\n/cc @icecrime @thaJeztah @docker/core-engine-maintainers "},{"labels":[null,null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nAdd a way to wait for service update result\r\nI think this is useful for Continous Deployment tools\r\nToday we have to query service api to get the \"UpdateStatus\" info waiting for result\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker service update --image myimage:version my_service\r\n\r\n**Describe the results you received:**\r\nmy_service (code 0) in a second\r\n\r\n**Describe the results you expected:**\r\nWith a new flag, wait for end of update and return status of update or RC like \r\n0 : completed\r\n1 : paused\r\n2: rolledback\r\n\r\n"},{"labels":[null,null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nAs a `docker user` I would like to validate the current `docker daemon` configuration through `docker info`\r\n\r\n**Steps to reproduce the issue:**\r\n1. create a file `/etc/docker/daemon.json` with `{\"init\":true}`\r\n2. restart the `docker daemon`\r\n3. type `docker info` or `ps ax | grep dockerd` or `curl --unix-socket /var/run/docker.sock http:/info` or ``\r\n\r\n**Describe the results you received:**\r\nThere is nothing showing that the `docker daemon` has the `--init` flag enabled. \r\n\r\nThe only way to check that is by creating a `container` and checking if `/dev/init` exists (e.g. `docker run -it --rm alpine ps ax`)\r\n\r\n**Describe the results you expected:**\r\n```\r\n$docker info\r\nContainers: 20\r\n Running: 0\r\n Paused: 0\r\n Stopped: 20\r\nImages: 81\r\nServer Version: 1.13.1\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n....\r\n....\r\n....\r\nInit: true\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.1\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 08:47:51 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.1\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 08:47:51 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 20\r\n Running: 0\r\n Paused: 0\r\n Stopped: 20\r\nImages: 81\r\nServer Version: 1.13.1\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: ibbu83l4rnqeiej4w9qp00789\r\n Is Manager: true\r\n ClusterID: u8ydyz8ln5hnjf6lbv46stil1\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 127.0.0.1\r\n Manager Addresses:\r\n  0.0.0.0:2377\r\n  127.0.0.1:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: aa8187dbd3b7ad67d8e5e3a15115d3eef43a7ed1\r\nrunc version: 9df8b306d01f59d3a8029be411de015b7304dd8f\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.8-moby\r\nOperating System: Alpine Linux v3.5\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 1.952 GiB\r\nName: moby\r\nID: T2GJ:O7QX:ALVL:YAC3:5ZQF:YA7A:D7ZC:PAYV:MC6M:YS5X:3G3U:I7FO\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 32\r\n Goroutines: 130\r\n System Time: 2017-02-27T11:44:11.143651252Z\r\n EventsListeners: 1\r\nNo Proxy: *.local, 169.254/16\r\nUsername: rogaha\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n"},{"labels":[null,null,"enhancement"],"text":"_docker ps service_name_ does not show container id or container name. This makes it very unintuitive to figure out the container id of the actual service.\r\n\r\nyou pretty much have to do a _sudo docker ps_ and try to match the \"name\" with the name of the service.\r\n\r\nWhat would be very helpful is an additional column that shows container name as well. This will help up run a _docker exec -it container_name /bin/bash_ into it. "},{"labels":[null,null,"enhancement",null],"text":"**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker network create --driver overlay --subnet 10.40.1.0/24 --gateway 10.40.1.1  mynetwork`\r\n2. Create test.yml file:\r\n```\r\nversion: '3'\r\nservices:\r\n  test:\r\n    image: nginx:stable-alpine\r\n    networks:\r\n     my_network:\r\n       ipv4_address: 10.40.1.20\r\n   \r\nnetworks:\r\n my_network:\r\n   external:\r\n     name: mynetwork\r\n```\r\n3. `docker stack deploy --compose-file=test.yml ng`\r\n4.  docker exec -it 0dbca750f536 hostname -i\r\n\r\n**Describe the results you received:**\r\n```\r\ndocker exec -it 0dbca750f536 hostname -i\r\n10.40.0.3\r\n```\r\n**Describe the results you expected:**\r\n```\r\ndocker exec -it 0dbca750f536 hostname -i\r\n10.40.1.20\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\ndocker version\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:55:28 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:55:28 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 2\r\n Running: 1\r\n Paused: 0\r\n Stopped: 1\r\nImages: 40\r\nServer Version: 1.13.0\r\nStorage Driver: overlay\r\n Backing Filesystem: xfs\r\n Supports d_type: false\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local local-persist\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: *******************\r\n Is Manager: true\r\n ClusterID: *******************\r\n Managers: 3\r\n Nodes: 5\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: ********\r\n Manager Addresses:\r\n******:2377\r\n******:2377\r\n******:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.10.0-514.6.1.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1.907 GiB\r\nName: ***.***.pl\r\nID: ******************\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: *****\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n"},{"labels":[null,"enhancement"],"text":"This is in regards to previous discussions which happened here in regards to Docker swarm mode within 1.13:\r\n\r\nhttps://github.com/docker/compose/issues/4305#issuecomment-282111823\r\n\r\nand here: \r\n\r\nhttps://github.com/docker/docker/issues/30404#issuecomment-274825244\r\n\r\nand as per recommendation to move this in the docker/docker issue tracker.\r\n\r\nAs mentioned [here ](https://github.com/docker/compose/issues/4305#issuecomment-282111823) I do understand the idea behind the \"fault-tolerance\" mechanism and that SWARM takes care to restart the container automatically. \r\n\r\nHowever as explained in the first thread this is not quite the same concept when it comes to \"_initialization_\"  where the \"_fault-taulerant_\" mechanism could (and actually does) make more issues than helping. \r\nAt a time when everything is completely stopped and while swarm starts the containers which have dependencies between each other it happen that every one get's restarted because the other one is still not running and so on, causing much more time lost during the startup instead of the opposite (faster boot). \r\nThe bigger problem is actually that the needed time ends up as a totally non-predictable value which makes it impossible to be planned during a maintenance window or to even have a rough idea in how much time one can restart/restore the infrastructure in case of a failure. So no scheduling at all. \r\n\r\nParticularly if there are multiple containers (in my case 7 in number) with more dependency logic than just wait for a say single DB container (probably the trivial and most common case)  the system could even enter into a kind of a \"deadlock\" state where one container gets restarted and during that another one because the first one is not available, but in the meantime the first is booted and sees the other one is not available and so on. \r\nOn my end I've waited for ~20 minutes and the swarm manager was keeping restarting but the correct order was still not identified. \r\n\r\nIn fact it really depends on the APP inside the container(s) - it really makes no sense to restart if the app actually takes more time to boot as this way startup would consume much more time for the start process. Instead if it follows a logic with the needed order the overall time would increase. \r\n\r\nHaving a mechanism like we used to have for clear \"dependency\" logic configuration would not make any issue of that kind at all + will actually make the boot process faster than relying on automatic restart and waiting until the proper order is get. \r\n\r\nWithout being able to predict, schedule during maintenance and knowing the behavior in general +  at least approximately plan how much time the docker-swarm needs to start the real production operation would be close to impossible for any more-complex than the trivial (as expected to be \"trivial\" mentioned above) microservice environment which has more dependency logic within it. \r\n\r\n\r\n\r\n\r\n\r\n\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nDocker accepts 2 conflicting security-opt types, only the last one given is used as the process label.\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker run -ti --security-opt label=type:container_t --security-opt label=type:spc_t fedora sleep 5\r\n\r\n\r\n\r\n**Describe the results you received:**\r\n\r\ndocker inspect results in:\r\n\r\n    \"MountLabel\": \"system_u:object_r:container_file_t:s0:c118,c497\",\r\n    \"ProcessLabel\": \"system_u:system_r:spc_t:s0:c118,c497\",\r\n\r\nsnip \r\n\r\n    \"SecurityOpt\": [\r\n    \"label=type:container_t\",\r\n    \"label=type:spc_t\"\r\n\r\n**Describe the results you expected:**\r\n\r\nThrow an error and make sure there can only be one type of each of the selinux labels, type, role, user, level, disable\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nDocker version 1.12.6, build ae7d637/1.12.6\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 13\r\nServer Version: 1.12.6\r\nStorage Driver: devicemapper\r\nLogging Driver: journald\r\nCgroup Driver: systemd\r\nPlugins:\r\n Volume: local\r\n Network: null host overlay bridge\r\nSwarm: inactive\r\nRuntimes: oci runc\r\nDefault Runtime: oci\r\nSecurity Options: seccomp selinux\r\nKernel Version: 4.9.9-200.fc25.x86_64\r\nOperating System: Fedora 25 (Cloud Edition)\r\nOSType: linux\r\nArchitecture: x86_64\r\nNumber of Docker Hooks: 2\r\nCPUs: 2\r\nTotal Memory: 992.6 MiB\r\nName: master-01\r\nID: O6ZD:L2TB:XTDS:GBOW:YUHF:YXLX:TDKU:7TF3:V67I:62I7:RQRF:CCGV\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: bridge-nf-call-iptables is disabled\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nRegistries: docker.io (secure)\r\n\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nvirtualbox"},{"labels":[null,null,"enhancement"],"text":"It would be nice to have the distinction between global and replicated services available as a filter.\r\n\r\n```bash\r\n$ docker service ls\r\nID            NAME          MODE        REPLICAS  IMAGE\r\ncybhf7tkt0ry  some-service  global      3/3       busybox:latest\r\nsvr8mwuujx1t  go-demo       replicated  1/1       vfarcic/go-demo:1.0\r\nys37uufktrsa  go-demo-db    replicated  1/1       mongo:3.2.10\r\n\r\n$ docker service ls --filter mode=global\r\nID            NAME          MODE        REPLICAS  IMAGE\r\ncybhf7tkt0ry  some-service  global      3/3       busybox:latest\r\n\r\n$ docker service ls --filter mode=replicated\r\nID            NAME          MODE        REPLICAS  IMAGE\r\nsvr8mwuujx1t  go-demo       replicated  1/1       vfarcic/go-demo:1.0\r\nys37uufktrsa  go-demo-db    replicated  1/1       mongo:3.2.10\r\n```"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nCurrently all services created with the latest compose format default to VIP load balancing mode. Is there plans to add support to `docker stack deploy` for that to be configurable?\r\n\r\nRight now the only way to change to round-robin mode is to update the service after it's been created with `docker stack deploy`.\r\n\r\n"},{"labels":[null,"enhancement",null],"text":"An error occurs when trying to create a /32 ipvlan L3 network. The use case behind this is to assign an ipvlan L3 network per container and then route the /32s out of the docker host. Since the docker container routes everything out of the parent interface, no gateway should be needed on this network. I believe this qualifies more as a feature request than a bug.\r\n\r\nOutput when trying to create the docker network as follows:\r\n\r\n```\r\n\\# docker network create -d ipvlan --subnet 10.32.82.2/32 -o ipvlan_mode=l3 -o parent=dummy0 ipvl3-1\r\nError response from daemon: failed to allocate gateway (): No available addresses on this pool\r\n\r\n\\# ip link list\r\n27: dummy0: <BROADCAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default\r\n    link/ether 36:22: ab:4b:39:c8 brd ff:ff:ff:ff:ff:ff\r\n```\r\n\r\n```\r\n\\# docker version\r\nClient:\r\n Version:      1.13.1\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 06:42:29 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.1\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 06:42:29 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n```\r\n\\# docker info\r\nContainers: 3\r\n Running: 0\r\n Paused: 0\r\n Stopped: 3\r\nImages: 2\r\nServer Version: 1.13.1\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 14\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: aa8187dbd3b7ad67d8e5e3a15115d3eef43a7ed1\r\nrunc version: 9df8b306d01f59d3a8029be411de015b7304dd8f\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 3.19.0-25-generic\r\nOperating System: Ubuntu 14.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 1.331 GiB\r\nName: net-docker-test\r\nID: 6BC7:TLAE:CWCJ:3GNF:T6DP:G7RG:RLN7:STQP:H3IA:W2I5:VGKS:GGFL\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nHttp Proxy: http://172.16.0.89:80\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nThe compose file parsing inside Docker can't parse Windows volumes.\r\n\r\nJust opening this to track and in case anyone else was experimenting with Swarm and Windows.\r\n\r\nTracking the status of: https://github.com/docker/docker/blob/master/cli/compose/convert/volume.go#L31\r\n\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a v3 docker-compose.yml spec with a service.volume spec.\r\n2. Run `docker stack deploy...` \r\n\r\n**Describe the results you received:**\r\n`undefined volume: C`\r\n\r\n**Describe the results you expected:**\r\n\r\nVolume mounts in a container\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.1\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 08:47:51 2017\r\n OS/Arch:      windows/amd64\r\n\r\nServer:\r\n Version:      1.13.1\r\n API version:  1.26 (minimum version 1.24)\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 08:47:51 2017\r\n OS/Arch:      windows/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 0\r\n Paused: 0\r\n Stopped: 1\r\nImages: 2\r\nServer Version: 1.13.1\r\nStorage Driver: windowsfilter\r\n Windows:\r\nLogging Driver: json-file\r\nPlugins:\r\n Volume: local\r\n Network: l2bridge l2tunnel nat null overlay transparent\r\nSwarm: active\r\n NodeID: ov0y3pqc1gjx15upv9mjru6bv\r\n Is Manager: true\r\n ClusterID: vkznxg9rw3dmput8jl9k291fz\r\n Managers: 1\r\n Nodes: 2\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 172.16.58.181\r\n Manager Addresses:\r\n  172.16.58.181:2377\r\nDefault Isolation: hyperv\r\nKernel Version: 10.0 15031 (15031.0.amd64fre.rs2_release.170204-1546)\r\nOperating System: Windows 10 Enterprise Insider Preview\r\nOSType: windows\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 3 GiB\r\nName: DESKTOP-UJ1M2UA\r\nID: 5JUS:Y5TR:DAEX:KQ7U:SC3Z:3IBV:3HBH:XNN5:K7SP:JDL6:J2OB:6FQR\r\nDocker Root Dir: C:\\ProgramData\\Docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: -1\r\n Goroutines: 131\r\n System Time: 2017-02-21T08:17:39.3966974-08:00\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"Currently, swarm mode support templating for the following elements :\r\n\r\n- Hostname\r\n- Mounts\r\n- Environment variables\r\n\r\nWe might want to extend this support in the future, and this issue is to track that. Based on the comments, we'll keep this list up-to-dated and will track implementation of it.\r\n\r\n- [ ] constraints ?\r\n- [ ] network ?\r\n- [ ] labels (label, container-label) ?\r\n\r\n"},{"labels":["enhancement",null,null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\ni'm trying to use top command with ps args. while the args working with ps command, it cause to error in the top command.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker run tomcat:8.0`\r\n2. `docker top [container_id] -e -o pid,lstart,pcpu,pmem,sz,ni,stat,cmd`\r\n\r\n**Describe the results you received:**\r\n\r\n    Error response from daemon: Error running ps: exit status 1\r\n\r\n**Describe the results you expected:**\r\nthe same as the output of the command (without the info of the ps command itself): \r\n`docker exec [container_id] ps -e -o pid,lstart,pcpu,pmem,sz,ni,stat,cmd`\r\n\r\n```\r\n PID                            STARTED  %CPU  %MEM        SZ  NI STAT  CMD\r\n    1 Sun Feb 19 07:20:26 2017       0.3      7.4     530522   0    Ssl   /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Djava.endorsed.dirs=/usr/local/tomcat/endorsed -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap start\r\n  128 Sun Feb 19 07:55:42 2017  0.0  0.1  2694   0 Rs   ps -e -o pid,lstart,pcpu,pmem,sz,ni,stat,cmd\r\n```\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.1\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 08:47:51 2017\r\n OS/Arch:      windows/amd64\r\n\r\nServer:\r\n Version:      1.13.1\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 08:47:51 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 14\r\n Running: 1\r\n Paused: 0\r\n Stopped: 13\r\nImages: 6\r\nServer Version: 1.13.1\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 101\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: aa8187dbd3b7ad67d8e5e3a15115d3eef43a7ed1\r\nrunc version: 9df8b306d01f59d3a8029be411de015b7304dd8f\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.8-moby\r\nOperating System: Alpine Linux v3.5\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 1.941 GiB\r\nName: moby\r\nID: KBZQ:5QHB:AZ3R:YD3F:C75G:OGQA:6YIT:D46Z:CDSI:D2N5:7MZT:MYTH\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 20\r\n Goroutines: 31\r\n System Time: 2017-02-19T07:57:39.1496673Z\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nworking on windows 10"},{"labels":[null,"enhancement"],"text":"Would it be useful to add the available filters for each command in the docker cli help \r\n\r\nthe online api documentation shows them , but not the cli\r\n\r\nhttps://docs.docker.com/engine/api/v1.25/\r\n\r\nI tried adding them , but the help page gets too big and hard to read so maybe there is a better way?\r\n\r\nthe only thing that comes to my mind is implementing --help for the parameters as well\r\n\r\ndocker containers --filter --help "},{"labels":[null,null,"enhancement"],"text":"Hi,\r\n\r\nCurrently, there is no way to update security profiles loaded into containers. We can only specify security profiles for seccomp/apparmor/SELinux during `docker run` using `--security-opt `. But there are many times when we want to update or dynamically change for example a set of syscalls using seccomp profile. \r\n\r\nIt would be nice to have the ability to update security profiles using `docker update --security-opt <>`. Is this planned or is there a fundamental limitation for doing this?"},{"labels":[null,null,"enhancement",null],"text":"**Description**\r\n\r\nThe `ContainerStatPath` method in the client library returns different errors than the error reported by the API.\r\nThe first case is when running the method on a container ID that doesn't exist, the other one if the container ID does exist, but the path does not exist.\r\n\r\n**Steps to reproduce the issue:**\r\n_case 1_\r\n\r\nCompile and run the following program:\r\n\r\n```go\r\nfunc TestContainerStatPathIssue(t *testing.T) {\r\n\tcli, err := client.NewEnvClient()\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\t_, err = cli.ContainerStatPath(context.TODO(), \"foo\", \"bar\")\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n}\r\n```\r\n\r\n_case 2_\r\nRun command `docker create --name=foo alpine:latest /bin/sh` and run the program from case 1 again.\r\n\r\n**Describe the results you received:**\r\nCase 1 returns:\r\n> Error: request returned Not Found for API route and version http://<ip>:4243/v1.24/containers/foo/archive?path=bar, check if the server supports the requested API version\r\n\r\nRunning `curl` on that URL returns:\r\n```json\r\n{\"message\":\"No such container: foo\"}\r\n```\r\n\r\nCase 2 returns:\r\n> Error: request returned Not Found for API route and version http://<ip>:4243/v1.24/containers/foo/archive?path=bar, check if the server supports the requested API version\r\n\r\nRunning `curl` on that URL returns:\r\n```json\r\n{\"message\":\"lstat /var/lib/docker/overlay/116c08d9b5aedea0fa868effeaa4562ed28cc8f8d2e06dcd1e557fca48815026/merged/bar: no such file or directory\"}\r\n```\r\n\r\n**Describe the results you expected:**\r\nIn both cases I expect an error that is more in line with the error returned from the API.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nThe docker version I'm running supports API version `1.25`, setting `DOCKER_API_VERSION=1.25` results in the same behavior as above.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:55:28 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:55:28 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 3\r\n Running: 0\r\n Paused: 0\r\n Stopped: 3\r\nImages: 9\r\nServer Version: 1.13.0\r\nStorage Driver: overlay\r\n Backing Filesystem: xfs\r\n Supports d_type: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.10.0-514.6.1.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 992.7 MiB\r\nName: localhost.localdomain\r\nID: MFQW:WT3K:SL5H:LVNK:NCRK:MMAV:YLUG:XCCG:LTO6:KXMN:2GBN:52TN\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nRunning on VirtualBox."},{"labels":[null,null,"enhancement"],"text":"I'd like to print image name from my current running containers, while showing statistics. The issue si similar to https://github.com/docker/docker/issues/20973, but referring to image name:\r\n\r\ndocker stats --format \"table {{.Container}}\\t{{.Name}}\\t{{**.ImageName**}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\\t{{.BlockIO}}\\t{{.NetIO}}\"\r\n\r\nRequest is: property .ImageName should be available to format shown statistics\r\n"},{"labels":[null,"enhancement"],"text":"I've tried specifying the mac_address in a docker compose v3 file and it doesn't give any parse errors or warnings when deploying via docker stack deploy but the mac_address parameter is not set for the service.\r\n\r\nIt appears that the option to set a mac address for a service is not available. This is required for services that use the mac address for e.g. licensing purposes."},{"labels":[null,"enhancement",null,null],"text":"\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nWhen using docker-compose up with the yml file below, container will run in priviledged mode.\r\n\r\n```\r\nversion: \"2.1\"\r\n\r\nservices:\r\n  web:\r\n    build: .\r\n    command: nodemon -L --debug=5858\r\n\r\n    privileged: true\r\n    tty: true\r\n    ports:\r\n      - \"3000:3000\"\r\n      - \"5858:5858\"\r\nnetworks:\r\n  default:\r\n    external:\r\n      name: nat\r\n```\r\n\r\n\r\ndockerfile for base node image\r\n\r\n```\r\nFROM microsoft/windowsservercore\r\n\r\nENV NPM_CONFIG_LOGLEVEL info  \r\n\r\nRUN powershell -Command \"wget -Uri https://nodejs.org/dist/v6.9.5/node-v6.9.5-x64.msi -OutFile node.msi -UseBasicParsing\"\r\n\r\nRUN msiexec.exe /q /i node.msi\r\n\r\nCMD [ \"node\" ]  \r\n```\r\n\r\ndockerfile for hapi server with serialport\r\n\r\n```\r\nFROM node:695\r\n\r\nWORKDIR /app\r\n\r\nRUN npm install --silent --progress=false -g nodemon@1.10.2\r\n\r\nCOPY package.json /app/package.json\r\nRUN npm install \r\n\r\n\r\nCOPY . /app\r\n\r\nCMD [\"npm.cmd\", \"start\"]\r\n```\r\n\r\n\r\nSteps to reproduce the behavior\r\n\r\n1. docker build -t node . (the first dockerfile from above)\r\n2. docker tag node:latest node:695\r\n3. docker-compose build\r\n4. docker-compose up\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\nWhen docker-compose up is run the initialization fails with the following error\r\n\r\nPS C:\\Users\\honeyws\\Documents\\ComApiNodeWindows> docker-compose up\r\nRemoving comapinodewindows_web_1\r\nRecreating f8442170a78f_comapinodewindows_web_1\r\n\r\nERROR: for web  Cannot start service web: container c1e3bd481a189a4f68566e4dd470af1ef385288ec7e831633e39363809358613 enc\r\nountered an error during CreateProcess: failure in a Windows system call: The system cannot find the file specified. (0x\r\n2) extra info: {\"ApplicationName\":\"\",\"CommandLine\":\"nodemon -L --debug=5858\",\"User\":\"\",\"WorkingDirectory\":\"C:\\\\app\",\"Env\r\nironment\":{\"NPM_CONFIG_LOGLEVEL\":\"info\"},\"EmulateConsole\":true,\"CreateStdInPipe\":true,\"CreateStdOutPipe\":true,\"CreateStd\r\nErrPipe\":false,\"ConsoleSize\":[0,0]}\r\nERROR: Encountered errors while bringing up the project.\r\nPS C:\\Users\\honeyws\\Documents\\ComApiNodeWindows>\r\n```\r\n\r\n**Describe the results you expected:**\r\n Container should start in priviledged mode\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nDiagnostic id:  ID 097756B2-58F4-436C-901D-FFC79D6EA8B6/2017-02-15_15-24-44\r\n\r\noriginally reported:\r\nhttps://github.com/docker/for-win/issues/501\r\n\r\n**Output of `docker version`:**\r\n```\r\nPS C:\\Users\\honeyws\\Documents\\ComApiNodeWindows> docker version\r\nClient:\r\n Version:      1.13.1\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 08:47:51 2017\r\n OS/Arch:      windows/amd64\r\n\r\nServer:\r\n Version:      1.13.1\r\n API version:  1.26 (minimum version 1.24)\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 08:47:51 2017\r\n OS/Arch:      windows/amd64\r\n Experimental: true\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nWindows 10 pro\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\ndockerd and docker-containerd are not compiled with Stack Canary (buffer overflow protection technique), or PIE (ASLR technique) for Fedora packages (and others I assume). They are compiled with partial RELRO (memory corruption mitigation technique) but not full RELRO. I have not checked the other binaries.\r\n\r\nThese findings might not be a big deal if you're not using the unsafe package or calling C from golang.\r\n\r\n**Steps to reproduce the issue:**\r\n1. To check for partial RELRO, expect to see output from these commands:\r\n- `readelf -l /usr/bin/dockerd | grep 'GNU_RELRO'`\r\n- `readelf -l /usr/bin/docker-containerd | grep 'GNU_RELRO'`\r\n2. To check for full RELRO, expect to see output from:\r\n- `readelf -d /usr/bin/dockerd | grep 'BIND_NOW'`\r\n- `readelf -d /usr/bin/docker-containerd | grep 'BIND_NOW'`\r\n\r\n3. To check for PIE and stack canaries:\r\nUse the `hardening-check`  tool (available in Fedora repos). `hardening-includes` package in Ubuntu repos.\r\n```\r\n➜  $ hardening-check $(which dockerd)\r\n/usr/bin/dockerd:\r\n Position Independent Executable: no, normal executable!\r\n Stack protected: no, not found!\r\n Fortify Source functions: no, only unprotected functions found!\r\n Read-only relocations: yes\r\n Immediate binding: no, not found!\r\n```\r\n\r\n**Describe the results you received:**\r\nBoth binaries had partial RELRO, no stack canaries, and no PIE enabled.\r\n\r\n**Describe the results you expected:**\r\nI expected full RELRO, stack canaries, and PIE enabled.\r\n\r\n**Output of `docker version`:**\r\n1.13.1\r\n"},{"labels":[null,"enhancement"],"text":" Right now container updating supports setting the `CpuPeriod` and `CpuQuota` options. Since `--cpus`, which sets `NanoCPUs` seems to be the new way to automatically set period and quota on container creation, I think they should be present in update as well!\r\n\r\n*opening an issue for this as requested in https://github.com/docker/docker/pull/27958#issuecomment-279928336`*"},{"labels":["enhancement"],"text":"Right now it is very painful to process a SIGINT when this function runs in my own tools. I would like it very much if I could interrupt the process.\r\n\r\nI don't think this'll be too complex but I could be wrong; either way if it's enough of a time/cost problem I can probably do the patch with a blessing in advance.\r\n\r\nThanks!"},{"labels":[null,null,"enhancement"],"text":"I am running docker version v1.13.0 with swarm (stand alone), the old swarm.\r\n\r\nWe have an overlay network and it seems that we can't create more than 255 containers. We get the following error:\r\n\r\n**Cannot start service logsparser: Error response from daemon: no available IPv4 addresses on this network's address pools**\r\n\r\n\r\nIs it a bug? Is there a way to increase this limitation?\r\n\r\nI attached 2 files with the output of:\r\n\r\ndocker info\r\n\r\nand\r\n\r\ndocker network inspect\r\n\r\n[d_info.txt](https://github.com/docker/docker/files/775585/d_info.txt)\r\n[d_network_inspect.txt](https://github.com/docker/docker/files/775586/d_network_inspect.txt)\r\n\r\n\r\n\r\n"},{"labels":[null,null,"enhancement"],"text":"Hi,\r\nthis is not an issue, merely a feature request.\r\nI begin to have quite some experience with docker, and I hang continually on the #docker IRC chan. I thus have a pretty good idea of the things the newcomers (or not :/) have the most issues with.\r\n\r\nThere is really a big misunderstanding of Docker Volumes. More specifically, people tend not to see the difference between creating a Docker Named Volume and mount it and sharing a host's directory.\r\n\r\nNamely, they don't make the difference between `-v /path/in/host:/path/in/container` and `-v volume-name:/path/in/container`.\r\n\r\nThis is even more troublesome when they actually have it written like this `-v ./sql-data:/var/lib/data` and then we suggest them to use a Named Volume instead, by giving them the `-v sql-data:/var/lib/data`!\r\n\r\nI have written an article about this [here](http://nschoe.com/articles/2017-01-28-Docker-Taming-the-Beast-Part-4.html) in order to try and demystify the Docker Volumes.\r\nBut that's a hard fight.\r\n\r\nWhen you include the Dockerfile `VOLUME` statement, it kills the remaining few people who seemed to have understood at first.\r\n\r\nI think a lot of confusion comes from the fact that experienced people tend to talk about \"volumes\" indifferently when they are talking about Docker Named Volumes or Shared Host's Directory.\r\n\r\nTherefore I think we would benefit to have a separate `--volume / -v` option which would mount a Docker Named Volume inside a container, the syntax would be the same `--volume / -v volume-name:/path/inside/container` and a `--mount` or even `--external-mount` which would mount a host's directory inside a container.\r\n\r\nFrom a developer's perspective, this would not change much (a simple name check when using `--volume / -v`), but that would clearly help newcomers. Because then, when we say \"volume\" it would clearly mean \"Docker Named Volume\", and when we say \"mounts\" (or \"external mounts\") we would clearly mean \"an external (thus host's) directory mounted in the container\".\r\n\r\nThis would be **much** easier to explain that \"external mounts shadow existing data in the container\" rather than \"when you're mounting a volume to a container, the target content is shadowed, but only if this volume is a host's directory\" (which is often said, but doesn't really make sense).\r\n\r\nI know this won't be accepted and probably discarded (and I understand: it breaks API, you'd need to make both options work at the same time to make the effective switch in 4-5 major versions), but I'm just saying that I see _a lot_ of confusion about this in IRC and this is something that I believe should be addressed.\r\n\r\nAnyway, that was my 2 cents of the topic, what do you say?"},{"labels":[null,null,"enhancement"],"text":"When using `docker version` with the `--format` option to get specific pieces of data, there are cases when the data is not really parseable. If format is provided the command should respect the data that is requested and not litter the output with other information.\r\n\r\nExample:\r\nI have a docker-machine with docker version 1.11.2 (api version 1.23) and a docker client version 1.13 (api version 1.26)\r\n\r\n```\r\n>>> docker version --format {{.Client.APIVersion}}\r\n1.24 (downgraded from 1.26)\r\nError response from daemon: client is newer than server (client API version: 1.24, server API version: 1.23)\r\n```\r\n\r\nIn this scenario I expect 1.24 (or 1.26) as the response, and I expressly don't expect the error messages from the daemon included.\r\n\r\nor another example\r\n\r\n```\r\n>>> docker version --format {{.Client.Version}}\r\n1.13.1\r\nError response from daemon: client is newer than server (client API version: 1.24, server API version: 1.23)\r\n```\r\n\r\nIn this scenario I expect 1.13.1 as the response, I was not requesting any info about the server.\r\n\r\nI think providing the error and even the downgraded message are totally fine when I'm not requesting a specific piece of data with `--format`.  When building tools to help debug errors for my users this makes it very difficult (or impossible) to provide meaningful information about errors to the users when we cannot extract specific data because of those errors.\r\n"},{"labels":[null,null,"enhancement"],"text":"Hi,\r\n\r\nSome improvements about docker stack :\r\n~~1. docker stack ls => the result is not sorted~~ #31085\r\n~~2. docker stack rm is limited to only one stack label~~ #32110\r\n3. docker stack start/stop add the possibility to stop a stack without remove it\r\n4. The env variable is not available with external network :\r\nI create an overlay network : \r\n```\r\ndocker network create --driver overlay nginx_network\r\n```\r\ni create a file docker-compose.yml\r\n\r\n```\r\nversion: '3'\r\nnetworks:\r\n     ${NETWORKNAME}:\r\n       external: true\r\nservices:\r\n  nginx:\r\n    image: nginx:latest\r\n    networks:\r\n    - ${NETWORKNAME}\r\n```\r\n\r\nI try to run it\r\n```\r\nNETWORKNAME=nginx_network docker stack deploy NGINX_STACK -c docker-compose.yml\r\n```\r\nI get\r\n\r\n> Creating network NGINX_STACK_nginx_network\r\n> invalid network: nginx_network\r\n\r\nIf i change the env variable with envsubst : \r\n```\r\nexport NETWORKNAME=nginx_network\r\ncat docker-compose.yml | envsubst > docker-compose-new.yml\r\ndocker stack deploy MYSTACK -c docker-compose-new.yml\r\n```\r\ni get : \r\n\r\n> Creating service MYSTACK_nginx\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nSame purpose/behavior as `docker-compose up`'s [`--remove-orphans` option](https://github.com/docker/docker.github.io/blob/20c29f7e47ade7567ee35f3587790f6235d17d59/docs/reference/up.md).  \r\n\r\n```\r\n    --remove-orphans           Remove containers for services not\r\n                               defined in the Compose file\r\n```\r\n\r\nI'd like to ensure that my compose file's representation of my stack's services is complete.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.1\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 08:47:51 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.1\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 08:47:51 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"As requested in https://forums.docker.com/t/how-to-specify-our-own-unique-node-id-or-node-name-to-docker-swarm-nodes/28109 it could be interesting to be able to make node names more friendly, having a **PlaceHolder {{.Node.Name}}** with the host's hostname in https://docs.docker.com/engine/reference/commandline/service_create/#/create-services-using-templates\r\n\r\nRegards"},{"labels":[null,"enhancement"],"text":"`docker images`doesn't look good, it's a bit messy.\r\n\r\nFor example:\r\n```sh\r\n[root@localhost ~]#docker images\r\nREPOSITORY          TAG                  IMAGE ID              CREATED             SIZE\r\nblack-hole/ubuntu   0.1                  38002e6cac22          2 days ago          469 MB\r\nblack-hole/ubuntu   0.2                  f1867zecbacd          2 days ago          469 MB\r\ntars/wordpress      4.2                  2970126d975b          3 days ago          842 MB\r\ntars/mysql          5.5                  c18a3d29edd9          4 days ago          842 MB\r\nubuntu              latest               f49eec89601e          4 days ago          129 MB\r\ntars/php-fpm        5.4                  08c10bfd8ae7          4 days ago          805 MB\r\ntars/centos         7.1                  e7596057d40c          4 days ago          706 MB\r\ncentos              centos7.1.1503       285396d0a019          5 months ago        212 MB\r\n```\r\nIf \"images\" is too much, it's hard to find one of them.\r\n\r\nI think Change this will be way better:\r\n```sh\r\n[root@localhost ~]#docker images -t\r\nblack-hole\r\n    ubuntu\r\n        0.1                      38002e6cac22         2 days ago          469 MB\r\n        0.2                      f1867zecbacd          2 days ago          469 MB\r\ntars\r\n    wordpress         4.2                      2970126d975b        3 days ago          842 MB\r\n    mysql             5.5                      c18a3d29edd9        4 days ago          842 MB\r\n    php-fpm           5.4                      08c10bfd8ae7        4 days ago          805 MB\r\n    centos            7.1                      e7596057d40c        4 days ago          706 MB\r\nnone\r\n    ubuntu            latest                   f49eec89601e         4 days ago        129 MB\r\n    centos            centos7.1.1503           285396d0a019         5 months ago      212 MB\r\n```\r\n\r\n```sh\r\n[root@localhost ~]#docker images -t -s black-hole\r\nblack-hole\r\n      ubuntu\r\n              0.1                      38002e6cac22         2 days ago          469 MB\r\n              0.2                      f1867zecbacd          2 days ago          469 MB\r\n```\r\n\r\n```sh\r\n[root@localhost ~]#docker images\r\nREPOSITORY          TAG                  IMAGE ID              CREATED             SIZE\r\nblack-hole/ubuntu   0.1                  38002e6cac22          2 days ago          469 MB\r\nblack-hole/ubuntu   0.2                  f1867zecbacd          2 days ago          469 MB\r\ntars/wordpress      4.2                  2970126d975b          3 days ago          842 MB\r\ntars/mysql          5.5                  c18a3d29edd9          4 days ago          842 MB\r\nubuntu              latest               f49eec89601e          4 days ago          129 MB\r\ntars/php-fpm        5.4                  08c10bfd8ae7          4 days ago          805 MB\r\ntars/centos         7.1                  e7596057d40c          4 days ago          706 MB\r\ncentos              centos7.1.1503       285396d0a019          5 months ago        212 MB\r\n```\r\n\r\n```sh\r\n[root@localhost ~]#docker images --help\r\n\r\nUsage:  docker images [OPTIONS] [REPOSITORY[:TAG]]\r\n\r\nList images\r\n\r\nOptions:\r\n  -a, --all             Show all images (default hides intermediate images)\r\n      --digests         Show digests\r\n  -f, --filter value    Filter output based on conditions provided (default [])\r\n      --format string   Pretty-print images using a Go template\r\n      --help            Print usage\r\n      --no-trunc        Don't truncate output\r\n   -t, --tree           Tree diagram display\r\n   -s, --specify        Specify the display of a version\r\n   -q, --quiet          Only show numeric IDs\r\n```\r\n"},{"labels":[null,null,"enhancement"],"text":"(running docker 1.13.1 experimental mode, using docker deploy w/composer v3 file).\r\n\r\nOccasionally, for reasons unknown to me, updating a stack by running `docker deploy` does not check for and pull newer images. I don't know how to recover from this other than doing `docker pull` on each host. `docker stack rm xxxxx`, and re-deploying does not help. \r\n\r\nSpecifying the sha would help, but I don't see how that is practical, when deployment is run from a CD pipeline.\r\n\r\nI searched for CLI flags or existing issues, but couldn't find any.\r\n\r\nWhen I issue a  `docker pull <image>` the swarm manager I'm talking to reports the image as up-to-date, but if I login to another manager, it *does* download the newer image."},{"labels":[null,null,null,"enhancement"],"text":"**Description**\r\n\r\nI have several GitLab repositories that deploy review apps to Docker in swarm mode. When done manually, this typically involves the following `docker` commands.\r\n\r\n```\r\ndocker stack rm my_project\r\ndocker stack deploy --compose-file=my_project.yml my_project\r\n```\r\n\r\n`docker stack rm` issues the remove command for services and networks and returns right away. When the above two commands are done in rapid succession by a CI job, I often get errors about missing networks. It seems that re-deploying a stack that has just been removed but whose services/networks haven't finished removing can result in errors.\r\n\r\nTo avoid errors, I've added additional steps between `rm` and `deploy` in my CI scripts to specifically wait for services and networks to finish removing before deploying again.\r\n\r\n```\r\nuntil [ -z \"$(docker service ls --filter label=com.docker.stack.namespace=$COMPOSE_PROJECT_NAME -q)\" ] || [ \"$limit\" -lt 0 ]; do\r\n  sleep 1;\r\ndone\r\n\r\nuntil [ -z \"$(docker network ls --filter label=com.docker.stack.namespace=$COMPOSE_PROJECT_NAME -q)\" ] || [ \"$limit\" -lt 0 ]; do\r\n  sleep 1;\r\ndone\r\n```\r\n\r\nIt would be useful if the `docker stack rm` command had an option like `--wait` that waits for services and networks to be fully removed for a stack before exiting. A `--wait-timeout` option could also be added to specify how long it should wait before exiting if services and networks take a long time to remove.\r\n\r\nThis would be a bit like the `docker container wait` command, but as an option for removing stacks.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n1. Have a stack definition (bundle or compose file) and deploy it.\r\n2. Remove the stack.\r\n3. Immediately deploy the stack again with the same name.\r\n\r\n**Describe the results you received:**\r\n\r\n`docker` reports errors with missing networks, sometimes. The networks from the previous deployment are actually still in the process of being removed.\r\n\r\n**Describe the results you expected:**\r\n\r\nThe stack should be redeployed.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nThis is most prevalent with large stacks. Networks are removed after all the services in them have stopped. This can take a little while.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n(swarm) →  ~ docker version\r\nClient:\r\n Version:      1.13.1\r\n API version:  1.26\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 08:47:51 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.1\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   092cba3\r\n Built:        Wed Feb  8 06:38:28 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n(swarm) →  ~ docker info\r\nContainers: 14\r\n Running: 14\r\n Paused: 0\r\n Stopped: 0\r\nImages: 71\r\nServer Version: 1.13.1\r\nStorage Driver: devicemapper\r\n Pool Name: docker-thinpool\r\n Pool Blocksize: 524.3 kB\r\n Base Device Size: 10.74 GB\r\n Backing Filesystem: xfs\r\n Data file: \r\n Metadata file: \r\n Data Space Used: 23.1 GB\r\n Data Space Total: 65.28 GB\r\n Data Space Available: 42.18 GB\r\n Metadata Space Used: 6.316 MB\r\n Metadata Space Total: 683.7 MB\r\n Metadata Space Available: 677.4 MB\r\n Thin Pool Minimum Free Space: 6.527 GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: true\r\n Deferred Deletion Enabled: false\r\n Deferred Deleted Device Count: 0\r\n Library Version: 1.02.135-RHEL7 (2016-11-16)\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: 3vz5vqb96ab3yh6mrhhbnf5ki\r\n Is Manager: true\r\n ClusterID: 9p9qjvvrp991zitpixlm8dpm5\r\n Managers: 4\r\n Nodes: 4\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 10.45.32.40\r\n Manager Addresses:\r\n  10.45.32.40:2377\r\n  10.45.32.41:2377\r\n  10.45.32.42:2377\r\n  10.45.32.43:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: aa8187dbd3b7ad67d8e5e3a15115d3eef43a7ed1\r\nrunc version: 9df8b306d01f59d3a8029be411de015b7304dd8f\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.10.0-514.6.1.el7.x86_64\r\nOperating System: Red Hat Enterprise Linux Server 7.3 (Maipo)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 15.51 GiB\r\nName: itpocnode01.ucalgary.ca\r\nID: OM7K:NNN7:75VJ:I34C:W5ZS:TCYH:ZGPH:SRXP:HINS:7O2W:IZQ4:QIGD\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: kinghuang\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\n"},{"labels":[null,null,null,"enhancement",null],"text":"**Description**\r\n\r\nI followed this tutorial from Docker to set up a secure way to communicate with a docker daemon : https://docs.docker.com/engine/security/https/\r\n\r\nBut I wanted to use an encrypted client private key instead of the unencrypted one. When I tried to launch a docker command (such as \"docker ps\") I got the error :\r\n\r\n> Could not load X509 key pair: crypto/tls: failed to parse private key. Make sure the key is not encrypted.\r\n\r\nThis message is clear. But why can't I use encrypted client key ? Is that a bug ?\r\n\r\n**Steps to reproduce the issue:**\r\nJust follow the tutorial here :  https://docs.docker.com/engine/security/https/\r\nBut instead of generating simple client private key without passphrase, use this command : \r\n\r\n> openssl genrsa -aes256 -out ca-key.pem 4096\r\n\r\n**Describe the results you received:**\r\n> $ docker ps\r\n> Could not load X509 key pair: crypto/tls: failed to parse private key. Make sure the key is not encrypted.\r\n\r\n**Describe the results you expected:**\r\n\r\nSomething like : \"enter your passphrase\"\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n\r\n```\r\nDocker version 1.12.1, build 23cf638\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 6\r\n Running: 3\r\n Paused: 0\r\n Stopped: 3\r\nImages: 290\r\nServer Version: 1.12.1\r\nStorage Driver: devicemapper\r\n Pool Name: docker-9:1-1974-pool\r\n Pool Blocksize: 65.54 kB\r\n Base Device Size: 107.4 GB\r\n Backing Filesystem: xfs\r\n Data file: /dev/loop0\r\n Metadata file: /dev/loop1\r\n Data Space Used: 5.015 GB\r\n Data Space Total: 107.4 GB\r\n Data Space Available: 14.1 GB\r\n Metadata Space Used: 14.53 MB\r\n Metadata Space Total: 2.147 GB\r\n Metadata Space Available: 2.133 GB\r\n Thin Pool Minimum Free Space: 10.74 GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: false\r\n Deferred Deletion Enabled: false\r\n Deferred Deleted Device Count: 0\r\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\r\n WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\r\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\r\n Library Version: 1.02.90 (2014-09-01)\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge null host overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options:\r\nKernel Version: 3.14.32-xxxx-grs-ipv6-64\r\nOperating System: Debian GNU/Linux 8 (jessie)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 31.32 GiB\r\nName: xxx\r\nID: ED7R:A7Y2:JXCY:J2UD:ENZM:WS5C:TT4Y:3NLI:ZHRV:NNWS:ANBE:EOVR\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No kernel memory limit support\r\nWARNING: No cpu cfs quota support\r\nWARNING: No cpu cfs period support\r\nInsecure Registries:\r\n 127.0.0.0/8\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nNot related."},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nWhen users configure a 3rd party logging driver, typically `docker logs` is unable to report any log output from existing containers.  This makes it difficult to troubleshoot problems without having to go into the weeds of the specific 3rd party logging system to extract the logs.\r\n\r\nWe should find some way to expose some amount of historical logging information from containers regardless of what the logging configuration is, so we can rely on `docker logs` producing output.\r\n\r\nDepending on the chosen model, we might want to expose a knob the user can tweak to control how much logging history is stored when a 3rd party driver that doesn't support read.\r\n"},{"labels":[null,null,"enhancement"],"text":"from my personal experience using the tooling. it would be great if these were resolved. please, do not take these as criticism.\r\n\r\n#### output:\r\n\r\ni  wonder how everyone deals with the docker ouput, in particular the core team. i suspect, i could be wrong, that people are handcrafting scripts for common operations.\r\n\r\ndealing with the ouput in a *unix env could be easier:\r\n\r\n* requires extra work when using common tools, e.g., `docker history <id>` -> `SIZE` contains spaces so `sort -h` doesn't handle it that well. one needs to do deal with Go templates.\r\n* some commands don't take formatting options so they're harder to deal with, e.g., `docker history`.\r\n* is there a standard delimiter that is always used for all the commands? quick test: remove just the `COMMENT` column, note the space(s) in the fields themselves.\r\n\r\n```sh\r\n$ docker history golang_1.4-alpine:latest\r\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\r\nf687c7a9c8fd        22 hours ago        /bin/sh -c echo 'hello world from' 'golang...   0 B                 \r\n0deea0c51a1c        11 months ago       /bin/sh -c #(nop) WORKDIR /go                   0 B                 \r\n<missing>           11 months ago       /bin/sh -c mkdir -p \"$GOPATH/src\" \"$GOPATH...   0 B                 \r\n<missing>           11 months ago       /bin/sh -c #(nop) ENV PATH=/go/bin:/usr/lo...   0 B                 \r\n<missing>           11 months ago       /bin/sh -c #(nop) ENV GOPATH=/go                0 B                 \r\n<missing>           11 months ago       /bin/sh -c set -ex  && apk add --no-cache ...   138 MB              \r\n<missing>           11 months ago       /bin/sh -c #(nop) ENV GOLANG_SRC_SHA1=486d...   0 B                 \r\n<missing>           11 months ago       /bin/sh -c #(nop) ENV GOLANG_SRC_URL=https...   0 B                 \r\n<missing>           11 months ago       /bin/sh -c #(nop) ENV GOLANG_VERSION=1.4.3      0 B                 \r\n<missing>           11 months ago       /bin/sh -c #(nop) ADD file:0f9cfb2e848f093...   4.79 MB             \r\n```\r\n\r\nwhat i'm trying to get at is, it would nice if the output played nicely with the tools folks are familiar with, and some consistent behaviour in the flags commands take (some take `--format` other don't).\r\n\r\n#### documentation\r\n\r\n#### aliasing\r\n\r\nthe alias, specifically, at the bottom is confusing, e.g., https://docs.docker.com/engine/reference/commandline/images/ says `docker images` -> ` docker image ls`:\r\n\r\n* why not then just redirect to the page?\r\n* the alias should be at the top of the page, it is less confusing that way.\r\n\r\n#### links to docs\r\n\r\nit would _really_ nice to have links to the docs directly (not the alias) available on the command line. something like below, or whatever fits your bill.\r\n\r\n```sh\r\n$ docker images --help\r\n\r\nhttps://docs.docker.com/engine/reference/commandline/image_ls/\r\n\r\nUsage:\tdocker images [OPTIONS] [REPOSITORY[:TAG]]\r\n\r\n...\r\n```\r\n\r\n---\r\n\r\n```sh\r\n$ docker version\r\n```\r\n\r\n```sh\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n```sh\r\n$ docker info\r\n```\r\n\r\n```sh\r\nContainers: 153\r\n Running: 0\r\n Paused: 0\r\n Stopped: 153\r\nImages: 1589\r\nServer Version: 1.13.0\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 1290\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.4-moby\r\nOperating System: Alpine Linux v3.5\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 1.952 GiB\r\nName: moby\r\nID: NRXU:ZFNJ:3AC2:WVYL:UODL:44GO:E7AA:AFWL:22FC:FJYR:URTX:SV2N\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 16\r\n Goroutines: 26\r\n System Time: 2017-02-09T19:26:55.917920388Z\r\n EventsListeners: 1\r\nNo Proxy: *.local, 169.254/16\r\nUsername: mohamedbana\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nWould like to copy a file from inside of a docker container to the outside host machine while maintaining the file permissions/ownership of the file. The user who owns the file in docker does not exist on the host machine and the file will be moved to another host machine. However I do not have sudo privilege to modify the ownership and file permissons on the host machines.\r\n\r\nI am thinking it would be an equivalent linux command like `cp -a` or `cp -p`.\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n"},{"labels":[null,null,"enhancement"],"text":"Nobody *wants* to run systemd in a container, but it's necessary because not all software is docker-ready, or docker is being used to simulate a production system.  \r\n\r\nIt's great that docker 1.13 can now deploy a docker-compose.yml to swarm, but the limited configuration support means systemd containers will not run. `docker stack deploy` reports:\r\n```\r\nIgnoring unsupported options: cap_add, devices, privileged, security_opt, tmpfs\r\n```\r\n\r\nWhen will stack/bundle/dab/whatever support systemd containers, which require `cap_add` and `tmpfs`? \r\n\r\n```\r\n    cap_add:\r\n      - SYS_ADMIN\r\n    tmpfs: /run\r\n    volumes:\r\n      - /sys/fs/cgroup:/sys/fs/cgroup:ro\r\n    environment:\r\n      - container=docker\r\n```\r\nRef https://github.com/docker/docker/issues/28614#issuecomment-261724902\r\n\r\nPS: `devices` would also be very useful especially if the value could somehow be made different on different swarm nodes."},{"labels":["enhancement"],"text":"<!--\r\n\r\n-->\r\n\r\n**Description**\r\n\r\nThe error message printed when trying to delete a paused container is incorrect/incomplete.\r\n\r\n**Steps to reproduce the issue:**\r\n1. create a container\r\ndocker create --name tmpredis redis\r\n\r\n2. start the container\r\ndocker start tmpredis\r\n\r\n3. pause the container\r\ndocker pause tmpredis\r\n\r\n4. remove the container\r\ndocker rm tmpredis\r\n\r\n**Describe the results you received:**\r\n\r\nError response from demon: You cannot remove a running container <id>. Stop the container before attempting removal or use -f\r\n\r\n**Describe the results you expected:**\r\nTo remove a paused container, we have to first unpause it then stop it for remove it using -f option. The error message needs to include a statement to unpause the container if it is paused.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n```\r\nClient:\r\n Version:      1.14.0-dev\r\n API version:  1.26\r\n Go version:   go1.7.4\r\n Git commit:   f018c0d-unsupported\r\n Built:        Tue Feb  7 19:33:51 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.14.0-dev\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.4\r\n Git commit:   f018c0d-unsupported\r\n Built:        Tue Feb  7 19:33:51 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 0\r\n Paused: 0\r\n Stopped: 1\r\nImages: 2\r\nServer Version: 1.14.0-dev\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 10\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 1137bf2ba633587aa5d8ce41862c3306519f074b\r\nrunc version: 51371867a01c467f08af739783b8beafc154c4d7\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-62-generic\r\nOperating System: Debian GNU/Linux 8 (jessie) (containerized)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 5.767 GiB\r\nName: 3ae83a9d4ce5\r\nID: PKSO:Y5K3:WFIY:6VCW:5YES:S7Q3:GKL6:RNQI:E66A:FHHH:JKXS:TOKE\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 13\r\n Goroutines: 22\r\n System Time: 2017-02-08T20:32:54.747933128Z\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nWARNING: bridge-nf-call-iptables is disabled\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,null,"enhancement",null],"text":"**Description**\r\n\r\nRepeatedly calling `docker service update` may trigger an `update out of sequence` error. This seems to happen because the api call to service inspect (`GET /services/{id}`) returns an old `Version.Index` even after the successful return of previous a call to service update (`POST /services/{id}/update`).\r\n\r\n**Steps to reproduce the issue:**\r\n```\r\ndocker swarm init\r\ndocker service create --name test busybox tail -f /dev/null\r\nwhile docker service update test --constraint-add \"node.labels.a != b\"; do true; done\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\nAfter some time repeatedly updating the service (~30s on my machine) the last command will fail with the error `Error response from daemon: rpc error: code = 2 desc = update out of sequence`.\r\n\r\n**Describe the results you expected:**\r\n\r\nI expected that successful calls to `POST /service/{id}/update` would guarantee that subsequent calls to `GET /service/{id}` returned an updated `Version.Index`. This seems not to be the case. I'm not sure if this behavior is intended or not, if this is working as expected I think a clarification in the API documentation would be nice.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n$ docker version\r\nClient:\r\n Version:      1.13.1-rc1\r\n API version:  1.25\r\n Go version:   go1.7.4\r\n Git commit:   2527cfc\r\n Built:        Fri Jan 27 21:54:54 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.1-rc1\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.4\r\n Git commit:   2527cfc\r\n Built:        Fri Jan 27 21:54:54 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\nThe problem also happens with `1.13.0` and `1.12.6`\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n$ docker info\r\nContainers: 26\r\n Running: 4\r\n Paused: 0\r\n Stopped: 22\r\nImages: 264\r\nServer Version: 1.13.1-rc1\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 397\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: n2osx48m66gag48ggoyg1w1pc\r\n Is Manager: true\r\n ClusterID: 66490aoqoo9cdx00t5n95hw34\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 192.168.50.4\r\n Manager Addresses:\r\n  192.168.50.4:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-31-generic\r\nOperating System: Ubuntu 16.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 2.915 GiB\r\nName: vagrant\r\nID: XXOL:4PPB:VZV3:W7ZD:QRDT:FY6D:L2WN:OI5T:3Z3I:HZS5:TI6Z:BXJN\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 58\r\n Goroutines: 153\r\n System Time: 2017-02-07T17:31:31.46294919Z\r\n EventsListeners: 1\r\nUsername: cezarsa\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nTested on Vagrant + Virtualbox and also on Ubuntu 14.04 on private Cloudstack."},{"labels":[null,"enhancement",null,null],"text":"**Description**\r\n\r\nTrying to play with the new templating features in 1.13, I wanted to template a volume name in a `docker-compose.yml` file for deployment with `docker stack deploy`. I got an `undefined volume` error, however.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a v3 `docker-compose.yml` file:\r\n```yaml\r\nversion: '3.1'\r\nservices:\r\n  redis:\r\n    image: redis\r\n    volumes:\r\n      - \"redisVol-{{.Task.Slot}}:/data\"\r\nvolumes:\r\n  redisVol-1:\r\n    driver: local\r\n```\r\n2. Attempt to deploy this:\r\n```console\r\n$ docker stack deploy -c docker-compose.yml redis\r\nundefined volume: redisVol-{{.Task.Slot}}\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\nAn error: `undefined volume: redisVol-{{.Task.Slot}}`\r\n\r\n**Describe the results you expected:**\r\n\r\nExpected the volume reference to match ;)\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.1-rc1\r\n API version:  1.25\r\n Go version:   go1.7.4\r\n Git commit:   2527cfc\r\n Built:        Sat Jan 28 00:43:00 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.1-rc1\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.4\r\n Git commit:   2527cfc\r\n Built:        Sat Jan 28 00:43:00 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 32\r\n Running: 7\r\n Paused: 0\r\n Stopped: 25\r\nImages: 250\r\nServer Version: 1.13.1-rc1\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: pxwtehg7hjvgrzd7z1yhwrna8\r\n Is Manager: true\r\n ClusterID: xerj566k9hqteqrznvkyg3z6f\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 192.168.65.2\r\n Manager Addresses:\r\n  192.168.65.2:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.6-moby\r\nOperating System: Alpine Linux v3.5\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 1.952 GiB\r\nName: moby\r\nID: 72VG:JFL4:ITTA:T6HT:HZ6S:3XKE:ASDY:YB3S:32EF:FQJS:AVHS:HFQQ\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 90\r\n Goroutines: 199\r\n System Time: 2017-02-06T20:13:35.084216266Z\r\n EventsListeners: 2\r\nNo Proxy: *.local, 169.254/16\r\nUsername: hairyhenderson\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nDocker for Mac Version 1.13.1-rc1-beta40 (15241)"},{"labels":[null,"enhancement",null],"text":"Extracted from #30024:\r\n\r\n> It seems really difficult to debug a v2 plugin - no `docker logs` or anything like that.\r\n\r\nThere is, unless I missed it, no way in the UI for the user/sysadmin to see whether the process is crashing and restarting, no way to look inside the container, no way to do anything beyond install it and list its name via the docker command.\r\n\r\nYou can drop down to `containerd` to find the ID, `exec` inside it and so forth, but this is somewhat ugly and undocumented."},{"labels":["enhancement"],"text":"follow up from #6889 and #27919\r\n\r\n* The problem:\r\n    some api help  is missing from the the cli and it is present in the rest api docs \r\n\r\n    docker image list --help  - no documentation for available filters\r\n    https://docs.docker.com/engine/api/v1.25/ - filters well documented here\r\n\r\n* In the following context\r\n       swagger is now used to generate the REST docs\r\n       swagger will be used to generate most or all  go api/types\r\n       api/types are shared between the client and the daemon\r\n\r\nI suggest that swagger is also used to generate one extra type that will be used when adding a cobra command\r\n\r\n* cobra command now\r\n      Use:   \"images [OPTIONS] [REPOSITORY[:TAG]]\",\r\n      Short: \"List images\",\r\n\r\n* cobra command after\r\n      Use:   types.cli.use,\r\n      Short: types.cli.short,\r\n\r\n**the command descriptions for the cli and the rest api should remain separate,  but should be in a single swagger file.**\r\n\r\nI am not familiar with swagger , but hopefully it allows adding custom fields which will be used to generate the go type used for the cobra command\r\n\r\n\r\nI think this has the following advantages:\r\n*  the maintainers don't have to remember to change the REST docs and the cli docs\r\n*  better UX \r\n*  reuse some command descriptions"},{"labels":[null,null,"enhancement"],"text":"For historical reasons, the local image store still has code in place to disallow removing \"parent\" images.\r\n\r\nFor example;\r\n\r\n```bash\r\n$ DOCKER_BUILDKIT=0 docker build -t parent -<<EOF\r\nFROM scratch\r\nLABEL foo=bar\r\nEOF\r\n\r\n$ DOCKER_BUILDKIT=0 docker build -t child -<<EOF\r\nFROM parent:latest\r\nLABEL foo=baz\r\nEOF\r\n\r\n\r\n$ docker images ls --filter reference=parent\r\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\nparent              latest              17789f62da6c        About an hour ago   0 B\r\n\r\n$ docker image rm 17789f62da6c\r\nError response from daemon: conflict: unable to delete 17789f62da6c (cannot be forced) - image has dependent child images\r\n```\r\n\r\nSince the introduction of the content-addressable store, the \"child/parent\" chain\r\nis only used for the local build cache, but is not required to run an image.\r\n\r\nI think we should allow removing the parent image if `--force` is used.\r\n\r\nNote that _removing_ the parent image will not cleanup any disk space (because\r\nthe _layers_ are still used by the child image), however it does allow users\r\nto clean up images they no longer need.\r\n\r\nrelated discussion https://github.com/docker/docker/issues/25602\r\n\r\n/cc @tonistiigi @dmcgowan "},{"labels":[null,null,"enhancement"],"text":"We run a lot of applications, and we are about to move them over to a swarm. Some of these applications use host names (for accessing external services such as a REST APIs or a databases) we cannot change (for different reasons). Different applications use the same host name to access different external resource. \r\n\r\nWhat we have done up until now is to run each application in its own network and in its own DNS zone. It could also just have been as simple as editing the /etc/hosts file on the service that application runs on to make sure that a given host name resolves to the correct external resource.\r\n\r\nIt seems this is not that easy in swarm mode. At first, we though it would be as easy as just creating services (for the REST APIs or databases) with names that corresponds to the host name which the application expect to find any given external resource on. However; we then realize that service names are unique. After having looked into this issue, we also realize that it is impossible to alias a given service (representing a REST API or a database) in the scope of a single overlay network. Maybe we could do some sort of DNS trickery with our own DNS service for each overlay network...but then again; I can't stop thinking that network scoped DNS entries for services would be a very useful thing not only for us.\r\n\r\nShort story : swarm mode requires that all service names (aka host names) are unique. Different applications in our stack expects to be able to resolve external resources (REST APIs or databases) using host names which would not be \"globally\" unique within the swarm.\r\n\r\nExample:\r\n\r\n**Overlay Network 1**\r\n\r\n- **Application A** resolves Database A with host name `db` and REST API with host name `api`.\r\n- Database A\r\n  - service name = `dbA` becomes host name = `dbA`. \r\n  - wanted : network scoped alias = `db`\r\n- REST API A\r\n  - service name = `restapiA` becomes host name = `restapiA`. \r\n  - wanted : network scoped alias = `api`\r\n\r\n**Overlay Network 2**\r\n\r\n- **Application B** resolves Database B with host name `db`\r\n- Database B\r\n  - service name = `dbB` becomes host name = `dbB`\r\n  - wanted : network scoped alias = `db`\r\n\r\n**Overlay Network 3**\r\n\r\n- **Application C** resolves REST API with host name `api`\r\n- REST API C\r\n  - service name = `restapiC` becomes host name = `restapiC`\r\n  - wanted : network scoped alias = `api`\r\n\r\nFirst of all; have I missed something? Is this doable with Docker Swarm mode as it is now (1.13.0)? If not, would this be something worth looking into?"},{"labels":[null,null,"enhancement"],"text":"We are seeing a variety of engagements where docker hosts participating in a swarm cluster somehow lose state in DNS,, thus containers on an overlay network fail to resolve other containers on the same overlay network.  The issue appears to be intermittent, but we have seen cases where all containers on a node have stopped resolving\r\n\r\nWould like a less disruptive way to refresh/re-establish a functional DNS on a docker node than too restart the docker daemon, which is too disruptive for some customers.\r\n\r\nThis issue with varying similar circumstances is known to have occurred on many customer support engagements.  \r\n\r\nAttempts to repro the issue within Docker labs thus far have not been successful, but here is a summary of the steps during a troubleshooting session that identifies the issue/symptoms\r\n\r\n1) can deploy containers on existing and newly created overlays via compose, view networks, etc.  Nodes seems to be communicating fine with the controllers\r\n\r\n2) Once multiple containers are deployed on existing overlay networks or on newly created overlays  via docker run or compose, during an unknown or specified timeline container(s) stop responding.  \r\n\r\n3) confirm those failing containers by entering into a container that has been reported that is running,idocker exec -it <container_id> sh and running nslookup to the other container that exists on the overlay network\r\n\r\n4) confirm container and overlays are being used by checking the output of docker inspect\r\n\r\nIssue has been reported using engine 1.12.x \r\n\r\n\r\n\r\n"},{"labels":[null,null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nwhen using `docker stack deploy`, the command fails on Invalid interpolation format for \"command\" option in service \r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n~~~\r\ndocker stack deploy --compose-file=docker-compose.yml monitoring\r\n~~~\r\nthe compose file I'm using is [this](https://gist.github.com/999404efd8e63ae1dfaa18ef1595d4c5)\r\n\r\n**Describe the results you received:**\r\n~~~\r\nhost$ docker stack deploy --compose-file=docker-compose.yml monitoring                                                            \r\nInvalid interpolation format for \"command\" option in service \"node-exporter\": \"^/(sys|proc|dev|host|etc)($|/)\"\r\n~~~\r\n\r\n**Describe the results you expected:**\r\nit should work, the same syntax works from the command line \r\n~~~\r\n\r\ndocker \\\r\n  service create --mode global \\\r\n  --name node-exporter \\\r\n  --mount type=bind,source=/proc,target=/host/proc \\\r\n  --mount type=bind,source=/sys,target=/host/sys \\\r\n  --mount type=bind,source=/,target=/rootfs \\\r\n  --mount type=bind,source=/etc/hostname,target=/etc/host_hostname \\\r\n  -e HOST_HOSTNAME=/etc/host_hostname \\\r\n  basi/node-exporter:latest \\\r\n  -collector.procfs /host/proc \\\r\n  -collector.sysfs /host/sys \\\r\n  -collector.filesystem.ignored-mount-points \"^/(sys|proc|dev|host|etc)($|/)\" \\\r\n  --collector.textfile.directory /etc/node-exporter/ \\\r\n  --collectors.enabled=\"conntrack,diskstats,entropy,filefd,filesystem,loadavg,mdadm,meminfo,netdev,netstat,stat,textfile,time,vmstat,ipvs\"\r\n\r\n\r\n~~~\r\n\r\n\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:58:26 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 2\r\n Running: 2\r\n Paused: 0\r\n Stopped: 0\r\nImages: 7\r\nServer Version: 1.13.0\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 66\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: c1y7qpqdobgltvwwwkzxiktod\r\n Is Manager: true\r\n ClusterID: 0m9lnp8bpx00ki5gligqmwlzh\r\n Managers: 1\r\n Nodes: 3\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 10.0.0.139\r\n Manager Addresses:\r\n  10.0.0.139:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-45-generic\r\nOperating System: Ubuntu 16.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 3.675 GiB\r\nName: ip-10-0-0-139\r\nID: 2MJO:BNSZ:FLU4:65YT:V7ED:3WY7:NDMM:F5X6:DKCY:VUMO:UXBX:FHU3\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: odedpriva\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,null,"enhancement"],"text":"I noticed that the plugin integration tests install plugins from the Hub:\r\n\r\n```go\r\npName             = \"tiborvass/sample-volume-plugin\"\r\npTag              = \"latest\"\r\npNameWithTag      = pName + \":\" + pTag\r\n\r\n...\r\n\r\nfunc (s *DockerSuite) TestPluginBasicOps(c *check.C) {\r\n        testRequires(c, DaemonIsLinux, IsAmd64, Network)\r\n        _, _, err := dockerCmdWithError(\"plugin\", \"install\", \"--grant-all-permissions\", pNameWithTag)\r\n        c.Assert(err, checker.IsNil)\r\n...\r\n```\r\n\r\nI see a few problems with this:\r\n\r\n- Use of the Hub means a service outage or transient network issue will make the test appear flaky. We've had this problem with some pull tests before, and have tried to use a local registry instead wherever possible.\r\n- If the plugin is updated on Hub to an incompatible version, it will break integration tests for older versions of Docker.\r\n- The plugin is installed with full privileges, so a compromise of this Hub account can lead to root access on any machine that runs the integration tests.\r\n- It's a waste of bandwidth to pull the plugins from the internet every time a test which uses them runs.\r\n\r\nI think it would be a lot better to bake the plugins into the Docker image and serve them from a local registry.\r\n\r\ncc @tiborvass @tonistiigi @vdemeester @anusha-ragunathan"},{"labels":[null,"enhancement"],"text":"I'm running the [install script](https://docs.docker.com/datacenter/ucp/2.0/reference/cli/install/) for a wonderful life-changing new piece of software, and I get the following error:\r\n\r\n```\r\n{\"level\":\"error\",\"msg\":\"Failed to launch proxy: Error response from daemon: oci runtime error: container_linux.go:247: starting container process caused \\\"process_linux.go:245: running exec setns process for init caused \\\\\\\"exit status 33\\\\\\\"\\\"\",\"time\":\"2017-02-01T00:46:14Z\"}\r\n```\r\n\r\nI have multiple levels of error here:\r\n\r\n- Failed to launch proxy\r\n  - Error response from daemon\r\n    - OCI runtime error\r\n      - container_linux.go:247: starting container process caused\r\n        - process_linux.go:245: running exec setns process for init caused\r\n          - exit status 33\r\n\r\n(I hope this is the correct nesting, but I'm not sure.) 🎄 \r\n\r\nAfter quickly interrogating available witnesses, it looks like the first level (\"Failed to launch proxy\") comes from the app that I am trying to run. I believe it is trying to start a container (perhaps with the Docker CLI), but this fails. The CLI (or the Docker API client) tells me \"Error response from daemon\". I'm pretty sure that we're talking about the Docker Engine here. So far, so good. 🔍 \r\n\r\nOK, what does the Docker Engine tell us? That some API call (I'm guessing a `start` API call, but a confirmation would be useful) failed. Why did it fail? Because the OCI runtime (on which we rely to faithfully manage our containers) said so. Let's ask the OCI runtime. _Politely._ 🙏 \r\n\r\n– OCI oh my OCI, why did my container fail?\r\n– Because its process could not be started. This happened at ~Ezekiel 25:17~ `container_linux.go:247`.\r\n– OCI oh my OCI, what is happening in `container_linux.go:247`?\r\n– Please refer to the [holy scriptures](https://github.com/opencontainers/runc/blob/master/libcontainer/container_linux.go#L247) for enlightenment. 📖 \r\n\r\nThe holy scriptures did not enlighten me. 😢  I tried alternative practices 🔮  which advised me to just go to the next clue, [process_linux.go:245](https://github.com/opencontainers/runc/blob/master/libcontainer/process_linux.go#L245).\r\n\r\nUnfortunately, we are facing here a typical case of diverging interpretation of the holy scriptures. In the opencontainers version above, line 245 doesn't seem relevant to the issue at hand. In the [official vendored version](https://github.com/docker/docker/tree/master/vendor/github.com/opencontainers/runc/libcontainer) the process_linux.go is nowhere to be found. What do, what do. 🤔 \r\n\r\nThis might happen in the `execSetns` function. Maybe. Maybe not. 🐈 \r\n\r\nI will figure this out, even if I have to submit my Docker Engine to the good services of the Holy Inquisition. But meanwhile, I think we can improve that reporting a tiny little bit. 🐜 \r\n\r\nThank you!\r\n\r\nPS: I just got an anonymous tip for the other hemisphere of my brain letting me know that this might be related to some resource limit (e.g. can't fork _because ulimit says so!_) but no conclusive proof was gathered so far. The culprits are still out there. 🏃  :running_woman:  Obimichaelcrosby you are my only hope.\r\n"},{"labels":[null,"enhancement"],"text":"I got an error while doing \"docker info\" with zfs backend driver.  It shows \"Zpool: error while getting pool information strconv.ParseUint: parsing \"\": invalid syntax\" on the page which seems to be an error or something is missing.  \r\n\r\nVersion details:\r\nHost: Ubuntu 14.04.5\r\nZFS: 0.6.5.8-1~trusty\r\nDocker: Docker version 1.13.0, build 49bf474\r\n\r\n\r\n```\r\nroot@atomic:/var/lib/docker# docker info\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 0\r\nServer Version: 1.13.0\r\nStorage Driver: zfs\r\n Zpool: error while getting pool information strconv.ParseUint: parsing \"\": invalid syntax\r\n Zpool Health: not available\r\n Parent Dataset: tank/docker\r\n Space Used By Parent: 139264\r\n Space Available: 797687267328\r\n Parent Quota: no\r\n Compression: lz4\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 3.13.0-107-generic\r\nOperating System: Ubuntu 14.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 7.761 GiB\r\nName: atomic\r\nID: WSPC:FV5I:4AMK:5QB7:NHMU:43P3:5LAI:JBE7:SZIN:KT6H:DMDZ:R26T\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\nI am not sure which command Docker uses, outputs of some ZFS commands for reference:\r\n\r\n```\r\nroot@atomic:/var/lib/docker# zpool list\r\nNAME   SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\r\ntank  2,72T  1,91T   830G         -      -    70%  1.00x  ONLINE  -\r\n```\r\n\r\n```\r\nroot@atomic:/var/lib/docker# zfs list\r\nNAME                    USED  AVAIL  REFER  MOUNTPOINT\r\ntank                   1,91T   743G   144K  none\r\ntank/burp               419G   743G   419G  /var/spool/burp\r\ntank/data              1,49T   743G  1,49T  /var/disk\r\ntank/docker             348K   743G   348K  /var/lib/docker\r\ntank/lxc               2,37G   743G  2,37G  /var/lib/lxc\r\ntank/lxc-abby          1,05G   743G  1,04G  /var/lib/lxc/abby\r\ntank/mysql             42,1M   743G  42,1M  /var/lib/mysql\r\ntank/postgresql        15,3M   743G  15,3M  /var/lib/postgresql\r\ntank/squeezeboxserver   491M   743G   185M  /var/lib/squeezeboxserver\r\ntank/twonky            2,88M   743G  2,88M  /var/lib/twonky\r\ntank/www                869M   743G   858M  /var/www\r\n```\r\n\r\n```\r\nroot@atomic:/var/lib/docker# zpool status tank\r\n  pool: tank\r\n state: ONLINE\r\nstatus: Some supported features are not enabled on the pool. The pool can\r\n        still be used, but some features are unavailable.\r\naction: Enable all features using 'zpool upgrade'. Once this is done,\r\n        the pool may no longer be accessible by software that does not support\r\n        the features. See zpool-features(5) for details.\r\n  scan: scrub repaired 0 in 6h9m with 0 errors on Sun Dec 27 21:07:45 2015\r\nconfig:\r\n\r\n        NAME                        STATE     READ WRITE CKSUM\r\n        tank                        ONLINE       0     0     0\r\n          mirror-0                  ONLINE       0     0     0\r\n            wwn-0x50014ee20aeb2e08  ONLINE       0     0     0\r\n            wwn-0x50014ee26040a451  ONLINE       0     0     0\r\n        cache\r\n          atomic--vg-zfsaarc        ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"Hi guys,\r\n\r\nNow that we have ingest pipeline shipped with elasticsearch, wouldn't it be great to have a docker log driver directly pushing to Elasticsearch ?\r\n"},{"labels":["enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nRight now checkpoints are stored per-container, but that only enables really simple use cases which involve checkpointing and restoring the same container. Checkpoints should be more of a global resource, to allow for use cases like cloning (restoring multiple containers from the same checkpoint) and container migration (live or offline). However, the crux of making this change seems to be around validating that it's safe to restore a given container from a given checkpoint.\r\n\r\nTo address this, I propose associating each checkpoint to a docker image, instead of to a single container. See https://github.com/docker/docker/issues/30443. As @xemul points out, a checkpoint is heavily coupled to the filesystem snapshot (i.e. docker image). Therefore, it makes sense for docker to add support for managing a checkpoint and a docker image as a pair. Copying from https://github.com/docker/docker/issues/30443#issuecomment-275534126:\r\n\r\nFor the simple case where the filesystem isn't modified between container creation and checkpoint creation, the associated docker image is simply the image of the container that was checkpointed.\r\n\r\nThe case where the filesystem is modified in between container and checkpoint creation necessitates a new docker image be created which is consistent with the checkpoint, and that's a little different. For that case, I propose a new ```--commit <image_name>``` flag for the checkpoint create command which will create a new docker image from the current state of the filesystem (i.e. docker commit) and then create the checkpoint. In this case, the checkpoint is associated with the newly created docker image. Making the commit and checkpoint creation a single atomic operation has other advantages as well. Currently, if you do a ```docker commit``` and then a ```docker checkpoint create``` (or vice-versa), there's a small period of time in between the two operations where the process in the container becomes unpaused, which means that the filesystem could be altered in between the commit and checkpoint creation (e.g. application writes to a log file). If this happens, the restore will fail as the checkpoint and the docker image will be inconsistent. Combining the two actions into a single atomic operation where the container process is paused throughout is the only way to guarantee this won't happen.\r\n\r\nOnce the checkpoint is associated to a docker image, the validation (e.g. on ```docker start --checkpoint <checkpoint_name>```) becomes just checking that the image associated to the checkpoint matches the container's image. Then we know the filesystems are compatible. Perhaps there's other validation that needs to happen too, but that's at least a start.\r\n\r\ncc @thaJeztah @boucher @xemul "},{"labels":[null,null,"enhancement"],"text":"hello,\r\n\r\nDocker compose documentation suggests the support of override files [here](https://docs.docker.com/compose/extends/#/multiple-compose-files) but I cannot find any support for that in 'docker stack' command (neither in the docs, not when trying, and nor for dab files). \r\n\r\nIntegrating compose as part of Docker Swarm is a huge step forward but for real life environment overriding is absolutely a must otherwise managing the diff between dev, test, prod and various size deployments becomes too much of a mess. \r\n\r\n"},{"labels":[null,"enhancement"],"text":"Currently Docker create and run commands supports the --storage-opt \"size\" param for a few graph drivers. \r\nDocumentation: https://docs.docker.com/engine/reference/commandline/run/#set-storage-driver-options-per-container\r\n\r\n> This (size) will allow to set the container rootfs size to 120G at creation time. This option is only \r\navailable for the devicemapper, btrfs, overlay2, windowsfilter and zfs graph drivers. For the \r\ndevicemapper, btrfs, windowsfilter and zfs graph drivers, user cannot pass a size less than the \r\nDefault BaseFS Size. For the overlay2 storage driver, the size option is only available if the backing fs is xfs and mounted with the pquota mount option. Under these conditions, user can pass any size less then the backing fs size.\r\n\r\n\r\nThis issue is to request that the optional \"<btrfs/dm/grafdriver>.default_size\" param be supported for the docker daemon --storage-opts for the supported drivers[in the above list] so that a global predicatble value could be set if the user does not pass any."},{"labels":[null,"enhancement"],"text":"Hi -\r\n\r\nI am working on an HPC project that would need to achieve CPU pinning for containers so that specific tasks run on the appropriate CPU as well as speak directly to the host network adapter (a topic covered well in  #25303). This could be achieved by having a head node task (perhaps on the swarm manager node) that uses docker swarm to run an agent container on each compute node launched as a service and then each agent container could then launch the appropriately configured task containers locally using docker run --cpuset-cpus=\"1,2\" etc. \r\nHowever, it would be much cleaner/nicer to manage the entire deployment process through docker swarm so that orchestration features would not need to be implemented in the agent nodes - something that would simplify the entire orchestration control structure.\r\n\r\nOn this basis,  please consider the inclusion of the --cpuset-cpus parameter.\r\n\r\nThanks,\r\nPiers Harding.\r\n"},{"labels":[null,null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nI'm trying to write tests against code consuming client.Client, I can see a number of ways of abstracting this but they all feel obtuse and repetitive.\r\n\r\nAre there efforts to create a testable / mockable version of the client.Client code in the `docker/docker` repository?\r\n\r\nGo includes modules to allow testing of http servers - so it seems like it could be an idiomatic pattern to provide a docker/docker client.Client version which can also provide a testable interface. \r\n\r\nI may be missing something here, but have had a chat with Justin and browsed the source, I didn't seem to find something like a way to create a client for unit testing.\r\n\r\nIdeally I could do something like this:\r\n\r\n```\r\ntestClient := client.NewTestClient{}\r\nerror := scaleService(alert, &testClient)\r\n\r\n....\r\n\r\nfunc scaleService(req requests.PrometheusAlert, c *client.Client) error {\r\n\tvar err error\r\n        serviceName := req.Alerts[0].Labels.FunctionName\r\n\tservice, _, inspectErr := c.ServiceInspectWithRaw(context.Background(), serviceName)\r\n\tif inspectErr == nil {\r\n\t\tvar replicas uint64\r\n\t\tif req.Status == \"firing\" {\r\n\t\t\tif *service.Spec.Mode.Replicated.Replicas < 20 {\r\n\t\t\t\treplicas = *service.Spec.Mode.Replicated.Replicas + uint64(5)\r\n\t\t\t} else {\r\n\t\t\t\treturn err\r\n\t\t\t}\r\n           // c.ServiceUpdate(......\r\n       }\r\n```\r\n\r\nI wondered if @dnephin might have some ideas?"},{"labels":[null,"enhancement"],"text":"There are many cases where it is useful to mount an immutable data volume into a container, with configuration or static data (or code, it could be an apt repo or whatever).\r\n\r\nI am proposing we add a new mount type so you can do (or whatever `--mount` is renamed to) - it would also be supported in compose files/bundles.\r\n\r\n```docker service create --mount type=image,src=alpine:3.5,dest=/alpine ...```\r\n\r\nThese would fetch the image from the repository if necessary, allowing tags or hashes to be specified, and mount it at the specified mountpoint. This would always be read only. Docker would unpack the image and mount it, presumably using layers but this would be an implementation detail, as being read only this would not be visible.\r\n\r\nUse cases include configuration data (not secrets, but miscellaneous scripts and config), actual data, apt repos, npm repos etc. If a hash was used it would be guaranteed to be consistent across multiple tasks in a service.\r\n\r\ncc @AkihiroSuda @tonistiigi @stevvooe "},{"labels":[null,null,"enhancement",null],"text":"**Description**\r\n\r\ndocker stack deploy from compose file not compatible with Publish mode=host (bypass ingress network)\r\n\r\n**Steps to reproduce the issue:**\r\ncat docker-compose.yml\r\n```\r\nversion: \"3\"\r\nservices:\r\n  nginx:\r\n    image: nginx\r\n    ports:\r\n      - \"mode=host,target=80,published=9005\"\r\n```\r\ndocker stack deploy --compose-file docker-compose.yml test\r\n\r\n**Describe the results you received:**\r\nInvalid containerPort: mode=host,target=80,published=9005\r\n\r\n**Describe the results you expected:**\r\nI would like this create service like : \r\n```\r\ndocker service create --name test_nginx --publish \"mode=host,target=80,published=9005\" nginx\r\n```\r\n\r\n**Output of `docker version`:**\r\n```\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:55:28 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:55:28 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n```\r\nContainers: 43\r\n Running: 27\r\n Paused: 0\r\n Stopped: 16\r\nImages: 258\r\nServer Version: 1.13.0\r\nStorage Driver: overlay2\r\n Backing Filesystem: xfs\r\n Supports d_type: true\r\n Native Overlay Diff: false\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: ******\r\n Is Manager: true\r\n ClusterID: *****\r\n Managers: ***\r\n Nodes: ****\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: ****\r\n Manager Addresses:\r\n  *****:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.10.0-514.2.2.el7.x86_64\r\nOperating System: Red Hat Enterprise Linux Server 7.3 (Maipo)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: ***\r\nTotal Memory: *** GiB\r\nName: *****\r\nID: *****\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\n storage-driver=overlay2\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\nThanx for reading"},{"labels":[null,"enhancement",null],"text":"`docker checkpoint ls` lists the checkpoints _for a given container_:\r\n\r\n```bash\r\n$ docker checkpoint ls --help | grep Usage\r\nUsage:  docker checkpoint ls [OPTIONS] CONTAINER\r\n```\r\nI have a usecase where I need a full list of _all checkpoints_: bash completion for `docker start --checkpoint`. At the point where the user specifies `--checkpoint`, the container is yet unknown.\r\nWithout this feature, I have to iterate over all containers and call `docker checkpoint ls` on each of them, which would result in lots of API calls."},{"labels":[null,"enhancement"],"text":"Hi, \r\n\r\nI was tryin out docker 1.13 (and compose v3 too). It's a huuuuge step on the road to production-readiness.\r\n\r\nI think it can be a good thing if each management commands gets aliased. I know that `docker ps` is still supported but one day, it won't be anymore and i'll be forced to type `docker container ls` which is much longer. I can do personal aliases in my `.profile` or` .bashrc`, of course, but this is ugly, and what will occur for production machine without any personnal account ? \r\n\r\nFor instance, I think that aliases like \"ct\" for containers and \"im\" or \"img\" for images can be great. Like in `docker ct ls`. \r\n\r\nThis is just thought/suggestion, a mini-minor improvement but that will make every user save time everyday :)\r\n\r\nIt4."},{"labels":[null,"enhancement",null],"text":"| Q                | A\r\n| ---------------- | -----\r\n| Bug report?      | no\r\n| Feature request? | yes\r\n\r\n**Description**\r\n\r\nCurrently Docker has a good logs system (`docker logs`) where we can see all the logs redirected to `stdout|stderr` but it lacks of logs rotate support. \r\n\r\nWhat this mean? Let's say I have a Fedora 25 running Docker (latest) and each morning when I arrive to office I need to update the Fedora OS and restart the PC. Of course doing this I will need to start my containers once again whether it's through `docker` or `docker-compose` the thing is logs are keep from the beginning of the time. \r\n\r\nWhen you have an application that's constantly logging as a way to debug it for example a web application some times is hard to waits for the latest logs because if you run `docker logs -f` it will start from the very beginning and if the container is a month older then it's a pain. \r\n\r\nI could use another approach here `tail -n <numer_of_lines>` but how do I know when the logs for today started? Is impossible AFAIK. \r\n\r\nSo I am asking if it's possible to add support to rotate logs by day automatically or manually maybe as a flag or don't know, what do you think about it?\r\n\r\n**Steps to reproduce the issue:**\r\n1. Have a Docker image from one or two months ago constantly logging\r\n2. Check `docker logs` you should get the logs from one or two month ago plus the current ones\r\n\r\n**Describe the results you received:**\r\nYou got all the logs once at a time\r\n\r\n**Describe the results you expected:**\r\nI can filter the logs for today at least"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nCurrently there's no way to execute an arbitrary command in a stopped container.\r\n\r\nAlthough this is by design right now, this design presents one problem: you cannot inspect a container whose build failed straight away.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker build --tag some-failing-image .`\r\n2. The build stops.\r\n3. `docker ps -a` shows that the container that failed is called `pensive_fermi`.\r\n4. OK, I want to check what happened there to fix the `Dockerfile`: `docker exec pensive_fermi sh`.\r\n\r\n**Describe the results you received:**\r\nError response from daemon: Container a212058e9a8bf7354c63bf293481c96a22d91e97fcd4c43421a99c9eb328cf1c is not running\r\n\r\n**Describe the results you expected:**\r\n`docker exec` should have a `--boot` (or similar) flag that let you boot the container with this command if it is stopped, or `docker start` should let you pass a new command when restarting it.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nPossible workarounds/drawbacks:\r\n\r\n- use volumes and mount them in a new container. Problem: if problem happened outside the volume.\r\n- If original container was started with bash, sleep or anything like that, you can `docker start` it and then `docker exec` it. Problem: what if not? What if the command simply fails and exits straight away?\r\n- Do this:\r\n  ```\r\n  docker commit pensive_fermi temp\r\n  docker run -it --rm temp <your commands>\r\n  docker rmi temp\r\n  ```\r\n  It should work, although it's not very straightforward nor documented. However, I guess that documenting this could be considered a fix.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n$ docker version\r\nClient:\r\n Version:         1.12.6\r\n API version:     1.24\r\n Package version: docker-common-1.12.6-5.git037a2f5.fc25.x86_64\r\n Go version:      go1.7.4\r\n Git commit:      037a2f5/1.12.6\r\n Built:           Wed Jan 18 12:11:29 2017\r\n OS/Arch:         linux/amd64\r\n\r\nServer:\r\n Version:         1.12.6\r\n\r\n API version:     1.24\r\n Package version: docker-common-1.12.6-5.git037a2f5.fc25.x86_64\r\n Go version:      go1.7.4\r\n Git commit:      037a2f5/1.12.6\r\n Built:           Wed Jan 18 12:11:29 2017\r\n OS/Arch:         linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n$ docker info\r\nContainers: 114\r\n Running: 0\r\n Paused: 0\r\n Stopped: 114\r\nImages: 1635\r\nServer Version: 1.12.6\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\nLogging Driver: journald\r\nCgroup Driver: systemd\r\nPlugins:\r\n Volume: local\r\n Network: bridge host null overlay\r\nSwarm: inactive\r\nRuntimes: oci runc\r\nDefault Runtime: oci\r\nSecurity Options: seccomp\r\nKernel Version: 4.9.4-201.fc25.x86_64\r\nOperating System: Fedora 25 (Workstation Edition)\r\nOSType: linux\r\nArchitecture: x86_64\r\nNumber of Docker Hooks: 2\r\nCPUs: 4\r\nTotal Memory: 6.761 GiB\r\nName: yajolap.yajodomain\r\nID: KUBN:F7JL:URX6:HO55:R3L2:SCUU:IWVY:EZ2O:F53G:WHTO:3G4D:R4YU\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nRegistries: docker.io (secure)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nlocal linux"},{"labels":[null,"enhancement",null],"text":"Using Docker 1.13:\r\n\r\n```\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n`docker stack deploy couchbase-master` gives the error message:\r\n\r\n```\r\nPlease specify either a bundle file (with --bundle-file) or a Compose file (with --compose-file).\r\n```\r\n\r\nThe directory already has a `docker-compose.yml`. A default file name should be use if none is specified.\r\n\r\n`docker stack deploy --compose-file docker-compose.yml couchbase-master` feels a bit redundant."},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nWhen I want to run a container to work in swarm mode overlay network, I found that if not run `docker run --name ...`, it can't be resolved with container name\r\n\r\n**Steps to reproduce the issue:**\r\n1.create a swarm mode cluster and create an overlay network with option --attachable\r\n2.create a service testA in swarm mode\r\n3.create a container testB with command `docker run ...` without option --name, it will get a random name, such as `lucid_lumiere`\r\n4.create a container testC with command `docker run --name testC ...`\r\n5.run command `nslookup lucid_lumiere` and `nslookup testC ` in testA\r\n\r\n**Describe the results you received:**\r\n```style\r\n[root@22ca477efa57 /]# nslookup lucid_lumiere\r\nServer:\t\t127.0.0.11\r\nAddress:\t127.0.0.11#53\r\n\r\nNon-authoritative answer:\r\n*** Can't find lucid_lumiere: No answer\r\n```\r\n```style\r\n[root@22ca477efa57 /]# nslookup testC\r\nServer:\t\t127.0.0.11\r\nAddress:\t127.0.0.11#53\r\n\r\nNon-authoritative answer:\r\nName:\ttestC\r\nAddress: 10.0.3.4\r\n```\r\n\r\n**Describe the results you expected:**\r\n`nslookup lucid_lumiere` could get the right answer\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:55:28 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:55:28 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 4\r\n Running: 2\r\n Paused: 0\r\n Stopped: 2\r\nImages: 98\r\nServer Version: 1.13.0\r\nStorage Driver: devicemapper\r\n Pool Name: docker-8:5-59899908-pool\r\n Pool Blocksize: 65.54 kB\r\n Base Device Size: 53.69 GB\r\n Backing Filesystem: xfs\r\n Data file: /dev/loop0\r\n Metadata file: /dev/loop1\r\n Data Space Used: 17.3 GB\r\n Data Space Total: 107.4 GB\r\n Data Space Available: 90.08 GB\r\n Metadata Space Used: 33.28 MB\r\n Metadata Space Total: 2.147 GB\r\n Metadata Space Available: 2.114 GB\r\n Thin Pool Minimum Free Space: 10.74 GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: false\r\n Deferred Deletion Enabled: false\r\n Deferred Deleted Device Count: 0\r\n Data loop file: /data/docker/devicemapper/devicemapper/data\r\n WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\r\n Metadata loop file: /data/docker/devicemapper/devicemapper/metadata\r\n Library Version: 1.02.107-RHEL7 (2016-06-09)\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: ye2ghne3j4rvmr5r7ljcq48vs\r\n Is Manager: true\r\n ClusterID: j7wn96ngop5xhxu6h333b7tdt\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 5000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 172.16.10.19\r\n Manager Addresses:\r\n  172.16.10.19:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.8.10-1.el7.elrepo.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 31.11 GiB\r\nName: log.aispeech.com\r\nID: 4Y6T:NCRV:K7J7:WOKO:ZIZY:LXYQ:IQRL:FMNL:PH4E:BPBX:6VU6:Y3ZB\r\nDocker Root Dir: /data/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 56\r\n Goroutines: 154\r\n System Time: 2017-01-22T16:43:58.12487325+08:00\r\n EventsListeners: 1\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nRegistry Mirrors:\r\n http://33391a94.m.daocloud.io/\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement",null],"text":"Swarm mode errors appear with the format `rpc error: code = # desc = <description>`.  It'd be more readable and more consistent with other docker APIs for them to just appear as `<description>`.\r\n\r\nThe daemon logs seem to already log the error with the RPC code, if we needed them for debugging:\r\n\r\n```\r\ntime=\"2017-01-20T22:27:30.360244689Z\" level=error msg=\"Error creating service : rpc error: code = 2 desc = name conflicts with an existing object\" \r\ntime=\"2017-01-20T22:27:30.360284788Z\" level=error msg=\"Handler for POST /v1.24/services/create returned error: rpc error: code = 2 desc = name conflicts with an existing object\"\r\n```\r\n\r\nThe error returned from the API does not need to have that information.  Maybe we can just call `grpc.Desc(error)` on errors returned from GRPC calls?\r\n\r\n**Steps to reproduce the issue, and results received:**\r\n```\r\n$  docker service create --name alpine alpine ping google.com\r\n1zklorj14lsau1pgw0un50byo\r\n$  docker service create --name alpine alpine ping google.com\r\nError response from daemon: rpc error: code = 2 desc = name conflicts with an existing object\r\n```\r\n\r\n**Results expected (similar command from docker run):**\r\n```\r\n$  docker run -d --name alpine alpine ping google.com\r\n93146c58d97f1254934f763deccfd1361881c5b14315412c28374d040a9bbc72\r\n$  docker run -d --name alpine alpine ping google.com\r\ndocker: Error response from daemon: Conflict. The container name \"/alpine\" is already in use by container 93146c58d97f1254934f763deccfd1361881c5b14315412c28374d040a9bbc72. You have to remove (or rename) that container to be able to reuse that name..\r\n```\r\n\r\n(As a note, this error message comment also seems to have 2 periods at the end)\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\nThis was also true of docker 1.12."},{"labels":[null,null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nADP runs a typical corporate firewall ala Blue Coat proxy which means there is no direct internet access or transparent proxy available. Docker for Mac helps with this but most machines which need Docker are running Linux or Windows 7.\r\n\r\n* I would like to have a concise way of setting a http-proxy override for the daemon to enable pulling from the Hub registry (without having to locate and hack the systemd unit file). This can be tricky for some people.\r\n\r\n* I would like to have a way of setting a one-time environmental variable or dotfile to configure proxy as per git, npm and bower. It would be explicitly honoured by all `docker build` commands. The `--build-arg` override is useful but overly verbose when specifying both http/https and arduous to type over and over. This has to be explained continually so improving the UX would be a time saver.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n### Scenario 1. \r\n\r\n`docker pull nginx:latest`\r\n\r\n### Scenario 2. \r\n\r\n`docker build -t mycontainer .`\r\n\r\n**Describe the results you received:**\r\n\r\n### Scenario 1. \r\n\r\n`Cannot dial 52....`\r\n\r\n### Scenario 2. \r\n\r\n`apt-get timed out reaching server debian.org`\r\n\r\n**Describe the results you expected:**\r\n\r\n### Scenario 1.\r\n\r\nInstead of hacking the systemd file, I'd rather the daemon could be more easily altered - perhaps in `daemon.json`? A cli command would be great, but I think this might be out of place for the CLI.\r\n\r\n### Scenario 2.\r\n\r\nInstead of `docker build --build-arg http_proxy=$http_proxy --build-arg https_proxy=$https_proxy mycontainer . I'd like to be able to do a one-time command that would affect a dotfile such as:\r\n\r\n`docker config proxy $http_proxy `\r\n`docker config https.proxy $https_proxy `\r\n\r\nOther tools that work this way: git, bower, npm:\r\n\r\nhttps://docs.npmjs.com/misc/config"},{"labels":[null,null,"enhancement"],"text":"Hey all,\r\n\r\n**Usecase**\r\nI have a front-end proxy which listens on ports `80` and `443`. My applications are deployed as microservices behind this proxy, which acts as a layer-7 load balancer for those applications. These applications are, for example, `php:7-apache` containers which simply listen on port `80` for HTTP requests. The idea is to have at least 2 replicas for a service, so they can be upgraded incrementally using the *rolling upgrade* functionality that comes with Docker Swarm.\r\n\r\n**Issue**\r\nAt this point, I'm not sure if zero-downtime deployments using the rolling update functionality of Docker Swarm are even possible, at least for my use case. There is one major issue for me here.\r\n\r\nWhen containers are stopped during the rolling update, they are always stopped using the same signal (SIGTERM, or SIGKILL after a certain period). Many images, like the aforementioned apache-based image, won't gracefully shutdown with a SIGTERM, but need a different signal to be sent for the container to shutdown in a graceful way. I created an issue (#25696) for this as well, but this didn't make the 1.13 release. I don't see how the current rolling upgrade system can work in any use case, except for the cases where containers *actually* are designed specifically to shutdown gracefully when receiving a SIGTERM. In my situation, upgrading the service leads to intermittent HTTP-502 errors until the upgrade is complete. I can't imagine this not being a problem for anyone else, unless I'm missing something obvious.\r\n\r\n**Possible workaround**\r\nWrap the main command of an image that needs to be able to shutdown gracefully in a wrapper script:\r\n\r\n```bash\r\nshut_down() {\r\n  kill -SIGWINCH ${SCRIPT_PID}\r\n}\r\n\r\ntrap 'shut_down' SIGTERM SIGINT\r\n\r\nstart_apache &\r\n\r\nSCRIPT_PID = \"$!\"\r\nwait ${SCRIPT_PID}\r\n```\r\n\r\nThis would immediately fix the issue I'm having, since any `SIGINT` or `SIGTERM` that reaches the container would simply be relayed as a `SIGWINCH`. In this case, this would gracefully shutdown my apache container. However, this would mean that I would have to modify every image I'm using to use this script. It's also a non-standard and to be fair, a nasty solution.\r\n\r\nWhat's the recommended course of action here? Am I missing a piece of the puzzle, or simply overseeing something? Also: even if *this* issue would be solved, would I get true **zero downtime** deployments with the rolling upgrade functionality of Docker Swarm Mode? In other words, are containers actually removed from the ingress load balancing pool prior to sending the `stop_signal` during the upgrade, or would I still get HTTP-502 errors from containers that are still being load-balanced *to*, but would be in the process of shutting down?"},{"labels":[null,"enhancement"],"text":"Single-file Docker volumes are great to replace / insert configuration files.\r\n\r\nBut the current implementation has some major limitations:\r\n\r\n1. you cannot replace the container without the file at place. (this makes the container dependant on the structure of the host fs).\r\n2. two containers cannot share the same file.\r\n3. a single-file volume is not listed as a volume.\r\n\r\nUse cases for single-file volumes cannot be transformed into use cases with folder-based volumes without forcing the application to restructure their file system."},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nCan't install `docker-engine` on Photon OS by using:\r\n\r\n```console\r\n$ curl -sSL get.docker.com | sh\r\n```\r\n\r\n**Steps to reproduce the issue:**\r\n1. Start a Photon OS machine\r\n2. Run `curl -sSL get.docker.com | sh`\r\n\r\n**Describe the results you received:**\r\n\r\n```console\r\n$ curl -sSL get.docker.com | sh\r\n+ sudo -E sh -c 'sleep 3; tdnf -y install docker-engine'\r\nError: Failed to synchronize cache for repo 'Docker main Repository' from 'https://yum.dockerproject.org/repo/main/photon/1.0'\r\nDisabling Repo: 'Docker main Repository'\r\nNo package docker-engine available\r\nError(1011) : No matching packages\r\n```\r\n\r\nVisit https://yum.dockerproject.org/repo/main and see no `photon` directory.\r\n\r\n**Describe the results you expected:**\r\n\r\nExpecting `docker-engine` to be installed on Photon OS."},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\n**Steps to reproduce the issue:**\r\n1. `$ docker images | sort -hk 7,8` \r\n\r\n**Describe the results you received:**\r\nSorts numerically but doesn't take into account the ` MB` or ` GB` (even though `-k 7,8` includes it).\r\n\r\n**Describe the results you expected:**\r\nMost commands, like `du -h` have no space before `MB` or `GB` and thus work directly with `sort -h`.\r\nThe workaround is an ugly hack `$ docker images | sed 's/ \\(MB\\|kB\\|GB\\)/\\1/' | sort -hk 7`.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nCommon tools that have the standard `K`, `M`, `G` without space (note that they don't have `GB` but just `G` but that doesn't seem to bother `sort -h`):\r\n\r\n - `du -h`\r\n - `df -h`\r\n - `ls -lh`\r\n - `tree -h`\r\n - `free -h`\r\n\r\nCommon tools that have a space before it:\r\n\r\n - `docker images`\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.12.3\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   6b644ec\r\n Built:        Wed Oct 26 21:44:32 2016\r\n OS/Arch:      linux/amd64\r\n```"},{"labels":[null,"enhancement"],"text":"Pull request 8588 set the minimum TLS version to 1.0 in code: https://github.com/docker/docker/pull/8588\r\n\r\nHowever, our security department wants to eliminate the use of TLSv1 entirely and docker has been identified as having this security risk. Is it possible to set the TLS version to 1.1 through the config without recompiling docker?\r\n\r\nIdeally this would be something in the daemon.json like:\r\n        \"tlsminversion\": tls.VersionTLS11\r\n\r\nBut it doesn't look like that's possible at the moment.\r\n\r\nAny suggestions? Would this be a reasonable feature request?\r\n\r\nThanks,\r\nKevin\r\n\r\n"},{"labels":[null,null,null,"enhancement",null],"text":"**Description**\r\n\r\n`docker service create --publish` can specify the published port mapping for the service. However, both `docker service ls` and `docker service ps` cannot show such information.\r\n\r\nIn `1.13`, one more column, `PORTS`, was added in `docker service ps` result, however, it's always empty.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n```bash\r\nroot@d1:~/stack-deploy# docker service create --name app1 -p 8001:80 nginx\r\naw1aubl9w8ki9dmw2bo6q6937\r\nroot@d1:~/stack-deploy# docker service ps app1\r\nID            NAME    IMAGE         NODE  DESIRED STATE  CURRENT STATE             ERROR  PORTS\r\n9c17a7wonz55  app1.1  nginx:latest  d2    Running        Preparing 12 seconds ago\r\n```\r\n**Update: the below problem was caused by mixing Docker 1.12 and 1.13 in the Swarm cluster, check the comment below about the details**\r\n\r\nI also tried with `--publish mode=host` which is introduced in `1.13`, but still nothing shows on `PORTS` column:\r\n\r\n```bash\r\nroot@d1:~/stack-deploy# docker service create --name app2 --publish mode=host,published=8002,target=80 nginx\r\n7evmnm2gau4c5bprds1hna7ft\r\nroot@d1:~/stack-deploy# docker service ps app2\r\nID            NAME    IMAGE         NODE  DESIRED STATE  CURRENT STATE          ERROR  PORTS\r\n2haoqqj8a12c  app2.1  nginx:latest  d2    Running        Running 7 seconds ago\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\nThe `PORTS` section is empty.\r\n\r\n**Describe the results you expected:**\r\n\r\nThere should be something like `80/tcp, 443/tcp`or `8001 => 80/tcp, 443/tcp` in the section.\r\n\r\nAnd `docker stack ps` inherit the same problem with empty `PORTS` column:\r\n\r\n```bash\r\nroot@d1:~/stack-deploy# docker stack ps lnmp -f desired-state=running\r\nID            NAME          IMAGE                      NODE  DESIRED STATE  CURRENT STATE           ERROR  PORTS\r\n1x6qiieam21p  lnmp_mysql.1  mysql:5.7                  d1    Running        Running 31 minutes ago\r\n7irrc6v9xnbo  lnmp_nginx.1  twang2218/lnmp-nginx:v1.2  d1    Running        Running 31 minutes ago\r\n2bq2kjm6xacn  lnmp_php.1    twang2218/lnmp-php:v1.2    d1    Running        Running 31 minutes ago\r\nedp0ed1k6u9w  lnmp_nginx.2  twang2218/lnmp-nginx:v1.2  d1    Running        Running 31 minutes ago\r\n1hlmkgtpf1pa  lnmp_php.2    twang2218/lnmp-php:v1.2    d2    Running        Running 31 minutes ago\r\n0xjjyu3tyewp  lnmp_php.3    twang2218/lnmp-php:v1.2    d2    Running        Running 31 minutes ago\r\ne9lgn25kyepx  lnmp_php.4    twang2218/lnmp-php:v1.2    d1    Running        Running 31 minutes ago\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nAfter dive into the code, I found the code of printing port information is at: \r\n\r\nhttps://github.com/docker/docker/blob/master/cli/command/task/print.go#L142-L154\r\n\r\n```golang\r\n\t\tfmt.Fprintf(\r\n\t\t\tout,\r\n\t\t\tpsTaskItemFmt,\r\n\t\t\tid,\r\n\t\t\tindentedName,\r\n\t\t\timage,\r\n\t\t\tnodeValue,\r\n\t\t\tcommand.PrettyPrint(task.DesiredState),\r\n\t\t\tcommand.PrettyPrint(task.Status.State),\r\n\t\t\tstrings.ToLower(units.HumanDuration(time.Since(task.Status.Timestamp))),\r\n\t\t\ttaskErr,\r\n\t\t\tportStatus(task.Status.PortStatus),\r\n\t\t)\r\n```\r\n\r\nAnd I inspect the task to see what's value of `task.Status.PortStatus`:\r\n\r\n```bash\r\n$ docker inspect 9c17a7wonz55\r\n[\r\n    {\r\n        \"ID\": \"9c17a7wonz55ti6k6uy9eus7g\",\r\n        \"Version\": {\r\n            \"Index\": 222\r\n        },\r\n        \"CreatedAt\": \"2017-01-18T05:38:22.69818533Z\",\r\n        \"UpdatedAt\": \"2017-01-18T05:38:37.380391831Z\",\r\n        \"Spec\": {\r\n            \"ContainerSpec\": {\r\n                \"Image\": \"nginx:latest@sha256:33ff28a2763feccc1e1071a97960b7fef714d6e17e2d0ff573b74825d0049303\"\r\n            },\r\n            \"Resources\": {\r\n                \"Limits\": {},\r\n                \"Reservations\": {}\r\n            },\r\n            \"RestartPolicy\": {\r\n                \"Condition\": \"any\",\r\n                \"MaxAttempts\": 0\r\n            },\r\n            \"Placement\": {},\r\n            \"ForceUpdate\": 0\r\n        },\r\n        \"ServiceID\": \"aw1aubl9w8ki9dmw2bo6q6937\",\r\n        \"Slot\": 1,\r\n        \"NodeID\": \"kcumt000ix5pldgrxfiq4yqu0\",\r\n        \"Status\": {\r\n            \"Timestamp\": \"2017-01-18T05:38:37.362853091Z\",\r\n            \"State\": \"running\",\r\n            \"Message\": \"started\",\r\n            \"ContainerStatus\": {\r\n                \"ContainerID\": \"3a42430ae3f06d7dc1cb74181c0158245ff5de17914b3111d0d1ca3de7c2485d\",\r\n                \"PID\": 23410\r\n            },\r\n            \"PortStatus\": {}\r\n        },\r\n        \"DesiredState\": \"running\",\r\n        \"NetworksAttachments\": [\r\n            {\r\n                \"Network\": {\r\n                    \"ID\": \"to761yyjzg5sa62etld8is1sh\",\r\n                    \"Version\": {\r\n                        \"Index\": 123\r\n                    },\r\n                    \"CreatedAt\": \"2017-01-17T23:58:46.668572186Z\",\r\n                    \"UpdatedAt\": \"2017-01-18T05:11:30.405150109Z\",\r\n                    \"Spec\": {\r\n                        \"Name\": \"ingress\",\r\n                        \"Labels\": {\r\n                            \"com.docker.swarm.internal\": \"true\"\r\n                        },\r\n                        \"DriverConfiguration\": {},\r\n                        \"IPAMOptions\": {\r\n                            \"Driver\": {},\r\n                            \"Configs\": [\r\n                                {\r\n                                    \"Subnet\": \"10.255.0.0/16\",\r\n                                    \"Gateway\": \"10.255.0.1\"\r\n                                }\r\n                            ]\r\n                        }\r\n                    },\r\n                    \"DriverState\": {\r\n                        \"Name\": \"overlay\",\r\n                        \"Options\": {\r\n                            \"com.docker.network.driver.overlay.vxlanid_list\": \"4096\"\r\n                        }\r\n                    },\r\n                    \"IPAMOptions\": {\r\n                        \"Driver\": {\r\n                            \"Name\": \"default\"\r\n                        },\r\n                        \"Configs\": [\r\n                            {\r\n                                \"Subnet\": \"10.255.0.0/16\",\r\n                                \"Gateway\": \"10.255.0.1\"\r\n                            }\r\n                        ]\r\n                    }\r\n                },\r\n                \"Addresses\": [\r\n                    \"10.255.0.8/16\"\r\n                ]\r\n            }\r\n        ]\r\n    }\r\n]\r\n```\r\n\r\nAs shown above, it's empty:\r\n\r\n```json\r\n        \"Status\": {\r\n            \"Timestamp\": \"2017-01-18T05:38:37.362853091Z\",\r\n            \"State\": \"running\",\r\n            \"Message\": \"started\",\r\n            \"ContainerStatus\": {\r\n                \"ContainerID\": \"3a42430ae3f06d7dc1cb74181c0158245ff5de17914b3111d0d1ca3de7c2485d\",\r\n                \"PID\": 23410\r\n            },\r\n            \"PortStatus\": {}\r\n        },\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n docker version\r\nClient:\r\n Version:      1.13.0-rc7\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   48a9e53\r\n Built:        Fri Jan 13 06:52:01 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.0-rc7\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   48a9e53\r\n Built:        Fri Jan 13 06:52:01 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nroot@d1:~/stack-deploy# docker info\r\nContainers: 7\r\n Running: 5\r\n Paused: 0\r\n Stopped: 2\r\nImages: 8\r\nServer Version: 1.13.0-rc7\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 60\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: rlt51or3rdev9ku00bsclb3he\r\n Is Manager: true\r\n ClusterID: ilt7bnrmlxdialuitpmyovs1h\r\n Managers: 2\r\n Nodes: 2\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 138.197.213.116\r\n Manager Addresses:\r\n  138.197.213.116:2377\r\n  138.197.221.47:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-59-generic\r\nOperating System: Ubuntu 16.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 488.4 MiB\r\nName: d1\r\nID: JDBE:R26Z:SLGS:WIBN:S33Z:XLM7:HRW4:FEPV:4TU6:6ZJH:H5UY:YC5X\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nLabels:\r\n provider=digitalocean\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"## Problem\r\nThe container object is used all over the code base as a place to store container configuration (both user config and config resolved by the daemon), runtime data.\r\nLikewise the container's mutex (which is really the `State` object's mutex) is also exposed all over the place.\r\n\r\nThe container mutex is locked when both reading and writing to the container (as expected). The mutex is also locked during certain container operations. Because of this, reading and writing to the container object is very much tied to typically very slow operations, and as we've found operations that tend to be buggy (see netlink failures). This has led to a number of cases causing deadlocks.\r\n\r\n## Goal\r\nIssues with a container should never prevent a user from querying the engine in any way, including the problem container itself.\r\n\r\n## Proposal\r\nMake the container a lock-free object.\r\nSwamkit already has a really nice model for this for storing each of it's object types using github.com/hashicorp/go-memdb. This same model should work for containers... see https://github.com/docker/swarmkit/tree/master/manager/state/store\r\nThis provides for indexing, filtering, partial matching (can drop usage of truncindex), and change notifications.\r\n\r\nEssentially this means instead of passing around a single pointer object that each function reads from/writes to, we make (deep) copies. The object database contains the single source of truth, anything that needs to modify the container will need to modify it by committing changes back to the object store. The container object itself should not require any mutex since each object would only have 1 reader or writer at a time.\r\n\r\nBecause we'll be passing around copies, we need to make sure there is no \"live\" state being copied... for instance https://github.com/docker/docker/blob/f19a293dd741583c66001799435f784f2af455e0/container/state.go#L32... or for that matter https://github.com/docker/docker/blob/f19a293dd741583c66001799435f784f2af455e0/container/state.go#L18\r\n\r\nWork to be done:\r\n\r\n1. (#28483) Move \"live\" state off the container object (or at least contain it)\r\n2. Implement deep copy support for the container object and each of the objects it uses.\r\n3. Replace `MemoryStore` with go-memdb object store\r\n    - Also requires replacing exec store\r\n    - Needs to implement versioning on the container data (does not need to persist)\r\n    - Implement notifications for changes to container state\r\n4. Add locking (per ID) at the container manager level (currently Daemon) to ensure that stateful actions against the container (start/stop/pause etc) are synchronized.\r\n\r\nYou can see a POC form of this here: https://github.com/docker/docker/compare/master...cpuguy83:container_memdb\r\n\r\nFor step 1, this is implemented in the mentioned PR.\r\n\r\nStep 2 is a little tricky, I do have a package to handle this (https://github.com/cpuguy83/go-generate/tree/master/deepcopy), but would prefer to use swarmkit's deep copy (generated from protobuf)... this is tricky because container uses so many different types and it may be difficult to force them to use protobufs."},{"labels":[null,null,"enhancement"],"text":"I am trying to understand the resource control related to the CPUs from Swarm Mode.\r\n\r\nFrom the docs I just see:\r\nhttps://docs.docker.com/engine/reference/commandline/service_create/\r\n--limit-cpu value                Limit CPUs (default 0.000)\r\n\r\nhttps://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#create-a-service\r\nNanoCPUs – CPU limit in units of 10-9 CPU shares.\r\n\r\nThe concept is completely different from the ones related to containers and I think there is a lack of documentation regarding the use of these parameters (limit-cpu and reserved-cpu)"},{"labels":["enhancement",null],"text":"**Description**\r\n\r\nThere's one other ambiguity in the documentation/implementation that I've come across. When I run:\r\n\r\n`docker cp --follow-link container_id:source/path dest/path`\r\n\r\nI would expect that when `source/path/` contains files that are symlinks, e.g. `source/path/file.symlink`, and subdirectories that are symlinks, e.g., `souce/path/dir.symlink/more_files`, that all of those would be resolved, and that I would get all the files at dest/path. This is not the case. Instead I end up with broken symlinks.\r\n\r\nMy best guess is that --follow-link refers to links that are in source path itself, e.g., `souce.symlink/path/`, but I haven't tested that. Even so, that doesn't really make so much sense to me as a feature. This is because as @jlhawn pointed out in issue https://github.com/docker/docker/issues/30082, there is already a *nix cp notion as to whether to resolve symlinks at the end of path (based on the trailing /), and it seems that symlinks in the middle of a path should always be resolved.\r\n\r\n**Steps to reproduce the issue:**\r\n1. run `docker --follow-link container_id:source/path dest/path` where source path contains symlinked files/subdir\r\n2. The files at `dest/path` will contain broken symlinks\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.12.6\r\n API version:  1.24\r\n Go version:   go1.6.4\r\n Git commit:   78d1802\r\n Built:        Wed Jan 11 00:23:16 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.12.6\r\n API version:  1.24\r\n Go version:   go1.6.4\r\n Git commit:   78d1802\r\n Built:        Wed Jan 11 00:23:16 2017\r\n OS/Arch:      linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 11\r\n Running: 0\r\n Paused: 0\r\n Stopped: 11\r\nImages: 52\r\nServer Version: 1.12.6\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 98\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: host bridge null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: seccomp\r\nKernel Version: 4.4.41-moby\r\nOperating System: Alpine Linux v3.4\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 1.951 GiB\r\nName: moby\r\nID: XEI4:KV5E:QUMI:STWS:2Y2X:MXS2:X5QC:FUHS:Z27W:ZW3I:4ZKO:O4KR\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 17\r\n Goroutines: 29\r\n System Time: 2017-01-12T00:03:34.814396512Z\r\n EventsListeners: 1\r\nNo Proxy: *.local, 169.254/16\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No kernel memory limit support\r\nInsecure Registries:\r\n 127.0.0.0/8\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nRunning on Mac OS 10.11.6\r\n"},{"labels":["enhancement",null],"text":"**Description**\r\n\r\nI create a container via two ways, one is CLI, the other is API.\r\n\r\n```\r\nroot@ubuntu:~# docker run -d --health-cmd \"curl localhost:3000\" --health-interval=-1s python-demo:v1.1\r\ndocker: --health-interval cannot be negative.\r\nSee 'docker run --help'.\r\n```\r\n**Client side will validate the healthcheck params.**\r\n\r\nWhen I use API to create a container with an **invalid parameter of negative interval**,\r\n```\r\nPOST 192.168.59.103:2375/containers/create\r\n\r\n{\r\n\t\"Image\":\"python-demo:v1.1\",\r\n\t\"Healthcheck\":{\r\n\t\t\"Interval\": -100000000\r\n\t}\r\n}\r\n```\r\n\r\nDocker Daemon works fine for this, and returns the response:\r\n```\r\n{\r\n  \"Id\": \"21d712248a1fe21e72c8198b3a0b2d784a9657725e180268c40db859e178b67b\",\r\n  \"Warnings\": null\r\n}\r\n```\r\n\r\nI think the docker daemon should validate the healthcheck params as well to provide robust API.\r\n\r\n\r\n**Steps to reproduce the issue:**\r\n1.\r\n2.\r\n3.\r\n\r\n**Describe the results you received:**\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nroot@ubuntu:~# docker version\r\nClient:\r\n Version:      1.13.0-rc4\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   88862e7\r\n Built:        Fri Dec 16 22:59:15 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.0-rc4\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   88862e7\r\n Built:        Fri Dec 16 22:59:15 2016\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\nroot@ubuntu:~#\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nroot@ubuntu:~# docker info\r\nContainers: 8\r\n Running: 6\r\n Paused: 0\r\n Stopped: 2\r\nImages: 37\r\nServer Version: 1.13.0-rc4\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 202\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: avzbj8l22g0rfl4ei2b1tm5zq\r\n Is Manager: false\r\n Node Address: 192.168.59.103\r\n Manager Addresses:\r\n  192.168.59.104:2377\r\n  192.168.59.105:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 51371867a01c467f08af739783b8beafc154c4d7\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 3.19.0-25-generic\r\nOperating System: Ubuntu 14.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1.954 GiB\r\nName: ubuntu\r\nID: Q2ZC:GWDN:27OH:GRMH:G6QU:W7QP:4TIX:Q5F6:YEVK:45XP:EXHC:HOB5\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 68\r\n Goroutines: 200\r\n System Time: 2017-01-17T01:11:48.938844162+08:00\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nRegistry Mirrors:\r\n https://a.b.c/\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement",null],"text":"I am working on a galera cluster using percona and when you scale up any new node need to connect to a running node to sync with the cluster\r\n\r\ncurrently what happens is that all tasks start at the same time and the sync is messed up and most containers will shutdown.\r\n\r\nsomething like docker service create --consecutively\r\nstart a new task only when all other tasks report halthy"},{"labels":[null,"enhancement",null],"text":"@dnephin \r\n\r\n**Description**\r\n\r\nit's not possible to set `attachable: true` in the global network key.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n```\r\nnetworks:\r\n  my-network:\r\n    driver: overlay\r\n    attachable: true\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\nattachable Additional property attachable is not allowed\r\n\r\n**Describe the results you expected:**\r\n\r\njust creating an attachable network\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.0-rc7\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   48a9e53\r\n Built:        Fri Jan 13 21:41:57 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.0-rc7\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   48a9e53\r\n Built:        Fri Jan 13 06:37:02 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 1\r\n Paused: 0\r\n Stopped: 0\r\nImages: 10\r\nServer Version: 1.13.0-rc7\r\nStorage Driver: overlay\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local local-persist\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: wx2cm0cgduy2r82wgkw0pgayy\r\n Is Manager: true\r\n ClusterID: mhdentu8t3yzti0z62r5h1hqh\r\n Managers: 3\r\n Nodes: 6\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 10.8.128.73\r\n Manager Addresses:\r\n  10.8.122.201:2377\r\n  10.8.127.73:2377\r\n  10.8.128.73:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nKernel Version: 4.8.14-docker-1\r\nOperating System: Debian GNU/Linux 8 (jessie)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 3.857 GiB\r\nName: d0001.ams.tc.xxxxxx.com\r\nID: I2Z7:LP7H:TNVC:WZ7A:3O6B:OKSD:XET5:G62C:YHRE:JQYZ:OZ3F:YPUU\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: raarts\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\n provider=generic\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"Currently, passing constraints to a swarm service only allows full string matching.  It would be a nice feature to have the ability to match on substrings (either globbing or regex) so that more complex labels (e.g., comma separated values) can be applied to hosts.\r\n\r\nExample:\r\n```\r\n[root@dkh01 ~]# docker node inspect dkh01.example.com | grep -C2 processor_flags\r\n                \"EngineVersion\": \"1.12.3\",\r\n                \"Labels\": {\r\n                    \"processor_flags\": \"aes,apic,avx,clflush,cmov,constant_tsc,cx16,cx8,de,eagerfpu,fpu,fxsr,hypervisor,lahf_lm,lm,mca,mce,mmx,msr,mtrr,nopl,nx,pae,pat,pclmulqdq,pge,pni,popcnt,pse,pse36,rdtscp,rep_good,sep,sse,sse2,sse4_1,sse4_2,ssse3,syscall,tsc,tsc_deadline_timer,vme,x2apic,xsave,xsaveopt\"\r\n                },\r\n                \"Plugins\": [\r\n```\r\n\r\n```\r\n[root@dkh01 ~]# docker service create --network root_net1 --replicas 3 --name cputest --constraint 'engine.labels.processor_flags =~ avx2' cputest\r\nError response from daemon: rpc error: code = 2 desc = constraint expected one operator from ==, !=\r\n```"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\nseems like `--tmpfs` argument isn't validated at all, which leads to cryptic errors (especially with relative paths and stuff under `/dev`).\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker run -it --tmpfs . fedora bash`\r\n2.\r\n3.\r\n\r\n**Describe the results you received:**\r\n\r\nHere are the different cases (the last one actually passed on my test with master code but fails on < master):\r\n```\r\n16:43:54 [github.com/docker/docker] ‹master› docker run -it --tmpfs /dev/null fedora bash\r\n/usr/bin/docker-current: Error response from daemon: oci runtime error: container_linux.go:247: starting container process caused \"process_linux.go:359: container init caused \\\"rootfs_linux.go:94: reopening /dev/null inside container caused \\\\\\\"Failed to open /dev/null - open /dev/null: is a directory\\\\\\\"\\\"\".\r\n\r\n16:43:55 [github.com/docker/docker] ‹master› docker run -it --tmpfs ./ docker.io/fedora bash\r\n/usr/bin/docker-current: Error response from daemon: oci runtime error: container_linux.go:247: starting container process caused \"process_linux.go:359: container init caused \\\"rootfs_linux.go:94: reopening /dev/null inside container caused \\\\\\\"Failed to open /dev/null - open /dev/null: permission denied\\\\\\\"\\\"\".\r\n\r\n16:45:00 [github.com/docker/docker] ‹master› docker run -it --tmpfs . docker.io/fedora bash   \r\n/usr/bin/docker-current: Error response from daemon: oci runtime error: container_linux.go:247: starting container process caused \"process_linux.go:359: container init caused \\\"rootfs_linux.go:94: reopening /dev/null inside container caused \\\\\\\"Failed to open /dev/null - open /dev/null: permission denied\\\\\\\"\\\"\".\r\n\r\n16:45:06 [github.com/docker/docker] ‹master› docker run -it --tmpfs ..///.///././ docker.io/fedora bash\r\n/usr/bin/docker-current: Error response from daemon: oci runtime error: container_linux.go:247: starting container process caused \"process_linux.go:359: container init caused \\\"rootfs_linux.go:90: jailing process inside rootfs caused \\\\\\\"pivot_root invalid argument\\\\\\\"\\\"\".\r\n\r\n16:45:11 [github.com/docker/docker] ‹master› docker run -it --tmpfs ..///.///././/././././././..///../..//././././././././././././././././..////./././././///./././././../..///../.././../..///.///././/././././././..///../..//././././././././././././././././..////./././././///./././././../..///../.././../..///.///././/././././././..///../..//././././././././././././././././..////./././././///./././././../..///../.././../..///.///././/././././././..///../..//././././././././././././././././..////./././././///./././././../..///../.././../..///.///././/././././././..///../..//././././././././././././././././..////./././././///./././././../..///../.././../..///.///././/././././././..///../..//././././././././././././././././..////./././././///./././././../..///../.././../..///.///././/././././././..///../..//././././././././././././././././..////./././././///./././././../..///../.././../..///.///././/././././././..///../..//././././././././././././././././..////./././././///./././././../..///../.././../..///.///././/././././././..///../..//././././././././././././././././..////./././././///./././././../..///../.././../..///.///././/././././././..///../..//././././././././././././././././..////./././././///./././././../..///../.././../..///.///././/././././././..///../..//././././././././././././././././..////./././././///./././././../..///../.././../ docker.io/fedora bash\r\n[root@70f74e6ae816 /]#\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nbetter error due to arg validation maybe\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.14.0-dev\r\n API version:  1.26\r\n Go version:   go1.7.4\r\n Git commit:   1eafa0f\r\n Built:        Sat Jan 14 16:42:29 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.14.0-dev\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.4\r\n Git commit:   1eafa0f\r\n Built:        Sat Jan 14 16:42:29 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 47\r\n Running: 0\r\n Paused: 0\r\n Stopped: 47\r\nImages: 56\r\nServer Version: 1.14.0-dev\r\nStorage Driver: overlay\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\nLogging Driver: journald\r\nCgroup Driver: systemd\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc oci\r\nDefault Runtime: oci\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: N/A (expected: 51371867a01c467f08af739783b8beafc154c4d7)\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\n selinux\r\nKernel Version: 4.8.16-300.fc25.x86_64\r\nOperating System: Fedora 25 (Workstation Edition)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 11.44 GiB\r\nName: runcom.usersys.redhat.com\r\nID: 62I5:E7N5:VHP3:HEWA:7I2L:JLUR:JQV7:SDPV:GYHS:LFYI:QBIH:KRRV\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 15\r\n Goroutines: 24\r\n System Time: 2017-01-14T16:47:43.518175993+01:00\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n brew-pulp-docker01.web.prod.ext.phx2.redhat.com:8888\r\n 172.30.0.0/16\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nIf we running a container with a tty(-t/--tty), the output gets a `\\n` translated to `\\r\\n` on output.This happens for both `docker run` and `docker exec`.\r\n\r\n```\r\n# docker run -t ubuntu:latest echo hello | cat -v\r\nhello^M\r\n# docker run -tid ubuntu:latest \r\n845634fb34f7d91112680303f302905ca260f7101e4f1305155476537e65a02e\r\n# docker exec -t 845634fb34f7d91112680303f302905ca260f7101e4f1305155476537e65a02e echo hello | cat -v\r\nhello^M\r\n```\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. use the latest docker\r\n2. run a container with `-t/-tty`\r\n\r\n```\r\n docker run -t ubuntu:latest echo hello | cat -v\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\n# docker run -t ubuntu:latest echo hello | cat -v\r\nhello^M\r\n```\r\n\r\n**Describe the results you expected:**\r\n```\r\n# docker run -t ubuntu:latest echo hello | cat -v\r\nhello\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n# docker version\r\nClient:\r\n Version:      1.14.0-dev\r\n API version:  1.26\r\n Go version:   go1.7.4\r\n Git commit:   1eafa0f\r\n Built:        Sat Jan 14 03:08:43 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.14.0-dev\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.4\r\n Git commit:   1eafa0f\r\n Built:        Sat Jan 14 03:08:43 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**"},{"labels":[null,"enhancement"],"text":"I'm currently testing the new `--compose-file` parameter for `docker stack deploy` in 1.13.0-rc6, but my use case requires deploying from multiple compose files (as possible with `docker-compose -f base.yml -f additionalService.yml -f production.yml`). At the moment, using globs or multiple `--compose-file` is not supported as far as I can see. Are there plans to support this in future releases? \r\n\r\nA simple workaround/implementation strategy is to merge the compose files before deploying, but I would rather want to avoid using temporary files."},{"labels":[null,"enhancement"],"text":"While reviewing #28499 in today's maintainer meeting we decided to take a step back and talk about whether we should consider this at a higher level.\r\n\r\nIt seems there are two issues at play here:\r\n1 - under what id should a certain Dockerfile command be executed under? root vs USER\r\n2 - is the `USER --chown` issue really a work-around because of the layering issue?\r\nMeaning, are people ok with `COPY`+`RUN chmod...` if an extra layer wasn't created? Which is related to @stevvooe's comment: https://github.com/docker/docker/issues/29853#issuecomment-271469066\r\n\r\nSo, we decided to open up this issue to have a discussion around how to move forward.\r\n\r\nLet's start with some questions:\r\n1 - If we were starting from scratch (and didn't have to worry about backwards compat) would people prefer for `USER` to change the id under which subsequent COPY/ADD/RUN/... commands run?\r\n2 - is it true that while being able to `COPY --user john ...` is perhaps easier to write than `COPY ...` + `RUN chmod ...`, are people ok with two commands if an extra layer were not created?\r\n\r\n\r\nPersonal opinion:\r\n- I think `USER` impacting subsequent commands is more natural, so I would be in favor of `USER --persist john` type of flag to enable this. This won't break backwards compat.\r\n- I liked @stevvooe's suggestion of using an `@` prefix on a Dockerfile command to indicate that we should skip the commit on this command.  I have no idea what kind of impact this would have on caching yet, but if we can make it work I like that better than previous suggestions of BEGIN/END type of commands.  It seems so non-intrusive. And yes I know we have a squash command on build now, but there are time when people still want layers, just not as many as they get today.\r\n\r\nPlease post your comments/opinions here instead of in #28499 and #29853 "},{"labels":[null,"enhancement"],"text":"I commented on this in https://github.com/docker/docker/pull/29405#issuecomment-270971356 and below, so making an issue.\r\n\r\nDocker build needs to accept multiple arguments to create the context, it should be modelled after eg `go build` not `git`, ie\r\n```\r\ndocker build file1 file2 directory ...\r\n```\r\nThis means you can construct a context additively, rather than having to use a .`dockerignore` file to remove parts of the context. This is especially useful if you have two Dockerfiles in the same directory that use different contexts. Currently I emulate this behaviour in every single build I do by using\r\n```\r\ntar cf - file1 file2 directory ... | docker build -\r\n```\r\nBut that is much more limiting, eg you cannot use with compose, you need tar which is not on Windows etc. This change is very simple and easy to understand, and is a big improvement.\r\n\r\nAs a future enhancement, add some more `tar` features, especially dereference symlinks (frequently requested) and `-C` to change directories.\r\n"},{"labels":[null,"enhancement"],"text":"The title speaks for itself, the docker deamon shows a cryptic error message (see below) when one tries to mount a file that is missing or not accessible on the host in an existing location on the container side.\r\nI have created a small repository containing all the instructions to reproduce the error message: [github.com/klingtnet/unintuitive-docker-error-message](https://github.com/klingtnet/unintuitive-docker-error-message)\r\n\r\nInstructions and error:\r\n```sh\r\n$ git clone https://github.com/klingtnet/unintuitive-docker-error-message.git && cd unintuitive-docker-error-message && ./trigger-error.sh \r\ndocker: Error response from daemon: oci runtime error: rootfs_linux.go:53: mounting \"/var/lib/docker/devicemapper/mnt/293288098b315e070975512b52113dc6cbe49d5de455f1cb6f113729520fe2e9/rootfs/usr/bin/foo\" to rootfs \"/var/lib/docker/devicemapper/mnt/293288098b315e070975512b52113dc6cbe49d5de455f1cb6f113729520fe2e9/rootfs\" caused \"not a directory\".\r\n```"},{"labels":[null,"enhancement"],"text":"In single host mode (no swarm and more complicated stuff) I have a host with multiple public IPs. It seems there is no way to configure which of those IPs containers use for outbound communication. Always the primary IP on the host is used. So I would need that different containers are seen on the Internet as using different IPs.\r\n\r\nMy use case is a mail server. I have an extra IP allocated to the server to use for sending e-mails so that forward and backwards DNS entries can match. The other IP address is used for HTTP virtual hosting and has many different DNS entries. Additionally using an extra IP for a dedicated mail server is in general a good practice.\r\n\r\nTried with Docker 1.12.5 on Linux (Ubuntu 16.04.1 LTS) with 4.8.0 kernel."},{"labels":[null,"enhancement"],"text":"## Description:\r\n\r\nwhen using docker ps on a terminal window that is less than screen width, the output does not look pretty\r\n\r\n\r\n## How to reproduce:\r\n\r\n1. open up a terminal (tested on mate-terminal on Ubuntu Mate 16.04 LTS)\r\n2. make the terminal windows width less than the screen (half even)\r\n3. run command: docker ps\r\n\r\n## Expected result\r\n\r\noutput is formatted so each container is on 1 row:\r\n\r\n\tCONTAINER ID        IMAGE                           COMMAND                  CREATED             STATUS              PORTS                       NAMES\r\n\t091ed00a9226        webdevops/php-apache:centos-7   \"/opt/docker/bin/entr\"   12 days ago         Up 20 seconds       80/tcp, 443/tcp, 9000/tcp   ps.c.xxxxxxxxx.www.2016-12-29\r\n\tbee109752240        mysql:5.6.34                    \"docker-entrypoint.sh\"   12 days ago         Up 34 seconds       3306/tcp                    ps.c.xxxxxxxxx.db.2016-12-29\r\n\td2d217622de8        webdevops/php-apache:centos-7   \"/opt/docker/bin/entr\"   8 weeks ago         Up 6 seconds        80/tcp, 443/tcp, 9000/tcp   infra.ais.core.www.2016-11-08\r\n\tbb422c0b9bf3        mysql:5.5.53        \r\n\r\n\r\n## Actual result:\r\n\r\noutput is formatted as if the terminal windows width is the same as the screens:\r\n\r\n\tCONTAINER ID        IMAGE                           COMMAND                  CREATED             STATUS              PORTS                       NAMES\r\n\t0912200a22f6        webdevops/php-apache:centos-7   \"/opt/docker/bin/entr\"   12 days ago         Up 20 seconds       80/tcp, 443/tcp, 9000/tcp   ps.c.xxxxxxxxx.\r\n\twww.2016-12-29\r\n\tbee109752240        mysql:5.6.34                    \"docker-entrypoint.sh\"   12 days ago         Up 34 seconds       3306/tcp                    ps.c.xxxxxxxxx.\r\n\tdb.2016-12-29\r\n\td2d217622de8        webdevops/php-apache:centos-7   \"/opt/docker/bin/entr\"   8 weeks ago         Up 6 seconds        80/tcp, 443/tcp, 9000/tcp   infra.ais.core.\r\n\twww.2016-11-08     \r\n\tbb422c0b9bf3        mysql:5.5.53                    \"docker-entrypoint.sh\"   8 weeks ago         Up 6 seconds        3306/tcp                    infra.ais.core.\r\n\tdb.2016-11-13\r\n\r\n"},{"labels":[null,"enhancement"],"text":"In 1.13 we pre parse the Dockerfile for errors, but the reporting sadly lacks the context you would get from the previous setup, as you can't see where in the Dockerfile you have got to. This came up while debugging yesterday where there were two places in the Dockerfile the error could occur, and the user had I think only checked the first one, but the error did not help.\r\n\r\neg with this Dockerfile\r\n```\r\nFROM alpine\r\nERROR\r\nRUN echo\r\nERROR\r\n```\r\n\r\nI get \r\n```\r\nbuckeye:error justin$ docker build .\r\nSending build context to Docker daemon 2.048 kB\r\nError response from daemon: Unknown instruction: ERROR\r\n```\r\n\r\nObviously here they are more obvious but with errors due to comments and continuations they are less so.\r\n"},{"labels":["enhancement"],"text":"Right now `docker system prune` removes resources which are not currently in use.\r\nThis can be problematic since some of these objects store persistent data which is important even if it's not in use at the moment prune is called.\r\n\r\nWe need some way to mask these items from `docker system prune` (and the object specific commands).\r\nThe simple thing may be to add support for filters... so it could be something like `docker system prune --filter label!=my_important_stuff`\r\n\r\nAnother approach may be to mark these objects in some way... `docker volume pin foo`, for example.\r\nThe problem with this particular verb is it can mean different things for images."},{"labels":[null,"enhancement"],"text":"All containers get this weird `/` prefix which we have to account for all over the code.\r\nThis was used, IIRC, for the graphdb but is not needed today (and arguable that it was ever really \"needed\").\r\n\r\nWhen I removed graphdb I chose to keep it so as to minimize the change and potential pieces that could break at the time.\r\n\r\nIt would be really great to remove this."},{"labels":[null,null,"enhancement",null],"text":"**Description**\r\n\r\nHi, All, \r\n\r\nI found that the error returned from swarmkit and returned to docker daemon, finally returned to client side is not so readable.\r\n\r\nSince we always get rpc error like `rpc error: code = 4 desc = context deadline exceeded`, however we always cannot decide which part returns such kind of error.\r\n\r\nWhen user calls an API, the handler may call swarmkit several times and each time may return an error like `rpc error: code = 4 desc = context deadline exceeded`, for programmers we need to debug \r\nwhere it returns, but obviously currently we cannot.\r\n\r\nping @aaronlehmann @tonistiigi \r\n\r\n**Steps to reproduce the issue:**\r\n1.\r\n2.\r\n3.\r\n\r\n**Describe the results you received:**\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nroot@ubuntu:~# docker version\r\nClient:\r\n Version:      1.13.0-rc4\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   88862e7\r\n Built:        Fri Dec 16 22:59:15 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.14.0-dev\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.4\r\n Git commit:   e75ca4f\r\n Built:        Mon Jan  9 02:24:51 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nroot@ubuntu:~# docker info\r\nContainers: 4\r\n Running: 0\r\n Paused: 0\r\n Stopped: 4\r\nImages: 37\r\nServer Version: 1.14.0-dev\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 194\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: 7ae07lyjg73yt09kdu2mrox49\r\n Error: rpc error: code = 4 desc = context deadline exceeded\r\n Is Manager: true\r\n ClusterID:\r\n Managers: 0\r\n Nodes: 0\r\n Orchestration:\r\n  Task History Retention Limit: 0\r\n Raft:\r\n  Snapshot Interval: 0\r\n  Heartbeat Tick: 0\r\n  Election Tick: 0\r\n Dispatcher:\r\n  Heartbeat Period: Less than a second\r\n CA Configuration:\r\n  Expiry Duration: Less than a second\r\n Node Address: 192.168.59.103\r\n Manager Addresses:\r\n  0.0.0.0:2377\r\n  192.168.59.103:2377\r\n  192.168.59.104:2377\r\n  192.168.59.105:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 51371867a01c467f08af739783b8beafc154c4d7\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.19.0-25-generic\r\nOperating System: Ubuntu 14.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1.954 GiB\r\nName: ubuntu\r\nID: Q2ZC:GWDN:27OH:GRMH:G6QU:W7QP:4TIX:Q5F6:YEVK:45XP:EXHC:HOB5\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 31\r\n Goroutines: 96\r\n System Time: 2017-01-09T10:48:33.835141601+08:00\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nRegistry Mirrors:\r\n https://a.b.c/\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"I am missing the ability to pass labels and the 'attachable' flag to the networks declared in a compose file. That would allow me to let these be managed by docker stack instead of having to create them externally."},{"labels":[null,"enhancement"],"text":"Please add the `--read-only` flag to Swarm services (i.e. `docker service create` and `docker service update`).\r\n\r\nI tried searching, but didn't find any issues that matched this request.  For others searching... when doing a `docker inspect`, the attribute is `HostConfig` / `ReadonlyRootFS`."},{"labels":[null,"enhancement"],"text":"Would be nice :)\r\nnaming can be up to the implementer, i dont care"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker build ssh://git@mygitrepo.com/repo.git\r\n\r\n\r\n**Describe the results you received:**\r\n\r\nunable to prepare context: unable to evaluate symlinks in context path: lstat `pwd`/ssh:: no such file or directory\r\n\r\n**Describe the results you expected:**\r\n\r\nI expected that the build would invoke git checkout \r\nhttps://github.com/docker/docker/blob/b248de7e332b6e67b08a8981f68060e6ae629ccf/pkg/gitutils/gitutils.go#L99\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nCan we just add ssh:// here:\r\nhttps://github.com/docker/docker/blob/b248de7e332b6e67b08a8981f68060e6ae629ccf/pkg/urlutil/urlutil.go#L11\r\n?\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\ndocker --version\r\nDocker version 1.12.5, build 7392c3b\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\ndocker info\r\nContainers: 174\r\n Running: 0\r\n Paused: 0\r\n Stopped: 174\r\nImages: 1940\r\nServer Version: 1.12.5\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 2290\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: host bridge null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: seccomp\r\nKernel Version: 4.4.39-moby\r\nOperating System: Alpine Linux v3.4\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 5.817 GiB\r\nName: moby\r\nID: ROGJ:KNVT:MHXG:Q7FS:2EV3:PPTL:WXQ7:RADW:FY2R:MR3R:INKU:HA46\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 17\r\n Goroutines: 29\r\n System Time: 2017-01-05T19:58:37.009670518Z\r\n EventsListeners: 1\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No kernel memory limit support\r\nInsecure Registries:\r\n[redacted]\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,null,"enhancement"],"text":"**Description**\r\nCurrently, if I run `docker stack deploy --compose-file STACKFILE.yml STACKNAME` multiple times, subsequent `deploy` commands update the stack to match the file, with one exception. If I remove a service, the service remains in the stack. This is a feature request to have the service removed if it isn't listed, or alternately have a `remove` key in the service definition."},{"labels":[null,"enhancement"],"text":"I'm trying to run the example in this documentation: https://godoc.org/github.com/docker/docker/client#hdr-Usage\r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n\t\"context\"\r\n\t\"fmt\"\r\n\r\n\t\"github.com/docker/docker/api/types\"\r\n\t\"github.com/docker/docker/client\"\r\n)\r\n\r\nfunc main() {\r\n\tcli, err := client.NewEnvClient()\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\r\n\tcontainers, err := cli.ContainerList(context.Background(), types.ContainerListOptions{})\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\r\n\tfor _, container := range containers {\r\n\t\tfmt.Printf(\"%s %s\\n\", container.ID[:10], container.Image)\r\n\t}\r\n}\r\n```\r\n\r\nBut for obvious reasons, when running the example against the latest release of Docker, 1.12.5, that doesn't work.\r\nThe error I get is: \r\n```\r\npanic: Error response from daemon: client is newer than server (client API version: 1.26, server API version: 1.24)\r\n```\r\nI understand that semantic versioning has no significance for this project, because the API breaks in point releases, but it would be cool if at least examples would work."},{"labels":[null,"enhancement",null],"text":"\r\n**Description**\r\n\r\nWhen I use an incorrect filter in `docker volume ls`, it reports all volumes.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker volume create `\r\n2. `docker volume ls` \r\n3. `docker volume ls -f driver=a`\r\n\r\n**Describe the results you received:**\r\n```\r\nroot@ubuntu:~# docker volume create\r\n0e613322f854ca38503f9c275608e273cebd29edebde0b3abb0a99565dd3aa27\r\nroot@ubuntu:~# docker volume ls -f driver=a\r\nDRIVER              VOLUME NAME\r\nlocal               0e613322f854ca38503f9c275608e273cebd29edebde0b3abb0a99565dd3aa27\r\n```\r\n\r\n\r\n**Describe the results you expected:**\r\nNone output since there is no volume with a driver of `a`.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nroot@ubuntu:~# docker version\r\nClient:\r\n Version:      1.13.0-rc4\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   88862e7\r\n Built:        Fri Dec 16 22:59:15 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.0-rc4\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   88862e7\r\n Built:        Fri Dec 16 22:59:15 2016\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nroot@ubuntu:~# docker info\r\nContainers: 4\r\n Running: 2\r\n Paused: 0\r\n Stopped: 2\r\nImages: 9\r\nServer Version: 1.13.0-rc4\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 24\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: osqmy8toia5ttwfya7759h1ua\r\n Is Manager: true\r\n ClusterID: r4w8gn22hv62q87rv05phm2xk\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 192.168.59.104\r\n Manager Addresses:\r\n  192.168.59.104:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 51371867a01c467f08af739783b8beafc154c4d7\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 3.19.0-25-generic\r\nOperating System: Ubuntu 14.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1.954 GiB\r\nName: ubuntu\r\nID: FXOY:JCOY:HKDI:VO5U:FYDM:UEXV:YIFN:AISM:NR6U:VMW5:V4MZ:RQWF\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 42\r\n Goroutines: 132\r\n System Time: 2017-01-04T14:40:44.387294614+08:00\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nLabels:\r\n d=a\r\n c=d\r\nExperimental: false\r\nInsecure Registries:\r\n a.b.c\r\n 127.0.0.0/8\r\nRegistry Mirrors:\r\n https://a.b.c/\r\n https://a.b.c/\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"Hello,\r\n\r\nWe are running our application in AWS ECS and we are trying to collect our container logs via TCP port using gelf log driver. But if I used TCP in --log-driver-options --gelf-address tcp://ipaddress:port my container is not starting.\r\n\r\ndocker: Error response from daemon: gelf: endpoint needs to be UDP.\r\n\r\nKindly suggest how to use TCP in gelf-address option.\r\n\r\nMy docker version is 1.12\r\n\r\nRegards,\r\nRaja "},{"labels":["enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1.go tool vet -all\r\n2.\r\n3.\r\n\r\n**Describe the results you received:**\r\nsrc\\github.com\\docker\\docker\\daemon\\logger\\etwlogs\\etwlogs_windows.go:132: syscall.GUID composite literal uses unkeyed fields\r\n\r\n**Describe the results you expected:**\r\ngo vet pass \r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"https://docs.docker.com/engine/reference/commandline/tag/\r\n\r\n````\r\nA tag name may contain lowercase and uppercase characters, digits, underscores, periods and dashes. A tag name may not start with a period or a dash and may contain a maximum of 128 characters.\r\n````\r\n\r\nWhat is a character? You mean ASCII Latin letters? The use of the word \"character\" is WRONG because a character refers to ANY code point! There is also more than one character referred to as dash and underscore in unicode. That needs to be specified properly too. I know you're probably referring to ASCII and not unicode, but you never actually write that."},{"labels":[null,"enhancement"],"text":"With 1.13 RC4\r\n\r\nI like the nicely organized Docker commands in 1.13. However there are still a few commands that are shown solo. \r\n\r\n```\r\nCommands:\r\n  build       Build an image from a Dockerfile\r\n  deploy      Deploy a new stack or update an existing stack\r\n  login       Log in to a Docker registry\r\n  logout      Log out from a Docker registry\r\n  run         Run a command in a new container\r\n  search      Search the Docker Hub for images\r\n  version     Show the Docker version information\r\n```\r\n\r\n`build` can be accommodated in `image` management command, `run` in `container`. Similarly other solo commands may be aligned with other management commands. "},{"labels":[null,null,"enhancement"],"text":"Right now if you create a container while the daemon's logging driver is set to json-file, then you restart the daemon with a different logging driver, it doesn't actually change the logging driver for the containers. There is no way to change the logging driver for containers once they are created. It can be done only for services and it's done by re-creating the container. I suspect that whether the logging driver in use by a container is \"default\" or not is not tracked at all right now, so I think this is an enhancement request, not a bug report."},{"labels":[null,"enhancement",null,null],"text":"**Description**\r\n\r\nContainers are not started in parallel when issuing multiple concurrent \"docker run\" commands for the same image.\r\n\r\nI expect this has to have been discussed in the past, but my searches have turned up empty. Spent the last two days on IRC attempting to get more details to no avail.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Issue \"docker run\" for 10 containers in the background, printing the start time.\r\n2. Wait for \"docker run\" to exit/return.\r\n3. Print the timestamp when \"docker run\" completes.\r\n\r\n**Describe the results you received:**\r\nThe first docker run completes in ~2 seconds, whereas the tenth docker run completes in ~25 seconds.\r\n\r\n**Describe the results you expected:**\r\nGiven enough resources on the host, all of these should start in parallel. All containers should start within ~2 seconds.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nGist with the timings:\r\nhttps://gist.github.com/sakserv/bc55f10fd35801bb374e8b8401bf07bb\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.12.5\r\n API version:  1.24\r\n Go version:   go1.6.4\r\n Git commit:   7392c3b\r\n Built:        Fri Dec 16 02:23:59 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.12.5\r\n API version:  1.24\r\n Go version:   go1.6.4\r\n Git commit:   7392c3b\r\n Built:        Fri Dec 16 02:23:59 2016\r\n OS/Arch:      linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 9\r\n Running: 5\r\n Paused: 0\r\n Stopped: 4\r\nImages: 48\r\nServer Version: 1.12.5\r\nStorage Driver: devicemapper\r\n Pool Name: vg01-docker--pool\r\n Pool Blocksize: 524.3 kB\r\n Base Device Size: 268.4 GB\r\n Backing Filesystem: xfs\r\n Data file:\r\n Metadata file:\r\n Data Space Used: 41.2 GB\r\n Data Space Total: 5.63 TB\r\n Data Space Available: 5.589 TB\r\n Metadata Space Used: 16.17 MB\r\n Metadata Space Total: 16.98 GB\r\n Metadata Space Available: 16.96 GB\r\n Thin Pool Minimum Free Space: 563 GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: false\r\n Deferred Deletion Enabled: false\r\n Deferred Deleted Device Count: 0\r\n Library Version: 1.02.107-RHEL7 (2015-12-01)\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: null host bridge overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: seccomp\r\nKernel Version: 3.10.0-327.13.1.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 32\r\nTotal Memory: 251.6 GiB\r\nName: foo.example.com\r\nID: 7DE4:CNIR:C3A5:HYGP:NUJN:DUJ2:3LYC:5TBZ:37EA:WWTT:UZPT:XLV3\r\nDocker Root Dir: /grid/0/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 38\r\n Goroutines: 40\r\n System Time: 2016-12-29T15:50:33.050576556Z\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: bridge-nf-call-iptables is disabled\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\nInsecure Registries:\r\n 127.0.0.0/8\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nPhysical node - centos 7.2"},{"labels":[null,null,"enhancement"],"text":"**Description**\r\nI found that `docker service update` command document is not so detailed in https://github.com/docker/docker/blob/master/docs/reference/commandline/service_update.md\r\n\r\nI think there are lots of missing options in explanation, such as `--rollback`, and so on.\r\n\r\nIn addition, in the doc it says:\r\n```\r\nThe parameters are the same as [`docker service create`](service_create.md).\r\n``` \r\n\r\nI think that is not true.\r\n\r\nHere is the evidence:\r\n```\r\nroot@ubuntu:~# docker service create --help | grep \"\\-\\-\" | wc -l\r\n45\r\nroot@ubuntu:~# docker service update --help | grep \"\\-\\-\" | wc -l\r\n57\r\n```\r\n So, there is at least 12 parameters that are not the same.\r\n\r\nI think we need a detailed `docker service update` doc for the users. "},{"labels":[null,"enhancement"],"text":"Feature request:\r\n\r\nRight now the shutdown in docker is a bit harsh with no warning outside schedulers to allow allow for things like AWS ECS deregister, https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_DeregisterContainerInstance.html  Can we switch https://github.com/docker/docker/blob/68dab9bd0910e22c96d5dfcbd4e972f8ce770a88/contrib/init/upstart/docker.conf#L4 to something more like:\r\n\r\n```stop on starting rc RUNLEVEL=[016]```\r\n\r\nthis would allow for adding in a pre-stop cleanup script that does not get killed in 20 seconds along the lines of:\r\n```\r\npre-stop script\r\n\tcluster=`jq -r .Data.Cluster /data/ecs/ecs_agent_data.json`\r\n\tregion=`jq -r .Data.ContainerInstanceArn /data/ecs/ecs_agent_data.json | awk -F: '{print $4}'`\r\n\tcontainer_instance_id=`jq -r .Data.ContainerInstanceArn /data/ecs/ecs_agent_data.json | awk -F/ '{print $2}'`\r\n\r\n\t#de-register the instance\r\n\t/usr/local/bin/aws --region $region ecs deregister-container-instance --cluster $cluster --container-instance $container_instance_id --force > /dev/null 2>&1\r\n\r\n\t#stop the ecs-agent if its running\r\n\tif (wget -qO- http://localhost:51678/v1/metadata > /dev/null 2>&1) ; then \r\n\t\tdocker stop `/usr/bin/docker ps | grep \\/amazon-ecs-agent | head -n 1 | /usr/bin/awk '// {print $1}'` > /dev/null 2>&1\r\n\tfi \r\n\r\n\t#remove the ecs_agent data\r\n\trm -f /data/ecs/ecs_agent_data.json\r\n\r\n\t#sleep to allow the deregister to take effect and connection draining to complete\r\n\tsleep 300\r\nend script\r\n```\r\n\r\nThis has a lot less moving parts then something like the life-cycle hooks based approach in https://engineering.circle.com/the-hook-the-message-and-the-function-16e7df31666c#.r5ft4tgpz"},{"labels":[null,"enhancement",null],"text":"**Description**\r\nCreating a service via a compose file in Docker 1.13-rc4 doesn't seem to support the DNS property, even though assigning a custom DNS server through the commandline tool is possible. The documentation for the new Docker Compose version 3 file doesn't indicate that the DNS property is unsupported either.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a swarm with `docker swarm init`\r\n2. Run `docker stack deploy --compose-file test.yml test` with the following compose file:\r\n\r\n```\r\n# test.yml\r\nversion: '3'\r\n\r\nservices:\r\n  hello:\r\n    image: hello-world:latest\r\n    dns: 8.8.8.8\r\n```\r\n\r\n**Describe the results you received:**\r\nRunning the deploy command shows the output below. Notice the \"Ignoring unsupported options: dns\" line.\r\n\r\n```\r\nIgnoring unsupported options: dns\r\n\r\nCreating network test_default\r\nCreating service test_hello\r\n```\r\n\r\n**Describe the results you expected:**\r\nThe service should be started with the custom DNS server, as happens when we create it via the commandline tool with `docker service create --dns 8.8.8.8 --name test hello-world:latest`.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.0-rc4\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   88862e7\r\n Built:        Sat Dec 17 01:34:17 2016\r\n OS/Arch:      windows/amd64\r\n\r\nServer:\r\n Version:      1.13.0-rc4\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   88862e7\r\n Built:        Sat Dec 17 01:34:17 2016\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 0\r\nServer Version: 1.13.0-rc4\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: j7e2dqmqhrahu9b4fnxp22esq\r\n Is Manager: true\r\n ClusterID: kbp1q4rd44axp9hyqk346u0xj\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 192.168.65.2\r\n Manager Addresses:\r\n  192.168.65.2:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 51371867a01c467f08af739783b8beafc154c4d7\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.8.15-moby\r\nOperating System: Alpine Linux v3.4\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 1.934 GiB\r\nName: moby\r\nID: AMFI:3SYV:IXKB:BNOT:TOQF:ITPN:VHK6:RMMR:PVIJ:QEQX:3KXW:TXX6\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 30\r\n Goroutines: 135\r\n System Time: 2016-12-24T16:07:23.4117973Z\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nTested on both Docker for Windows and AWS EC2 with equal results. The `docker info` and `docker version` outputs were created in the Docker for Windows environment."},{"labels":[null,"enhancement"],"text":"We have several logging options, but we can't change the log file location.\r\n\r\nUsing the **json-file** driver we can find the file at /var/lib/docker/<container_id>/<container_id>-json.log (default), but it would be useful to set the log file name and path as we can set the max-size and max-file on json-file.\r\nhttps://docs.docker.com/engine/admin/logging/overview/#/json-file-options\r\n\r\ne.g.: *docker run -d --log-driver=json-file --log-opt max-file=5 **--log-opt file-path=/var/log/my-container.log** my-image*"},{"labels":[null,null,"enhancement"],"text":"I am using Docker stack (1.13 rc4). When I am using the flowing command:\r\nCmd: `docker volume create --opt type=none --opt device=/my-path-on-host --opt o=bind –name try3`\r\neverything is working fine as you can see:\r\ncmd: “docker volume inspect try3”:\r\n\r\n```json\r\n[\r\n    {\r\n        \"Driver\": \"local\",\r\n        \"Labels\": {},\r\n        \"Mountpoint\": \"/var/lib/docker/volumes/try3/_data\",\r\n        \"Name\": \"try2\",\r\n        \"Options\": {\r\n            \"device\": \"/my-path-on-host \",\r\n            \"o\": \"bind\",\r\n            \"type\": \"none\"\r\n        },\r\n        \"Scope\": \"local\"\r\n    }\r\n]\r\n```\r\n\r\nBut when I am using the command in yaml compose file format for docker stack like this: \r\n\r\n```yaml\r\nversion: '3'\r\nvolumes:\r\n  try3:\r\n    driver_opts:\r\n      device: =/my-path-on-host\r\n      o: bind\r\n      type: none\r\n    external: true\r\n```\r\n\r\nThe volume try3 is created but all under driver_opts is ignored!\r\nCmd: “docker volume inspect try3”:\r\n\r\n```json\r\n[\r\n    {\r\n        \"Driver\": \"local\",\r\n        \"Labels\": {},\r\n        \"Mountpoint\": \"/var/lib/docker/volumes/try3/_data\",\r\n        \"Name\": \"try2\",\r\n        \"Options\": { },\r\n        \"Scope\": \"local\"\r\n    }\r\n]\r\n```\r\nWhat am I doing wrong? Or it’s a docker bug? If so when do you going to fix it?\r\nThanks!"},{"labels":[null,null,"enhancement"],"text":"Description\r\nVersion: docker-1.11.2\r\n\r\nSteps to reproduce the issue:\r\n1. Commit a stop container\r\n2. At the same time remove this container\r\n\r\nThe expected result should be:\r\nWhen container is in commit operation, the delete should not be allowed.\r\n \r\n\r\n\r\n\r\n"},{"labels":[null,null,"enhancement"],"text":"\r\n**Description**\r\n\r\nCould we please add/modify the systemd unit file to include a default environment file so you have the option to persist startup/daemon options on upgrades. Something like:\r\n\r\n```\r\nEnvironmentFile=-/etc/default/docker\r\nExecStart=/usr/bin/dockerd -H fd:// $DOCKER_OPTS\r\n\r\n```"},{"labels":[null,"enhancement"],"text":"Hi, All, \r\n\r\nI found that currently dockerd does not support reload config registry mirror, while today I ran into a case that we have chance to add this, but there are already some workload on the docker daemon.\r\n\r\nIf we hope to add registry mirror, we have to edit the config file and restart dockerd which is not OK for the workload. \r\n\r\nSo, I hope to have registry mirror config reload support in docker daemon.\r\n\r\nP.S.\r\nHere are the reload options support in docker daemon: https://github.com/docker/docker/blob/master/daemon/daemon.go#L1009-L1019\r\n```\r\n// Reload reads configuration changes and modifies the\r\n// daemon according to those changes.\r\n// These are the settings that Reload changes:\r\n// - Daemon labels.\r\n// - Daemon debug log level.\r\n// - Daemon insecure registries.\r\n// - Daemon max concurrent downloads\r\n// - Daemon max concurrent uploads\r\n// - Cluster discovery (reconfigure and restart).\r\n// - Daemon live restore\r\n// - Daemon shutdown timeout (in seconds).\r\n```"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n`docker service create` does not fetch image tags\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n1. Create a swarm cluster. Only a single node is enough.\r\n2. Do `docker service create nginx:latest` and make sure the image is pulled and the service is created.\r\n3. Do `docker images | grep nginx`\r\n\r\n**Describe the results you received:**\r\n\r\nThe image itself is pulled but the tag is not fetched.\r\n\r\n```\r\nnginx                  <none> ...\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nThe tag should be fetched as well (as in `docker run`).\r\n\r\n```\r\nnginx                  latest ...\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.14.0-dev\r\n API version:  1.26\r\n Go version:   go1.7.4\r\n Git commit:   2cf32ee\r\n Built:        Tue Dec 20 06:31:26 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.14.0-dev\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.4\r\n Git commit:   2cf32ee\r\n Built:        Tue Dec 20 06:31:26 2016\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 1\r\n Paused: 0\r\n Stopped: 0\r\nImages: 152\r\nServer Version: 1.14.0-dev\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\n NodeID: dwbro2r4r28kjov8vkdpc0rw6\r\n Is Manager: true\r\n ClusterID: ob7adda1s0jgp13u8jgxq2mfv\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 10.146.0.2\r\n Manager Addresses:\r\n  10.146.0.2:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 51371867a01c467f08af739783b8beafc154c4d7\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-53-generic\r\nOperating System: Ubuntu 16.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 14.69 GiB\r\nName: ws01\r\nID: SN3T:CNK6:JKQD:54CY:XKF2:BRX3:CIRU:DQBT:6DVZ:VWQ2:Q5ET:F23L\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 36\r\n Goroutines: 130\r\n System Time: 2016-12-20T08:38:32.848334921Z\r\n EventsListeners: 1\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement",null],"text":"Hi, All, \r\n\r\nI ran into an issue, and here is the case:\r\n\r\nI have a laptop with 3 VMs created by virtualbox.\r\n1. when I was in my work office, my VMs are in the network of 10.1.4.0/24,ip is  10.1.4.103, 10.1.4.104, 10.1.4.105. I make them be a swarm cluster via command.\r\n2. When I moved to my house, three VMs will change their IPs, this time IPs are 192.168.59.103, 192.168.59.104, 192.168.59.105.\r\n\r\nBut I found that when I start docker daemon, it fails with the dockerd log \r\n```\r\ntime=\"2016-12-20T02:05:50.419437798+08:00\" level=fatal msg=\"Error creating cluster component: could not find local IP address: dial udp 10.1.4.105:2377: connect: network is unreachable\"\r\n```\r\nWe can find that this a fatal log here https://github.com/docker/docker/blob/master/cmd/dockerd/daemon.go#L289,\r\nit makes dockerd exit immediately.\r\n\r\n**Actually I only change the work place and IP changes, then I have no ability to use docker daemon. I think this is UNACCEPTABLE for users.**\r\n\r\nI suggest that when swarm cluster init fails, it only affects the swarm part, not related to original API parts. \r\n\r\n**Steps to reproduce the issue:**\r\n1.\r\n2.\r\n3.\r\n\r\n**Describe the results you received:**\r\nDockerd fails to start with log \r\n```\r\ntime=\"2016-12-20T02:05:50.419437798+08:00\" level=fatal msg=\"Error creating cluster component: could not find local IP address: dial udp 10.1.4.105:2377: connect: network is unreachable\"\r\n```\r\n\r\n**Describe the results you expected:**\r\nDockerd can start with an error log without fatal log.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.12.5\r\n API version:  1.24\r\n Go version:   go1.6.4\r\n Git commit:   7392c3b\r\n Built:        Fri Dec 16 02:30:42 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.12.5\r\n API version:  1.24\r\n Go version:   go1.6.4\r\n Git commit:   7392c3b\r\n Built:        Fri Dec 16 02:30:42 2016\r\n OS/Arch:      linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nroot@ubuntu:~# docker info\r\nContainers: 4\r\n Running: 0\r\n Paused: 0\r\n Stopped: 4\r\nImages: 37\r\nServer Version: 1.12.5\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 194\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: null host bridge overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: apparmor\r\nKernel Version: 3.19.0-25-generic\r\nOperating System: Ubuntu 14.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1.954 GiB\r\nName: ubuntu\r\nID: Q2ZC:GWDN:27OH:GRMH:G6QU:W7QP:4TIX:Q5F6:YEVK:45XP:EXHC:HOB5\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 14\r\n Goroutines: 23\r\n System Time: 2016-12-20T02:31:47.207169643+08:00\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nInsecure Registries:\r\n 127.0.0.0/8\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"Currently, there is no checksum on the release page's download section:\r\n\r\n> # Download\r\n> deb/rpm install: curl -fsSL https://test.docker.com/ | sh\r\n> Linux 64bits tgz: https://test.docker.com/builds/Linux/x86_64/docker-1.13.0-rc4.tgz\r\n> Darwin/OSX 64bits client tgz: https://test.docker.com/builds/Darwin/x86_64/docker-1.13.0-rc4.tgz\r\n> Linux 32bits arm tgz: https://test.docker.com/builds/Linux/armel/docker-1.13.0-rc4.tgz\r\n> Windows 64bits zip: https://test.docker.com/builds/Windows/x86_64/docker-1.13.0-rc4.zip\r\n> Windows 32bits client zip: https://test.docker.com/builds/Windows/i386/docker-1.13.0-rc4.zip\r\n\r\nIt would be great if there is a checksum file for those binaries, so we can verify the downloaded file's integrity. Thanks."},{"labels":[null,null,"enhancement"],"text":"When Docker build is used in batch mode (like Jenkins) the output of \"Sending context to docker daemon...\" repeats many times, fill the console output with useless output. currently we have to use -q option but it is not the proper solutions because We want the build output.\r\nmaven has the same problem. they provide an option (-B) for this situation. I think this would be nice if we have a similar option in docker"},{"labels":[null,"enhancement"],"text":"### Proposal\r\n\r\n`docker swarm join-token` should give out temporary expiring use tokens in addition to the \"root\" tokens provided today.\r\n\r\nThese would be valid to join a swarm only for a specified duration, and would become de-activated once an expected number of nodes had joined with that token, or the TTL was reached.\r\n\r\nExample:\r\n\r\n```console\r\n$ docker swarm join-token manager -q \\\r\n    --ttl \"30minutes\" \\\r\n    --expect 2\r\nSWMTMPTKN-1-1dd5wxkrb8x04yeiohem5j5m3y32ew4z9mivyjiz1h8sdrctk7-ff87lcz63x6cl3187m3r0tt2b\r\n```\r\n\r\nThe token about will be valid to `join` manager nodes for 30 minutes, and will stop allowing nodes to `join` with it after 2 more nodes have joined.\r\n\r\n### Motivation\r\n\r\nIn many situations it is likely that you would want to `join` by passing the tokens around via channels where their transport is reasonably secure but which may, accidentally or by design, leave the information for the symmetric token hanging around somewhere _not_ presumed to be secure forever (e.g., pass in via custom data, drop into an S3 bucket, pass around using subnet-internal HTTP, etc).\r\n\r\nHaving temporary tokens available allows for a better security story and could help automation of creating swarms by giving implementers leeway to not worry that the passed tokens _must_ remain secure forever and/or the existing root tokens _must_ be rotated after all anticipated nodes have joined.\r\n\r\nIn theory, this feature could potentially allow two nice other features:\r\n\r\n1. This could be used to construct an audit log for \"join events\", so that if you _only_ allowed joining via temporary use tokens, a history of \"who has joined when\" is theoretically possible.\r\n2. Ability to \"error out\" failed scale events.  e.g., if you have a temporary token for an event intended to add X nodes to the swarm, and only X-1 nodes have reported in by the time of the TTL, a system or administrator managing Swarm could know that something went wrong (network partition, etc.) and debug / retry it, rather then ending up in an awkward half-joined situation where the error might not even be detected.\r\n\r\n__tl;dr:__ It would help automation and security-concious administrators if Swarm could issue time-limited join tokens.\r\n\r\ncc @NathanMcCauley @diogomonica @stevvooe @aaronlehmann  @kencochrane @ddebroy @friism  FYI"},{"labels":[null,null,null,"enhancement"],"text":"**Description**\r\n\r\nThere is very little information from API documentation that can explain how to use REST request to promote or demote swarm node from remote side. It seems I should post \"/nodes/id/update\" with parameters attached. However, some parameters like 'version' are vague. Can you provide an example to show promote/demote?\r\n\r\nSomething like:\r\n`curl --unix-socket /var/run/docker.sock -X POST -H 'Content-Type: application/json' -d '{\"version\": .., ..}' http:/nodes/id/update`\r\n\r\nThanks"},{"labels":[null,null,null,"enhancement"],"text":"**Description**\r\n\r\nSpecifying an invalid shebang will show a very terse error message.\r\n\r\nYes, this is a known issue (see #10668 and #10738 and perhaps #27780), but I would like to be that guy:\r\n![image](https://cloud.githubusercontent.com/assets/171481/21197147/fbb76c86-c1ff-11e6-9c1d-1e795abda88a.png)\r\n... i.e. I'd like to fight for the users! 🤘 \r\n\r\n(Also, `bash` is handling this issue more gracefully than we are, and there's no way we gotta get no shame from a shell whose source code is not even on GitHub... no hard feelings, bash, you're still my favorite 🐚 , but I can't even easily link to a snippet of that wonderful code of yours!)\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n```\r\nmkdir shebang\r\necho '#!/bin/nonexistent' >shebang/shebang\r\nchmod +x shebang/shebang\r\ncat >shebang/Dockerfile <<EOF\r\nFROM alpine\r\nCOPY shebang /\r\nENTRYPOINT [\"/shebang\"]\r\nEOF\r\ndocker build -t shebang shebang\r\ndocker run --rm shebang\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\nstandard_init_linux.go:175: exec user process caused \"no such file or directory\"\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nIf I execute this entrypoint directly from the shell, I get instead:\r\n```\r\n$ shebang/shebang\r\n-bash: shebang/shebang: /bin/nonexistent: bad interpreter: No such file or directory\r\n```\r\n\r\n... which is more helpful to track down the issue.\r\n\r\nOf course, this is just `bash` being fancy and all. If you try to `os.system(\"shebang/shebang\")` from a Python script, or even `ash -c shebang/shebang`, you will go back to a good, old, unhelpful, UNIX-unfriendly error message.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/171481/21197310/a39faf30-c200-11e6-9e5d-931439b7ff84.png)\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nI would like to thank [@anotherhowie](https://twitter.com/AnotherHowie/status/809097681729650689) for pointing out that we could do better.\r\n\r\nOh, and this issue is particularly prone to happen if you are using Windows, where line-breaks are different from Linux. You can _very easily_ end up with an invalid shebang, because it will be `#!/usr/bin/env python^M` and Linux will be like \"y'all can take your caret em' and shove it where it ain't no sunshine, I'm not executing that stuff!\" (Linux can be cranky at times.)\r\n\r\n**Output of `docker version`:**\r\n\r\nTested with 1.13.0-rc3 and 1.12.1.\r\n\r\n**Output of `docker info`:**\r\n\r\nWhoa there, whoa there. We don't want to get into _that_ conversation, do we? I already added two silly images, you don't want me to paste pages and pages of irrelevant information, do you?\r\n\r\n```\r\nNOPE NOPE NOPE\r\n```\r\n\r\n... I thought so!\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nI don't know if we should try to be smart (like bash and zsh do) on the Engine side, or the client side. Client side would probably be good enough, but iono!"},{"labels":[null,"enhancement"],"text":"A manager that's stuck outside of quorum doesn't fail in a good way. For a test, I started a 3-manager swarm and then terminated two managers. The 3rd managers now believes itself to be outside the quorum.\r\n\r\n## What I expected\r\n\r\nRunning commands that hit the Swarm API should result in a prompt error saying something like \"This node is outside the Swarm quorum\", maybe with a recommendation to use one of the managers that are supposed to still be in quorum (those could perhaps be listed) and alternatively suggesting rebuilding quorum from this manager (if that's possible).\r\n\r\n## What I got\r\n\r\nSwarm-related commands (eg. `docker node ls`) hang or time out.\r\n\r\n## Additional info\r\n\r\n```\r\n~ $ docker info\r\nContainers: 5\r\n Running: 4\r\n Paused: 0\r\n Stopped: 1\r\nImages: 5\r\nServer Version: 1.13.0-rc3\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: awslogs\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: h3tx9ctw18xgbj1u1ogx6ba5l\r\n Error: rpc error: code = 4 desc = context deadline exceeded\r\n Is Manager: true\r\n ClusterID:\r\n Managers: 0\r\n Nodes: 0\r\n Orchestration:\r\n  Task History Retention Limit: 0\r\n Raft:\r\n  Snapshot Interval: 0\r\n  Heartbeat Tick: 0\r\n  Election Tick: 0\r\n Dispatcher:\r\n  Heartbeat Period: Less than a second\r\n CA Configuration:\r\n  Expiry Duration: Less than a second\r\n Node Address: 172.31.7.108\r\n Manager Addresses:\r\n  172.31.20.235:2377\r\n  172.31.20.236:2377\r\n  172.31.7.108:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 51371867a01c467f08af739783b8beafc154c4d7\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.8.12-moby\r\nOperating System: Alpine Linux v3.4\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 3.67 GiB\r\nName: ip-172-31-7-108.us-west-2.compute.internal\r\nID: O7M6:AEHD:XE3J:BF3H:M4SI:CWYD:FEVN:5RCH:MUKI:HBYD:ZLVS:BU5L\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 61\r\n Goroutines: 115\r\n System Time: 2016-12-13T20:07:24.406563198Z\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nThis is a request to add quota support for ext4 for the overlay and overlay2 storage drivers, similar to what has been done in https://github.com/docker/docker/pull/24771 and https://github.com/docker/docker/pull/24807.\r\n\r\nhttps://github.com/docker/docker/pull/24771#issuecomment-250274944 mentions that ext4 quota support was added in kernel 4.5. The next LTS Linux kernel (4.9) was just released this past Sunday, which will have this support, so maybe it's worth looking into this again.\r\n"},{"labels":["enhancement",null,null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nIt would be nice if Docker supported Linux style paths for `--volume` even when operating on Windows. Keeps things uniform and enables the same automation/scripting be used regardless of platform.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n1. `docker run --volume /tmp/:/tmp/ microsoft/windowsservercore`\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\nc:\\Program Files\\docker\\docker.exe: Error response from daemon: invalid bind mount spec \"/tmp/:/tmp/\": invalid volume specification: '\\tmp\\:\\tmp\\'.\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nSuccessful mounting of `C:\\tmp\\` to `C:\\tmp\\`.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nMany other Windows tools (e.g. PowerShell and .NET) work fine with Linux style paths and treat a Linux root as the root of the \"current\" drive. Obviously the drive ambiguity is not solvable in this case but as far as all I care about is a single drive, life is well improved with this enhancement.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.14.0-dev\r\n API version:  1.26\r\n Go version:   go1.7.3\r\n Git commit:   e308f6b\r\n Built:        Tue Nov 22 18:15:55 2016\r\n OS/Arch:      windows/amd64\r\n\r\nServer:\r\n Version:             1.14.0-dev\r\n API version:         1.26\r\n Minimum API version: 1.24\r\n Go version:          go1.7.3\r\n Git commit:          e308f6b\r\n Built:               Tue Nov 22 18:15:55 2016\r\n OS/Arch:             windows/amd64\r\n Experimental:        false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 3\r\n Running: 0\r\n Paused: 0\r\n Stopped: 3\r\nImages: 35\r\nServer Version: 1.14.0-dev\r\nStorage Driver: windowsfilter\r\n Windows:\r\nLogging Driver: json-file\r\nPlugins:\r\n Volume: local\r\n Network: l2bridge l2tunnel nat null overlay transparent\r\nSwarm: inactive\r\nDefault Isolation: hyperv\r\nKernel Version: 10.0 14393 (14393.479.amd64fre.rs1_release.161110-2025)\r\nOperating System: Windows 10 Enterprise\r\nOSType: windows\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 15.92 GiB\r\nName: jose\r\nID: 7YUR:ZJ6B:47ZU:ZJ2G:SNCO:FDE3:QMAX:BXBK:F3H2:3RYF:NRM2:VRVD\r\nDocker Root Dir: C:\\ProgramData\\Docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: saaresaxinom\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nPhysical pc"},{"labels":[null,"enhancement"],"text":"Just making an enhancement request for support for auto-creation of a new Log Group (using a CreateLogGroup call in AWS) if the specified `awslogs-group` does not exist.\r\n\r\nCurrently this fails as follows:\r\n\r\n```\r\nResourceNotFoundException: The specified log group does not exist.\r\n```\r\n"},{"labels":[null,"enhancement"],"text":"In v1.13.0-rc3 it's not possible to deploy a stack using a compose-file that contains services extending other services using  _extends_.\r\n\r\n**Example** \r\ncompose.yml:\r\n```\r\nversion: '3.0'\r\n    services:\r\n      service2:\r\n        extends:\r\n           file: ../common-services.yml\r\n           service: service1\r\n        deploy:\r\n            replicas: 2\r\n```\r\nDocker  responses with the following error message when running \r\n\r\n> docker stack deploy --compose-file compose.yml my_stack\r\n```\r\nCompose file contains unsupported options:\r\n\r\nextends: Support for `extends` is not implemented yet. Use `docker-compose config` to generate a configuration with all `extends` options resolved, and deploy from that.\r\n```\r\nThe workaround with 'docker-compose config' is not really an option, as version 3.0 is not supported by docker-compose and _deploy_ is not available in version 2.1 of docker-compose.\r\n\r\nIt will be great if the support for extend could be added soon.\r\n\r\n\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nsorry for the poor title; but basically, if you do this:\r\n\r\nDockerfile (we call this image 'broken' below):\r\n\r\n```\r\nFROM debian\r\nVOLUME /var/tmp\r\n```\r\n\r\n```\r\ndocker create --name cont broken\r\ndocker cp foo cont:/var/tmp\r\ndocker commit cont broken\r\ndocker run -it broken ls /var/tmp\r\n```\r\n\r\nNo output because the file is not there; it was saved in the volume created by the image directions.\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. See above\r\n2. Test example\r\n3. Watch in wonder\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nI'm happy to write a patch for this, but it seems like it could go a few ways:\r\n\r\n* patch to `docker build` to implement the old `NOVOLUME` or `VOLUME []` suggestions to clear volumes from the list, and an equivalent API to do so (I would really prefer this method)\r\n* Changes to `docker cp` or `docker commit` to manage the volume reference in a persistent way\r\n* Changes to `docker create` to explicitly ignore mounting volumes (this is the most elegant, but less practical than the build solution IMO)\r\n\r\n**Output of `docker version`:**\r\n\r\n1.12.3\r\n"},{"labels":[null,null,"enhancement"],"text":"The help text for `--replicas` on `service update` and `service create` says the default is \"none\".\r\n\r\n```\r\n      --replicas uint                    Number of tasks (default none)\r\n```\r\n\r\nIn fact, creating a replicated service without `--replicas` uses 1 replica, not zero as this description might suggest."},{"labels":[null,null,"enhancement"],"text":"The current plugin used for testing (https://github.com/docker/docker/blob/23ea9e45fd6281e82c389e6112457a7bb70f8d89/integration-cli/docker_cli_network_unix_test.go#L773-L795) [tiborvass/test-docker-netplugin](https://hub.docker.com/r/tiborvass/test-docker-netplugin/) does not offer any functionality.\r\n\r\nWe should have a better example plugin\r\n\r\n/cc @mavenugo @anusha-ragunathan "},{"labels":[null,"enhancement",null,null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nI get the following error after upgrading to 1.13.0rc3 on Docker for Mac beta channel today.\r\n\r\n```\r\n$ docker inspect something_that_does_not_exist\r\n[]\r\nError response from daemon: Swarm certificates have expired. To replace them, leave the swarm and join again.\r\n```\r\n\r\nThe error only appears for containers that do not exist (as listed by `docker ps -a`):\r\n\r\n```\r\n$ docker inspect something_that_exists\r\n[…] normal output\r\n```\r\n\r\nBelow is some diagnostic information.\r\n\r\n```\r\n$ docker version\r\nClient:\r\n Version:      1.13.0-rc3\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   4d92237\r\n Built:        Tue Dec  6 01:15:44 2016\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.0-rc3\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   4d92237\r\n Built:        Tue Dec  6 01:15:44 2016\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n\r\n$ docker info\r\nContainers: 71\r\n Running: 3\r\n Paused: 0\r\n Stopped: 68\r\nImages: 95\r\nServer Version: 1.13.0-rc3\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 250\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: error\r\n NodeID:\r\n Error: Swarm certificates have expired. To replace them, leave the swarm and join again.\r\n Is Manager: false\r\n Node Address: 192.168.65.2\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 51371867a01c467f08af739783b8beafc154c4d7\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.8.12-moby\r\nOperating System: Alpine Linux v3.4\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 1.952 GiB\r\nName: moby\r\nID: IEMY:6NDR:XEEJ:5HXA:AZCF:NJMX:KBDX:ATTA:K4RN:O3PF:P5LN:JFR7\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 55\r\n Goroutines: 77\r\n System Time: 2016-12-08T08:27:54.125249557Z\r\n EventsListeners: 1\r\nNo Proxy: *.local, 169.254/16\r\nUsername: shykes\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\nThank you!"},{"labels":[null,"enhancement"],"text":"I've discovered fact that if you try to create a bunch of networks with default parameters you run out of available networks REALLY fast.\r\n\r\n**Steps to reproduce:**\r\n1) Spin up fresh environment with Docker (VirtualBox, Ubuntu 16.04, Docker 1.12.3 in my case).\r\n2) Try to create 50 networks.\r\n```\r\nfor i in {1..50}; do docker network create net-$i; done\r\n```\r\n3) Notice error messages after some amount of created networks\r\n4) Take a look at result.\r\n```\r\nfor i in $(docker network ls -q); do docker network inspect --format='{{.Name}} {{.IPAM.Config}}' $i; done\r\n```\r\n\r\n```\r\nnet-1 [{172.18.0.0/16  172.18.0.1/16 map[]}]\r\nnet-2 [{172.19.0.0/16  172.19.0.1/16 map[]}]\r\nnet-3 [{172.20.0.0/16  172.20.0.1/16 map[]}]\r\nnet-4 [{172.21.0.0/16  172.21.0.1/16 map[]}]\r\nnet-5 [{172.22.0.0/16  172.22.0.1/16 map[]}]\r\nnet-6 [{172.23.0.0/16  172.23.0.1/16 map[]}]\r\nnet-7 [{172.24.0.0/16  172.24.0.1/16 map[]}]\r\nnet-8 [{172.25.0.0/16  172.25.0.1/16 map[]}]\r\nnet-9 [{172.26.0.0/16  172.26.0.1/16 map[]}]\r\nnet-10 [{172.27.0.0/16  172.27.0.1/16 map[]}]\r\nnet-11 [{172.28.0.0/16  172.28.0.1/16 map[]}]\r\nnet-12 [{172.29.0.0/16  172.29.0.1/16 map[]}]\r\nnet-13 [{172.30.0.0/16  172.30.0.1/16 map[]}]\r\nnet-14 [{172.31.0.0/16  172.31.0.1/16 map[]}]\r\nnet-15 [{192.168.0.0/20  192.168.0.1/20 map[]}]\r\nnet-16 [{192.168.16.0/20  192.168.16.1/20 map[]}]\r\nnet-17 [{192.168.32.0/20  192.168.32.1/20 map[]}]\r\nnet-18 [{192.168.48.0/20  192.168.48.1/20 map[]}]\r\nnet-19 [{192.168.64.0/20  192.168.64.1/20 map[]}]\r\nnet-20 [{192.168.80.0/20  192.168.80.1/20 map[]}]\r\nnet-21 [{192.168.96.0/20  192.168.96.1/20 map[]}]\r\nnet-22 [{192.168.112.0/20  192.168.112.1/20 map[]}]\r\nnet-23 [{192.168.128.0/20  192.168.128.1/20 map[]}]\r\nnet-24 [{192.168.144.0/20  192.168.144.1/20 map[]}]\r\nnet-25 [{192.168.160.0/20  192.168.160.1/20 map[]}]\r\nnet-26 [{192.168.176.0/20  192.168.176.1/20 map[]}]\r\nnet-27 [{192.168.192.0/20  192.168.192.1/20 map[]}]\r\nnet-28 [{192.168.208.0/20  192.168.208.1/20 map[]}]\r\nnet-29 [{192.168.224.0/20  192.168.224.1/20 map[]}]\r\nnet-30 [{192.168.240.0/20  192.168.240.1/20 map[]}]\r\n```\r\n\r\nIt was able to create 30 networks total. Seems like default subnet allocation rules are like this:\r\n1) Docker uses RFC1918 networks 172.16/12 and 192.168/16 \r\n2) Network masks are picked in a way that would divide both networks into 16 subnets.\r\n\r\n**Why it's important?**\r\n\r\nThis issue becomes prominent when you deploy apps using Compose in your CI pipeline.\r\nCompose creates separate network for each project by default, which seems like a reasonable behavior.\r\nHowever you are limited to 30 projects that can be deployed with default subnet allocation policy.\r\nI want to avoid hardcoding networks in `docker-compose.yml` file at all costs since it makes them unportable and human error-prone.\r\n\r\n**What is my proposal?**\r\n\r\nDocker daemon already has option `--fixed-cidr` that allows to change address pool for IP addresses assigned to containers by default.\r\nAdding options like `--default-subnet-pool` and `--default-subnet-mask` (names are subject to change) will allow user to control default subnet allocation policy and with values like `--default-subnet-pool=10.0.0.0/8 --default-subnet-mask=/24` will virtually eliminate issue of subnet depletion without affecting Compose files in CI pipelines.\r\n"},{"labels":[null,"enhancement"],"text":"Currently we can load a docker image from a tar file that was previously generated via `docker save`. It will be great if  we could load that same image from a folder with the contents of tar file. So,\r\n\r\n`docker load -i abc.tar`\r\nor\r\n```\r\ntar xzvf abc.tar\r\ndocker load -d {folder_name with tar contents unzipped} \r\n```\r\n\r\nWhy?\r\nWe are shipping some private docker images using `docker save` and `docker load`. To reduce the size of the final archive, we have written a [script](https://github.com/appscode/baler) that dedups the layers inside the tar file. The issue is for loading, we have to rebuild the original tars. This takes time. If docker could load images from unzipped folder with exact same tar contents then it could save time during the reload phase.\r\n\r\nThanks."},{"labels":[null,null,null,"enhancement",null],"text":"Currently, `docker service logs` expects a service name or ID and will stream the combined output of every task that is part of the service.\r\n\r\nIt would be neat to support `docker service logs <task ID>`.\r\n\r\nThis is already implemented in the SwarmKit backend, so it's really a matter of plumbing it into the API/CLI.\r\n\r\nRelevant bits: In `daemon/cluster/cluster.go` there is:\r\n```go\r\n\tstream, err := state.logsClient.SubscribeLogs(ctx, &swarmapi.SubscribeLogsRequest{\r\n\t\tSelector: &swarmapi.LogSelector{\r\n\t\t\tServiceIDs: []string{service.ID},\r\n\t\t},\r\n\t\tOptions: &swarmapi.LogSubscriptionOptions{\r\n\t\t\tFollow: config.Follow,\r\n\t\t},\r\n\t})\r\n```\r\n\r\nIn order to stream logs for a task using SwarmKit, one would need to use the exact same code, using a different selector: `TaskIDs: []string{task.ID}`.\r\n\r\nI think the workflow would look like:\r\n1) The user calls `docker service logs <something>`\r\n2) The CLI attempts to call `/services/<something>/inspect`\r\n3) If that succeeds, then it continues with `/services/<something>/logs`, and the rest of the code is the same\r\n4) If it fails, then it attempts to call `/tasks/<something>/inspect`\r\n5) If that works, it calls `/tasks/<something>/logs` and continues with the rest of the code\r\n6) if that also fails, the CLI errors out saying <something> is neither a service nor a task\r\n\r\n/cc @thaJeztah @aaronlehmann "},{"labels":[null,"enhancement",null],"text":"Currently _docker node update_ accepts one host at a time. Would be nice if docker node update can accept an array of nodes.\r\n\r\nIf someone wants to add same node label to a cluster of hosts , as per current specification it has to run one at a time. \r\n\r\n**docker version**\r\n\r\nClient:\r\nVersion: 1.12.3\r\nAPI version: 1.24\r\nGo version: go1.6.3\r\nGit commit: 6b644ec\r\nBuilt:\r\nOS/Arch: linux/amd64\r\n\r\nServer:\r\nVersion: 1.12.3\r\nAPI version: 1.24\r\nGo version: go1.6.3\r\nGit commit: 6b644ec\r\nBuilt:\r\nOS/Arch: linux/amd64\r\n"},{"labels":[null,"enhancement"],"text":"If you build a Linux container from a Windows Docker client, you get this warning in stderr: `SECURITY WARNING: You are building a Docker image from Windows against a non-Windows Docker host. All files and directories added to build context will have '-rwxr-xr-x' permissions. It is recommended to double check and reset permissions for sensitive files and directories.`\r\n\r\nSee also https://github.com/docker/docker/issues/20397 for illustrative purposes.\r\n\r\nIt would be desirable to be able to disable this specific error, without being forced to ignore any stderr output.\r\n\r\nReason: we have automated build processes and we are aware of this limitation and accept it. We want to ignore it. However, because it is written to stderr, the automation considers it an error. We tried just telling it to not treat stderr as an error but this actually hid some error conditions that apparently signaled exit code 0, so this workaround may lead to actual errors not being discovered."},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nunauthorized: authentication required when creating service with attachable network access.\r\n\r\nThis is using the ARM 32-bit release from: https://github.com/docker/docker/releases/tag/v1.13.0-rc3\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n```\r\ndocker network create --attachable -d overlay funker\r\nmrythowxwse3ocwo92l5k80hl\r\n\r\ndocker service create --name add --network funker add\r\nunable to pin image add to digest: errors:\r\ndenied: requested access to the resource is denied\r\nunauthorized: authentication required\r\n\r\ntgpvi3qvcafui6tuh7evrykbr\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\npi@pi2swarm7:~ $ docker version\r\nClient:\r\n Version:      1.13.0-rc3\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   4d92237\r\n Built:        Tue Dec  6 01:48:38 2016\r\n OS/Arch:      linux/arm\r\n\r\nServer:\r\n Version:      1.13.0-rc3\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   4d92237\r\n Built:        Tue Dec  6 01:48:38 2016\r\n OS/Arch:      linux/arm\r\n Experimental: false\r\n\r\nLinux pi2swarm7 4.4.19-v7+ #906 SMP Tue Aug 23 15:53:06 BST 2016 armv7l GNU/Linux\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\nSlightly redacted:\r\n\r\n```\r\nContainers: 7\r\n Running: 0\r\n Paused: 0\r\n Stopped: 7\r\nImages: 17\r\nServer Version: 1.13.0-rc3\r\nStorage Driver: overlay\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n Is Manager: true\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 192.168.0.34\r\n Manager Addresses:\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 51371867a01c467f08af739783b8beafc154c4d7\r\ninit version: 949e6fa\r\nKernel Version: 4.4.19-v7+\r\nOperating System: Raspbian GNU/Linux 8 (jessie)\r\nOSType: linux\r\nArchitecture: armv7l\r\nCPUs: 4\r\nTotal Memory: 925.5 MiB\r\nName: pi2swarm7\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: alexellis2\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nWARNING: No kernel memory limit support\r\nWARNING: No cpu cfs quota support\r\nWARNING: No cpu cfs period support\r\nWARNING: No cpuset support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"Source: [http://semver.org/spec/v2.0.0.html#spec-item-10](http://semver.org/spec/v2.0.0.html#spec-item-10)\r\n\r\n> Build metadata MAY be denoted by appending a plus sign and a series of dot separated identifiers immediately following the patch or pre-release version. Identifiers MUST comprise only ASCII alphanumerics and hyphen [0-9A-Za-z-]. Identifiers MUST NOT be empty. Build metadata SHOULD be ignored when determining version precedence. Thus two versions that differ only in the build metadata, have the same precedence. Examples: 1.0.0-alpha+001, 1.0.0+20130313144700, 1.0.0-beta+exp.sha.5114f85.\r\n\r\nCurrently, the inclusion of a plus sign (+), as denoted for build metadata, is rejected as an invalid tag. I would have hoped that the full spec of semver be considered valid for use as a docker tag."},{"labels":[null,null,"enhancement",null],"text":"**Description**\r\n\r\nSome services i use, require static ip addresses, but instead of manualy adding them, i'd like to have an option below.\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker service create --name=someservice nginx:latest\r\n2. restart all/some nodes\r\n\r\n**Describe the results you received:**\r\nall containers got new ip\r\n\r\n**Describe the results you expected:**\r\nsome option ( if not out of the box) for setting docker-dhcp lease per container id.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.0-rc3\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   4d92237\r\n Built:        Mon Dec  5 19:05:57 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.0-rc3\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   4d92237\r\n Built:        Mon Dec  5 19:05:57 2016\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 5\r\n Running: 5\r\n Paused: 0\r\n Stopped: 0\r\nImages: 3\r\nServer Version: 1.13.0-rc3\r\nStorage Driver: devicemapper\r\n Pool Name: docker-202:1-263647-pool\r\n Pool Blocksize: 65.54 kB\r\n Base Device Size: 10.74 GB\r\n Backing Filesystem: xfs\r\n Data file: /dev/loop0\r\n Metadata file: /dev/loop1\r\n Data Space Used: 631.4 MB\r\n Data Space Total: 107.4 GB\r\n Data Space Available: 8.215 GB\r\n Metadata Space Used: 1.835 MB\r\n Metadata Space Total: 2.147 GB\r\n Metadata Space Available: 2.146 GB\r\n Thin Pool Minimum Free Space: 10.74 GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: false\r\n Deferred Deletion Enabled: false\r\n Deferred Deleted Device Count: 0\r\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\r\n WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\r\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\r\n Library Version: 1.02.110 (2015-10-30)\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: 1b5xsu6ys0k6yjyszu3qtp04u\r\n Is Manager: true\r\n ClusterID: 9k72vaabjxl175012or013hn4\r\n Managers: 3\r\n Nodes: 3\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 172.31.50.222\r\n Manager Addresses:\r\n  172.31.50.222:2377\r\n  172.31.54.237:2377\r\n  172.31.60.184:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 51371867a01c467f08af739783b8beafc154c4d7\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-53-generic\r\nOperating System: Ubuntu 16.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 3.674 GiB\r\nName: ip-172-31-50-222\r\nID: U2WW:7VP2:KGSJ:46MX:XKHY:JIOU:3ZSR:6T4A:OCE4:MV7U:O26B:6WG7\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nBaremetal/AWS.\r\n\r\nThanks a lot in advance! Really hoping there's some way of achieving this already, that i did not find."},{"labels":[null,null,"enhancement"],"text":"Just opening this as a separate issue here from https://github.com/docker/docker/issues/25303 so it can be tracked. There was as discussion about adding `--entrypoint` to service create. This would be extremely useful to have for 1.13 because right now a large number of docker images are unusable as services without modification."},{"labels":[null,null,null,null,"enhancement"],"text":"During my test on `docker stack rm` commands, I found one difference from `docker-compose down`, `docker stack rm` doesn't have `-v` options, which is used to remove the volumes defined in `docker-compose.yml` file.\r\n\r\n`-v` option is very useful, especially in the swarm clustering environment. In staging cluster, it's quite often we need to clean all the data from last staging testing.\r\n\r\n`docker stack deploy` is very helpful to setup the environment, however, when we remove the stack via command `docker stack rm`, it became a little bit painful, as all the volumes are left in the cluster, spread out across many hosts.\r\n\r\nAs the stack has been removed, there is no way to know which volume was on which host. We have to go through each host in the cluster, and `docker volume ls` to check is there any volume for this just-removed-stack left, and remove it manually if anything found. \r\n\r\nThis is very annoying. And volume we missed will be payback in next test running, which might have wired result because some tasks are using the data from last testing. We have to be very careful to identify the remaining volumes.\r\n\r\nBy providing the `-v` option in `docker stack rm` command, the volumes of this stack can be removed across the swarm cluster during the stack take down, it will easy the life a lot."},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nI'm able to do rolling update without any \"Connection Refused\" if I have 2 replicas but not if I have only one.\r\n\r\nFor people with small VPS, it would be useful to be able to do \"rolling updates\" with only one node and having the containers running only once.\r\n\r\n**Steps to reproduce the issue:**\r\n1. run `docker service create -p 8080:80 --name hello --update-delay 10s --stop-grace-period 10s dockercloud/hello-world`\r\n2. run `siege http://127.0.0.1:8080`\r\n3. run `docker service update --image dockercloud/hello-world:staging hello`\r\n4. wait for the update to finish\r\n5. stop 'siege'\r\n\r\n**Describe the results you received:**\r\nI see a lot of \"Connection refused\" and a non 100% availability.\r\n\r\n**Describe the results you expected:**\r\nno \"Connection refused\" and 100% availability.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.14.0-dev\r\n API version:  1.26\r\n Go version:   go1.7.4\r\n Git commit:   1c96879\r\n Built:        Mon Dec  5 16:14:53 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.14.0-dev\r\n API version:  1.26 (minimum version 1.12)\r\n Go version:   go1.7.4\r\n Git commit:   1c96879\r\n Built:        Mon Dec  5 16:14:53 2016\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 5\r\n Running: 1\r\n Paused: 0\r\n Stopped: 4\r\nImages: 70\r\nServer Version: 1.14.0-dev\r\nStorage Driver: devicemapper\r\n Pool Name: docker-8:2-24772686-pool\r\n Pool Blocksize: 65.54 kB\r\n Base Device Size: 10.74 GB\r\n Backing Filesystem: xfs\r\n Data file: /dev/loop0\r\n Metadata file: /dev/loop1\r\n Data Space Used: 4.849 GB\r\n Data Space Total: 107.4 GB\r\n Data Space Available: 102.5 GB\r\n Metadata Space Used: 8.417 MB\r\n Metadata Space Total: 2.147 GB\r\n Metadata Space Available: 2.139 GB\r\n Thin Pool Minimum Free Space: 10.74 GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: false\r\n Deferred Deletion Enabled: false\r\n Deferred Deleted Device Count: 0\r\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\r\n WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\r\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\r\n Library Version: 1.02.136 (2016-11-05)\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: 68a62tley9l6zk71erj1wnr6t\r\n Is Manager: true\r\n ClusterID: 0nad6052izxvfvwllyscr0s0v\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 10.1.10.94\r\n Manager Addresses:\r\n  10.1.10.94:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 9c21822f344b7894ec2e55ac4b6d4e86118907dd (expected: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e)\r\nrunc version: N/A (expected: 51371867a01c467f08af739783b8beafc154c4d7)\r\ninit version: N/A (expected: 949e6facb77383876aeff8a6944dde66b3089574)\r\nKernel Version: 4.8.11-1-ARCH\r\nOperating System: Arch Linux\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 7.801 GiB\r\nName: bruno\r\nID: FQPI:5DOA:ESFX:4DUW:A3RE:GQ6A:UDLN:HRLV:I5HU:UTO4:3HX2:CCP2\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: bbigras\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nphysical"},{"labels":[null,null,"enhancement"],"text":"To test `docker stack deploy --compose-file` function, I load one of my sample `docker-compose.yml`:\r\n\r\n```yaml\r\nversion: '3'\r\nservices:\r\n    nginx:\r\n        image: \"${DOCKER_USER}/lnmp-nginx:v1.2\"\r\n        build:\r\n            context: .\r\n            dockerfile: Dockerfile.nginx\r\n        ports:\r\n            - \"80:80\"\r\n        networks:\r\n            - frontend\r\n        depends_on:\r\n            - php\r\n    php:\r\n        image: \"${DOCKER_USER}/lnmp-php:v1.2\"\r\n        build:\r\n            context: .\r\n            dockerfile: Dockerfile.php\r\n        networks:\r\n            - frontend\r\n            - backend\r\n        environment:\r\n            MYSQL_PASSWORD: Passw0rd\r\n        depends_on:\r\n            - mysql\r\n    mysql:\r\n        image: mysql:5.7\r\n        volumes:\r\n            - mysql-data:/var/lib/mysql\r\n        environment:\r\n            TZ: 'Asia/Shanghai'\r\n            MYSQL_ROOT_PASSWORD: Passw0rd\r\n        command: ['mysqld', '--character-set-server=utf8']\r\n        networks:\r\n            - backend\r\nvolumes:\r\n    mysql-data:\r\n\r\nnetworks:\r\n    frontend:\r\n    backend:\r\n```\r\n\r\nIn the `image` section of service `nginx` and `php`, I used `${DOCKER_USER}` to get the docker id from environment variables. And if I use `docker-compose up`, it will load `.env` file as default envvar files, which content is:\r\n\r\n```bash\r\nDOCKER_USER=twang2218\r\n```\r\n\r\nHowever, if I use `docker stack` to deploy this `docker-compose.yml`, I will got following errors:\r\n\r\n```bash\r\n$ docker stack deploy --compose-file docker-compose.yml lnmp\r\nIgnoring unsupported options: build\r\n\r\nCreating network lnmp_frontend\r\nCreating network lnmp_backend\r\nCreating network lnmp_default\r\nCreating service lnmp_php\r\nError response from daemon: rpc error: code = 3 desc = ContainerSpec: \"/lnmp-php:v1.2\" is not a valid repository/tag\r\n```\r\n\r\nAs you can see, as `docker stack deploy` command didn't load `.env` file, the `${DOCKER_USER}` was replaced by empty string, which cause image name become invalid.\r\n\r\nIf `.env` file was loaded, the final image name should be `twang2218/lnmp-php:v1.2`.\r\n\r\nThe environment substitution is actually working, if I run the command this way:\r\n\r\n```bash\r\n$ DOCKER_USER=twang2218 docker stack deploy --compose-file docker-compose.yml lnmp\r\nIgnoring unsupported options: build\r\n\r\nCreating network lnmp_frontend\r\nCreating network lnmp_backend\r\nCreating network lnmp_default\r\nCreating service lnmp_mysql\r\nCreating service lnmp_nginx\r\nCreating service lnmp_php\r\n```\r\n\r\nAnd we can verify it's working by `docker service inspect` command:\r\n\r\n```bash\r\n$ docker service inspect lnmp_php | grep Image\r\n                    \"Image\": \"twang2218/lnmp-php:v1.2@sha256:4f1aef1350aeef3f757f6b6da8f2e1a79ff849f61382320e4b668bfe2b0d1c5a\",\r\n```\r\n\r\nThe image name is `twang2218/lnmp-php:v1.2`, which is correct.\r\n\r\nI tested this feature on Digtial Ocean droplet, which installed docker 1.13.0-rc2 via `docker-machine`.\r\n\r\nHere is the version:\r\n\r\n```bash\r\n$ docker version\r\nClient:\r\n Version:      1.13.0-rc2\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   1f9b3ef\r\n Built:        Wed Nov 23 06:32:39 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:             1.13.0-rc2\r\n API version:         1.25\r\n Minimum API version: 1.12\r\n Go version:          go1.7.3\r\n Git commit:          1f9b3ef\r\n Built:               Wed Nov 23 06:32:39 2016\r\n OS/Arch:             linux/amd64\r\n Experimental:        false\r\n```\r\n\r\nHere is the `docker info`:\r\n\r\n```bash\r\nroot@d1:~/docker-lnmp# docker info\r\nContainers: 7\r\n Running: 1\r\n Paused: 0\r\n Stopped: 6\r\nImages: 4\r\nServer Version: 1.13.0-rc2\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 43\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: vyf3mgcj3uonrnh5xxquasp38\r\n Is Manager: true\r\n ClusterID: jb8rxvd6ptrn3psfkiixxed7r\r\n Managers: 1\r\n Nodes: 3\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 138.197.195.206\r\n Manager Addresses:\r\n  138.197.195.206:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 51371867a01c467f08af739783b8beafc154c4d7\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-51-generic\r\nOperating System: Ubuntu 16.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 488.5 MiB\r\nName: d1\r\nID: E6UB:PHX6:I2KY:Q35T:PCCI:MFDQ:ZMMN:2X7K:DEOZ:PAP7:4BUC:FP6X\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nLabels:\r\n provider=digitalocean\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n"},{"labels":[null,"enhancement"],"text":"When providing multiple names to `docker secret rm`, the non-existing names are ignored.\r\n\r\n**When providing multiple names, non-existing names are ignored**\r\n\r\n```bash\r\n$ echo \"foo\" | docker secret create foo\r\nra4t9sb7bvd30rczgin4if7pr\r\n\r\n$ docker secret rm foo bar baz\r\nra4t9sb7bvd30rczgin4if7pr\r\n```\r\n\r\nOnly the existing secret is attempted to be deleted, but no error is shown for the non-existing secrets;\r\n\r\n```\r\nDEBU[1151] Calling GET /v1.26/secrets?filters=%7B%22names%22%3A%7B%22bar%22%3Atrue%2C%22baz%22%3Atrue%2C%22foo%22%3Atrue%7D%7D\r\nDEBU[1151] Calling DELETE /v1.26/secrets/ra4t9sb7bvd30rczgin4if7pr\r\nDEBU[1151] secret removed                                method=RemoveSecret secret.ID=ra4t9sb7bvd30rczgin4if7pr\r\n```\r\n\r\nThis is inconsistent with other `rm` commands, which leave resolution to the daemon;\r\n\r\n```bash\r\n$ docker create --name foo busybox\r\n8820a306e95aab39b633045771de202dbfd6101409f40742530b959906ee4759\r\n\r\n$ docker rm foo bar baz\r\nfoo\r\nError response from daemon: No such container: bar\r\nError response from daemon: No such container: baz\r\n```\r\n\r\nWhich makes these API calls;\r\n\r\n```\r\nDEBU[2445] Calling DELETE /v1.26/containers/baz\r\nERRO[2445] Handler for DELETE /v1.26/containers/baz returned error: No such container: baz\r\nDEBU[2445] Calling DELETE /v1.26/containers/foo\r\nDEBU[2445] Calling DELETE /v1.26/containers/bar\r\nERRO[2445] Handler for DELETE /v1.26/containers/bar returned error: No such container: bar\r\n```\r\n\r\n**Proposed solution**\r\n\r\nThis issue looks to be introduced by https://github.com/docker/docker/commit/70d2cefd51e853ca4f3bbf8eb0386360809e026b (https://github.com/docker/docker/pull/28716), which added support for removing multiple secrets.\r\n\r\nGiven that the API supports both \"IDs\" and \"names\", there should not be a need to lookup ID's in advance.\r\n\r\nOne thing that _should_ be tested is that the provided names cannot be mis-used, e.g. that `docker secret rm ../containers/foo` does not lead to deleting a container named \"foo\" 😇 \r\n\r\n/cc @ehazlett \r\n"},{"labels":[null,"enhancement",null,null],"text":"**Description**\r\n\r\n`docker node inspect` prints out the following regarding the platform:\r\n\r\n```\r\n            \"Platform\": {\r\n                \"Architecture\": \"x86_64\",\r\n                \"OS\": \"linux\"\r\n            }\r\n```\r\n\r\nwhereas `docker info` prints out:\r\n\r\n```\r\nOperating System: Boot2Docker 1.13.0-rc2 (TCL 7.2); HEAD : 51fa426 - Wed Nov 23 20:08:44 UTC 2016\r\nOSType: linux\r\nArchitecture: x86_64\r\n```\r\n\r\nThis is inconsistent.  I'd suggest to add a field in `docker node inspect` to print out it's Operating System info as well.\r\n\r\n**Steps to reproduce the issue:**\r\n`docker node inspect` and `docker info`\r\n\r\n**Describe the results you received:**\r\nInconsistent behavior for those two commands regarding platform\r\n\r\n**Describe the results you expected:**\r\nConsistent behavior\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\ndocker version\r\nClient:\r\n Version:      1.12.3\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   6b644ec\r\n Built:        Wed Oct 26 23:26:11 2016\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.0-rc2\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   1f9b3ef\r\n Built:        Wed Nov 23 17:40:58 2016\r\n OS/Arch:      linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 12\r\n Running: 11\r\n Paused: 0\r\n Stopped: 1\r\nImages: 23\r\nServer Version: 1.13.0-rc2\r\nStorage Driver: aufs\r\n Root Dir: /mnt/sda1/var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 92\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: etjbw0zbhyc5leu4w7j2b7d1k\r\n Is Manager: true\r\n ClusterID: 94lfc4rioa02efy24nqo4h4hz\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  External CAs:\r\n    cfssl: https://192.168.99.100:12381/api/v1/cfssl/sign\r\n Node Address: 192.168.99.100\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: seccomp\r\nKernel Version: 4.4.34-boot2docker\r\nOperating System: Boot2Docker 1.13.0-rc2 (TCL 7.2); HEAD : 51fa426 - Wed Nov 23 20:08:44 UTC 2016\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 2.386 GiB\r\nName: node1\r\nID: D2W3:TL4K:CKZ5:VUKI:CIUW:6UBA:CDJ6:KZSO:4JKB:XLGP:IZ6Q:2J3D\r\nDocker Root Dir: /mnt/sda1/var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 122\r\n Goroutines: 219\r\n System Time: 2016-12-02T23:00:46.012882451Z\r\n EventsListeners: 2\r\nUsername: ry4nz\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\n provider=virtualbox\r\nInsecure Registries:\r\n 127.0.0.0/8\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nBoot2docker\r\n"},{"labels":[null,"enhancement"],"text":"## What is different from previous issues/ideas\r\n1. It doesn't try to share /var/lib/docker. Image layers are stored on a separate mount if shared \r\n2. It is agnostic to chosen shared filesystem\r\n3. Changes are isolated to graphdriver code (except for RO mount function in pkg/loopback)\r\n3. Implementation is functional and based on HEAD of master(as of Dec 1)\r\n\r\n## Goals\r\n1. Storage of image layers on common cluster filesystems\r\n2. Share image layers between docker daemons on different hosts\r\n3. Provide layered image abstraction on top of filesystems that don't support overlayfs\r\n\r\n## Proposal\r\n- A loop_overlay2 graphdriver than stores layers as loopback images (XFS) and merges layers together using overlayfs for running containers\r\n\r\n## Implementation\r\nDocker fork:\r\nhttps://github.com/bacaldwell/docker/tree/stateless4\r\nGraphdriver code:\r\nhttps://github.com/bacaldwell/docker/commit/48159c8376c23da850f266423a7bb133a00a794d\r\nREADME:\r\nhttps://github.com/bacaldwell/docker/blob/stateless4/daemon/graphdriver/loop_overlay2/README.md\r\n\r\n## Features of loop_overlay2\r\n- Layers are mounted on-demand as loopback devices\r\n  - Read-write mode for ApplyDiff\r\n  - Read-only for Get\r\n- Upper writable layer for the container resides in /var/lib/docker. On 'docker commit', the upper layer gets written to a loopback device\r\n- The scheme from the overlay2 driver is reused for multiple read-only lower layers identified by symbolic links to maximize number of layers supported\r\n- Supports a common cluster computing/high performance computing shared filesystem organization scheme that allows a set of layers (and associated Layer Store) to be shared read-only, and protected by UNIX access permissions. When root Layer Store is not writable, a configurable fallback location is used for storing layers added by the daemon. This can be persistent or ephemeral. On graph driver initialization, the stores are merged into a new store (using overlayfs) in the daemon's expected place (e.g. /var/lib/docker/image)\r\n\r\n## Target use case\r\nCluster environments, especially high-performance computing environments, already have shared filesystems such as NFS, Lustre, or GPFS. The distributed and parallel variants Lustre and GPFS may have petabytes of storage capacity, are designed to be highly reliable, and shared by tens of thousands of nodes. Furthermore, due to complexity and failure rate concerns, nodes that share these filesystems are often \"diskless\" or \"stateless\" nodes without any local persistent disks.\r\n- The loop_overlay2 graphdriver allows these \"stateless\" nodes to run Docker and on startup, have a consistent view of images available on shared storage.\r\n- Changes made by \"stateless\" nodes can be captured in a per-node/per-user directory on shared storage or in a ramdisk (ephemeral)\r\n- Note that the use of loopback devices mitigate excessive metadata traffic and leverage the block performance of parallel filesystems\r\n\r\n## Related issues\r\n#9935\r\n#15887\r\n\r\n# Presentation with early architecture\r\nhttps://www.youtube.com/watch?v=8yfMd96ll4s\r\n\r\n## Status\r\nFeature-complete and passes most integration tests. One test that elicits race conditions (DockerSuite.TestEventsLimit) is problematic. Despite refcounting loopback mounts, sometimes mounting a layer returns device or resource busy\r\n"},{"labels":[null,"enhancement"],"text":"In current version of docker, the settings of a container is fixed whenever it is created. For example, when I do `docker run -dit -p 5566:7788 ubuntu`, the port-forwarding settings is fixed. There seems no way that I can edit this setting except recreating a new container. Is it possible to support this feature?"},{"labels":[null,"enhancement"],"text":"I installed Docker via https://apt.dockerproject.org/repo. After installation, the completion file for Bash is not in `/etc/bash_completion.d`; I have to install it manually (with `curl`). I think it should be installed by the package, or at least the completion file should be provided somewhere (so that I can symlink to it). I searched `/usr/share/docker-engine/contrib/` but the `completion` folder is not there.\r\n\r\nNote: For Zsh I installed the completion in `/usr/local/share/zsh/site-functions/`, but I guess (from folder name) that the `.deb` package should install the completion in `/usr/share/zsh/vendor-completions`."},{"labels":[null,"enhancement"],"text":"Hello\r\n\r\nUntil now (v. 1.12.3), if you pull an existing image from a private registry but you are not authenticated, the `docker pull` command returns:\r\n```\r\n$ docker pull privateregistry.com/existingimage\r\nUsing default tag: latest\r\nPulling repository privateregistry.com/existingimage\r\nError: image existingimage:latest not found\r\n```\r\nwith exit code = 1.\r\n\r\nAs is it, we cannot determine if it fails because image really does not exists, or if it's because we are not authenticated.\r\n\r\nIt should return a specific message like:\r\n```\r\nError: please log in to privateregistry.com before pull\r\n```\r\nwith a specific exit code.\r\n\r\nThanks and keep on the great work"},{"labels":[null,"enhancement"],"text":"When I inspect a container, the named volumes appear to mount a location on the host to a location on the container:\r\n\"Mounts\": [\r\n            {\r\n                \"Type\": \"volume\",\r\n                \"Name\": \"e1_serverlog\",\r\n                \"Source\": \"/mnt/sda1/var/lib/docker/volumes/e1_serverlog/_data\",\r\n                \"Destination\": \"/log\",\r\n                \"Driver\": \"local\",\r\n                \"Mode\": \"\",\r\n                \"RW\": true,\r\n                \"Propagation\": \"\"\r\n            }\r\n        ],\r\nWe need a way by which, docker can be told to use a custom directory from the host such as /usr/data/myvolumes as the Source for the named volumes. "},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nwhen do `docker build` the intermediate image not protected! This is a big issue when do `docker rmi <none>:<none>` at the same time!\r\n\r\nthe real example is in our CI system, we have two jobs, one for build image and one for clean images.\r\n\r\n**Steps to reproduce the issue:**\r\n1. do `docker build`\r\n2. do `docker rmi <none>:<none>` at the same time\r\n3. docker build will fail due to intermediate image removed by `docker rmi`\r\n\r\n**Describe the results you received:**\r\n\r\ndocker build fail\r\n\r\n**Describe the results you expected:**\r\n\r\ndocker build success\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.12.1\r\n API version:  1.24\r\n Go version:   go1.7.1\r\n Git commit:   6f9534c\r\n Built:        Thu Sep  8 10:31:18 2016\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.12.1\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   23cf638\r\n Built:        Thu Aug 18 17:52:38 2016\r\n OS/Arch:      linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 17\r\n Running: 8\r\n Paused: 0\r\n Stopped: 9\r\nImages: 45\r\nServer Version: 1.12.1\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 170\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: overlay null host bridge\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: seccomp\r\nKernel Version: 4.4.20-moby\r\nOperating System: Alpine Linux v3.4\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 6\r\nTotal Memory: 1.952 GiB\r\nName: moby\r\nID: FYWR:2TN5:XXJD:OYS6:GWJX:YSZY:ZQVK:GO5J:NV2L:LEOW:Y4AA:IIXZ\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 62\r\n Goroutines: 100\r\n System Time: 2016-12-01T05:14:17.493898224Z\r\n EventsListeners: 3\r\nRegistry: https://index.docker.io/v1/\r\nInsecure Registries:\r\n 127.0.0.0/8\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\njenkins ci"},{"labels":[null,"enhancement"],"text":"I am noticing on boot that I get the warning \"mountpoint for pids not found\". I find this rather confusing and undocumented. \r\n\r\nI suspect this is cause we are running on a pre 4.3 kernel and don't have pids resource management in cgroups. \r\n\r\nCan error be either suppressed on boot, or clarified, eg:\r\n\r\n\"INFO: mountpoint for pids not found, pids in cgroups are a Linux 4.3 feature\" "},{"labels":[null,"enhancement"],"text":"Doing more test on consistency, it looks like `--secret-rm` is using the wrong property for removing a secret;\r\n\r\nTo reproduce; create a secret, and a service that uses the same secret _twice_:\r\n\r\n```bash\r\necho \"world\" | docker secret create hello\r\n\r\ndocker service create --name test --secret source=hello,target=world --secret source=hello,target=other-world nginx:alpine\r\n```\r\n\r\nVerify that the secret is used twice;\r\n\r\n```bash\r\ndocker service inspect test --format '{{ json .Spec.TaskTemplate.ContainerSpec.Secrets}}' | jq\r\n```\r\n\r\n```json\r\n[\r\n  {\r\n    \"File\": {\r\n      \"Name\": \"world\",\r\n      \"UID\": \"0\",\r\n      \"GID\": \"0\",\r\n      \"Mode\": 292\r\n    },\r\n    \"SecretID\": \"jis2m9o0w5b2h4b1hu1ejjbwa\",\r\n    \"SecretName\": \"hello\"\r\n  },\r\n  {\r\n    \"File\": {\r\n      \"Name\": \"other-world\",\r\n      \"UID\": \"0\",\r\n      \"GID\": \"0\",\r\n      \"Mode\": 292\r\n    },\r\n    \"SecretID\": \"jis2m9o0w5b2h4b1hu1ejjbwa\",\r\n    \"SecretName\": \"hello\"\r\n  }\r\n]\r\n```\r\n\r\nUpdate the service, and remove a secret;\r\n\r\n```bash\r\ndocker service update --secret-rm hello test\r\n```\r\n\r\nBoth uses of the secret are removed;\r\n\r\n```bash\r\ndocker service inspect test --format '{{ json .Spec.TaskTemplate.ContainerSpec.Secrets}}' | jq\r\nnull\r\n```\r\n\r\n\r\nThe _target_ should be leading here, because a single secret can be used multiple times. This is also consistent with other options (e.g. `--mount-rm`)\r\n\r\n/cc @stevvooe @ehazlett \r\n"},{"labels":[null,"enhancement"],"text":"Hi,\r\n\r\nSome Docker workloads expect to spin new Docker instances by talking to the daemon over /var/run/docker.sock from within an image. These are typically run with:\r\n`-v /var/run/docker.sock:/var/run/docker.sock`\r\n\r\nThis is a bother when trying to run multiple daemons or have multiple versions of Docker installed on the same host as the original images will only work if the socket is at the right location.\r\n\r\nThis could be avoided with a flag to express sharing the docker daemon connection with the image, e.g. something like:\r\n`--share-docker-sock`\r\n\r\nSadly, it will take some time for images and scripts authors to adopt such a flag, so this will only be useful on the long run but it's definitely worth providing a solution to harcoded pathnames in people's code.\r\n\r\nCheers,\r\n-Loïc Minier"},{"labels":[null,"enhancement"],"text":"`docker inspect --type task` only accepts task IDs. Unfortunately, these are hard to get.\r\n\r\n```bash\r\n# I have a service \"some-service\" with a task.\r\n$ docker service ps some-service\r\nNAME                         IMAGE           NODE          DESIRED STATE  CURRENT STATE           ERROR  PORTS\r\nsome-service.1.zb7vcagaa927  busybox:latest  8fe154b748c2  Running        Running 36 seconds ago\r\n# trying to inspect the task by its name:\r\n$ docker inspect some-service.1.zb7vcagaa927\r\n[]\r\nError: No such object: some-service.1.zb7vcagaa927\r\n# guessing partial task ID from the task name (last part) => success\r\n$ docker inspect zb7vcagaa927\r\n[...]\r\n        \"ID\": \"zb7vcagaa9276a0td3t10sswn\",\r\n[...]\r\n```\r\nThings would be easier if `docker inspect --type task` accepted task names.\r\n\r\nReferences\r\n- #23614 added support for inspecting tasks\r\n- #28944 suffers from this issue (bash completion)\r\n- #28213 will make task IDs more easily accessable by adding `--format` to `docker service ps`\r\n\r\nping @yongtang PTAL"},{"labels":[null,"enhancement"],"text":"Possibly just a \"nit\"; but now that services \"pin\" an image by digest, the _pull_ also pulls by digest, resulting in local images to not have a \"tag\" visible;\r\n\r\nbefore creating the service; no images are available;\r\n\r\n```bash\r\n$ docker images\r\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n```\r\n\r\ncreate a service using the `nginx:alpine` image\r\n\r\n```bash\r\n$ docker service create --name web nginx:alpine\r\nx72opmu78pv8cklr7fu4j2w18\r\n```\r\n\r\nAfter the service is created, the `nginx` image is present locally, but tag is empty (`<none>`);\r\n\r\n```\r\n$ docker images\r\n\r\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\nnginx               <none>              d964ab5d0abe        5 hours ago         54.9 MB\r\n```\r\n\r\nNot sure this can be solved easily, but I can see this being confusing.\r\n\r\n/cc @nishanttotla @aaronlehmann "},{"labels":[null,"enhancement"],"text":"When using the docker stats api currently each request to the API will take up to 2 seconds. As explained in https://github.com/docker/docker/issues/23188 the docker API needs two measurement points to calculate an average. \r\n\r\nFor stats collectors like [Metricbeat](https://github.com/elastic/beats/tree/master/metricbeat) it would be useful to get the raw cpu values and either do the calculations on the collector side or directly when viewing the data on the storage system (in our case Elasticsearch).\r\n\r\nI'm proposing to add a flag/param to the docker stats API request that skips average calculations and return only raw values instead. This will speed up the API request and will potentially also reduce the load on the docker service as the request is open for a much shorter time.\r\n\r\nCollecting stats for a large number of containers has currently to be done one by one which leads to lots of open http requests and is quite slow because of the above issue. This will potentially be solved with https://github.com/docker/docker/pull/25361 by @WeiZhang555 but also as part of this bulk request it would be nice to have the flag to disable calculations."},{"labels":[null,"enhancement"],"text":"I'm currently using docker in a puppet environment, where puppet handles all firewall-rules and this conflicts with docker generating it's own firewall rules. Puppet can handle this by adding an ignore list for which rules NOT to purge from the different chains - but it's very hard to identify the rules created by docker from puppet. But adding a comment saying \"Managed by docker\", will make this a piece of cake.\r\n\r\nI've created this untested patch - hopefully it will inspire someone with the ability to test and run docker from source to create this feature :)\r\n\r\n```\r\ndiff --git a/vendor/github.com/docker/libnetwork/iptables/iptables.go b/vendor/github.com/docker/libnetwork/iptables/iptables.go\r\nindex 3884257..e9422b5 100644\r\n--- a/vendor/github.com/docker/libnetwork/iptables/iptables.go\r\n+++ b/vendor/github.com/docker/libnetwork/iptables/iptables.go\r\n@@ -281,6 +281,7 @@ func (c *ChainInfo) Link(action Action, ip1, ip2 net.IP, port int, proto string,\r\n // rule is not already present in the chain. Reciprocally,\r\n // it removes the rule only if present.\r\n func ProgramRule(table Table, chain string, action Action, args []string) error {\r\n+       args = append(args, \"-m comment --comment 'Managed by docker'\")\r\n        if Exists(table, chain, args...) != (action == Delete) {\r\n                return nil\r\n        }\r\n```\r\n\r\nIt may be best to add it as an option to docker, so it will be 100% backward compatible and up to the user to use it."},{"labels":[null,null,"enhancement",null],"text":"I have been playing around with ZFS and Docker and the whole thing seems really interesting and promising.\r\n\r\nOne thing that is missing though is being able to set volume size on `docker volume create` when using zfs as the storage driver, since it supports quotas.\r\n\r\nRight now trying to use the `size` option in volumes, results in the following error:\r\n\r\n```\r\nroot@ubuntu-1gb-nyc1-01:~# docker volume create --opt type=zfs --opt size=1G\r\nError response from daemon: create 6feea583bc598a514c804f1c0bc64e7b813897741ba2643e0a69f2112d36ce39: invalid option key: \"size\"\r\n```\r\n\r\n**Docker info**\r\n```\r\nContainers: 1\r\n Running: 0\r\n Paused: 0\r\n Stopped: 1\r\nImages: 1\r\nServer Version: 1.12.3\r\nStorage Driver: zfs\r\n Zpool: zpool-docker\r\n Zpool Health: ONLINE\r\n Parent Dataset: zpool-docker/docker\r\n Space Used By Parent: 124416\r\n Space Available: 10336655360\r\n Parent Quota: no\r\n Compression: off\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: host bridge null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: apparmor seccomp\r\nKernel Version: 4.4.0-47-generic\r\nOperating System: Ubuntu 16.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 992.5 MiB\r\nName: ubuntu-1gb-nyc1-01\r\nID: Z6QZ:IDVT:EEDV:FWDV:LTNM:I3KS:6UYE:OQSU:HU3O:WFIT:V3Q7:ZIYP\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nInsecure Registries:\r\n 127.0.0.0/8\r\n```\r\n\r\n**Docker version**\r\n```\r\nClient:\r\n Version:      1.12.3\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   6b644ec\r\n Built:        Wed Oct 26 22:01:48 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.12.3\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   6b644ec\r\n Built:        Wed Oct 26 22:01:48 2016\r\n OS/Arch:      linux/amd64\r\n```"},{"labels":[null,null,"enhancement",null,null],"text":"**Description**\r\n\r\nService logs only works with `json-file` and `journald` drivers (fine). Running `docker service logs <service with other log driver> will just hang. \r\n\r\n**Describe the results you received:**\r\n\r\nDocker cli hangs\r\n\r\n**Describe the results you expected:**\r\n\r\nError saying that `docker service logs` is not supported for this service since it uses a log driver that doesn't support that.\r\n\r\n\r\n**Output of `docker version`:**\r\n```\r\n $ docker version\r\nClient:\r\n Version:      1.13.0-rc2\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   1f9b3ef\r\n Built:        Wed Nov 23 17:40:58 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:             1.13.0-rc2\r\n API version:         1.25\r\n Minimum API version: 1.12\r\n Go version:          go1.7.3\r\n Git commit:          1f9b3ef\r\n Built:               Wed Nov 23 17:40:58 2016\r\n OS/Arch:             linux/amd64\r\n Experimental:        true\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nDocker for AWS"},{"labels":[null,null,"enhancement"],"text":""},{"labels":[null,"enhancement"],"text":"The output of `docker version` is now not nicely \"balanced\" due to `Minimum Api Version` being added in 1.13; Can we come up with an alternative title, so that it looks nice again?\r\n\r\nBefore:\r\n\r\n```\r\nClient:\r\n Version:      1.12.3\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   6b644ec\r\n Built:        Thu Oct 27 00:09:21 2016\r\n OS/Arch:      darwin/amd64\r\n Experimental: true\r\n\r\nServer:\r\n Version:      1.12.3\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   6b644ec\r\n Built:        Thu Oct 27 00:09:21 2016\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\nAfter:\r\n\r\n```\r\nClient:\r\n Version:      1.14.0-dev\r\n API version:  1.26\r\n Go version:   go1.7.3\r\n Git commit:   839e405\r\n Built:        Wed Nov 23 20:32:14 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:             1.14.0-dev\r\n API version:         1.26\r\n Minimum API version: 1.12\r\n Go version:          go1.7.3\r\n Git commit:          839e405\r\n Built:               Wed Nov 23 20:32:14 2016\r\n OS/Arch:             linux/amd64\r\n Experimental:        true\r\n```\r\n\r\n/cc @jhowardmsft @tiborvass "},{"labels":[null,"enhancement",null,null],"text":"The `docker checkpoint ls` command's error messages can be slightly\r\nimproved;\r\n\r\n```bash\r\n$ docker checkpoint ls nosuchcontainer\r\nError response from daemon: No such container: nosuchcontainer\r\n```\r\n\r\nThis is inconsistent with other commands, which don't print `Error response from daemon:`;\r\n\r\n```bash\r\n$ docker service ps nosuchthing\r\nError: No such service: nosuchthing\r\n\r\n$ docker container inspect nosuchthing\r\n[]\r\nError: No such container: nosuchthing\r\n```\r\n\r\nI also found this;\r\n\r\n```bash\r\n$ docker checkpoint ls ../../\r\nError response from daemon: page not found\r\n```\r\n\r\nWhereas (e.g.)\r\n\r\n```bash\r\n$ docker service ps ../../\r\nError: No such service: ../../\r\n```\r\n\r\nIt's really a corner case, and I'm not sure why other commands succeed in this case, but would be nice\r\nto improve if it's an easy fix :)\r\n"},{"labels":[null,"enhancement",null],"text":"Unlike other commands (`docker volume create`, `docker network create`),\r\nthe `docker checkpoint` create command does not return the \"id\" or \"name\"\r\nof the checkpoint;\r\n\r\n```bash\r\n$ docker checkpoint create my-container charlie\r\n<nothing printed>\r\n```\r\n\r\nTo be consistent with other commands, the name (or \"id\") should be printed;\r\n\r\n```bash\r\n$ docker checkpoint create my-container charlie\r\ncharlie\r\n```\r\n"},{"labels":[null,"enhancement",null,null],"text":"When creating a checkpoint for a container, and a checkpoint with that\r\nname already exists for the container, a cryptic error message is printed;\r\n\r\n```bash\r\n$ docker checkpoint create cr checkpoint2\r\nError response from daemon: Cannot checkpoint container cr: rpc error: \r\ncode = 6 desc = mkdir /var/lib/docker/containers/c4ba0c844c93a31db50dcda57fc0126fd17ec1c4a73cd550a501aac117685a6c/checkpoints/checkpoint2:\r\nfile exists\r\n```\r\n\r\n`docker checkpoint create` should;\r\n\r\n- check if a checkpoint already exists\r\n- if exists, print a proper error message\r\n\r\nExpected behavior;\r\n\r\n```bash\r\n$ docker checkpoint create cr checkpoint2\r\nError response from daemon: checkpoint with name foo already exists for container cr\r\n```\r\n\r\n/cc @boucher"},{"labels":[null,null,"enhancement"],"text":"Just adding this for discussion so that we can find how to resolve this; this occurred to me while reviewing https://github.com/docker/docker/pull/28734\r\n\r\nJekyll requires some non-standard markdown markup markers for certain parts of the documentation (`{% raw %}` / `{% end raw %}`). Before the switch to Jekyll, these markers were not needed, so it was possible to copy / re-use the same markdown for both the CLI reference, and the man-pages.\r\n\r\nIf we want to generate both the CLI docs and man-pages using the same source, we may have to make some changes so that the man-page generation understands those markers\r\n\r\n/cc @johndmulhausen @FrenchBen @dnephin @cpuguy83 "},{"labels":[null,"enhancement"],"text":"[Official docker cp documentation](https://docs.docker.com/engine/reference/commandline/cp/)\r\n\r\nIn the file docker/contrib/completion/zsh/_docker line 616 the autocomplete declaration only supports the following docker cp usage format:\r\n```\r\ndocker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|-\r\n```\r\nIt would be awesome if it also supported the other format of:\r\n```\r\ndocker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH\r\n```\r\nIt's annoying that the autocompletion doesn't work when you want to copy a file to a running container. I don't think this would be overly complicated to solve for someone who understands zsh autocompletion."},{"labels":[null,null,"enhancement",null],"text":"HI, All, \r\n\r\nI found currently docker does not support managing plugin via ID. It is only plugin name can be used. While we need to specify plugin name+tag to make it work.\r\n\r\nI think using ID to manage plugin will be really convenient for users.\r\n\r\nHere is `docker plugin ls` output:\r\n```\r\nroot@10-11-11-54:~# docker plugin ls\r\nNAME                  TAG                 DESCRIPTION                ENABLED\r\ntiborvass/no-remove   latest              A test plugin for Docker   true\r\n```\r\nI wish to have an `ID` column there.\r\n\r\nAnd when I inspect a plugin, it has an `Id` field there:\r\n```\r\nroot@10-11-11-54:~# docker plugin inspect tiborvass/no-remove\r\n[\r\n    {\r\n        \"Config\": {\r\n            ....\r\n        },\r\n        \"Enabled\": true,\r\n        \"Id\": \"9fb474b1a72ef5628da6371f0e573373d5cd63917cc66d0726f9ed6627cbe04c\",\r\n        \"Name\": \"tiborvass/no-remove\",\r\n        ....\r\n    }\r\n]\r\n```\r\n\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nroot@10-11-11-54:~# docker version\r\nClient:\r\n Version:      1.13.0-rc1\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   75fd88b\r\n Built:        Fri Nov 11 19:41:05 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:             1.13.0-rc1\r\n API version:         1.25\r\n Minimum API version: 1.12\r\n Go version:          go1.7.3\r\n Git commit:          75fd88b\r\n Built:               Fri Nov 11 19:41:05 2016\r\n OS/Arch:             linux/amd64\r\n Experimental:        false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,null,"enhancement"],"text":"There's currently no option to deploy a stack that's using private images. We should add a `--with-registry-auth` flag to `docker stack deploy` to allow this (was discussing with @vdemeester).\r\n\r\n/cc @aanand @dnephin @vdemeester "},{"labels":["enhancement",null],"text":"Starting with docker 1.13 we allow newer clients to work with older\r\ndaemon versions, and \"detect\" if experimental features are enabled\r\non the daemon.\r\n\r\nGiven that experimental features can (and in many cases, _will_) change\r\nbefore they leave experimental, this can result in unexpected behavior.\r\n\r\nFor example, if the \"checkpoint / restore\" feature leaves experimental\r\nin 1.14, and a 1.14 client communicates with a 1.13 daemon, it will no\r\nlonger check if experimental features are enabled on the daemon (because\r\naccording to the client, the feature is no longer experimental).\r\n\r\nThe _daemon_ on its side, will allow requests to the \"checkpoint\" API \r\nendpoints if the `--experimental` flag is set.\r\n\r\nI think we should prevent this, for example:\r\n\r\n1. a client should only allow running experimental options if the client\r\n  and daemon versions match\r\n2. a daemon should also only allow access to experimental API endpoints\r\n  if the client has the same version (_actual_ version, not downgrade\r\n  / upgraded API version)\r\n3. if a feature leaves experimental, the minimum API version of the \r\n  related endpoints should be set to the version in which it left experimental.\r\n\r\nI'm not sure if `2.` is possible; does the client request contain information\r\nthat it downgraded / upgraded the API version?\r\n\r\nPerhaps parts of this are already in place, but opening this issue\r\nso that we don't forget checking before GA :smile:\r\n\r\n/cc @vieux\r\n"},{"labels":[null,null,"enhancement",null],"text":"It would be nice to have something like `docker plugin ls --filter enabled=true`.\r\nThis would enable me to refine bash completion for `docker plugin enable` and `docker plugin disable` with official Docker CLI means."},{"labels":[null,null,"enhancement",null],"text":"When pulling images, Docker displays progress information (`docker pull`, implicitly on `docker run`).\r\nAs plugins are images, I would expect `docker plugin install` to display the same information during operation. In fact, it just stalls until the pull is complete.\r\n\r\n```bash\r\nroot@da8d205c7472:~#  docker version\r\nClient:\r\n Version:      1.14.0-dev\r\n API version:  1.26\r\n Go version:   go1.7.3\r\n Git commit:   96f50e9\r\n Built:        Sat Nov 19 15:39:43 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:             1.14.0-dev\r\n API version:         1.26\r\n Minimum API version: 1.12\r\n Go version:          go1.7.3\r\n Git commit:          96f50e9\r\n Built:               Sat Nov 19 15:39:43 2016\r\n OS/Arch:             linux/amd64\r\n Experimental:        false\r\n```"},{"labels":[null,"enhancement"],"text":"This limit can prevent from running out of disk space by any one of applications inside the container using \"dd if=/dev/zero of=/attack bs=1M\", since \"docker run\" already supports this options.\r\n"},{"labels":[null,"enhancement"],"text":"This limit can prevent from OS resource damage by applications inside the container using fork bomb attack.\r\n\r\nAnd this issue can also be easy to implement referencing from \"--limit-memory\".\r\n\r\n=> Setting pids value to: /sys/fs/cgroup/pids/docker/**$container**/pids.max"},{"labels":[null,null,null,"enhancement"],"text":"**Description**\r\n\r\nDocker volume command documentation files do not fully describe expected behavior for contributors writing volume drivers. In particular, volume management in swarm mode can be troublesome for clustered storage systems when determining which node to create volumes and where to report volume list. This can lead to cartesian bugs when creating volumes (every node creates a volume based on the volume create request). Some plugin authors have been confused as to where to report a volume as available depending on how their storage system manages volumes.\r\n\r\n@anusha-ragunathan is aware of this and will be following up with more info.\r\n\r\nCC'ing @thaJeztah @stevvooe @vieux per Anusha's request."},{"labels":[null,null,"enhancement",null],"text":"I've found few references to this issue:\r\n- #15187 \r\n- [at linux foundation.com](https://events.linuxfoundation.org/sites/events/files/slides/User%20Namespaces%20-%20ContainerCon%202015%20-%2016-9-final_0.pdf)\r\n- [and at this blog.](https://integratedcode.us/2015/10/13/user-namespaces-have-arrived-in-docker/)\r\n\r\n but there's no ticket regarding this problem, AFAIK.\r\n\r\nIt would be more than useful to create per-container user namespaces (remapped root UID/GID) in one of my projects. I haven't even found any workaround, creating or changing {U,G}ID doesn't resolve my issues.\r\n\r\nExpected behaviour might be sth like\r\n`docker run -map-uid=$HOSTUID:$CONTAINERUID`\r\n`docker run -map-gid=$HOSTGID:$CONTAINERGID`\r\n\r\n"},{"labels":[null,"enhancement"],"text":"While testing https://github.com/docker/docker/issues/24394, I considered using `--update-max-failure-ratio` (added in https://github.com/docker/docker/pull/26421) to prevent docker from endlessly spinning up faulty containers;\r\n\r\n```\r\ndocker service create --name faulty --update-max-failure-ratio 0.25 nginx:alpine sh\r\n```\r\n\r\nHowever, docker does not `pause` spinning up containers even if 100% fail.\r\n\r\nI realize this was not in the original design for this flag, but was wondering if it would be a nice enhancement; stop starting new tasks/containers if `xx %` of those containers fail.\r\n\r\n/cc @aaronlehmann "},{"labels":[null,"enhancement",null],"text":"\r\n**Description**\r\nI use \"docker secret create secret.json\" command to create a secret named secret.json,  and enter \"mhv17xfe3gh6xc4rij5orpfds\".  But there is no response and no secret ID returns. So I touch a secret.json file with  \"mhv17xfe3gh6xc4rij5orpfds\" within it, and use \"cat secret.json | docker secret create secret.json\" command, then it works and returns the secret ID. \r\n\r\n**Steps to reproduce the issue:**\r\n1. input \"docker secret create secret.json\" command\r\n2. click enter key\r\n3. input \"mhv17xfe3gh6xc4rij5orpfds\" \r\n4. click enter again\r\n\r\n**Describe the results you received:**\r\nThere is no response and no secret ID received.\r\n\r\n**Describe the results you expected:**\r\nI desire the command \"docker secret create secret.json\" could work normally\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.13.0-rc1\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   75fd88b\r\n Built:        Fri Nov 11 19:41:05 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:             1.14.0-dev\r\n API version:         1.26\r\n Minimum API version: 1.12\r\n Go version:          go1.7.3\r\n Git commit:          b1c0b5b\r\n Built:               Fri Nov 18 08:52:29 2016\r\n OS/Arch:             linux/amd64\r\n Experimental:        false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 8\r\n Running: 0\r\n Paused: 0\r\n Stopped: 8\r\nImages: 117\r\nServer Version: 1.14.0-dev\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 109\r\n Dirperm1 Supported: false\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: 2pu8f2j98lqjla2q8i1khyux4\r\n Is Manager: true\r\n ClusterID: x4mgejsryxshqvyasegj1vxkn\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 10.11.11.54\r\n Manager Addresses:\r\n  10.11.11.54:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 8517738ba4b82aff5662c97ca4627e7e4d03b531\r\nrunc version: ac031b5bf1cc92239461125f4c1ffb760522bbf2\r\ninit version: 4a92b9e (expected: 949e6fa)\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.13.0-100-generic\r\nOperating System: Ubuntu 14.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 3.861 GiB\r\nName: 10-11-11-54\r\nID: LJ7Q:CII3:2HUV:5EJK:GEEO:5MD3:36NZ:RLIJ:L3DG:IMI2:NFHC:C7ME\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"enhancement"],"text":"For example I am running a Docker container (within AWS ECS ) which allocates dynamic ports (by specifying servicePort=0), inside my container I need to know the actual hostPort it bounded to.\r\n\r\nI would need host and port information, I could get the host (both public & private) information using 'ec2-instance-metadata' but there is no equivalent information to get the port.\r\n\r\nIn short we need something like this inside the container, outside of container we can get information from docker host like this 'docker inspect --format='{{(index (index .NetworkSettings.Ports \"8080/tcp\") 0).HostPort}}' CONTAINER_NAME'.\r\n\r\nOther platform (like mesosphere etc) have a way ( ex $PORT_8080) to inspect this info, but AWS doesn't have it."},{"labels":[null,"enhancement"],"text":"I want to change the value of net.core.somaxconn and net.core.netdev_max_backlog. Now it is changed by excute sysctl after setting --privileged=true when docker run. but this is fussy,Is any one can help me to save these values in docker image,I hope the value is right when container start."},{"labels":[null,"enhancement"],"text":"```\r\ndocker network inspect success11204\r\n[\r\n    {\r\n        \"Name\": \"success11204\",\r\n        \"Id\": \"79ba293ff4dad75497cb698af925777596d714b79e720664bff5569663ab2a15\",\r\n        \"Created\": \"2016-11-18T01:25:44.508047151Z\",\r\n        \"Scope\": \"global\",\r\n        \"Driver\": \"calico\",\r\n        \"EnableIPv6\": false,\r\n        \"IPAM\": {\r\n            \"Driver\": \"calico-ipam\",\r\n            \"Options\": {},\r\n            \"Config\": [\r\n                {\r\n                    \"Subnet\": \"192.169.0.0/16\"\r\n                }\r\n            ]\r\n        },\r\n        \"Internal\": false,\r\n        \"Attachable\": false,\r\n        \"Containers\": {},\r\n        \"Options\": {},\r\n        \"Labels\": {}\r\n    }\r\n]\r\n```\r\n\r\nbut docker is actually storing more information\r\n```\r\netcdctl get /docker/network/v1.0/network/79ba293ff4dad75497cb698af925777596d714b79e720664bff5569663ab2a15 |jq\r\n{\r\n  \"addrSpace\": \"CalicoGlobalAddressSpace\",\r\n  \"attachable\": false,\r\n  \"created\": \"2016-11-18T01:25:44.508047151Z\",\r\n  \"enableIPv6\": false,\r\n  \"generic\": {\r\n    \"com.docker.network.enable_ipv6\": false,\r\n    \"com.docker.network.generic\": {}\r\n  },\r\n  \"id\": \"79ba293ff4dad75497cb698af925777596d714b79e720664bff5569663ab2a15\",\r\n  \"inDelete\": false,\r\n  \"ingress\": false,\r\n  \"internal\": false,\r\n  \"ipamOptions\": {},\r\n  \"ipamType\": \"calico-ipam\",\r\n  \"ipamV4Config\": \"[{\\\"PreferredPool\\\":\\\"192.169.0.0/16\\\",\\\"SubPool\\\":\\\"\\\",\\\"Gateway\\\":\\\"\\\",\\\"AuxAddresses\\\":null}]\",\r\n  \"ipamV4Info\": \"[{\\\"IPAMData\\\":\\\"{\\\\\\\"AddressSpace\\\\\\\":\\\\\\\"CalicoGlobalAddressSpace\\\\\\\",\\\\\\\"Gateway\\\\\\\":\\\\\\\"0.0.0.0/0\\\\\\\",\\\\\\\"Pool\\\\\\\":\\\\\\\"192.169.0.0/16\\\\\\\"}\\\",\\\"Meta\\\":{\\\"com.docker.network.gateway\\\":\\\"0.0.0.0/0\\\"},\\\"PoolID\\\":\\\"192.169.0.0/16\\\"}]\",\r\n  \"labels\": {},\r\n  \"name\": \"success11204\",\r\n  \"networkType\": \"calico\",\r\n  \"persist\": true,\r\n  \"postIPv6\": false,\r\n  \"scope\": \"global\"\r\n}\r\n```\r\n\r\nNote the PreferredPool and AddressSpace entries.\r\n\r\n@aboch @mavenugo"},{"labels":[null,"enhancement",null],"text":"At the moment the log body is sent over as structPayload.data field\r\n\r\nI would like to be able to configure the driver to send the log body as textPayload field as it is more readable through the Google Logging console.\r\n\r\nRelates to https://github.com/docker/docker/issues/28515 and https://github.com/docker/docker/issues/28508"},{"labels":[null,"enhancement"],"text":""},{"labels":[null,"enhancement"],"text":"Primary and secondary key labels are used by Google UI to provide a structural, drop down menus.\r\n\r\nI would like to be able to set these values to organise the log hierarchy.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/5507517/20369383/cff7e7a0-acac-11e6-94eb-b5b06ad12737.png)\r\n\r\nThe keys are set through labels:\r\n\r\ncustom.googleapis.com/primary_key\r\ncustom.googleapis.com/secondary_key\r\n\r\n\r\n    {'resource': {'type': 'global'}, \r\n     'labels': {'custom.googleapis.com/primary_key': 'DEMOTEST', \r\n            'custom.googleapis.com/secondary_key': 'TEST2'}, \r\n     'logName': 'projects/project/logs/TEST', \r\n     'severity': 'ERROR', 'textPayload': 'Something went wrong'}\r\n\r\nUsing the current labels to set them up does not work. \r\n\r\n    --log-opt labels=custom.googleapis.com/primary_key,location \\ \r\n    --label custom.googleapis.com/primary_key=TESTS \\\r\n\r\nResults in \r\n\r\n![image](https://cloud.githubusercontent.com/assets/5507517/20369550/b4dd17a0-acad-11e6-918b-1d53259f7c2c.png)\r\n\r\nGcloud API example that works:\r\n\r\n    #!/usr/bin/env python\r\n    from gcloud import logging\r\n    logging_client = logging.Client()\r\n    logger = logging_client.logger(\"PYTHON_TEST\")\r\n    labels = {'custom.googleapis.com/primary_key' : 'DEMOTEST','custom.googleapis.com/secondary_key' : 'TEST2' }\r\n    logger.log_text('Something went wrong', severity='ERROR', labels=labels)"},{"labels":[null,"enhancement"],"text":"All logs delivered through gcplogs driver are put into a single bucket - gcplogs-docker-driver.\r\n\r\nAs an administrator over extended docker infrastructure I would like to be able to define custom log name to help isolate logs from different components.\r\n\r\nI believe it is a low hanging fruit - should be easy to pass on an option to replace the hardcoded \"gcplogs-docker-driver\" value. Similar to gcp-log-cmd or gcp-project\r\n"},{"labels":[null,"enhancement"],"text":"https://github.com/docker/docker/pull/26108 is fantastic. However, it doesn't handle this use case: a CI server where I need to keep images around for a couple days for caching, but need to prune them after that. I would like to have a time parameter (in days or hours) to exclude image/container/etc pruning. An LRU system would be ideal, but more technically complicated. For context, I am handling this with the script below. It's a little gross, but gets the job done.\r\n```\r\n#!/bin/bash\r\nfunction check_age(){\r\n    while read cid\r\n    do\r\n        age=$(date -d $(docker inspect -f '{{.Created}}' $cid) +%s)\r\n        if [ $age -lt $TOO_OLD ]; then\r\n            echo \"$cid\"\r\n        fi\r\n    done\r\n}\r\n\r\nget_names(){\r\n    while read cid\r\n    do\r\n        docker inspect -f '{{.RepoTags}}' $cid | tr -d '[]'\r\n    done\r\n}\r\n\r\necho \"cleaning containers\"\r\nTOO_OLD=$(echo \"$(date +%s)-60*60*6\" | bc)\r\ndocker rm -v $(docker ps --filter status=exited -q 2>/dev/null)\r\ndocker rm -f $(docker ps -aq --no-trunc | check_age)\r\n\r\necho \"cleaning images\"\r\ndocker rmi $(docker images --no-trunc -q --filter \"dangling=true\")\r\nTOO_OLD=$(echo \"$(date +%s)-60*60*24*7\" | bc)\r\ndocker rmi $(docker images -q --no-trunc | check_age | get_names)\r\n\r\nexit 0\r\n```"},{"labels":[null,null,"enhancement",null],"text":"**Description**\r\n\r\nThe reason i report this issue is because i encountered a kernel(version 4.6.4) panic when running with docker containers(version 1.10.3). The panic seems like a kernel bug which was fixed and patched in kernel 4.8(https://patchwork.kernel.org/patch/9179461/).  \r\n\r\nExcept for the panic, i also found so many kernel logs like below, i suspect if these mounts caused the kernel bug: \r\n\r\n> EXT4-fs (dm-10): mounted filesystem with ordered data mode. Opts: (null)\r\n> ...\r\n> EXT4-fs (dm-10): mounted filesystem with ordered data mode. Opts: (null)\r\n\r\nSo i decide to investigate the docker code(Version 1.10.3), add some debug and build my own binary and found each time i call container inspect with size, docker daemon will mount the current layer of container(which is already mounted) and mount the base layer of container(which is not mounted usually), calculate the diff size and unmount them. my program will call container inspect every second, so the mount and unmount will be executed for base layer every second.\r\n\r\nFrankly speaking,  such frequent mounts is caused by my frequent container inspect call. but i wonder if this will bring burden to kernel and if there are better ways to calculate the diff size without mount and unmount base layer ? e.g. store the base layer size in a memory map and update it when base layer is updated?\r\n\r\nActually i did not find any relations between many mount operations and kernel bug,  ignore the kernel panic problem if you guys also think so.\r\n\r\n**Steps to reproduce the issue:**\r\n1. call the container inspect with size every second\r\n\r\n**Describe the results you received:**\r\n1. you will find so many mount operations in kernel log\r\n\r\n**Describe the results you expected:**\r\n1. not so many mount operations\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.10.3\r\n API version:  1.22\r\n Go version:   go1.6.2\r\n Git commit:   20f81dd-unsupported\r\n Built:        Tue Nov 15 18:46:33 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.10.3\r\n API version:  1.22\r\n Go version:   go1.6.2\r\n Git commit:   20f81dd-unsupported\r\n Built:        Tue Nov 15 18:46:33 2016\r\n OS/Arch:      linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 2\r\n Running: 2\r\n Paused: 0\r\n Stopped: 0\r\nImages: 2\r\nServer Version: 1.10.3\r\nStorage Driver: devicemapper\r\n Pool Name: docker-thinpool-tpool\r\n Pool Blocksize: 524.3 kB\r\n Base Device Size: 10.74 GB\r\n Backing Filesystem: ext4\r\n Data file:\r\n Metadata file:\r\n Data Space Used: 634.9 MB\r\n Data Space Total: 456.1 GB\r\n Data Space Available: 455.5 GB\r\n Metadata Space Used: 770 kB\r\n Metadata Space Total: 4.798 GB\r\n Metadata Space Available: 4.798 GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: false\r\n Deferred Deletion Enabled: false\r\n Deferred Deleted Device Count: 0\r\n Library Version: 1.02.77 (2012-10-15)\r\nExecution Driver: native-0.2\r\nLogging Driver: json-file\r\nPlugins:\r\n Volume: local\r\n Network: host null\r\nKernel Version: 4.6.4-040604-generic\r\nOperating System: Ubuntu 14.04.5 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 40\r\nTotal Memory: 251.8 GiB\r\nName: nq79\r\nID: FT5L:U3O3:4OZV:THVT:7GYC:TJFE:BBXD:2M3U:I7IN:XMEY:3IBM:S5WM\r\nDebug mode (server): true\r\n File Descriptors: 22\r\n Goroutines: 30\r\n System Time: 2016-11-16T11:27:15.488018082+08:00\r\n EventsListeners: 0\r\n Init SHA1: 85330d6c18d3ef97dd88b6f4cb58d75b661f72cb\r\n Init Path:\r\n Docker Root Dir: /var/lib/docker\r\nWARNING: No memory limit support\r\nWARNING: No swap limit support\r\nWARNING: No oom kill disable support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nNo\r\n"},{"labels":[null,"enhancement",null,null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nThe output from docker info includes a section *Plugins*, within which there is *Network*. Typically this shows four items: *bridge null host overlay* (in no particular order); but on my docker installations it only shows three: *bridge null host* .\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\nPlugins:\r\n Volume: local\r\n Network: bridge null host\r\n```\r\n\r\n**Describe the results you expected:**\r\n```\r\nPlugins:\r\n Volume: local\r\n Network: bridge null host overlay\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n```\r\nClient:\r\n Version:         1.10.3\r\n API version:     1.22\r\n Package version: docker-common-1.10.3-46.el7.centos.10.x86_64\r\n Go version:      go1.6.3\r\n Git commit:      d381c64-unsupported\r\n Built:           Thu Aug  4 13:21:17 2016\r\n OS/Arch:         linux/amd64\r\n\r\nServer:\r\n Version:         1.10.3\r\n API version:     1.22\r\n Package version: docker-common-1.10.3-46.el7.centos.10.x86_64\r\n Go version:      go1.6.3\r\n Git commit:      d381c64-unsupported\r\n Built:           Thu Aug  4 13:21:17 2016\r\n OS/Arch:         linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 3\r\n Running: 3\r\n Paused: 0\r\n Stopped: 0\r\nImages: 3\r\nServer Version: 1.10.3\r\nStorage Driver: devicemapper\r\n Pool Name: docker-253:2-395657-pool\r\n Pool Blocksize: 65.54 kB\r\n Base Device Size: 10.74 GB\r\n Backing Filesystem: xfs\r\n Data file: /dev/loop0\r\n Metadata file: /dev/loop1\r\n Data Space Used: 644.3 MB\r\n Data Space Total: 107.4 GB\r\n Data Space Available: 7.127 GB\r\n Metadata Space Used: 1.552 MB\r\n Metadata Space Total: 2.147 GB\r\n Metadata Space Available: 2.146 GB\r\n Udev Sync Supported: true\r\n Deferred Removal Enabled: false\r\n Deferred Deletion Enabled: false\r\n Deferred Deleted Device Count: 0\r\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\r\n WARNING: Usage of loopback devices is strongly discouraged for production use. Either use `--storage-opt dm.thinpooldev` or use `--storage-opt dm.no_warn_on_loop_devices=true` to suppress this warning.\r\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\r\n Library Version: 1.02.107-RHEL7 (2016-06-09)\r\nExecution Driver: native-0.2\r\nLogging Driver: json-file\r\nPlugins:\r\n Volume: local\r\n Network: bridge null host\r\nKernel Version: 3.10.0-327.28.3.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nNumber of Docker Hooks: 2\r\nCPUs: 2\r\nTotal Memory: 31.26 GiB\r\nName: webdmz8-10-250-130-124\r\nID: 2FHN:3AEV:SPGX:QHHJ:34HL:A3MW:LGR5:MCPB:D7HQ:6CPQ:A4WV:WLVX\r\nWARNING: bridge-nf-call-iptables is disabled\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\nRegistries: docker.io (secure)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nA set of virtual machines each running CentOS 7.0; the same problem is observed on all seven.\r\n\r\nNote: I understand that the [overlay driver should now be supported on kernel 3.10](https://github.com/docker/docker/releases/tag/v1.10.0) however there might be some remaining incompatibility problem?\r\n\r\n"},{"labels":[null,"enhancement"],"text":"When you use service update --image, it would be nice if there were an option for docker to delete the old image once the service has updated.  Cleaning out old images from a bunch of nodes is going to get painful."},{"labels":[null,null,"enhancement"],"text":"We need the ability to map a port to a specific container for a service running in swarm mode.  This would allow debugging / monitoring a specific instance of a service.  Consider the case where a service instance is consuming 100% CPU and the other instances are using almost no CPU.  It would be great to be able to remote attach a tool to that specific container to monitor the CPU usage, memory, threads, take a profiling trace, etc."},{"labels":[null,"enhancement"],"text":"## what \r\nIt would be helpful if an action could be triggered when a service status is unhealthy.\r\n\r\n## why\r\nIt would be sane trigger an action for a container when a status is unhealthy. Tying this into the restart policy would make sense. \r\n\r\nCurrent alternatives appear to be listening to event changes from a third part script. That doesn't support simple scaling.\r\n\r\n## ideal configuration\r\ncombining \r\n* HEALTHCHECK command to toggle the health status\r\n* restart argument passed with `on-failure:N` that will restart the container once `failStreak` is N"},{"labels":[null,null,"enhancement"],"text":"in addition to the currently supported archive formats zip, bz2 and tar.gz, it would be great to add support for the free and open ZSTD compression recently open sourced by Facebook.\r\nThe format provides excellent compression and speed, and is already supported by a variety of standard tools.\r\n\r\n[zstd on Github](https://github.com/facebook/zstd)\r\n[related article](https://code.facebook.com/posts/1658392934479273/smaller-and-faster-data-compression-with-zstandard/)"},{"labels":[null,null,"enhancement",null],"text":"**Description**\r\n\r\nHard to protect Docker Swarm Mode node publish, port always binds to 0.0.0.0.\r\nAccording to [docker service create](https://docs.docker.com/engine/reference/commandline/service_create/) and [Use swarm mode routing mesh](https://docs.docker.com/engine/swarm/ingress/) there are two options for publishing service `--publish <PUBLISHED-PORT>:<TARGET-PORT>`.\r\nAccording to [dockerd options](https://docs.docker.com/engine/reference/commandline/dockerd/) there is an option:\r\n```--ip=0.0.0.0                           Default IP when binding container ports```\r\nChanging `--ip` option for `dockerd` has no impact on `--publish` option of `docker service create` command.\r\nAlthough, there is no explicit indication that swarm mode publish port should obey `--ip` option of `dockerd`, implicitly it would be expected.\r\n\r\n**Steps to reproduce the issue:**\r\n1. configure dockerd to start with `--ip=127.0.0.1` option specified\r\n2. create docker swarm\r\n3. create docker service with `--publish 80:80` option specified\r\n\r\n**Describe the results you received:**\r\nRegardless of `--ip` option in `dockerd`, published service always gets bound to 0.0.0.0:\r\n```\r\ntcp6       0      0 :::80                   :::*                    LISTEN      6110/dockerd    \r\n``` \r\n\r\n**Describe the results you expected:**\r\nSince `--ip=127.0.0.1` option specified in `dockerd` it is expected that service is bound to 127.0.0.1 like:\r\n```\r\ntcp       0      0 127.0.0.1:80                   0.0.0.0:*                    LISTEN      6110/dockerd    \r\n``` \r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nThere are also a number of other issues (like #26696) reported for the similar case where users want to have service `--publish`ed port bind to specific ip/interface, I didn't find similar point of view as explained above.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.12.3\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   6b644ec\r\n Built:        Wed Oct 26 22:01:48 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.12.3\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   6b644ec\r\n Built:        Wed Oct 26 22:01:48 2016\r\n OS/Arch:      linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 1\r\n Paused: 0\r\n Stopped: 0\r\nImages: 35\r\nServer Version: 1.12.3\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 34\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: overlay bridge null host\r\nSwarm: active\r\n NodeID: 50b50557xu7l6fexck2mni7tx\r\n Is Manager: true\r\n ClusterID: 3bedltspub354myauu2qpy4o9\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 127.0.0.1\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: apparmor seccomp\r\nKernel Version: 4.4.0-47-generic\r\nOperating System: Ubuntu 16.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 7.678 GiB\r\nName: lab1\r\nID: 63ZL:DTO7:J5ZM:EQRI:W6CT:XAMD:OKH7:Q6QB:SX3S:V6MJ:MGBC:BEDT\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: muradm\r\nRegistry: https://index.docker.io/v1/\r\nInsecure Registries:\r\n 127.0.0.0/8\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nUbuntu 16.04"},{"labels":["enhancement",null],"text":"**Description**\r\n\r\n\"docker rm -f\" does not remove the container but cause error:\r\n\r\n```\r\nError response from daemon: Could not kill running container 0859eb46e6f5e76161b06f12678bd95c1b55dde8c6f4645ca660faa6da201450, cannot remove - Container 0859eb46e6f5e76161b06f12678bd95c1b55dde8c6f4645ca660faa6da201450 is paused. Unpause the container before stopping\r\n```\r\n\r\nYes, I can \"unpause\" before killing the container. But, it is quite nice if \"docker rm -f\" is the final command to remove the container. Also, strictly speaking, even if I call \"docker unpause\" just before \"docker rm -f\", Someone may \"unpause\" the container between those commands.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Run a container.\r\n\r\n```\r\n# docker run -d ubuntu sleep 1000\r\nacf0571d8fdc581a61428b7e9312b12f378d4d649a55cd6e0c199c597a53596e\r\n```\r\n\r\n2. Pause the container\r\n```\r\n# docker pause acf0571d8fdc581a61428b7e9312b12f378d4d649a55cd6e0c199c597a53596e\r\nacf0571d8fdc581a61428b7e9312b12f378d4d649a55cd6e0c199c597a53596e\r\n```\r\n\r\n3. Remove the container\r\n```\r\n# docker rm -f acf0571d8fdc581a61428b7e9312b12f378d4d649a55cd6e0c199c597a53596e\r\nError response from daemon: Could not kill running container acf0571d8fdc581a61428b7e9312b12f378d4d649a55cd6e0c199c597a53596e, cannot remove - Container acf0571d8fdc581a61428b7e9312b12f378d4d649a55cd6e0c199c597a53596e is paused. Unpause the container before stopping\r\n```\r\n\r\n**Describe the results you received:**\r\nError message:\r\n\r\n```\r\nError response from daemon: Could not kill running container acf0571d8fdc581a61428b7e9312b12f378d4d649a55cd6e0c199c597a53596e, cannot remove - Container acf0571d8fdc581a61428b7e9312b12f378d4d649a55cd6e0c199c597a53596e is paused. Unpause the container before stopping\r\n```\r\n\r\n\r\n**Describe the results you expected:**\r\nRemoving the container\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.12.2\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   bb80604\r\n Built:        Tue Oct 11 18:29:41 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.12.2\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   bb80604\r\n Built:        Tue Oct 11 18:29:41 2016\r\n OS/Arch:      linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 16\r\n Running: 13\r\n Paused: 1\r\n Stopped: 2\r\nImages: 4\r\nServer Version: 1.12.2\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 150\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: apparmor seccomp\r\nKernel Version: 4.4.0-43-generic\r\nOperating System: Ubuntu 16.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 3.858 GiB\r\nName: paizaterm-prod4\r\nID: 757W:AQM2:WNUV:CQSJ:URNH:5I5W:65OW:4DXQ:QYLV:M57U:P5AG:WBAV\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nLabels:\r\n provider=generic\r\nInsecure Registries:\r\n 127.0.0.0/8\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nAWS\r\n\r\n"},{"labels":[null,"enhancement"],"text":"Hi, All,\r\n\r\nRecently, when I do more experience with my colleague, he told me that **If a user can assign outer port range on host.** \r\n\r\nBy default, docker daemon automatically assigns port from 32768. For example like below:\r\n```\r\ndocker run -d -P nginx:latest sleep 100000\r\n62697ba1a6e1ef79eee3eaabc4efa50ec985da13382a9a7151a7eef1c3452235\r\nroot@10-11-11-54:~# docker ps\r\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                                           NAMES\r\n62697ba1a6e1        nginx               \"sleep 100000\"      12 hours ago        Up 12 hours         0.0.0.0:32769->80/tcp, 0.0.0.0:32768->443/tcp   cranky_minsk\r\n```\r\n\r\nSome need, for example, a user can specify automatic port range is 30000-35000. Since in some enterprise cases, some port range is reserved for some reason. And this feature request **WILL NOT** influence docker daemon's back compatibility, and enhance the port management for host instead.\r\n\r\nFeature Request:\r\n**User can define automatic port range for container creating**"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nThe [release.sh](https://github.com/docker/docker/blob/e6866492c4492db3bb9546afa2fbaed20f4c1883/hack/release.sh#L217) script used during docker's release process is failing because it cannot handle solaris builds.\r\n\r\n**Steps to reproduce the issue:**\r\n```\r\n$ docker run ... docker-dev:1-13-x hack/release.sh\r\n...\r\nerror: can't convert solaris to an appropriate value for 'uname -s'\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\nMost of the binaries uploaded to s3 except for solaris and windows.\r\n\r\n**Describe the results you expected:**\r\n\r\nStuff to work.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\nsomething recent\r\n\r\n**Output of `docker info`:**\r\n\r\nstuff\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":["enhancement",null],"text":"Without `-i`, stdin doesn't work with `docker run` and `docker exec`:\r\n\r\n`docker run -i alpine cat < /etc/hostname` prints the hostname of the host\r\n`docker run alpine cat < /etc/hostname` does nothing\r\n\r\nI wonder if there is any situation where you would *not* want to specify `-i`, otherwise it should just be the default (and the flag removed)."},{"labels":[null,"enhancement"],"text":"If we update a service's image, e.g.\r\n\r\n```\r\ndocker service update --image <someurl>\r\n```\r\nit will first stop the service, then attempt to download the image.  \r\n\r\nHowever, our registry is in the US and our docker services are in manila, so depending on the day's routes in Asia, sometimes this can unpredictably become extremely slow..\r\n\r\nIt would be better if the update would first download the image and then start updating services to minimize the downtime...\r\n\r\n\r\n\r\n"},{"labels":[null,null,"enhancement"],"text":"These were part of the 1.12 cli, but never implemented in the daemon, so the CLI options were taken out in https://github.com/docker/docker/pull/25646.\r\n\r\nHaving this feature implemented would be great."},{"labels":[null,"enhancement"],"text":"the `docker tag` command has always confused me, because the two arguments can be easily mixed resulting wrong tag overwrite, because docker client doesn't even complain that tag exists.\r\n\r\nthe usage, printed is not useful at all indicating which of the arguments is SOURCE and which is TARGET tag, i.e which one would be REFERENCED and which one will be CREATED.\r\n\r\n```\r\n$ docker tag --help\r\n\r\nUsage:  docker tag IMAGE[:TAG] IMAGE[:TAG]\r\n\r\nTag an image into a repository\r\n\r\nOptions:\r\n      --help   Print usage\r\n\r\n```\r\n\r\nSo to get definitive answer one has to visit [reference documentation](https://docs.docker.com/engine/reference/commandline/tag/#/examples) or just experiment which command works. that is not productive at all.\r\n\r\ni think the major confusion cames due the fact that the arguments are reversed from `git(1)` usage.\r\n\r\n```\r\n       git tag [-a | -s | -u <keyid>] [-f] [-m <msg> | -F <file>]\r\n               <tagname> [<commit> | <object>]\r\n```\r\n\r\nProposed solutions:\r\n- rename arguments in usage to `REFERENCE[:TAG] IMAGE[:TAG]`\r\n- reject tag creation if tag already exists, unless `-f` option added\r\n- add flag to tag command to reject tag creation if exists (opposite of previous suggestion)"},{"labels":["enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nAs we all know, docker is getting bigger and bigger. In some scenarios, we hope to get a smaller docker binary, because the resource is limited, especially embedded systems.\r\n\r\nInspired by the Linux kernel, how about to make the docker be **customized** and **configurable** before compile.\r\n* some core functions are always compiled into docker.\r\n* some optional features can be configurable, if people do not want them, they can disable them and do not compile them.(we can use the go build tag to implement this goal)\r\n\r\n**Any ideas are welcome**\r\n\r\n"},{"labels":[null,"enhancement"],"text":"Now that we are on our way to changing the experimental features from a compile time option to a runtime configuration with #26713, there is still a few more items we need to take care of before release.\r\n\r\nIn the upcoming 1.13.0 release, there will NOT be a separate experimental binary release.\r\n\r\n- [ ] update docs explaining all the ways to enable experimental in configuration (command line option and `daemon.json`) and how to tell if they are enabled, e.g. `docker version`, and how it will not be enable-able via live reload\r\n- [ ] update changelog about experimental changes\r\n- [ ] update [deprecation docs](https://github.com/docker/docker.github.io/blob/949a89e6ea4d47e8306451d84752654d18c5253b/engine/deprecated.md) about experimental changes\r\n- [ ] update experimental [instructions](https://experimental.docker.com/builds/) for manual install of static binaries"},{"labels":[null,null,"enhancement"],"text":"Suppose that `--with-registry-auth` was accidentally specified when creating a service which uses a public image. The operator may want to remove the registry credentials from the service. However, there doesn't appear to be a way to do this. I expected that logging out and then updating the service with `--with-registry-auth` would clear the authentication credentials, but this does not seem to be the case. If no credentials are provided by the client, the existing credentials are reused:\r\n\r\n```go\r\n        if encodedAuth != \"\" {\r\n                ...\r\n        } else {\r\n                // this is needed because if the encodedAuth isn't being updated then we\r\n                // shouldn't lose it, and continue to use the one that was already present\r\n                ...\r\n                serviceSpec.Task.GetContainer().PullOptions = ctnr.PullOptions\r\n```"},{"labels":[null,"enhancement"],"text":"With the addition of Solaris (see #28056), a few routine were copied (almost) verbatim into the `_solaris.go` files.\r\n\r\nThis can end up being problematic as we may forget to update the Solaris version when making update to those routines as we have no CI running on Solaris (yet?).\r\n\r\nThis issue is here to prevent us to forget that these need to be deduplicated :)"},{"labels":[null,"enhancement"],"text":"The bridge libnetwork driver allows use of networks based on bridges as networks within Docker. I would be nice to be also able to attach a Docker network to a pre-existing bridge without affecting the configuration of that bridge."},{"labels":[null,"enhancement"],"text":"I'd like the API **container commit** event to include the ID of the image it created. \r\n\r\nI have a small util which tracks how recently docker images have been used, and cleans up dangling images after some time if they have not been reused.\r\nTo do this I need to keep track of image creation, however layers created by docker build come from container commit events, which do not tell you the ID of the image.\r\n "},{"labels":[null,null,"enhancement"],"text":"It would be amazing if Docker would only bind to the ports published by services on the hosts that meet the constraints for that service (maybe make it a flag to do so?).\r\n\r\nExample:\r\n- I have X hosts that are a \"management layer\". They run the load balancers and other stuff. Load balancers are constrained to these nodes by node labels.\r\n- I have Y hosts that are an \"application layer\". They run app servers, no ports published here via the mesh network, load balancers connect to them via the overlay networks. App services only run on these hosts by constraints via node labels.\r\n- I have Z hosts that are a \"DB layer\" which have access to special distributed storage, etc. DB services only run on these hosts by constraints via node labels.\r\n\r\nIt sounds reasonable that Docker would only listen for ports published by the load balancers only on the \"management\" nodes and for ports published by the databases on their respective nodes."},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\nPer the discussion on https://github.com/docker/docker/pull/27958#issuecomment-258598258, I think we should rename this flag, to keep consistency. Opening a new issue to track the discussion on this\r\n\r\nping @crosbymichael @tiborvass @yongtang @albers "},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nDuring push docker interprets 404 responses and allows a registry to return a custom message to the user, but during login it does not. It simply prints `Error response from daemon: login attempt to http://172.17.0.1/v2/ failed with status: 404 Not Found`\r\n\r\nMore specifically, during push the error message looks like:\r\n\r\n```\r\nerror parsing HTTP 404 response body: invalid character 'C' looking for beginning of value: \"Custom error information here\\n\"\r\n```\r\nThat's good enough for us, but that could also be improved.\r\n\r\n**Steps to reproduce the issue:**\r\n1. create a docker registry with a custom 404 page that returns a hint as to why the registry is not accessible\r\n2. try to docker login into the registry\r\n\r\n**Describe the results you received:**\r\n```\r\nError response from daemon: login attempt to http://172.17.0.1/v2/ failed with status: 404 Not Found\r\n```\r\n\r\n**Describe the results you expected:**\r\n```\r\nError response from daemon: login attempt to http://172.17.0.1/v2/ failed with status: 404 Not Found and body: Please do XYZ to configure the registry before using it.\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n$ docker version\r\nClient:\r\n Version:      1.12.2-cs2\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   d5fda6e\r\n Built:        Sun Oct 23 07:48:43 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.0-dev\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   17aaa08-unsupported\r\n Built:        Tue Nov  1 04:28:42 2016\r\n OS/Arch:      linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n$ docker info\r\nContainers: 31\r\n Running: 17\r\n Paused: 0\r\n Stopped: 14\r\nImages: 280\r\nServer Version: 1.13.0-dev\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: q0i8qz897t67xphfahdlz2sa3\r\n Is Manager: true\r\n ClusterID: k33d8hpd40dri5jiukx2zgxze\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  External CAs:\r\n    cfssl: https://172.17.0.1:12381/api/v1/cfssl/sign\r\n Node Address: 172.17.0.1\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: apparmor seccomp\r\nKernel Version: 4.8.3\r\nOperating System: NixOS 17.03.git.210b3b3 (Gorilla)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 15.53 GiB\r\nName: thisisfine\r\nID: 4XB6:RAOL:2UT7:W3TT:JBSR:ILHC:LREA:HUVU:5U7V:TI7E:WUT2:WQTS\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 203\r\n Goroutines: 346\r\n System Time: 2016-11-04T16:40:28.015262135-07:00\r\n EventsListeners: 2\r\nUsername: viktorstanchev\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No cpu cfs quota support\r\nWARNING: No cpu cfs period support\r\nInsecure Registries:\r\n 172.17.0.1\r\n 127.0.0.0/8\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nPhysical"},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\nWhen creating a volume using a volume driver that has options, there is no way to inspect the volume to see additional options.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a volume with options:\r\n```\r\n$ docker volume create --driver local --opt type=nfs --opt o=addr=localhost,rw --opt device=:/data/myapp --name foo\r\n```\r\n2. Inspect the volume:\r\n```\r\n$ docker volume inspect foo\r\n[\r\n    {\r\n        \"Name\": \"foo\",\r\n        \"Driver\": \"local\",\r\n        \"Mountpoint\": \"/var/lib/docker/volumes/foo/_data\",\r\n        \"Labels\": {},\r\n        \"Scope\": \"local\"\r\n    }\r\n]\r\n```\r\n\r\n**Describe the results you received:**\r\nI receive the above info.\r\n\r\n**Describe the results you expected:**\r\nI'd expect to be able to get the addition options specified.  I have no way to inspect the volume to see that it actually is backed by a NFS server.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n$ docker version\r\nClient:\r\n Version:      1.12.1\r\n API version:  1.24\r\n Go version:   go1.7.1\r\n Git commit:   6f9534c\r\n Built:        Thu Sep  8 10:31:18 2016\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.12.1\r\n API version:  1.24\r\n Go version:   go1.6.3\r\n Git commit:   23cf638\r\n Built:        Thu Aug 18 17:52:38 2016\r\n OS/Arch:      linux/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n$ docker info\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 79\r\nServer Version: 1.12.1\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 350\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge null host overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: seccomp\r\nKernel Version: 4.4.20-moby\r\nOperating System: Alpine Linux v3.4\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 1.953 GiB\r\nName: moby\r\nID: ALCY:J7RH:6BAJ:WTXS:ZALF:HRZC:QA4R:5E4I:TGAB:CNPP:O4U2:4NQ7\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 17\r\n Goroutines: 29\r\n System Time: 2016-11-04T15:41:26.497104057Z\r\n EventsListeners: 1\r\nNo Proxy: *.local, 169.254/16\r\nUsername: mbentley\r\nRegistry: https://index.docker.io/v1/\r\nInsecure Registries:\r\n 127.0.0.0/8\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nDocker for Mac, CS engine, OSS engine - all same behavior."},{"labels":["enhancement"],"text":"Recently I am building kubernetes on mips64el platform and getting the following error. Through googling , I find the source from https://github.com/docker/docker/blob/master/pkg/system/stat_linux.go line 13. Any suggestions for me？\r\n\r\n**The error info：**\r\n```\r\ntest/e2e_node/e2e_node.test\r\nk8s.io/kubernetes/vendor/github.com/google/cadvisor/fs\r\nk8s.io/kubernetes/vendor/github.com/docker/docker/pkg/system\r\nk8s.io/kubernetes/vendor/github.com/google/certificate-transparency/go/x509\r\n# k8s.io/kubernetes/vendor/github.com/docker/docker/pkg/system\r\nvendor/github.com/docker/docker/pkg/system/stat_linux.go:13: cannot use s.Rdev (type uint32) as type uint64 in field value\r\n# k8s.io/kubernetes/vendor/github.com/google/cadvisor/fs\r\nvendor/github.com/google/cadvisor/fs/fs.go:422: cannot use buf.Dev (type uint32) as type uint64 in argument to major\r\nvendor/github.com/google/cadvisor/fs/fs.go:423: cannot use buf.Dev (type uint32) as type uint64 in argument to minor\r\n# k8s.io/kubernetes/vendor/github.com/google/certificate-transparency/go/x509\r\nvendor/github.com/google/certificate-transparency/go/x509/x509.go:342: undefined: elliptic.P224\r\nvendor/github.com/google/certificate-transparency/go/x509/x509.go:355: undefined: elliptic.P224\r\nvendor/github.com/google/certificate-transparency/go/x509/x509.go:1461: undefined: elliptic.P224\r\nMakefile:79: recipe for target 'all' failed\r\nmake: *** [all] Error 1\r\n\r\n```\r\n**segment of the source file **stat_linux.go：**\r\n```\r\n// fromStatT converts a syscall.Stat_t type to a system.Stat_t type\r\nfunc fromStatT(s *syscall.Stat_t) (*StatT, error) {\r\n\treturn &StatT{size: s.Size,\r\n\t\tmode: s.Mode,\r\n\t\tuid:  s.Uid,\r\n\t\tgid:  s.Gid,\r\n\t\trdev: s.Rdev,\r\n\t\tmtim: s.Mtim}, nil\r\n}\r\n```\r\n**my platform info**\r\n\r\n```\r\n[root@dscn2 kubernetes]# uname -a\r\nLinux dscn2 4.4.21+ #3 SMP PREEMPT Wed Oct 12 09:14:23 CST 2016 mips64 mips64 mips64 GNU/Linux\r\n[root@dscn2 kubernetes]# go env\r\nGOARCH=\"mips64le\"\r\nGOBIN=\"\"\r\nGOEXE=\"\"\r\nGOHOSTARCH=\"mips64le\"\r\nGOHOSTOS=\"linux\"\r\nGOOS=\"linux\"\r\nGOPATH=\"/root/mygo\"\r\nGORACE=\"\"\r\nGOROOT=\"/usr/lib/golang\"\r\nGOTOOLDIR=\"/usr/lib/golang/pkg/tool/linux_mips64le\"\r\nCC=\"gcc\"\r\nGOGCCFLAGS=\"-fPIC -mabi=64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build115989482=/tmp/go-build -gno-record-gcc-switches\"\r\nCXX=\"g++\"\r\nCGO_ENABLED=\"1\"\r\n\r\n```\r\n"},{"labels":[null,null,null,"enhancement"],"text":"We could benefit from revisiting our existing tests for security options (capabilities, seccomp, apparmor, etc.) similar to https://github.com/docker/docker/pull/27634.  Filing this as a tracking issue, in some scenarios it may be worthwhile to port the tests to the runtime (runc) as well.\r\n\r\ncc @justincormack \r\n"},{"labels":[null,"enhancement"],"text":"Looks like starting docker container with **-ti** sets terminal settings while processes inside container are already up and running. This causes some programs (like **rlwrap**) to fail due to the uninitialized terminal settings.\r\n\r\nIf you put **sleep** even for a very short period of time in front of the command, terminal settings become initialized properly.\r\n\r\nThis issue was reproduced on Ubuntu and MacOS. However, since this is a race condition, it depends on system speed and current load.\r\n\r\nConfiguration:\r\n```\r\n>uname -a\r\nLinux ldev1 4.4.0-45-generic #66-Ubuntu SMP Wed Oct 19 14:12:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n>docker -v\r\nDocker version 1.12.1, build 23cf638\r\n```\r\n\r\n**rlwrap** works only after sleep command:\r\n```\r\n>docker run -ti ubuntu_with_rlwrap bash -c \"rlwrap ls\"\r\nrlwrap: error: My terminal reports width=0 (is it emacs?)  I can't handle this, sorry!\r\n>docker run -ti ubuntu_with_rlwrap bash -c \"sleep 0.0001;rlwrap ls\"\r\nbin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\r\n```\r\n\r\nRunning **stty** 3 times in a row, with only the last run showing correct terminal setting:\r\n```\r\n>docker run -ti ubuntu bash -c \"stty -a;stty -a;sleep 0.01;stty -a\"\r\nspeed 38400 baud; rows 0; columns 0; line = 0;\r\nintr = ^C; quit = ^\\; erase = ^?; kill = ^U; eof = ^D; eol = <undef>;\r\neol2 = <undef>; swtch = <undef>; start = ^Q; stop = ^S; susp = ^Z; rprnt = ^R;\r\nwerase = ^W; lnext = ^V; discard = ^O; min = 1; time = 0;\r\n-parenb -parodd -cmspar cs8 -hupcl -cstopb cread -clocal -crtscts\r\n-ignbrk -brkint -ignpar -parmrk -inpck -istrip -inlcr -igncr icrnl ixon -ixoff\r\n-iuclc -ixany -imaxbel -iutf8\r\nopost -olcuc -ocrnl onlcr -onocr -onlret -ofill -ofdel nl0 cr0 tab0 bs0 vt0 ff0\r\nisig icanon iexten echo echoe echok -echonl -noflsh -xcase -tostop -echoprt\r\nechoctl echoke -flusho -extproc\r\nspeed 38400 baud; rows 0; columns 0; line = 0;\r\nintr = ^C; quit = ^\\; erase = ^?; kill = ^U; eof = ^D; eol = <undef>;\r\neol2 = <undef>; swtch = <undef>; start = ^Q; stop = ^S; susp = ^Z; rprnt = ^R;\r\nwerase = ^W; lnext = ^V; discard = ^O; min = 1; time = 0;\r\n-parenb -parodd -cmspar cs8 -hupcl -cstopb cread -clocal -crtscts\r\n-ignbrk -brkint -ignpar -parmrk -inpck -istrip -inlcr -igncr icrnl ixon -ixoff\r\n-iuclc -ixany -imaxbel -iutf8\r\nopost -olcuc -ocrnl onlcr -onocr -onlret -ofill -ofdel nl0 cr0 tab0 bs0 vt0 ff0\r\nisig icanon iexten echo echoe echok -echonl -noflsh -xcase -tostop -echoprt\r\nechoctl echoke -flusho -extproc\r\nspeed 38400 baud; rows 67; columns 198; line = 0;\r\nintr = ^C; quit = ^\\; erase = ^?; kill = ^U; eof = ^D; eol = <undef>; eol2 = <undef>; swtch = <undef>; start = ^Q; stop = ^S; susp = ^Z; rprnt = ^R; werase = ^W; lnext = ^V; discard = ^O;\r\nmin = 1; time = 0;\r\n-parenb -parodd -cmspar cs8 -hupcl -cstopb cread -clocal -crtscts\r\n-ignbrk -brkint -ignpar -parmrk -inpck -istrip -inlcr -igncr icrnl ixon -ixoff -iuclc -ixany -imaxbel -iutf8\r\nopost -olcuc -ocrnl onlcr -onocr -onlret -ofill -ofdel nl0 cr0 tab0 bs0 vt0 ff0\r\nisig icanon iexten echo echoe echok -echonl -noflsh -xcase -tostop -echoprt echoctl echoke -flusho -extproc"},{"labels":[null,"enhancement",null],"text":"**Description**\r\n\r\n`docker service ls` outputs full text of COMMAND by default. It impacts readability when COMMAND is long.\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\ndchen@vm2:~/$ docker service ls\r\nID            NAME                 REPLICAS  IMAGE                      COMMAND\r\nlfsyu3jnfi64  test                 1/1       dongluochen/simpleweb:0.5  simpleweb -e xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nTruncate output of command.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nGood to follow `docker ps` and provide `--no-trunc` option. \r\n\r\n"},{"labels":[null,"enhancement"],"text":"Authz plugins should be able to optionally specify a min and max supported Docker API version since the API can introduce new calls and deprecate old ones.\r\n\r\nOne proposal is to have the daemon call `GetCapabilities()` on the plugin during registration time, and have the plugin respond with `API_MIN_VERSION, API_MAX_VERSION` back.\r\n\r\ncc @anusha-ragunathan @tonistiigi @liron-l \r\n\r\nRelated to https://github.com/docker/docker/issues/21927"},{"labels":[null,"enhancement",null],"text":"\r\n**Description**\r\n\r\nDefault logging config for docker is \"until my computer runs out of disk space\" \r\n\r\nWhen doing some performance testing I noticed that after a couple weeks my services were crashing. After digging into the issue I noticed that some of my servers were out of disk space. \r\n\r\nI know you can set log-opt manually to these parameters, in daemon config. I think it might be safer out of the box to just do that instead of growing unbounded. Thoughts?\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a very small server (disk space)\r\n2. Install docker with no config changes\r\n3. Run a container that generates a lot of logs\r\n3. wait until there's no disk space left\r\n\r\n**Describe the results you received:**\r\n\r\nIssues running commands on server (cause there is no disk space left)\r\n\r\n**Describe the results you expected:**\r\n\r\nSane defaults for logging configs to prevent docker from consuming the entirety of available disk space\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n    Client:\r\n     Version:      1.12.3-cs3\r\n     API version:  1.24\r\n     Go version:   go1.6.3\r\n     Git commit:   781bb16\r\n     Built:        Thu Oct 27 17:45:19 2016\r\n     OS/Arch:      linux/amd64\r\n\r\n    Server:\r\n     Version:      1.12.3-cs3\r\n     API version:  1.24\r\n     Go version:   go1.6.3\r\n     Git commit:   781bb16\r\n     Built:        Thu Oct 27 17:45:19 2016\r\n     OS/Arch:      linux/amd64```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n    Containers: 0\r\n     Running: 0\r\n     Paused: 0\r\n     Stopped: 0\r\n    Images: 11\r\n    Server Version: 1.12.3-cs3\r\n    Storage Driver: devicemapper\r\n     Pool Name: docker-202:1-149203-pool\r\n     Pool Blocksize: 65.54 kB\r\n     Base Device Size: 10.74 GB\r\n     Backing Filesystem: ext4\r\n     Data file: /dev/loop0\r\n     Metadata file: /dev/loop1\r\n     Data Space Used: 697.2 MB\r\n     Data Space Total: 107.4 GB\r\n     Data Space Available: 6.352 GB\r\n     Metadata Space Used: 1.638 MB\r\n     Metadata Space Total: 2.147 GB\r\n     Metadata Space Available: 2.146 GB\r\n     Thin Pool Minimum Free Space: 10.74 GB\r\n     Udev Sync Supported: true\r\n     Deferred Removal Enabled: false\r\n     Deferred Deletion Enabled: false\r\n     Deferred Deleted Device Count: 0\r\n     Data loop file: /var/lib/docker/devicemapper/devicemapper/data\r\n     WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\r\n     Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\r\n     Library Version: 1.02.77 (2012-10-15)\r\n    Logging Driver: json-file\r\n    Cgroup Driver: cgroupfs\r\n    Plugins:\r\n     Volume: local\r\n     Network: bridge overlay null host\r\n    Swarm: inactive\r\n    Runtimes: runc\r\n    Default Runtime: runc\r\n    Security Options: apparmor\r\n    Kernel Version: 3.13.0-96-generic\r\n    Operating System: Ubuntu 14.04.5 LTS\r\n    OSType: linux\r\n    Architecture: x86_64\r\n    CPUs: 1\r\n    Total Memory: 3.676 GiB\r\n    Name: ip-172-31-37-98\r\n    ID: 5WMS:A4PC:JLN4:6VV7:3UHO:5WPF:WLG4:HW2O:ID3L:GUD4:QWWZ:2ODA\r\n    Docker Root Dir: /var/lib/docker\r\n    Debug Mode (client): false\r\n    Debug Mode (server): false\r\n    Username: fxdgear\r\n    Registry: https://index.docker.io/v1/\r\n    WARNING: No swap limit support\r\n    Insecure Registries:\r\n     127.0.0.0/8```\r\n"},{"labels":[null,"enhancement"],"text":"**Description**\r\n\r\nUsing `--build-arg` is convenient for setting proxy settings for build but it isn't ideal in many scenario, most of all when using compose. To get our images to build behind corporate firewall, we have to add the `arg` section to all our compose file. Ex:\r\n\r\n```\r\n args:\r\n        - http_proxy=${HTTP_PROXY}\r\n        - https_proxy=${HTTP_PROXY}\r\n        - HTTP_PROXY=${HTTP_PROXY}\r\n        - HTTPS_PROXY=${HTTP_PROXY}\r\n```\r\n\r\nThis solves our own images but if we contribute on Dockefile from anyone else, we have to patch those files with these args to get them to build.\r\n\r\nIt would be nice if in addition to being able to specify command line build args, it was possible to specify global ones using `DOCKER_BUILD_` environment variables. This would allow those of us unfortunate enough to be behind proxy to have our build arguments specific to our environment in env vars and not anywhere else. The `DOCKER_BUILD_` vars could be merged with the cli `--build-arg`, with the cli options taking precedence.\r\n"},{"labels":[null,"enhancement"],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nUse docker stats byname to display the statistic of the container and then rename the container,\r\nthe container name display in `CONTAINER` column does not change.\r\n\r\n````\r\n[lei@fedora docker]$ docker run -tid --name test busybox\r\n308fb0316bc67722a242b7ca50b5ad57735318bd1b38002e5820ed8ea0ca9d6d\r\n[lei@fedora docker]$ docker ps\r\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n308fb0316bc6        busybox             \"sh\"                3 seconds ago       Up 2 seconds                            test\r\n[lei@fedora docker]$ docker rename test foo\r\n[lei@fedora docker]$ docker ps\r\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n308fb0316bc6        busybox             \"sh\"                34 seconds ago      Up 33 seconds                           foo\r\n\r\n````\r\n\r\n````\r\nCONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS\r\ntest                0.00%               168 KiB / 3.858 GiB   0.00%               0 B / 648 B         0 B / 0 B           1\r\n\r\n````\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker run -tid --name test busybox \r\n2. docker stats test\r\n3. docker rename test foo\r\n\r\n**Describe the results you received:**\r\nthe container name in `CONTAINER` column remain unchanged after docker rename\r\n\r\n**Describe the results you expected:**\r\nthe container name in `CONTAINER` of docker stats should changed after docker rename\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,null,null,null,"enhancement"],"text":"We currently have various flags for managing cpu resources of a container.  These flags are still needed for advanced use cases around cpu affinity but for the 90% use case having these multiple flags adds confusing for people.\r\n\r\nI propose we add a `--cpus` flag that takes a float for specifying the number of cpus a user wishes to limit the container to.  If a user wishes the container to have 1 cpu then they can simple put `--cpus 1.0` as the value and Docker will configure the container correctly.  This also will allow users to add partial cpus enabling `--cpus 1.5`, `--cpus 2.3`, `--cpus 0.5`, etc...\r\n\r\nUnder the hood this will be implemented by cpu-quota and cpu-period on linux and cpu percent on Windows.  This will be based on time so that it is possible to allocate percentage of a core on the system.  \r\n\r\nThis will also allow swarm to have a unified configuration option across all platforms for scheduling without having to parse platform specific options or leave users trying to decide if they should use cpu-shares( you never should ) or how cpu quota works and what number they should input.  \r\n\r\nFor people wanting to manage container resources simply then this will really help out and result in a simpler docker run cmdline.  \r\n\r\n```bash\r\ndocker run --cpus 2.5 --memory 1g ubuntu bash\r\n```"},{"labels":[null,null,"enhancement"],"text":"This is a tracking issue for a discussion I was having with @anusha-ragunathan about https://github.com/docker/docker/pull/27804.\r\n\r\nCurrently, if a uses specifies a non-existing authz plugin, the daemon will still start, but the first API request trying to call the authz plugin will fail (due to \"lazy loading\" plugins).\r\n\r\nSpecifying a non-existent authz plugin should be verified during startup, and the daemon should fail to start, otherwise a user may _think_ the authz plugin is enabled, but actually not.\r\n\r\n/cc @riyazdf "},{"labels":["enhancement",null],"text":"**Description**\r\n\r\nMounting a loopback device in a container requires `CAP=SYS_ADMIN` or even better `--privileged`. However, the `/dev/loop*` do not appear in the container when created dynamically.\r\n\r\nThis is particularly a problem when manipulating disk `.img` files inside a container.\r\n\r\nI am using `ubuntu:16.04` as a base to reproduce, but this should be the same with any base.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker run --rm -it --privileged ubuntu:16.04 bash`\r\n2. In container: `apt update && apt install -y gdisk`\r\n3. In container: make a 500MB disk file`dd if=/dev/zero of=/ofile.img bs=1M count=500`\r\n4. In container: make partitions in the disk`sgdisk -n 1:2048:194559 -n 2:195560:300000 /ofile.img`\r\n5. In container: scan file and create loop devices for partitions:`losetup -f /ofile.img -P --show`\r\n6. In container: see that partition files have **not** been created`ls -l /dev/loop*`\r\n7. In host: see that partition files *_have*\\* been created: `ls -l /dev/loop*` \r\n\r\n**Describe the results you received:**\r\nIn container:\r\n\r\n```\r\n/dev/loop0\r\n/dev/loop1\r\n...\r\n```\r\n\r\nOn host:\r\n\r\n```\r\n/dev/loop0\r\n/dev/loop0p1\r\n/dev/loop0p2\r\n/dev/loop1\r\n...\r\n```\r\n\r\nEssentially, when running `--privileged`, docker copies the `/dev/` files over to the container, but does not keep them updated.\r\n\r\nSame problem exists if you have the files before and then run `losetup -D` (on host or in container): on host the `/dev/loop0p1` etc. files are gone, but linger in container.\r\n\r\n**Describe the results you expected:**\r\nWhen in privileged mode (or cap=sys_admin), since mounting is possible, devices should be synced up where relevant.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nDocker version 1.12.1, build 23cf638\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 1\r\n Paused: 0\r\n Stopped: 0\r\nImages: 735\r\nServer Version: 1.12.1\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 365\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: null bridge host overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nSecurity Options: apparmor seccomp\r\nKernel Version: 4.4.0-42-generic\r\nOperating System: Ubuntu 16.04.1 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 7.674 GiB\r\nName: avi-Latitude-E6320\r\nID: SVT2:XSJY:6LYB:O4NB:NGZO:JTXJ:MVPZ:Z2BG:JOPO:IZXS:XD2M:5FCN\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nInsecure Registries:\r\n 127.0.0.0/8\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nPhysical laptop running Ubuntu.\r\n"},{"labels":[null,"enhancement",null],"text":"**Description**\n\nAs discussed in #27281, if `docker pull` encounters any problems using the v2 API (such as a 401 error due to not having logged in), it falls back and tries the v1 API. If the registry doesn't implement the v1 API, this in turn causes a 404 error, and the user sees only the message \"not found\", and the original error via the v2 API is lost.\n\n**Steps to reproduce the issue:**\n1. Install a registry (I use the image registry:2.5) with htpasswd auth.\n2. Log in to the registry and push an image\n3. On another machine (or after logging out of the registry), run `docker pull <registry>:<port>/<image>`.\n\n**Describe the results you received:**\n\nDocker output:\n\n```\nUsing default tag: latest\nPulling repository registry:5000/bmerry/abacuscm\nError: image bmerry/abacuscm:latest not found\n```\n\nRegistry log file:\n\n```\ntime=\"2016-10-11T09:14:34Z\" level=warning msg=\"error authorizing context: basic authentication challenge for realm \\\"Registry Realm\\\": invalid authorization credential\" go.version=go1.6.3 http.request.host=\"registry:5000\" http.request.id=2ec10df4-0f18-4d69-b69b-e017b720b5ea http.request.method=GET http.request.remoteaddr=\"172.17.0.1:32768\" http.request.uri=\"/v2/\" http.request.useragent=\"docker/1.12.1 go/go1.6.3 git-commit/23cf638 kernel/4.4.0-38-generic os/linux arch/amd64 UpstreamClient(Docker-Client/1.12.1 \\\\(linux\\\\))\" instance.id=be638723-6e27-43a5-a2d6-1fc5254d7b71 version=v2.5.0 \n172.17.0.1 - - [11/Oct/2016:09:14:34 +0000] \"GET /v2/ HTTP/1.1\" 401 87 \"\" \"docker/1.12.1 go/go1.6.3 git-commit/23cf638 kernel/4.4.0-38-generic os/linux arch/amd64 UpstreamClient(Docker-Client/1.12.1 \\\\(linux\\\\))\"\n2016/10/11 09:14:34 http: TLS handshake error from 172.17.0.1:32772: tls: first record does not look like a TLS handshake\n172.17.0.1 - - [11/Oct/2016:09:14:34 +0000] \"GET /v1/_ping HTTP/1.1\" 404 19 \"\" \"docker/1.12.1 go/go1.6.3 git-commit/23cf638 kernel/4.4.0-38-generic os/linux arch/amd64 UpstreamClient(Docker-Client/1.12.1 \\\\(linux\\\\))\"\n172.17.0.1 - - [11/Oct/2016:09:14:34 +0000] \"GET /v1/repositories/bmerry/abacuscm/images HTTP/1.1\" 404 19 \"\" \"docker/1.12.1 go/go1.6.3 git-commit/23cf638 kernel/4.4.0-38-generic os/linux arch/amd64 UpstreamClient(Docker-Client/1.12.1 \\\\(linux\\\\))\"\n```\n\n**Describe the results you expected:**\n\nDocker Engine should print a diagnostic when access with the v2 API fails, indicating the cause of the failure, and warn that it is falling back to the v1 API. That way the real error is not suppressed.\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\nThe fundamental problem (that the real error returned by the registry is not reported, and a misleading one is instead) keeps getting reported (#17721, #18263, #27281, https://bugzilla.redhat.com/show_bug.cgi?id=1347805, probably more I missed), and they keep getting closed as \"by design\". I don't actually care how it gets fixed, but this is a suggested fix that I'm hoping will be acceptable and hopefully not too difficult to implement.\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:22:43 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:22:43 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 2\n Running: 1\n Paused: 0\n Stopped: 1\nImages: 385\nServer Version: 1.12.1\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 634\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: overlay bridge host null\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: apparmor\nKernel Version: 4.4.0-38-generic\nOperating System: Ubuntu 14.04.5 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 8\nTotal Memory: 31.36 GiB\nName: kryton\nID: MFLN:XRFL:372N:VPKR:3AWK:USXB:3Q3E:EAH2:66ZK:T5U2:NEOP:TSCH\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n\nPhysical box running Ubuntu 14.04.\n"},{"labels":[null,"enhancement",null],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nGENERAL SUPPORT INFORMATION\n---------------------------------------------------\n\nThe GitHub issue tracker is for bug reports and feature requests.\nGeneral support can be found at the following locations:\n\n- Docker Support Forums - https://forums.docker.com\n- IRC - irc.freenode.net #docker channel\n- Post a question on StackOverflow, using the Docker tag\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\n**Description**\n\n<!--\nBriefly describe the problem you are having in a few paragraphs.\n-->\n\nI'am facing ip conflict with my enterprise network, when i let Docker creating a network with it's default strategy.\n\nBy default :\n1 - Docker0 IP configuration is \n\n```\nDocker0\ninet 172.17.0.1  netmask 255.255.0.0\n```\n\n2 - When an another bridge network is created (with command line **_docker network create_** or with docker compose) the \"NEXT\" available sub network is used : \n\n```\nbr-xxxxx\ninet 172.18.0.1  netmask 255.255.0.0\n```\n\nSince 172.18.x.x is used on my company, there is an IP Conflict\n\nI tried to solve my problem with Docker0 IP customization\n\n**Steps to reproduce the issue:**\n1 - customize Docker0 subnet with the deamon \"bip\" parameter with \"192.170.0.1/24\"\n\n```\nDocker0\ninet 192.170.0.1  netmask 255.255.255.0\n```\n\n2 - create a new bridge network with docker network create mynetwork\n\n**Describe the results you received:**\nThe new network characteristics are : \n\n```\nbr-xxxxx\ninet 172.17.0.1  netmask 255.255.0.0\n```\n\n**Describe the results you expected:**\nThe subnet of \"mynetwork\" should be the next available subnet based on Docker0 configration.\n\n```\nbr-xxxxx\ninet 192.170.1.1  netmask 255.255.255.0\n```\n\nA another solution is to add a new daemon parameter to set the first subnet created with docker network create command. \n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.3\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   6b644ec\n Built:\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.3\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   6b644ec\n Built:\n OS/Arch:      linux/amd64\n```\n"},{"labels":[null,"enhancement",null],"text":"**Description**\n\nI've got a docker container running in it's own bridge network on a docker host <dockerhost>. The containers port 8080 is exposed. I've got iptables default policy on the host set to DROP. Docker daemon manages the docker iptables rules.\n\nIf I run curl outside my container to http://<dockerhost>:8080 I get HTTP status 200 and the correct website my container is running.\n\nHowever the same curl from inside my container on that host (to both the IP or hostname) fails to connect:\n\ncurl: (7) Failed to connect to <dockerhost> on port 8080: Host is unreachable\nThis doesn't happen when trying this from a Docker container from another host or from the host itself.\n\nI can solve this by adding the following iptables rule:\n`sudo iptables -I INPUT 1 -i <docker-bridge-interface> -j ACCEPT`\nWhere `<docker-bridge-interface>` is the name of the bridge interface, in which the docker container is running.\n\nNow I can do curl -v dockerhost:port from my container running in the docker-bridge-interface network and exposing above port on the dockerhost.\n\nI think this should be solved by docker: whenever a bridge network is created, a firewall rule should be set to allow the containers in this network to reach the host on it's own exposed (host) port. Reason we need this is that some webapplications use a baseurl instead relative urls internally.\n\n**Steps to reproduce the issue:**\n\n```\n$ sudo iptables -P INPUT DROP\n$ docker network create -d bridge mynetwork\n$ docker run -d -p 8000:8000 --net=mynetwork --name=revealjs amouat/revealjs:latest \n$ docker exec -ti revealjs /bin/bash\n# curl -v <dockerhost-ip>:8000\ncurl: (7) Failed to connect to <dockerhost-ip> port 8000: Connection refused\nOr depending on your overall firewall rules, you could also get a timeout.\n```\n\n**Describe the results you received:**\n\n```\ncurl -v http://curie1.ccveu.local:8080\n* Rebuilt URL to: http://curie1.ccveu.local:8080/\n*   Trying 192.168.56.5...\n* connect to 192.168.56.5 port 8080 failed: Connection refused\n* Failed to connect to curie1.ccveu.local port 8080: Connection refused\n* Closing connection 0\ncurl: (7) Failed to connect to curie1.ccveu.local port 8080: Connection refused\n```\n\nThis doesn't happen when trying this from a Docker container from another host or from the host itself.\n\n**Describe the results you expected:**\n\n```\ncurl -v http://curie1.ccveu.local:8080\n* Rebuilt URL to: http://curie1.ccveu.local:8080/\n*   Trying 192.168.56.5...\n* Connected to curie1.ccveu.local (192.168.56.5) port 8080 (#0)\n> GET / HTTP/1.1\n> Host: curie1.ccveu.local:8080\n> User-Agent: curl/7.47.0\n> Accept: */*\n> \n< HTTP/1.1 302 Found\n< Server: Apache-Coyote/1.1\n< X-AREQUESTID: 477x3x1\n< Location: /startup.jsp?returnTo=%2Fdefault.jsp\n< Content-Type: text/html;charset=UTF-8\n< Content-Length: 0\n< Date: Thu, 27 Oct 2016 07:57:49 GMT\n< \n* Connection #0 to host curie1.ccveu.local left intact\n```\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n\nI can solve this by adding an iptables rule to allow connections from docker network to host:\n`$ sudo iptables -I INPUT 1 <interfacename-mynetwork> -j ACCEPT`\n\nHowever, IMHO docker should add this rule when a bridge or other network is created.\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:33:38 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:33:38 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 8\n Running: 6\n Paused: 0\n Stopped: 2\nImages: 15\nServer Version: 1.12.1\nStorage Driver: aufs\n Root Dir: /data/docker/docker/aufs\n Backing Filesystem: extfs\n Dirs: 187\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: null bridge host overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: apparmor seccomp\nKernel Version: 4.4.0-28-generic\nOperating System: Ubuntu 16.04 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 1.954 GiB\nName: curie.ccveu.local\nID: XKC4:VJXM:XIAR:E7LT:4ELP:G7LS:4KPE:MFIB:T7E4:2YGT:AQCR:IIRE\nDocker Root Dir: /data/docker/docker\nDebug Mode (client): false\nDebug Mode (server): false\nHttp Proxy: http://proxy:8080\nHttps Proxy: http://proxy:8080\nNo Proxy: .ccveu.local\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\nLabels:\n debops.ccv.architecture=Ubuntu 16.04\n debops.ccv.environment=production\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nThis is reproducable on the following environments:\n- VirtualBox Ubuntu 16.04\n- ESX Virtual Machine Ubuntu 16.04\n- Virtual Box with Debian Jessie.\n- Physical Fedora 19 laptop with docker 1.9.1\n"},{"labels":[null,null,"enhancement",null],"text":"**Description**\nDocker run --help command has be low options, \n      --blkio-weight value          Block IO (relative weight), between 10 and 1000\n      --blkio-weight-device value   Block IO weight (relative device weight) (default [])\n  -c, --cpu-shares int              CPU shares (relative weight)\n1. Does all wight should be integer.?\n2. blkio-weight accepts 0 also. Help message shows that it should be in range 10-1000\n3. If 0 is default then in help, we should show default value.\n\n**Steps to reproduce the issue:**\n1. Check docker run --help  \n2. Create container with blkio weight as 0\n\n**Describe the results you received:**\n1. Docker run --help command has below options related to blkio weight\n      --blkio-weight value          Block IO (relative weight), between 10 and 1000\n      --blkio-weight-device value   Block IO weight (relative device weight) (default [])\n2. docker run --blkio 0 busybox executed successfully.\n\n**Describe the results you expected:**\n1. Docker run --help command should show blkio-weight value as Int as weight is Int and range is between 10-1000.\n      --blkio-weight int         Block IO (relative weight), between 10 and 1000\n      --blkio-weight-device int   Block IO weight (relative device weight) (default [])\n2. docker run --blkio 0 busybox.\n  As blkio range is out of 10-1000, we should get proper error message saying \" Range of blkio weight is from 10 to 1000\"\n1. If 0 is default value then we should see default value in docker run --help\n\n**Output of `docker version`:**\n\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:\n OS/Arch:      linux/amd64\n\n**Output of `docker info`:**\n[root@nfs-server cloud-user]# docker info\nContainers: 6\n Running: 0\n Paused: 0\n Stopped: 6\nImages: 2\nServer Version: 1.12.1\nStorage Driver: devicemapper\n Pool Name: docker-253:1-747269-pool\n Pool Blocksize: 65.54 kB\n Base Device Size: 10.74 GB\n Backing Filesystem: xfs\n Data file: /dev/loop0\n Metadata file: /dev/loop1\n Data Space Used: 209.1 MB\n Data Space Total: 107.4 GB\n Data Space Available: 41.06 GB\n Metadata Space Used: 1.069 MB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 2.146 GB\n Thin Pool Minimum Free Space: 10.74 GB\n Udev Sync Supported: true\n Deferred Removal Enabled: false\n Deferred Deletion Enabled: false\n Deferred Deleted Device Count: 0\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\n WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\n Library Version: 1.02.107-RHEL7 (2016-06-09)\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: null bridge host overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 3.10.0-327.36.1.el7.x86_64\nOperating System: CentOS Linux 7 (Core)\nOSType: linux\nArchitecture: x86_64\nCPUs: 2\nTotal Memory: 7.797 GiB\nName: nfs-server.cisco.com\nID: YCHJ:GOSU:POEO:MGF3:C3QV:ZWIR:T5ZK:POHY:XBQS:CH7O:RLVB:75AZ\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nWARNING: bridge-nf-call-iptables is disabled\nWARNING: bridge-nf-call-ip6tables is disabled\nInsecure Registries:\n 127.0.0.0/8\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n"},{"labels":[null,"enhancement"],"text":"This is a feature request / request for clarification.\n\nI have the following use-case: I want to access Docker containers from the host machine via their internal network name. As an example, I want to create a user-defined network with containers named \"foo\", \"bar\" and \"baz\". I then want to access these containers from the host machine via their internal DNS names (e.g. \"ping foo\"). As far as I understand, Docker exposes a custom DNS server to each container, which allows them to resolve the names in the network. It seems that this server only listens to the local port though and cannot be accessed from outside the container (https://github.com/docker/docker/blob/b826bebda0cff2cc2d3083b954c810d2889eefe5/vendor/src/github.com/docker/libnetwork/resolver.go).\n\nIt would be great to make the internal DNS server of a given network available to the host machine (e.g. at the gateway address of the network), such that one could use it to resolve DNS names of the containers from the host machine. This would be interesting for various use cases (e.g. to simulate a network for a CI setup and make the machines in the network reachable by their internal names from the outside).\n\nOf course, if there's an alternative way for doing this I'd be happy to hear about it.\n"},{"labels":["enhancement",null],"text":"When running containers manually, I often need to know what `docker run` command was originally used to spin up the container.\n\nFor example, when I need to attach more volumes to the container, I need to `docker stop` and `docker rm` the container and `docker run` it again (since `docker update` can't handle volume changes yet). Now, what was the original `docker run` sequence I used before?\n\nIt would be useful if `docker inspect` was able to help me out:\n\n``` bash\n$ docker inspect --format='{{.RunCmd}}' postgres\ndocker run -d --restart=always -p 5432:5432 -v /home/user/postgres:/var/lib/postgresql/data --log-opt max-size=500m --log-opt max-file=10 --memory-swappiness=10 --memory=1G --name=postgres postgres:9.6\n```\n\nso I could grab this string and re-use it to create new container.\n"},{"labels":["enhancement"],"text":"I miss the ability to check if a certificate was revoked.\nThis would be important mainly for client certificates, but I can also imagine a circumstance where this is needed for the validity of server certs.\nAn implementation should ideally consider CRL or OCSP checks. This should be be considered by the docker client as well as the docker daemon accepting the connection.\n"},{"labels":[null,"enhancement"],"text":"**To reproduce:**\n\n```\n# docker exec -ti <tab>\nzsh: do you wish to see all 3886 possibilities (972 lines)?\n```\n\n**Expected result:**\nList of started containers.\n\nNote: Everything is OK when there are no options used in the command line. e.g.:\n\n```\n$ docker exec <tab>\ndetermined_heyrovsky  7b4ec7cf2109  --      10 minutes, mystartedcontainer\n```\n"},{"labels":["enhancement",null],"text":"**Description**\n\nI am trying to configure a container differently depending on detected daemon features.\nFor this feature detection I am running `docker info` and grep for the feature in use.\nThis is a generic pattern, you could also use it for other features such as used storage backend or enabled security like seccomp or apparmor.\n\nI have noticed that `docker info` gives no information about whether or not the daemon is running namespaced or not. There are three indicators at the moment\n1. `/var/lib/docker/<uid>.<gid>` exists\n2. `/etc/subuid` / `/etc/subgid` contain `dockremap` (weak: only if default is used)\n3. docker log contains this line:  \n   `\n   User namespaces: ID ranges will be mapped to subuid/subgid ranges of: dockremap:dockremap\n   `\n\n**Describe the results you expected:**\n\nI would expect `docker info` to print userns configuration if this feature is enabled.\ne.g.\n\n```\n(...)\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nUser Namespaces:\n enabled: true\n user: dockremap\nKernel Version: 4.4.17-boot2docker\nOperating System: Boot2Docker 1.12.1 (TCL 7.2); HEAD : ef7d0b4 - Thu Aug 18 21:18:06 UTC 2016\nOSType: linux\nArchitecture: x86_64\n(...)\n```\n\nAlternatively it could appear in the \"Security Options\" section.\n\n**Output of `docker version`:**\n\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 17:52:38 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 17:52:38 2016\n OS/Arch:      linux/amd64\n\n**Output of `docker info`:**\n\nContainers: 15\n Running: 0\n Paused: 0\n Stopped: 15\nImages: 8\nServer Version: 1.12.1\nStorage Driver: aufs\n Root Dir: /mnt/sda1/var/lib/docker/165536.165536/aufs\n Backing Filesystem: extfs\n Dirs: 42\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge null host overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 4.4.17-boot2docker\nOperating System: Boot2Docker 1.12.1 (TCL 7.2); HEAD : ef7d0b4 - Thu Aug 18 21:18:06 UTC 2016\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 995.9 MiB\nName: whisk\nID: 2GG5:Y3O4:OYJW:PYDV:NN3G:AYU5:VJZD:XZU3:MFIN:LNNK:YWQW:GVGR\nDocker Root Dir: /mnt/sda1/var/lib/docker/165536.165536\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 15\n Goroutines: 26\n System Time: 2016-10-25T13:52:24.728753253Z\n EventsListeners: 0\nRegistry: https://index.docker.io/v1/\nLabels:\n provider=virtualbox\nInsecure Registries:\n 127.0.0.0/8\n"},{"labels":[null,"enhancement"],"text":"Here is some example output of docker node ls\n\n```\n08lo1p3lgvwus4pdu26z8tvus *  swarm5.programster.org  Ready   Active        Reachable\n5138uwpoamzthsftyw80otpqt    swarm4.programster.org  Ready   Active        Reachable\n61x0ziccdy4wi200th9ddt9t8    swarm6.programster.org  Ready   Active        \n7tzbno6vc9pk4m7suekxadkey    swarm3.programster.org  Ready   Drain         Reachable\n8m1yi1eo2n6vol6v3z44lxg7i    swarm1.programster.org  Ready   Active        Leader\nctgf1abx5c22h8eivz4sf11mf    swarm2.programster.org  Ready   Active        Reachable\n```\n\nIt appears that the output is sorted by ID by default which isn't particularly useful as the IDs appear to be random or pseudo-random. It would make more sense if the list was sorted by any of the other columns (I would pick hostname). Better yet, it would be great if the user could manually specify what to sort by with a new `--sort-by` parameter that can take either `id`, `hostname`, `status`, `availability`, or `manager-status` as options or a comma separated list of these to sort by one and then the other.\n\nFor example: `docker node ls --sort-by=manager-status,hostname`\n"},{"labels":[null,"enhancement"],"text":"Today this is what `docker service ls` outputs:\n\n```\nID            NAME            REPLICAS  IMAGE                                 COMMAND\nbnssidbnmtko  mulva           2/2       nginx:1.11.5-alpine\ncd97qo6tqx7g  logentries      global    logentries/docker-logentries          -t d33b91d9-6e5a-414e-ad1f-03be075aff81 -j\n```\n\nWould be VERY nice to see how many global instance a service have:\n\n```\nID            NAME            REPLICAS  IMAGE                                 COMMAND\nbnssidbnmtko  mulva           2/2       nginx:1.11.5-alpine\ncd97qo6tqx7g  logentries      7/7 global    logentries/docker-logentries          -t d33b91d9-6e5a-414e-ad1f-03be075aff81 -j\n```\n\nMake sense?\n"},{"labels":[null,"enhancement"],"text":"The parenthesized key=value pairs are comma-separated but there's no protection for commas, making it challenging to parse something like:\n\n```\n  (Release=56, Vendor=Red Hat, Inc., Name=rhel7/rhel, ...)\n```\n\nThere are many possible ways to address this. Some possibilities:\n\n```\n* Backslash-escape commas within key or value: Vendor=Red Hat\\, Inc.\n* Escape commas as HTML entities or using URL escaping: Vendor=Red Hat&#2C; Inc. or =Red Hat%2C Inc.\n* Double-quote values that contain space/comma/other: Vendor=\"Red Hat, Inc.\"\n```\n\nNote that commas aren't the only characters needing escape: also equal sign, and possibly close-paren. And, given any of the above mechanisms, its corresponding escape character itself (backslash, ampersand, percent, double-quote).\n\nThis is a can of worms, and there is not currently a compelling reason to parse this output... but there will be. Best to sort this out early and document it clearly.\n\nReproducer:\n\n``` sh\n$ docker run --label test=\"a, really) not machine=readable)label,\" busybox\n\n$ docker events # truncated...\n705f13ff94b3fd52138f76 (image=busybox, name=adoring_feynman, test=a, really) not machine=readable)label,)\n```\n\n/cc @duglin @thaJeztah @vdemeester \n"},{"labels":[null,"enhancement",null],"text":"**Description**\nHere is the service constraint description:\n\n| node attribute | matches | example |\n| :-- | :-- | :-- |\n| node.id | node's ID | `node.id == 2ivku8v2gvtg4` |\n| node.hostname | node's hostname | `node.hostname != node-2` |\n| node.role | node's manager or worker role | `node.role == manager` |\n| node.labels | node's labels added by cluster admins | `node.labels.security == high` |\n| engine.labels | Docker Engine's labels | `engine.labels.operatingsystem == ubuntu 14.04` |\n\nWhile when I execute `docker service create --constraint node.role==managerr ubuntu:14.04 sleep 100000` with wrong spelling of `manger` in `managerr`, it will succeed. I think maybe it is proper to throw out an error.\n\nIn addition, some keys in constraints are not valid, I think it is better to throw out error as well, like `docker service create --constraint node.rle==manager ubuntu:14.04 sleep 100000` with `node.rle==manager` wrong spelling of `role`.\n\n**Steps to reproduce the issue:**\n1. execute `docker service create --constraint node.rle==managerr ubuntu:14.04 sleep 100000`\n\n**Describe the results you received:**\n\n```\nroot@ubuntu:~# docker service create --constraint node.rle==managerr ubuntu:14.04 sleep 100000\nbzihlpg3cbclq21gvjpfrumn3\n```\n\n**Describe the results you expected:**\nreturn an error since constraints are not valid.\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n\n**Output of `docker version`:**\n\n```\nroot@ubuntu:~# docker version\nClient:\n Version:      1.12.2\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   bb80604\n Built:        Tue Oct 11 18:19:35 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.2\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   bb80604\n Built:        Tue Oct 11 18:19:35 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nroot@ubuntu:~# docker info\nContainers: 6\n Running: 1\n Paused: 0\n Stopped: 5\nImages: 64\nServer Version: 1.12.2\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 468\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge host null overlay\nSwarm: active\n NodeID: 2iu5iv9fk2sdxppoqnttfaaa8\n Is Manager: true\n ClusterID: 0c0xhuqs7vvw0t8azx2b5zth9\n Managers: 1\n Nodes: 1\n Orchestration:\n  Task History Retention Limit: 5\n Raft:\n  Snapshot Interval: 10000\n  Heartbeat Tick: 1\n  Election Tick: 3\n Dispatcher:\n  Heartbeat Period: 5 seconds\n CA Configuration:\n  Expiry Duration: 5 weeks\n Node Address: 192.168.59.103\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: apparmor\nKernel Version: 3.19.0-25-generic\nOperating System: Ubuntu 14.04.3 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 1.954 GiB\nName: ubuntu\nID: HRBI:EV6M:3NAC:CXZ3:BDS7:UOKL:PD6Y:ZCK4:V3PF:V7MR:CU2Y:3LGG\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 38\n Goroutines: 136\n System Time: 2016-10-23T01:02:28.073294251+08:00\n EventsListeners: 1\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n"},{"labels":[null,"enhancement",null],"text":"**Description**\nI execute `dockerd -h` in docker 1.12.2. And I found some options has no default value while some has. I think some options has a default value and it is just no showing in the output. \n\ncommand `dockerd -d`:\n\n```\nroot@ubuntu:~# dockerd -h\nUsage: dockerd [OPTIONS]\n\nA self-sufficient runtime for containers.\n\nOptions:\n\n  --add-runtime=[]                         Register an additional OCI compatible runtime\n  --api-cors-header                        Set CORS headers in the remote API\n  --authorization-plugin=[]                Authorization plugins to load\n  -b, --bridge                             Attach containers to a network bridge\n  --bip                                    Specify network bridge IP\n  --cgroup-parent                          Set parent cgroup for all containers\n  --cluster-advertise                      Address or interface name to advertise\n  --cluster-store                          URL of the distributed storage backend\n  --cluster-store-opt=map[]                Set cluster store options\n  --config-file=/etc/docker/daemon.json    Daemon configuration file\n  --containerd                             Path to containerd socket\n  -D, --debug                              Enable debug mode\n  --default-gateway                        Container default gateway IPv4 address\n  --default-gateway-v6                     Container default gateway IPv6 address\n  --default-runtime=runc                   Default OCI runtime for containers\n  --default-ulimit=[]                      Default ulimits for containers\n  --disable-legacy-registry                Disable contacting legacy registries\n  --dns=[]                                 DNS server to use\n  --dns-opt=[]                             DNS options to use\n  --dns-search=[]                          DNS search domains to use\n  --exec-opt=[]                            Runtime execution options\n  --exec-root=/var/run/docker              Root directory for execution state files\n  --fixed-cidr                             IPv4 subnet for fixed IPs\n  --fixed-cidr-v6                          IPv6 subnet for fixed IPs\n  -G, --group=docker                       Group for the unix socket\n  -g, --graph=/var/lib/docker              Root of the Docker runtime\n  -H, --host=[]                            Daemon socket(s) to connect to\n  -h, --help                               Print usage\n  --icc=true                               Enable inter-container communication\n  --insecure-registry=[]                   Enable insecure registry communication\n  --ip=0.0.0.0                             Default IP when binding container ports\n  --ip-forward=true                        Enable net.ipv4.ip_forward\n  --ip-masq=true                           Enable IP masquerading\n  --iptables=true                          Enable addition of iptables rules\n  --ipv6                                   Enable IPv6 networking\n  -l, --log-level=info                     Set the logging level\n  --label=[]                               Set key=value labels to the daemon\n  --live-restore                           Enable live restore of docker when containers are still running\n  --log-driver=json-file                   Default driver for container logs\n  --log-opt=map[]                          Default log driver options for containers\n  --max-concurrent-downloads=3             Set the max concurrent downloads for each pull\n  --max-concurrent-uploads=5               Set the max concurrent uploads for each push\n  --mtu                                    Set the containers network MTU\n  --oom-score-adjust=-500                  Set the oom_score_adj for the daemon\n  -p, --pidfile=/var/run/docker.pid        Path to use for daemon PID file\n  --raw-logs                               Full timestamps without ANSI coloring\n  --registry-mirror=[]                     Preferred Docker registry mirror\n  -s, --storage-driver                     Storage driver to use\n  --selinux-enabled                        Enable selinux support\n  --storage-opt=[]                         Storage driver options\n  --swarm-default-advertise-addr           Set default address or interface for swarm advertised address\n  --tls                                    Use TLS; implied by --tlsverify\n  --tlscacert=~/.docker/ca.pem             Trust certs signed only by this CA\n  --tlscert=~/.docker/cert.pem             Path to TLS certificate file\n  --tlskey=~/.docker/key.pem               Path to TLS key file\n  --tlsverify                              Use TLS and verify the remote\n  --userland-proxy=true                    Use userland proxy for loopback traffic\n  --userns-remap                           User/Group setting for user namespaces\n  -v, --version                            Print version information and quit\n```\n\nWe will see that like `--icc=true                               Enable inter-container communication` it has a default value of true.\n\nWhile I think the default value of `--live-restore                           Enable live restore of docker when containers are still running` is false. But it did not show this default value.\n\nMaybe it is command displaying issue.\n\n**Steps to reproduce the issue:**\n1.\n2.\n3.\n\n**Describe the results you received:**\n\n**Describe the results you expected:**\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n\n**Output of `docker version`:**\n\n```\nroot@ubuntu:~# docker version\nClient:\n Version:      1.12.2\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   bb80604\n Built:        Tue Oct 11 18:19:35 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.2\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   bb80604\n Built:        Tue Oct 11 18:19:35 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nroot@ubuntu:~# docker info\nContainers: 29\n Running: 5\n Paused: 0\n Stopped: 24\nImages: 64\nServer Version: 1.12.2\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 514\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: overlay bridge null host\nSwarm: active\n NodeID: 2iu5iv9fk2sdxppoqnttfaaa8\n Is Manager: true\n ClusterID: 0c0xhuqs7vvw0t8azx2b5zth9\n Managers: 1\n Nodes: 1\n Orchestration:\n  Task History Retention Limit: 5\n Raft:\n  Snapshot Interval: 10000\n  Heartbeat Tick: 1\n  Election Tick: 3\n Dispatcher:\n  Heartbeat Period: 5 seconds\n CA Configuration:\n  Expiry Duration: 5 weeks\n Node Address: 192.168.59.103\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: apparmor\nKernel Version: 3.19.0-25-generic\nOperating System: Ubuntu 14.04.3 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 1.954 GiB\nName: ubuntu\nID: HRBI:EV6M:3NAC:CXZ3:BDS7:UOKL:PD6Y:ZCK4:V3PF:V7MR:CU2Y:3LGG\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 57\n Goroutines: 193\n System Time: 2016-10-22T23:52:43.511871935+08:00\n EventsListeners: 5\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\nLabels:\n a=b\nInsecure Registries:\n 192.168.1.22:5500\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n"},{"labels":[null,"enhancement",null],"text":"Search for image that is not installed:\n\nSteps:\n1. docker image does-not-exist; echo $?\n\nExpected results:\nReturns none zero and no output\n\nActual results:\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n0\n\nWhy?\nPrograms need to be able to determine whether or not an image is installed.\n\nroot@node1:~# docker info\nContainers: 0\n Running: 0\n Paused: 0\n Stopped: 0\nImages: 0\nServer Version: 1.12.1\nStorage Driver: overlay\n Backing Filesystem: extfs\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: host bridge overlay null\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: apparmor\nKernel Version: 4.4.0-43-generic\nOperating System: Ubuntu 16.04.1 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 2\nTotal Memory: 992.3 MiB\nName: node1\nID: XHPN:4KOB:NWD2:DIVF:GGJR:QKXJ:JH7Y:BCWN:HVN2:7FES:ZGBV:ISNS\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\nInsecure Registries:\n 127.0.0.0/8\n"},{"labels":[null,"enhancement"],"text":"per the discussion on https://github.com/docker/docker/pull/25122, docker should send this header to prevent transparent decompression of 'x-gzip' content\n\n/cc @stevvooe @dmcgowan \n"},{"labels":[null,"enhancement",null],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nGENERAL SUPPORT INFORMATION\n---------------------------------------------------\n\nThe GitHub issue tracker is for bug reports and feature requests.\nGeneral support can be found at the following locations:\n\n- Docker Support Forums - https://forums.docker.com\n- IRC - irc.freenode.net #docker channel\n- Post a question on StackOverflow, using the Docker tag\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\n**Description**\nWhile running \"docker swarm init\" one of the processes of docker daemon gets SIGSEGV.\n\n<!--\nBriefly describe the problem you are having in a few paragraphs.\n-->\n\n**Steps to reproduce the issue:**\n1. Run docker daemon with gdb: \"gdb --args dockerd -D -s devicemapper\".\n2. (gdb) run\n3. In other terminal session do: \"docker swarm init --advertise-addr $IP\"\n\n**Describe the results you received:**\nDocker daemon gets segfault: \n\n```\nDEBU[0406] Assigning addresses for endpoint ingress-endpoint's interface on network ingress \nDEBU[0406] RequestAddress(LocalDefault/10.255.0.0/16, 10.255.0.3, map[]) \nDEBU[0406] Assigning addresses for endpoint ingress-endpoint's interface on network ingress \nDetaching after fork from child process 11405.\n\nThread 9 \"dockerd\" received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7ffff1a02700 (LWP 11055)]\n0x000000000066bf12 in net.networkNumberAndMask ()\n```\n\n**Describe the results you expected:**\nWork fine without segfault.\n\n**Output of `docker version`:**\n\n```\ndocker version\nClient:\n Version:      1.12.2\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   bb80604\n Built:        \n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.2\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   bb80604\n Built:        \n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nBacking Filesystem: xfs\n Data file: /dev/loop0\n Metadata file: /dev/loop1\n Data Space Used: 16.91 MB\n Data Space Total: 107.4 GB\n Data Space Available: 95 GB\n Metadata Space Used: 585.7 kB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 2.147 GB\n Thin Pool Minimum Free Space: 10.74 GB\n Udev Sync Supported: true\n Deferred Removal Enabled: false\n Deferred Deletion Enabled: false\n Deferred Deleted Device Count: 0\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\n Library Version: 1.02.122 (2016-04-09)\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: overlay bridge null host\nSwarm: active\n NodeID: 29ql6fig3b3xu21k8ysckisyg\n Is Manager: true\n ClusterID: 06j5qk4aln8uqo6c11gnt6pzb\n Managers: 1\n Nodes: 1\n Orchestration:\n  Task History Retention Limit: 5\n Raft:\n  Snapshot Interval: 10000\n  Heartbeat Tick: 1\n  Election Tick: 3\n Dispatcher:\n  Heartbeat Period: 5 seconds\n CA Configuration:\n  Expiry Duration: 3 months\n Node Address: 10.30.29.163\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 4.7.7-200.snorch.fc24.x86_64\nOperating System: Fedora 24 (Workstation Edition)\nOSType: linux\nArchitecture: x86_64\nCPUs: 4\nTotal Memory: 15.54 GiB\nName: dhcp-10-30-29-163.sw.ru\nID: FOBI:53SZ:3XT7:5E4Q:JJND:QPOY:PQAT:GPCK:IVQ6:L2AO:IZPY:QWKU\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 34\n Goroutines: 119\n System Time: 2016-10-21T15:16:15.941817613+03:00\n EventsListeners: 0\nRegistry: https://index.docker.io/v1/\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nFedora 24, docker-1.12.2\n\ngdb with symbols:\n\n```\n#0  0x000000000066dcb2 in net.networkNumberAndMask (n=0x0, ip=..., m=...) at /usr/local/go/src/net/ip.go:433\n#1  0x000000000066e002 in net.(*IPNet).String (n=0x0, ~r0=\"\") at /usr/local/go/src/net/ip.go:483\n#2  0x00000000004b3e1a in fmt.(*pp).handleMethods (p=0xc8204369c0, verb=118, depth=0, handled=true) at /usr/local/go/src/fmt/print.go:730\n#3  0x00000000004b43a9 in fmt.(*pp).printArg (p=0xc8204369c0, arg=..., verb=118, depth=0, wasString=false)\n    at /usr/local/go/src/fmt/print.go:806\n#4  0x00000000004bb70d in fmt.(*pp).doPrintf (p= []uint8, format=\"error setting interface %q IPv6 to %v\", a= []interface {} = {...})\n    at /usr/local/go/src/fmt/print.go:1238\n#5  0x00000000004aeeef in fmt.Sprintf (format=\"error setting interface %q IPv6 to %v\", a= []interface {} = {...}, ~r2=\"\")\n    at /usr/local/go/src/fmt/print.go:203\n#6  0x0000000000d50a52 in github.com/docker/libnetwork/osl.configureInterface (nlh=0xc820b4d8a0, iface=..., i=0xc820436d00, ~r3=...)\n    at /go/src/github.com/docker/docker/vendor/src/github.com/docker/libnetwork/osl/interface_linux.go:333\n#7  0x0000000000d4f25a in github.com/docker/libnetwork/osl.(*networkNamespace).AddInterface (n=0xc8200f7080, srcName=\"ov-000100-eko1d\", \n    dstPrefix=\"br\", options= []github.com/docker/libnetwork/osl.IfaceOption = {...}, ~r3=...)\n    at /go/src/github.com/docker/docker/vendor/src/github.com/docker/libnetwork/osl/interface_linux.go:297\n#8  0x0000000000d82538 in github.com/docker/libnetwork/drivers/overlay.(*network).setupSubnetSandbox (n=0xc820158820, s=0xc820148a00, \n    brName=\"ov-000100-eko1d\", vxlanName=\"vx-000100-eko1d\", ~r3=...)\n    at /go/src/github.com/docker/docker/vendor/src/github.com/docker/libnetwork/drivers/overlay/ov_network.go:473\n#9  0x0000000000d82e5c in github.com/docker/libnetwork/drivers/overlay.(*network).initSubnetSandbox (n=0xc820158820, s=0xc820148a00, \n    restore=false, ~r2=...) at /go/src/github.com/docker/docker/vendor/src/github.com/docker/libnetwork/drivers/overlay/ov_network.go:507\n#10 0x0000000000d99756 in github.com/docker/libnetwork/drivers/overlay.(*network).joinSubnetSandbox.func1 ()\n    at /go/src/github.com/docker/docker/vendor/src/github.com/docker/libnetwork/drivers/overlay/ov_network.go:229\n#11 0x000000000070d584 in sync.(*Once).Do (o=0xc8209139e0, f={void (void)} 0xc820b45658) at /usr/local/go/src/sync/once.go:44\n#12 0x0000000000d7fd0a in github.com/docker/libnetwork/drivers/overlay.(*network).joinSubnetSandbox (n=0xc820158820, s=0xc820148a00,\n```\n"},{"labels":[null,"enhancement"],"text":"**Description**\n\nPush restarts from scratch after timeouts. This means my pushes take several days to complete. If only, when it restarts, it could continue from where it was with a partially uploaded chunk, it would complete after only a few re-tries.\n\nEven worse. With the example below, the chunk 92313c584362 often gets to the stage were it says 'Pushed'... but, when it restarts, it forgets this, and starts the push all over again.\n\nThe example below has been re-run repeatedly for about 10 hours. It times out after 19 minutes. Each time the load of 92313c584362 nearly gets to 177Mb, then the timeout happens. When it restarts, it starts all over again. If it could remember where it was and continue the push from there, this would have completed hours ago.\n\nThe push refers to a repository [docker.synapse.org/syn7364819/blue]\n93d8703dde2a: Pushed \n934396e52c9d: Pushed \n8efaddf4ca67: Pushed \n0397e998d125: Pushed \n92313c584362: Pushing [===========================================>       ] 153.4 MB/177 MB\n4a4f58055766: Pushing 1.536 kB\nadb25476d2c1: Retrying in 1 second \n655a6c01124a: Retrying in 1 second \n0de3b0993fca: Pushing [==================================================>] 49.15 kB\n2253b7eea5e6: Retrying in 1 second \n05b90b2afe02: Retrying in 1 second \nec96d31f8eaf: Retrying in 1 second \ne3ac10d00c60: Retrying in 1 second \n2c2153fbd032: Retrying in 1 second \nd9a069c1d0fc: Retrying in 1 second \na5eb0fc1decb: Retrying in 1 second \nb2ac5371e0f2: Retrying in 1 second \n142a601d9793: Retrying in 1 second \n\n**Steps to reproduce the issue:**\n1. Docker push\n\n**Describe the results you received:**\nEach section restarts after the 19 minute time out.\n\n**Describe the results you expected:**\nIt would be good if it remembers where it was, within chunks, and, if, say, it had pushed 100MB, it started from there the next time, with the 101st MB.\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\nThis happens every time. It takes days for just one push to complete.\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.2\n Git commit:   23cf638\n Built:        Tue, 27 Sep 2016 12:25:38 +1300\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.2\n Git commit:   23cf638\n Built:        Tue, 27 Sep 2016 12:25:38 +1300\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 12\n Running: 0\n Paused: 0\n Stopped: 12\nImages: 75\nServer Version: 1.12.1\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 69\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: overlay bridge null host\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: apparmor\nKernel Version: 4.4.0-43-generic\nOperating System: Ubuntu 16.04.1 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 2\nTotal Memory: 17.87 GiB\nName: govern\nID: U36Y:L74X:VLZA:MJIS:7I5S:SFOD:BDRG:AFYB:6WNQ:AWRR:M3LL:B7UY\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nUsername: fustbariclation\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\nInsecure Registries:\n 127.0.0.0/8\n```\n\nRunning on Ubuntu under Virtualbox on OS/X - latest versions of all\n\nLinux govern 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n"},{"labels":[null,null,"enhancement"],"text":"I'm tired to have to set the http_proxy through build arg everytime I run a build. We should be able to set environment variables that are specific to our local build scenario but generic enough to be used all the time (and of course, should not be in the dockerfile) in a configuration file.\n"},{"labels":[null,"enhancement",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- IRC - irc.freenode.net #docker channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nCurrently the splunk log driver will include as attributes the environment variables that match the ones named in the <code>--env</code> flag passed as part of <code>--log-opts</code> when starting the Docker daemon. For example, supplying <code>--env=\"APP_VAR_1,APP_VAR2\"</code> will inspire the driver to attach the environment variables that have names that exactly match APP_VAR1 and APP_VAR2. \r\n\r\nI would like to be have an additional option <code>--splunk-env-regex=\"\" </code> to specify a regex to match environment variable names instead. \r\nSo, supplying <code>--splunk-env-regex=\"^APP_\" </code> would match APP_VAR1 and APP_VAR2\r\n\r\n29-November-2016 - update\r\nIn response to comments from @cpuguy83 and @outcoldman I have generalized the functionality to all log drivers. The flag is <code> --env-regex</code>\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n"},{"labels":[null,"enhancement"],"text":"Current `docker swarm create` has options for `--reserve-cpu` and `--reserve-memory` but no `--reserve-disk`. There should be a way to reserve an amount of disk space for each container.\n\nIssue spawned from discussion on https://github.com/docker/docker/issues/26259#issuecomment-248059992.\n"},{"labels":[null,"enhancement"],"text":"### Feature idea\n\nI belive docker service is missing a way to restart all replicas of service. \n\nThe reason is, that if you want to for example reload config file you updated in volume, you have to restart whole container. Without this feature you have to remove the service and create new, which cause downtime...\n\n**Steps to reproduce the issue:**\n\n```\ndocker service update --restart ...\n```\n\nThis command would send HUP command to all replicas of service. `--update-parallelism` and `update-delay` would affect parallelism and delay of this action \n"},{"labels":[null,"enhancement",null],"text":"Now that Windows 10 Anniversary Edition has been released, Windows 10 client (such as Pro or Enterprise) can support `docker run --isolation=process ...`\n\nIn Windows 10, with the containers feature and Hyper-V feature configured to run, I got things setup and I could run in both `hyperv` and `process` isolation modes.\n\nI regularly need to use VMware Workstation and cannot have Hyper-V turned on at the same time.  Turning Hyper-V on or off requires rebooting.  Very much not in the ethos of \"quick\".\n\nI rebooted into Hyper-V off mode (through [a bcdedit option](https://blog.nicholasrogoff.com/2013/12/27/create-a-no-hyper-v-boot-option/)) and can now run both VMware Workstation and `docker run` command, but I have to specify the isolation mode every time:\n\n```\n> docker run --isolation=process -it microsoft/nanoserver cmd\n```\n\nSetting the default options for `dockerd` are blocked from being changed to that default:\n\n```\n> dockerd.exe --exec-opt isolation=process\nError starting daemon: error setting default isolation mode: Windows client operating systems only support Hyper-V containers\n```\n\nAnother good reason is that running Docker in process mode is quicker than running it an Hyper-V container mode.  Even the second and subsequent times.\n\nProposal is that these lines in `deamon_windows.go` be modified to check for a minimum Windows client version, rather than blanket denying it:\n\n```\n        if containertypes.Isolation(val).IsProcess() {\n            if system.IsWindowsClient() {\n                return fmt.Errorf(\"Windows client operating systems only support Hyper-V containers\")\n            }\n            daemon.defaultIsolation = containertypes.Isolation(\"process\")\n        }\n```\n\n@jhowardmsft, @mebersol, and @taylorb-microsoft were on merge #22774 that put this restriction into place.\n"},{"labels":[null,"enhancement"],"text":"This is related to docker/distribution#1479. For telemetry purposes, I'd like to be able to correlate events on the registry so that failures can be analyzed and root caused efficiently.\r\n\r\nThere's already adequate support for single event multiple log correlation on the registry (for example: `\"http.request.id\"`), but it's still not easy to correlate log entries spanning different events coming from the same user. For example, assume there's issues in the storage used by a registry. A user might successfully push but have errors pulling; investigating this on an environment with a large amount of events might not be trivial. In general, correlating events would come handy when unexpected issues might be related.\r\n\r\nBy means of adding a session ID header to all the requests going to the registry, and having this session ID get a new value each time a client's daemon starts we can:\r\n- Obtain adequate correlation telemetry data\r\n- Maintain the user's anonymity\r\n- Keep the code changes small and concise\r\n\r\nThis feature would require changes in 1) the docker repo and 2) the distribution repo.\r\n\r\nThe change to the docker repo would be easily implemented by extending the `DockerHeaders()` function found in `/registry/registry.go` and send something like:\r\n\r\n``` go\r\n\"Docker-Session-ID\": []string{SessionID},\r\n```\r\n\r\nWhere the value of SessionID can be a global variable declared in the same file:\r\n\r\n``` go\r\n    // SessionID is a UUID that identifies the current execution context\r\n    // and is added to the docker headers in all requests against a registry\r\n    SessionID = uuid.Generate().String()\r\n```\r\n\r\nThe change to the distribution repo would be an addition to the `GetRequestLogger()` function in `/context/http.go` to also include a metric `\"http.request.sessionid\"`, and a change to `httpRequestContext.Value()` in the same file to return the appropriate value, the same way we do it today with `\"Content-Type\"`.\r\n\r\nI'll go ahead of myself and send a PR for the docker repository side of the changes. All suggestions and considerations are welcome.\r\n"},{"labels":[null,"enhancement"],"text":"Although it's a docker.com issue, perhaps someone in here (who considers this request interesting) can ping someone there. Otherwise, just close ;)\n\nScrolling trough https://github.com/docker/docker/releases I'm not able to find a way to install a specific release, cause all share the same:\n\n> curl -fsSL https://get.docker.com/ | sh\n\nI was wondering if adding a TAG version to URL could work:\n\n> curl -fsSL https://get.docker.com/v1.11.0 | sh\n\nInstead of tricks like: https://forums.docker.com/t/how-can-i-install-a-specific-version-of-the-docker-engine/1993\n\nRegards\n"},{"labels":[null,null,"enhancement",null],"text":"**Description**\nDocker run with --sysctl option works with --net=host. In Document, it is said that it should not work with --net=host.\nnet.ipv4.ip_forward value should be either 1 or 0. It accepts anything other than 1 and 0\nDocument link:\nhttps://docs.docker.com/engine/reference/commandline/run/#/full-container-capabilities---privileged\n\n**Steps to reproduce the issue:**\n1. Create container with --net=host an --sysctl \n   docker run -it --sysctl net.ipv4.ip_forward=1 --net=host ubuntu\n   which should not be allowed \n2. docker run -it --sysctl net.ipv4.ip_forward=a --net=host ubuntu \n   Should give proper error saying 'option value should be 0 or 1'\n\n**Describe the results you received:**\n1. Create container with --net=host an --sysctl \n   docker run -it --sysctl net.ipv4.ip_forward=1 --net=host ubuntu\n   :- Accepted --net=host with --sysctl\n2. docker run -it --sysctl net.ipv4.ip_forward=a --net=host ubuntu \n:-  Failed with error \"docker: Error response from daemon: oci runtime error: write /proc/sys/net/ipv4/ip_forward: invalid argument.\n\"\n**Describe the results you expected:**\n1. Create container with --net=host an --sysctl \n   docker run -it --sysctl net.ipv4.ip_forward=1 --net=host ubuntu\n   **:- Should not accept --net=host with --sysctl**\n2. docker run -it --sysctl net.ipv4.ip_forward=a --net=host ubuntu \n:-     Should give proper error saying 'option value should be 0 or 1'\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\nIssue happens every time\n\n**Output of `docker version`:**\n[root@nfs-server infra_manager]# docker version\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:\n OS/Arch:      linux/amd64\n\n**Output of `docker info`:**\nContainers: 6\n Running: 0\n Paused: 0\n Stopped: 6\nImages: 1\nServer Version: 1.12.1\nStorage Driver: devicemapper\n Pool Name: docker-253:1-747269-pool\n Pool Blocksize: 65.54 kB\n Base Device Size: 10.74 GB\n Backing Filesystem: xfs\n Data file: /dev/loop0\n Metadata file: /dev/loop1\n Data Space Used: 209.2 MB\n Data Space Total: 107.4 GB\n Data Space Available: 1.883 GB\n Metadata Space Used: 1.065 MB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 1.883 GB\n Thin Pool Minimum Free Space: 10.74 GB\n Udev Sync Supported: true\n Deferred Removal Enabled: false\n Deferred Deletion Enabled: false\n Deferred Deleted Device Count: 0\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\n WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\n Library Version: 1.02.107-RHEL7 (2016-06-09)\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge host null overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 3.10.0-327.36.1.el7.x86_64\nOperating System: CentOS Linux 7 (Core)\nOSType: linux\nArchitecture: x86_64\nCPUs: 2\nTotal Memory: 7.797 GiB\nName: nfs-server.cisco.com\nID: YCHJ:GOSU:POEO:MGF3:C3QV:ZWIR:T5ZK:POHY:XBQS:CH7O:RLVB:75AZ\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nUsername: dattatrayakumbhar04\nRegistry: https://index.docker.io/v1/\nInsecure Registries:\n 127.0.0.0/8\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nInstance is on aws ..\nOS: ubuntu/centos\n"},{"labels":[null,null,"enhancement"],"text":"### Description\n\nHotplugged CPUs are currently not recognized by Docker.\n### Steps to reproduce the issue:\n\n**Environment:**\n\nThe tests have been done on a s390x Ubuntu Xenial z/VM guest with three virtual CPUs and latest Docker built via `make deb`:\n\n```\nroot@r3545010:~# cat /etc/os-release \nNAME=\"Ubuntu\"\nVERSION=\"16.04.1 LTS (Xenial Xerus)\"\n\nroot@r3545010:~# uname -a\nLinux r3545010 4.4.0-21-generic #37-Ubuntu SMP Mon Apr 18 18:31:26 UTC 2016 s390x s390x s390x GNU/Linux\n\nroot@r3545010:~# docker version\nClient:\n Version:      1.13.0-dev\n API version:  1.25\n Go version:   go1.7.1\n Git commit:   c505486\n Built:        Tue Sep 27 16:49:25 2016\n OS/Arch:      linux/s390x\n```\n\n**Test procedure:**\n-   Stop docker daemon\n-   Enable one CPU\n-   Start docker daemon\n-   Start first container and start two CPU burner processes\n-   Enable second CPU\n-   Check if second CPU is used by first container\n-   Start second container and start two CPU burner processes\n-   Check if second CPU is used by second container\n\n**Describe the results you received:**\n\n```\nroot@r3545010:~# lscpu -e\nCPU NODE BOOK SOCKET CORE L1d:L1i:L2d:L2i ONLINE CONFIGURED POLARIZATION ADDRESS\n0   0    0    0      0    0:0:0:0         yes    yes        horizontal   0\n1   0    -    -      -    :::             no     yes        horizontal   1\n2   0    -    -      -    :::             no     yes        horizontal   2\n\nroot@r3545010:~# systemctl start docker \n\nroot@r3545010:~# docker run -ti --rm rhel:7.1 bash\n\nbash-4.2# while [ true ]; do true; done &\n[1] 7\nbash-4.2# while [ true ]; do true; done &\n[2] 8\n\nroot@r3545010:~# top\ntop - 19:22:08 up 1 day,  1:13,  2 users,  load average: 0.59, 0.83, 0.78\nTasks: 113 total,   3 running, 110 sleeping,   0 stopped,   0 zombie\n%Cpu0  : 97.4 us,  2.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.3 st\nKiB Mem :   881040 total,   385944 free,    78736 used,   416360 buff/cache\nKiB Swap:        0 total,        0 free,        0 used.   755592 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND\n23703 root      20   0    3536   1756   1416 R 50.0  0.2   0:08.83 bash\n23702 root      20   0    3536   1760   1420 R 49.7  0.2   0:10.43 bash\n\nroot@r3545010:~# chcpu -e 1\nCPU 1 enabled\n\nroot@r3545010:~# lscpu -e\nCPU NODE BOOK SOCKET CORE L1d:L1i:L2d:L2i ONLINE CONFIGURED POLARIZATION ADDRESS\n0   0    0    0      0    0:0:0:0         yes    yes        horizontal   0\n1   0    1    1      1    1:1:1:1         yes    yes        horizontal   1\n2   0    -    -      -    :::             no     yes        horizontal   2\n\nroot@r3545010:~# top\ntop - 19:23:19 up 1 day,  1:14,  2 users,  load average: 1.56, 1.08, 0.87\nTasks: 110 total,   3 running, 107 sleeping,   0 stopped,   0 zombie\n%Cpu0  : 97.3 us,  2.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.3 st\n%Cpu1  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem :   881040 total,   384772 free,    79844 used,   416424 buff/cache\nKiB Swap:        0 total,        0 free,        0 used.   754436 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n23703 root      20   0    3536   1756   1416 R  50.0  0.2   0:44.16 bash\n23702 root      20   0    3536   1760   1420 R  49.7  0.2   0:45.75 bash\n```\n\nLooking at the cpusets it turned out that Docker and its containers only use CPU 0. The global cpuset also contains the new CPU 1:\n\n```\nroot@r3545010:/sys/fs/cgroup/cpuset# cat ./cpuset.cpus\n0-1\nroot@r3545010:/sys/fs/cgroup/cpuset# cat ./docker/cpuset.cpus\n0\nroot@r3545010:/sys/fs/cgroup/cpuset# cat ./docker/c22..217/cpuset.cpus\n0\n```\n\nIt is possible to manually enable the CPU in the cpusets. First enable docker/cpuset.cpus and then do it for the container:\n\n```\nroot@r3545010:/sys/fs/cgroup/cpuset# echo \"0-1\" > ./docker/cpuset.cpus\nroot@r3545010:/sys/fs/cgroup/cpuset# echo \"0-1\" > ./docker/c22..217/cpuset.cpus\n```\n\nAfter this change the container uses the new CPU:\n\n```\ntop - 19:55:28 up 1 day,  1:46,  2 users,  load average: 2.00, 1.99, 1.86\nTasks: 110 total,   3 running, 107 sleeping,   0 stopped,   0 zombie\n%Cpu0  : 97.7 us,  2.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu1  : 97.7 us,  2.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem :   881040 total,   384200 free,    80144 used,   416696 buff/cache\nKiB Swap:        0 total,        0 free,        0 used.   754060 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n23703 root      20   0    3536   1756   1416 R 100.0  0.2  19:46.40 bash\n23702 root      20   0    3536   1760   1420 R 100.0  0.2  19:47.84 bash\n```\n\nWhen disabling and afterwards enabling CPU 1 again, the kernel (and Docker) do not include that CPU in the cpuset:\n\n```\nroot@r3545010:/sys/fs/cgroup/cpuset# chcpu -d 1\nCPU 1 disabled\nroot@r3545010:/sys/fs/cgroup/cpuset# chcpu -e 1\nCPU 1 enabled\nroot@r3545010:/sys/fs/cgroup/cpuset# cat docker/cpuset.cpus \n0\nroot@r3545010:/sys/fs/cgroup/cpuset# cat ./docker/c22..217/cpuset.cpus\n0\n```\n\nAlso newly created containers only use CPU 0.\n### Describe the results you expected:\n\nAlready running and newly started containers should be able to use additional online CPUs.\n### Initial proposal\n\nContainers which are not started with a `--cpuset-cpus` or `--cpuset-mems` option are called **unrestricted** in the following, the others are called **restricted**.\n\nFor each unrestricted container we skip the current creation of a cpuset cgroup sub-directory under the Docker cgroup directory. Then all tasks within that container will be automatically associated to the root cpuset cgroup. Because the root cpuset cgroup is managed by the kernel its cpuset.cpus will always contain all online CPUs. Therefore unrestricted containers will use all online CPUs.\n\nThis issue was created together with @michael-holzheu.\n"},{"labels":["enhancement",null],"text":"**Description**\nAs container name and volume name must be strict with regex `[a-zA-Z0-9][a-zA-Z0-9_.-]`. While network name is so casual that just not be empty. I think it is not so reasonable.\n\nIn addition, One confusion that why one character name for a container or volume is invalid. ping @vdemeester @vdemeester \n\n**Steps to reproduce the issue:**\n1. `docker network create ^`, it works\n\n**Describe the results you received:**\nIt works.\n\n```\nroot@ubuntu:~# docker network create ^\neb995b6be7ef87ce0ed14c6a8143e7bef1aa7627ab027475df304ec96588f0fd\n```\n\n**Describe the results you expected:**\n\n```\ndocker: Error response from daemon: Invalid network name (^), only [a-zA-Z0-9][a-zA-Z0-9_.-] are allowed.\n```\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n\n**Output of `docker version`:**\n\n```\nroot@ubuntu:~# docker version\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:22:43 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:22:43 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nroot@ubuntu:~# docker info\nContainers: 3\n Running: 0\n Paused: 0\n Stopped: 3\nImages: 63\nServer Version: 1.12.1\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 455\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: overlay bridge host null\nSwarm: active\n NodeID: 4yghf0i19aa9a65dgsyitqns3\n Is Manager: true\n ClusterID: 7bgsisgkf388rd3fpz5i4k71s\n Managers: 1\n Nodes: 1\n Orchestration:\n  Task History Retention Limit: 10\n Raft:\n  Snapshot Interval: 10000\n  Heartbeat Tick: 1\n  Election Tick: 3\n Dispatcher:\n  Heartbeat Period: 5 seconds\n CA Configuration:\n  Expiry Duration: 3 months\n Node Address: 192.168.59.103\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: apparmor\nKernel Version: 3.19.0-25-generic\nOperating System: Ubuntu 14.04.3 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 1.954 GiB\nName: ubuntu\nID: HRBI:EV6M:3NAC:CXZ3:BDS7:UOKL:PD6Y:ZCK4:V3PF:V7MR:CU2Y:3LGG\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 32\n Goroutines: 118\n System Time: 2016-10-08T01:49:16.635764341+08:00\n EventsListeners: 0\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\nLabels:\n a=b\nInsecure Registries:\n 192.168.1.22:5500\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n"},{"labels":[null,"enhancement"],"text":"For those containers that are logging to console .....\n\nSuch a basic feature and not implemented yet ?\n"},{"labels":["enhancement",null],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\n**Description**\n\n<!--\nBriefly describe the problem you are having in a few paragraphs.\n-->\n\nSeems like CLI completion for the new commands introduced in v 1.12 is no working for Fish.\n\n**Steps to reproduce the issue:**\n1. Open a fish session.\n2. Write `docker serv[TAB]`, `docker nod[TAB]`, `docker netw[TAB]` or `docker sw[TAB]`.\n\n**Describe the results you received:**\nNo autocompletion\n\n**Describe the results you expected:**\nAutocompletion\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:         1.12.1\n API version:     1.24\n Package version: docker-1.12.1-13.git9a3752d.fc25.x86_64\n Go version:      go1.7.1\n Git commit:      9a3752d/1.12.1\n Built:           \n OS/Arch:         linux/amd64\n\nServer:\n Version:         1.12.2\n API version:     1.24\n Package version: \n Go version:      go1.6.3\n Git commit:      bb80604\n Built:           Tue Oct 11 17:00:50 2016\n OS/Arch:         linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 1\n Running: 1\n Paused: 0\n Stopped: 0\nImages: 2\nServer Version: 1.12.2\nStorage Driver: aufs\n Root Dir: /mnt/sda1/var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 4\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge null host overlay\nSwarm: active\n NodeID: c80xbjtf78e8m749eme0u2oaa\n Is Manager: true\n ClusterID: am3yc8thmdbcwvayv7d3ruqr9\n Managers: 2\n Nodes: 3\n Orchestration:\n  Task History Retention Limit: 5\n Raft:\n  Snapshot Interval: 10000\n  Heartbeat Tick: 1\n  Election Tick: 3\n Dispatcher:\n  Heartbeat Period: 5 seconds\n CA Configuration:\n  Expiry Duration: 3 months\n Node Address: 192.168.99.100\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 4.4.24-boot2docker\nOperating System: Boot2Docker 1.12.2 (TCL 7.2); HEAD : 9d8e41b - Tue Oct 11 23:40:08 UTC 2016\nOSType: linux\nArchitecture: x86_64\nNumber of Docker Hooks: 2\nCPUs: 1\nTotal Memory: 995.9 MiB\nName: localnode1\nID: PRIA:B32W:QXXQ:P5CI:GKNE:U4PM:DT6Q:63DV:JYAE:GYYA:DC6U:ZJ65\nDocker Root Dir: /mnt/sda1/var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 40\n Goroutines: 150\n System Time: 2016-10-14T10:48:11.529998984Z\n EventsListeners: 1\nRegistry: https://index.docker.io/v1/\nLabels:\n provider=virtualbox\nInsecure Registries:\n 127.0.0.0/8\nRegistries: \n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nVirtualBox, Fedora 24 (with docker packages from F25).\n"},{"labels":[null,"enhancement"],"text":"**Description**\n\nIt seems that to have a different volume for each task in a service, you need to use anonymous volumes, which apparently only work with the local driver.\nIf you want named volumes, each task gets the same volume.\n\n**Steps to reproduce the issue:**\nCreate a service with a named volume using a shared storage driver:\n\n```\n docker service create  --name my-service  --replicas 3 \\\n  --mount type=volume,volume-driver=somedriver,source=my-volume,destination=/path/in/container \\\n  nginx:alpine\n```\n\n**Describe the results you received:**\n\nThe same volume with the same name is presented to all tasks\n\n**Describe the results you expected:**\n\nVolumes of with a recognisable name, but unique for each tasks, eg. \"servicename.taskid\" should be created.\n"},{"labels":[null,"enhancement",null],"text":"Current implementation of cloudwatchlogs allows selecting the stream name from either the container ID or a daemon parameter. It would be tremendously useful if we could select the stream name from a label on the container if it exists, and failing with the current solution for default. I'd like to specify the name of the label in the docker daemon.\n"},{"labels":[null,"enhancement"],"text":"We are containerizing the NetApp Docker Volume Plugin for use with 'docker plugin'. \n\nThe current version of the volume plugin is a native binary that can be run multiple times with different configuration details and driver names for each instance.  Storage administrators can do this in order to register differently-named volume drivers with different characteristics, eg. different types of backing storage systems, or systems with different service levels or performance characteristics.\n\nAs we transition to the new plugin model, we would prefer to maintain these instances as separate entities rather than bundle them into a monolithic container that would need additional orchestration logic to understand how to schedule the different back-ends that might have been provided. (The volume-provisioning driver seems like the wrong place for this kind of logic.)\n\nWhile we could easily run multiple containers with 'docker run', it is not clear how we would instantiate multiple instances when the driver is executed as a docker plugin instead.\n\ncc @anusha-ragunathan\n"},{"labels":[null,"enhancement"],"text":"I'd really like to be able to create anonymous volumes for containers without having to explicitly delete or garbage collect the volumes when the container is removed.\n\nThe current method of deleting anonymous volumes is `docker rm -v <container>`, but this requires knowing/tracking that the container has an anonymous volume and does not work with docker-compose.\n\nIdeally anonymous volumes would by default be lifecycle bound to the container that created them. _Persistence should be something you opt in to, not out of._ Host mounted volumes should probably always be persistent, but anonymous volumes should only be persistent if you use a special volume syntax. Ex: `docker run -v //:/<container path>` (I'm not married to the example syntax, it just needs to use the existing volume string so that docker-compose and container orchestration gets it for free.)\n"},{"labels":[null,null,"enhancement",null],"text":"**Description**\nWhen the Docker Engine performs a search of a private registry it ignores any response apart from HTTP status code 200 in which case it expects the registry to return JSON representing the SearchResults object.\n\nBut my private registry needs to return some meaningful text to the user in certain error situations (e.g. permissions errors, licensing errors, invalid search criteria, out of disk space in the registry etc).\n\n**Steps to reproduce the issue:**\n1. Install a private registry that will return some HTTP status other than 200 OK in response to a GET request on the path \"/v1/search\". \n2. Perform a 'docker search' command on the above private registry\n3. Look at the response\n\n**Describe the results you received:**\nThe user gets a message such as the following which just has the status code and no further information.\n  Error response from daemon: Unexpected status code 500\n\n**Describe the results you expected:**\nWould be good if it returned the actual text of the response from the registry as well as the status code...for example:\n  Error response from daemon: Unexpected status code 500:  Not enough space on registry volume \n\n**Additional information you deem important (e.g. issue happens only occasionally):**\nI can see where this could be fixed in the code in the file \"docker-master/registry/session.go\" in the SearchRepositories function.  Currently the code reads:\n\n```\n    res, err := r.client.Do(req)\n    if err != nil {\n            return nil, err\n    }\n    defer res.Body.Close()\n    if res.StatusCode != 200 {\n            return nil, httputils.NewHTTPRequestError(fmt.Sprintf(\"Unexpected status code %d\",\n                res.StatusCode), res)\n    }\n```\n\nBut maybe it could be modified to something like the following (so that the body of a non-200 response is returned to the user):\n\n```\n    res, err := r.client.Do(req)\n    if err != nil {\n            return nil, err\n    }\n    defer res.Body.Close()\n    if res.StatusCode != 200 {\n            errBody, err := ioutil.ReadAll(res.Body)\n            if err != nil {\n            logrus.Debugf(\"Error reading response body: %s\", err)\n            }\n            return nil, httputils.NewHTTPRequestError(fmt.Sprintf(\"Unexpected status code %d: %q\",\n                res.StatusCode, errBody), res)\n    }\n```\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.0\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   8eab29e\n Built:        Thu Jul 28 23:54:00 2016\n OS/Arch:      windows/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 17:52:38 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 0\n Running: 0\n Paused: 0\n Stopped: 0\nImages: 0\nServer Version: 1.12.1\nStorage Driver: aufs\n Root Dir: /mnt/sda1/var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 0\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: overlay null bridge host\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 4.4.17-boot2docker\nOperating System: Boot2Docker 1.12.1 (TCL 7.2); HEAD : ef7d0b4 - Thu Aug 18 21:18:06 UTC 2016\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 995.9 MiB\nName: default\nID: 4G5L:6XL6:5TEQ:IOG7:CYJQ:3UJU:SXFJ:QDQ6:JJ7W:BCMB:VKNM:JSZH\nDocker Root Dir: /mnt/sda1/var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 13\n Goroutines: 23\n System Time: 2016-10-11T13:25:47.028739034Z\n EventsListeners: 0\nRegistry: https://index.docker.io/v1/\nLabels:\n provider=virtualbox\nInsecure Registries:\n st8502:5000\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nMy test was on Windows using Docker Toolbox, but environment does not matter for this issue.\n"},{"labels":[null,"enhancement"],"text":"Currently the `--userns-remap=` option is acting quite unintuitively for someone not familiar with user namespaces. It takes some sort of namespace name and stores the mappings in some extra files you need to be aware of.\n\nThis might be naive, but why can't there be an easier option like \"--userns-remap-user=1:2000 --userns-remap-user=2:2001 ...\" which directly remaps root to id 2000, user 2 to 2001, ...?\n\nIf that requires adding some system-wide extra namespace, assign it some namespace name and enter that mapping somehow, why can't docker figure that out for me?\nAdding such an option might make user namespaces much more intuitive, since right now it seems unnecessarily complicated to enable them.\n"},{"labels":[null,"enhancement",null],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\nWhen using a private registry (I haven't tested it on Docker Hub) and there are no credentials cached, running `docker pull` gives the error `not found`. Looking at the logs for the registry, it appears that the registry is returning HTTP status 401 (authorization required) for /v2, but Docker is then trying to fall back to the /v1 and is getting a 404 from there since the registry does not support the v1 API.\n\n**Steps to reproduce the issue:**\n1. Install a registry (I use the image registry:2.5) with htpasswd auth.\n2. Log in to the registry and push an image\n3. On another machine (or after logging out of the registry), run `docker pull <registry>:<port>/<image>`.\n\n**Describe the results you received:**\n\nDocker output:\n\n```\nUsing default tag: latest\nPulling repository registry:5000/bmerry/abacuscm\nError: image bmerry/abacuscm:latest not found\n```\n\nRegistry log file:\n\n```\ntime=\"2016-10-11T09:14:34Z\" level=warning msg=\"error authorizing context: basic authentication challenge for realm \\\"Registry Realm\\\": invalid authorization credential\" go.version=go1.6.3 http.request.host=\"registry:5000\" http.request.id=2ec10df4-0f18-4d69-b69b-e017b720b5ea http.request.method=GET http.request.remoteaddr=\"172.17.0.1:32768\" http.request.uri=\"/v2/\" http.request.useragent=\"docker/1.12.1 go/go1.6.3 git-commit/23cf638 kernel/4.4.0-38-generic os/linux arch/amd64 UpstreamClient(Docker-Client/1.12.1 \\\\(linux\\\\))\" instance.id=be638723-6e27-43a5-a2d6-1fc5254d7b71 version=v2.5.0 \n172.17.0.1 - - [11/Oct/2016:09:14:34 +0000] \"GET /v2/ HTTP/1.1\" 401 87 \"\" \"docker/1.12.1 go/go1.6.3 git-commit/23cf638 kernel/4.4.0-38-generic os/linux arch/amd64 UpstreamClient(Docker-Client/1.12.1 \\\\(linux\\\\))\"\n2016/10/11 09:14:34 http: TLS handshake error from 172.17.0.1:32772: tls: first record does not look like a TLS handshake\n172.17.0.1 - - [11/Oct/2016:09:14:34 +0000] \"GET /v1/_ping HTTP/1.1\" 404 19 \"\" \"docker/1.12.1 go/go1.6.3 git-commit/23cf638 kernel/4.4.0-38-generic os/linux arch/amd64 UpstreamClient(Docker-Client/1.12.1 \\\\(linux\\\\))\"\n172.17.0.1 - - [11/Oct/2016:09:14:34 +0000] \"GET /v1/repositories/bmerry/abacuscm/images HTTP/1.1\" 404 19 \"\" \"docker/1.12.1 go/go1.6.3 git-commit/23cf638 kernel/4.4.0-38-generic os/linux arch/amd64 UpstreamClient(Docker-Client/1.12.1 \\\\(linux\\\\))\"\n```\n\n**Describe the results you expected:**\nDocker should report that authorization is required, and possibly the CLI should prompt for credentials when run interactively.\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\nThe symptoms are the same as described in #17721, but I was asked to open a separate issue.\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:22:43 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:22:43 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 2\n Running: 1\n Paused: 0\n Stopped: 1\nImages: 385\nServer Version: 1.12.1\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 634\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: overlay bridge host null\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: apparmor\nKernel Version: 4.4.0-38-generic\nOperating System: Ubuntu 14.04.5 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 8\nTotal Memory: 31.36 GiB\nName: kryton\nID: MFLN:XRFL:372N:VPKR:3AWK:USXB:3Q3E:EAH2:66ZK:T5U2:NEOP:TSCH\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n\nPhysical box running Ubuntu 14.04.\n"},{"labels":["enhancement",null],"text":"**Description**\n\nDocker [tag](https://docs.docker.com/engine/reference/commandline/tag/) is enforcing a strict pattern on the tag names:\n\n> A tag name may contain lowercase and uppercase characters, digits, underscores, periods and dashes. A tag name may not start with a period or a dash and may contain a maximum of 128 characters.\n\nThis prevents users to tag with the de-facto standard: semver, as semver 2.0 can potentially use a '+' character to include [metadata](http://semver.org/#spec-item-10).\n\n**Steps to reproduce the issue:**\n1. \n`docker tag 1fa4b226ec62 1.0.0-alpha+001`\n\n**Describe the results you received:**\n\n`Error parsing reference: \"1.0.0-alpha+001\" is not a valid repository/tag`\n\n**Describe the results you expected:**\n\nReturn code 0.\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n\nHappens every time.\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.2-rc1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   45bed2c\n Built:        Tue Sep 27 23:38:15 2016\n OS/Arch:      darwin/amd64\n Experimental: true\n\nServer:\n Version:      1.12.2-rc1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   45bed2c\n Built:        Tue Sep 27 23:38:15 2016\n OS/Arch:      linux/amd64\n Experimental: true\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 5\n Running: 0\n Paused: 0\n Stopped: 5\nImages: 81\nServer Version: 1.12.2-rc1\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 79\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge host null overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 4.4.22-moby\nOperating System: Alpine Linux v3.4\nOSType: linux\nArchitecture: x86_64\nCPUs: 4\nTotal Memory: 1.952 GiB\nName: moby\nID: GMUV:VFBH:VPNG:VPSB:HIZE:CV37:XVXC:QU22:VHLC:UEZF:AMSH:MUAA\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 15\n Goroutines: 27\n System Time: 2016-10-10T23:57:23.513758787Z\n EventsListeners: 1\nNo Proxy: *.local, 169.254/16\nRegistry: https://index.docker.io/v1/\nExperimental: true\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n\nMacOSX \n"},{"labels":["enhancement"],"text":"I suppose would be rather very fine if there was a way to say \"This container need to be able to call Docker client\" with just one flag for `docker run` (`--with-engine-api-access`?) and have client library or command that would work with older and newever versions of the daemon (same major version?).\n"},{"labels":[null,"enhancement",null],"text":"**Description:**\n\nWhen trying to pull down an image with a system user that does not belong to the `docker` group without sudo, it fails as expected but the error reported is not very helpful.\n\n**Steps to reproduce the issue:**\n1. Install Docker\n2. Login to the system with a user not in the docker group\n3. docker pull hello-world\n\n**Describe the results you received:**\n\n```\nvagrant@pulltest:~$ docker pull hello-world\nUsing default tag: latest\nWarning: failed to get default registry endpoint from daemon (Cannot connect to the Docker daemon. Is the docker daemon running on this host?). Using system default: https://index.docker.io/v1/\nCannot connect to the Docker daemon. Is the docker daemon running on this host?\n```\n\n**Describe the results you expected:**\n\n```\nWarning: user `vagrant` does not have permission to access the Docker daemon. Does `vagrant` belong to the `docker` group?\n```\n\n**Output of `docker version`:**\n\n```\nvagrant@pulltest:~$ docker --version\nDocker version 1.12.1, build 23cf638\n```\n\n**Output of `docker info`:**\n\n**Note**: Even a `docker info` with fail here too with _Cannot connect to the Docker daemon. Is the docker daemon running on this host?_ when the user does not belong to the group.\n\nI have added the vagrant user to the docker group to run `docker info` successfully.\n\n```\nvagrant@pulltest:~$ docker info\nContainers: 0\n Running: 0\n Paused: 0\n Stopped: 0\nImages: 1\nServer Version: 1.12.1\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 7\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: null overlay host bridge\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options:\nKernel Version: 3.16.0-4-amd64\nOperating System: Debian GNU/Linux 8 (jessie)\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 494.4 MiB\nName: pulltest\nID: PW76:TH7V:BLUW:LKZU:IEJF:DLOY:XR7O:5PEV:WECQ:ESG4:O7WR:VNLM\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nWARNING: No memory limit support\nWARNING: No swap limit support\nWARNING: No kernel memory limit support\nWARNING: No oom kill disable support\nWARNING: No cpu cfs quota support\nWARNING: No cpu cfs period support\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n\nThe test was performed in a `debian/contrib-jessie64` Vagrant driven VirtualBox.\n"},{"labels":[null,null,"enhancement",null],"text":"**Description**\n\nHi, I have started to use 1.12 with it's new features lately. I have a 4 node cluster (1 manager, 3 workers with swarm mode) and try to put workloads on dedicated machines. While doing that I came accross either a bug or something that is missing from documentation?\n\nI have added a label to one of my nodes and then try to execute `docker node ls` with a filter as described here: https://docs.docker.com/engine/reference/commandline/node_ls/#filtering. Unfortunately I don't get the expected response?\n\n**Steps to reproduce the issue:**\n\n```\n# docker node update --label-add test dev-swarm-04\ndev-swarm-04\n\n# docker node inspect dev-swarm-04\n[\n    {\n        \"ID\": \"7p470z31yf2u27t7wx48og9b0\",\n        \"Version\": {\n            \"Index\": 8545\n        },\n        \"CreatedAt\": \"2016-10-05T13:42:10.303870096Z\",\n        \"UpdatedAt\": \"2016-10-08T12:15:32.624496559Z\",\n        \"Spec\": {\n            \"Labels\": {\n                \"test\": \"\"\n            },\n            \"Role\": \"worker\",\n            \"Availability\": \"active\"\n        },\n        \"Description\": {\n            \"Hostname\": \"dev-swarm-04\",\n            \"Platform\": {\n                \"Architecture\": \"x86_64\",\n                \"OS\": \"linux\"\n            },\n            \"Resources\": {\n                \"NanoCPUs\": 4000000000,\n                \"MemoryBytes\": 8371408896\n            },\n            \"Engine\": {\n                \"EngineVersion\": \"1.12.1\",\n                \"Plugins\": [\n                    {\n                        \"Type\": \"Network\",\n                        \"Name\": \"bridge\"\n                    },\n                    {\n                        \"Type\": \"Network\",\n                        \"Name\": \"host\"\n                    },\n                    {\n                        \"Type\": \"Network\",\n                        \"Name\": \"null\"\n                    },\n                    {\n                        \"Type\": \"Network\",\n                        \"Name\": \"overlay\"\n                    },\n                    {\n                        \"Type\": \"Volume\",\n                        \"Name\": \"local\"\n                    }\n                ]\n            }\n        },\n        \"Status\": {\n            \"State\": \"ready\"\n        }\n    }\n]\n\n# docker node ls -f label=test\nID  HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS\n\n```\n\n**Describe the results you received:**\nNo nodes found.\n\n**Describe the results you expected:**\nOne node (dev-swarm-04) found.\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\nAdditonally I couldn't find any documentation how I would use a value with my label.\n\n`# docker node ls -f label=test=value`?\n`# docker node ls -f label=test:value`?\n`# docker node ls -f label.test=value`?\n\nUsing the label as a constraint to create a service works flawlessly :+1:.\nThanks for your help :).\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:33:38 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:33:38 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 16\n Running: 3\n Paused: 0\n Stopped: 13\nImages: 204\nServer Version: 1.12.1\nStorage Driver: devicemapper\n Pool Name: docker-253:1-916657-pool\n Pool Blocksize: 65.54 kB\n Base Device Size: 10.74 GB\n Backing Filesystem: xfs\n Data file: /dev/loop0\n Metadata file: /dev/loop1\n Data Space Used: 11.68 GB\n Data Space Total: 107.4 GB\n Data Space Available: 69.87 GB\n Metadata Space Used: 19.42 MB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 2.128 GB\n Thin Pool Minimum Free Space: 10.74 GB\n Udev Sync Supported: true\n Deferred Removal Enabled: false\n Deferred Deletion Enabled: false\n Deferred Deleted Device Count: 0\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\n WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\n Library Version: 1.02.110 (2015-10-30)\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: overlay bridge host null\nSwarm: active\n NodeID: 5uzh00gzxmquexfcmurnq6ypm\n Is Manager: true\n ClusterID: 7vryc5c1s75o0jkttokhyo6dc\n Managers: 1\n Nodes: 4\n Orchestration:\n  Task History Retention Limit: 5\n Raft:\n  Snapshot Interval: 10000\n  Heartbeat Tick: 1\n  Election Tick: 3\n Dispatcher:\n  Heartbeat Period: 5 seconds\n CA Configuration:\n  Expiry Duration: 3 months\n Node Address: 10.133.13.82\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: apparmor seccomp\nKernel Version: 4.4.0-36-generic\nOperating System: Ubuntu 16.04.1 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 12\nTotal Memory: 31.42 GiB\nName: dev-swarm-01\nID: 3FQW:TNXG:PUD2:FW4B:FU4Y:YS6E:NYCR:RGET:6SKY:LCOF:FW2G:UOE6\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nUsername: sebgieghost\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nUbuntu 16.04\n"},{"labels":[null,null,"enhancement"],"text":"`docker ps` can read the formatting options from command line as well as from `config.json`.\n\nA user noted that `docker service ps` does not support this configured formatting stored in `config.json`, meaning you have to provide formatting each time you run the command.\n\n// cc: @amouat \n"},{"labels":[null,"enhancement"],"text":"Just wanted to voice an idea to implement a couple of new --filter's in docker cli tool\n\nOften, I find myself looking for containers that run or publish certain ports, or sometime find all containers that publish ports and filter out all containers that don't publish and don't expose ports.\n\nDoes anyone think that they could also benefit from something like that?\n\nIt should allow matches by complete number and/or support wildcard or regex in the search mask. \nProposed syntax:\ndocker ps --filter publishes=27017\ndocker ps --filter exposes=27017\n\ndocker ps --filter publishes=270*\ndocker ps --filter exposes=270*\n\ndocker ps --filter publishes=*\ndocker ps --filter exposes=*\n"},{"labels":[null,"enhancement"],"text":"With the latest release of docker-engine I would like to be able to publish container ports to a specific interface identified by its DNS assigned hostname rather than a static IP assignment.\n\nThe rationale behind this is, that the IP address is assigned dynamically and the DNS records get updated  once in a while, causing the originally assigned IP address to change.\n\nSo, in addition to\n\n```\ndocker create -p <IpAddress>:80:8080 <Image>:<Tag>\n```\n\nI would like to be able to\n\n```\ndocker create -p <Fqdn>:80:8080 <Image>:<Tag>\n```\n\nAnd, upon each (re)start of the container, docker will figure out the actual IP address using a forward DNS lookup.\n\nAn extension to this would be to specify the name of the interface, e.g.\n\n```\ndocker create -p <eth0|eth1|eth1:1>:80:8080 <Image>:<Tag>\n```\n\nincluding also the ability to have multiple ip addresses bound to a specific interface, e.g. eth0:1 etc.\n\nPlease note that this is in no way related to #18269.\n"},{"labels":[null,"enhancement",null],"text":"<!--\nIf you are reporting a new issue, make sure that we do not have any duplicates\nalready open. You can ensure this by searching the issue list for this\nrepository. If there is a duplicate, please close your issue and add a comment\nto the existing issue instead.\n\nIf you suspect your issue is a bug, please edit your issue description to\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\ninformation within 7 days, we cannot debug your issue and will close it. We\nwill, however, reopen it if you later provide the information.\n\nFor more information about reporting issues, see\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#reporting-other-issues\n\n---------------------------------------------------\nBUG REPORT INFORMATION\n---------------------------------------------------\nUse the commands below to provide key information from your environment:\nYou do NOT have to include this information if this is a FEATURE REQUEST\n-->\n\n**Description**\n\nWhen I generate a Docker goroutine stack dump by sending SIGUSR1 to the daemon, the dump is all on a single line, which makes it very hard to read. \n\n**Steps to reproduce the issue:**\n1. Send SIGUSR1 to the daemon to generate a stack dump. \n\nExample stack dump is attached: \n[docker_stack_dump.txt](https://github.com/docker/docker/files/511937/docker_stack_dump.txt)\n\n**Describe the results you received:**\n\nThe stack dump is constrained to a single line.\n\n**Describe the results you expected:**\n\nI expect the stack dump to be multiline and neatly formatted, so that it's easy to read.\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:22:43 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:22:43 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nContainers: 5\n Running: 5\n Paused: 0\n Stopped: 0\nImages: 21\nServer Version: 1.12.1\nStorage Driver: overlay\n Backing Filesystem: extfs\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: null bridge host overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: apparmor\nKernel Version: 4.4.19-040419-generic\nOperating System: Ubuntu 14.04.5 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 2\nTotal Memory: 14.94 GiB\nName: ip-10-97-1-212\nID: NOMS:HFPO:S7RK:NVWV:UNVF:N3SH:SG5H:UNKX:NET5:G3VN:OXRM:ZTY7\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 54\n Goroutines: 94\n System Time: 2016-10-05T17:44:08.882124958Z\n EventsListeners: 0\nRegistry: https://index.docker.io/v1/\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\n\nAWS\n"},{"labels":[null,"enhancement"],"text":"I have build scripts for building various components using Docker containers. During normal development, I don't mind having some things cached. During CI builds, I don't ever want to cache.\n\nI could plumb around switches/flags for adding or omitting the `--no-cache` flag to `docker build` but in my ideal world, I would just be able to set `DOCKER_NO_CACHE` in my build environment and avoid all of the boilerplate that would be needed throughout Makefiles/build shell scripts.\n\n**Expected:** Setting `DOCKER_NO_CACHE=1` causes `docker build` to act as if `--no-build` was supplied.\n\n**Actually:** It's ignored.\n\nThanks for any consideration!\n"},{"labels":[null,"enhancement"],"text":"**Problem**\nThe problem arises when the data volumes are shared between multiple docker-engines using a volume plugin.\nRemoving a data volume created by the plugin (docker volume rm vol-name) on one of the docker-engine removes the entry for the volume in the metadata.db of that particular docker host. The removed volume entry is still present in the metadata.db of the other docker hosts which are running the other docker-engines.\nThis leads to a situation where a volume cannot be created again with the name same as the one deleted in one of the docker hosts which have a shared view of the volumes using the plugin. \nOther commands such as 'docker volume ls/rm/inspect' behave as expected and report that the volume is not present.\n\n**Since, the existence of a volume is checked with the plugin for every command, I believe that it is appropriate to check with the plugin when a 'docker volume create -d plugin --name vol-name' is issued instead of just checking the metadata.db alone and concluding that the volume is created/present where the reality is different.**\n\nIt is undesirable to restart the docker daemon in other hosts to rebuild the metadata.db for every 'docker volume rm vol-name' issued against the volumes from the plugin.\n"},{"labels":[null,"enhancement",null],"text":"**Description**\nWhen \"oci runtime error: no such file or directory\" appears no file or directory is provided\n\n**Describe the results you received:**\n'oci runtime error: no such file or directory'\n\n**Describe the results you expected:**\n'oci runtime error: no such file or directory \"the/missing/path\"'\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.7\n Git commit:   23cf638\n Built:        Fri Aug 19 02:03:02 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.7\n Git commit:   23cf638\n Built:        Fri Aug 19 02:03:02 2016\n OS/Arch:      linux/amd64\n```\n"},{"labels":[null,"enhancement",null],"text":"**Description**\n\nWhen inspecting the history of containers for a service, the ordering is confusing, or I just don't understand what is going on.\n\n**Steps to reproduce the issue:**\nI haven't been able to reproduce, but this is what I did.\n1. Update a running service to an image that doesn't exist.\n2. Update the same service to an image that DOES exist.\n\n**Describe the results you received:**\n\n2/3 of the new containers either died or were shutdown. 1 of the new containers successfully started. The odd thing is that in `docker service ps kong`, the service that is running is not the one that is left-most or at the top of the list.\n\n![image](https://cloud.githubusercontent.com/assets/571756/19038566/1755a122-894a-11e6-9641-83efb5b43fee.png)\n\n**Describe the results you expected:**\nI expected 1 service running, with it being at the top of the list.\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\nTo reproduce this, we might need to have a container that has an occasional failure on startup--I'm not sure.\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:33:38 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:33:38 2016\n OS/Arch:      linux/amd64\n```\n\n**Output of `docker info`:**\n\n```\nroot@swarmdev:/home/ubuntu# docker info\nContainers: 14\n Running: 14\n Paused: 0\n Stopped: 0\nImages: 12\nServer Version: 1.12.1\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 146\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: null bridge host overlay\nSwarm: active\n NodeID: 5hiqzkjsb1jbkny1vuvh6bh7w\n Is Manager: true\n ClusterID: 0d17s2mvdj4i4w6j6eyj06ob2\n Managers: 1\n Nodes: 1\n Orchestration:\n  Task History Retention Limit: 5\n Raft:\n  Snapshot Interval: 10000\n  Heartbeat Tick: 1\n  Election Tick: 3\n Dispatcher:\n  Heartbeat Period: 5 seconds\n CA Configuration:\n  Expiry Duration: 3 months\n Node Address: 10.4.0.4\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: apparmor seccomp\nKernel Version: 4.4.0-36-generic\nOperating System: Ubuntu 16.04.1 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 2\nTotal Memory: 3.359 GiB\nName: swarmdev\nID: TSZI:POAG:YCAO:L37D:ISMX:M2NE:B2HE:EZJF:W3CR:JMIM:MEEE:TURK\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nUsername: odysseyops\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\nInsecure Registries:\n 127.0.0.0/8\n```\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nThis on an Azure VM and I cannot access the previously failed-to-start containers since we are using docker-cleanup to cleanup old containers/images.\n\nThe custom kong image is just an image with custom plugins installed. The command at container init is exactly the same as the public, official kong image.\n"}]