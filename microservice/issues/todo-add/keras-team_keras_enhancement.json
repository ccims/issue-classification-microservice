[{"labels":["enhancement",null],"text":"If someone makes a PR which changes the documentation, a bot should build the documentation and comment on the pull request with the html page rendered as pdf. This should allow PR reviewers to see very quickly if there is a problem of formatting, and it would be easy to fix it early. That would make huge time savings in the long run.\r\n\r\nNearly all CI projects authorize using docker and https://github.com/arachnys/athenapdf seems good enough to convert html to PDF. Furthermore pygithub is a very cool package allowing to interact with github super easily with python (ideal to make bots).\r\n\r\nThose tools should make the task not easy, but at least feasable given enough time.\r\n\r\nThis issue is also valid for keras-contrib, see https://github.com/keras-team/keras-contrib/issues/472"},{"labels":["enhancement",null,null],"text":"Hi all!\r\n\r\nI have an error when I am trying to use **ImageDataGenerator** with **flow_from_directory** function for transfer learning of NasNet model from **keras.applications**.\r\n\r\nOS: ArchLinux\r\nTensorflow version: 1.12.0\r\nKeras version: 2.2.4 (updated from master)\r\nGPUs: GeForce GTX 1080 Ti\r\nCUDA version: 9.0.176-4\r\nCUDNN version: 7.0.5-2\r\n\r\nMy code:\r\n\r\n```from keras.applications.nasnet import NASNetMobile\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.models import Model\r\nfrom keras.layers import Dense, GlobalAveragePooling2D\r\nfrom keras import backend as K\r\nfrom keras.optimizers import SGD, Adam\r\nfrom keras.callbacks import ModelCheckpoint\r\n\r\nTRAIN_SAMPLES = 2081\r\nTEST_SAMPLES = 904\r\nBATCH_SIZE = 32\r\n\r\nTRAIN_DATA_DIR = './train'\r\nTEST_DATA_DIR = './test'\r\n\r\n# create the base pre-trained model\r\nbase_model = NASNetMobile(weights='imagenet', include_top=False)\r\n\r\n# add a global spatial average pooling layer\r\nx = base_model.output\r\nx = GlobalAveragePooling2D()(x)\r\n# let's add a fully-connected layer\r\nx = Dense(1024, activation='relu')(x)\r\n# and a logistic layer -- let's say we have 200 classes\r\npredictions = Dense(200, activation='softmax')(x)\r\n\r\n# this is the model we will train\r\nmodel = Model(inputs=base_model.input, outputs=predictions)\r\n\r\n# first: train only the top layers (which were randomly initialized)\r\n# i.e. freeze all convolutional InceptionV3 layers\r\nfor layer in base_model.layers:\r\n    layer.trainable = False\r\n\r\n# prepare train dataset\r\ntrain_gen = ImageDataGenerator(rescale=1. / 255,\r\n                               shear_range=0.2,\r\n                               zoom_range=0.2,\r\n                               horizontal_flip=True)\r\n\r\ntrain_gen.flow_from_directory(directory=TRAIN_DATA_DIR,\r\n                              target_size=base_model.input.shape[1:3],\r\n                              class_mode='categorical',\r\n                              batch_size=BATCH_SIZE,\r\n                              shuffle=True)\r\n\r\n# prepare test dataset\r\nval_gen = ImageDataGenerator(rescale=1. / 255)\r\n\r\nval_gen.flow_from_directory(directory=TEST_DATA_DIR,\r\n                            target_size=base_model.input.shape[1:3],\r\n                            class_mode='categorical',\r\n                            batch_size=BATCH_SIZE)\r\n\r\n# compile the model (should be done *after* setting layers to non-trainable)\r\nmodel.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy')\r\n\r\ncheckpoint = ModelCheckpoint(filepath=\"./NasNet_mobile_model_weights.h5\",\r\n                             monitor=[\"acc\"],\r\n                             verbose=1,\r\n                             mode='max')\r\n\r\n# train the model on the new data for a few epochs\r\nmodel.fit_generator(train_gen,\r\n                    steps_per_epoch=TRAIN_SAMPLES//BATCH_SIZE,\r\n                    epochs=50,\r\n                    validation_data=val_gen,\r\n                    validation_steps=TEST_SAMPLES//BATCH_SIZE,\r\n                    callbacks=[checkpoint],\r\n                    verbose=2, \r\n                    workers=4\r\n                    )\r\n```\r\n\r\nOutput I get:\r\n\r\n```\r\nUsing TensorFlow backend.\r\n2019-02-01 10:07:40.704532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.76GiB\r\n2019-02-01 10:07:40.856400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2019-02-01 10:07:41.022663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 2 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:82:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2019-02-01 10:07:41.204602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 3 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2019-02-01 10:07:41.206764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-02-01 10:07:42.265616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-01 10:07:42.265657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3 \r\n2019-02-01 10:07:42.265663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y N N \r\n2019-02-01 10:07:42.265667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N N N \r\n2019-02-01 10:07:42.265670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N Y \r\n2019-02-01 10:07:42.265674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   N N Y N \r\n2019-02-01 10:07:42.266421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10401 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2019-02-01 10:07:42.266867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10403 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2019-02-01 10:07:42.267168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10403 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)\r\n2019-02-01 10:07:42.267448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10403 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\nFound 2081 images belonging to 15 classes.\r\nFound 904 images belonging to 15 classes.\r\nTraceback (most recent call last):\r\n  File \"/home/smirnvla/PycharmProjects/keras-nasnet/train.py\", line 118, in <module>\r\n    verbose=2\r\n  File \"/usr/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/lib/python3.6/site-packages/keras/engine/training.py\", line 1418, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File \"/usr/lib/python3.6/site-packages/keras/engine/training_generator.py\", line 133, in fit_generator\r\n    if len(validation_data) == 2:\r\nTypeError: object of type 'ImageDataGenerator' has no len()\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nI tried to debug the code and seems like in this line https://github.com/keras-team/keras/blob/e59570ae26670f788d6c649191031e4a8824f955/keras/engine/training_generator.py#L110 if statement is false due **val_gen == False**. In the code above variable **val_gen** could be initialized as False because generator has no **\\_\\_next\\_\\_** or **next** functions.\r\n\r\nIs it normal behavior?\r\n\r\n"},{"labels":["enhancement"],"text":"The [pretrained embeddings example](https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py) replaces any unknown tokens with the zero vector. This can actually cause poorer results, as the `UNK` token is designed to replace unkonwn / low frequency words for the [glove](https://groups.google.com/forum/#!searchin/globalvectors/unk|sort:date/globalvectors/9w8ZADXJclA/hRdn4prm-XUJ) and other embeddings. \r\n\r\nThe example should replace words not in the embedding w/ UNK instead of zeros. \r\n\r\nBH\r\n\r\n\r\nPlease make sure that the boxes below are checked before you submit your issue.\r\nIf your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.\r\n\r\nThank you!\r\n\r\n- [X] Check that you are up-to-date with the master branch of Keras. You can update with:\r\n`pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps`\r\n\r\n- [X] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).\r\n\r\n- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\r\n\r\n\r\n"},{"labels":["enhancement",null],"text":"In the directory `keras/keras`, we should only use relative imports for keras classes and funtions. In the directory `keras/tests` we should only use absolute imports. \r\n\r\nThe should be a tool to automatically run a check that those rules are respected and this tool should run in travis CI.\r\n\r\nThis is related to #11782."},{"labels":["enhancement",null,null],"text":"Hi,\r\nI was trying to write a sequential generator for videos. Instead of using while True inside __getitem__ I decided to rely on 'inidex', as __getitem__ is defined to have a parameter list of (self,index).\r\nI specified in my model the number of validation and the number of train steps.\r\nWhat is surprising to me was that the indices which keras calls __getitem__ at some point get far above the defined for instance steps per epoch. Is that expected behavior or I am doing something wrong?\r\n\r\nI just cannot understand the reasoning in this case, because if I am concurrently forever looping around more than 1 videos, then this may read a video in randomised order, not sequential and even worse, I may repeat batches of frames. A video for a hundred frames I would read as 10 batches of 10 frames, for instance. Simply counting on how many times __getitem__ returned tuples to the model in such a setup will not not ensure proper training or will it? Am I wrong or is there a bug? \r\n"},{"labels":["enhancement",null,null],"text":"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.\r\n\r\nThank you!\r\n\r\n- [x] Check that you are up-to-date with the master branch of Keras. You can update with:\r\npip install git+git://github.com/keras-team/keras.git --upgrade --no-deps\r\n\r\n- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).\r\n\r\n- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:\r\npip install git+git://github.com/Theano/Theano.git --upgrade --no-deps\r\n\r\n- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\r\n\r\n```python\r\nfrom keras import Model\r\nfrom keras.layers import Input, Dense\r\n\r\ninput_tensor = Input((8,))\r\nx = Dense(4)(input_tensor)\r\nmodel = Model(input_tensor, x)\r\n\r\nmodel.compile('sgd', 'mse', metrics={'random_inexistant_output_name': 'mae'})\r\n# the previous line should throw an error listing the output names available and \r\n# printing the output name that  the user provided. It currently fails silently.\r\n```\r\n\r\nWe should have an error message. Currently, the training works and the metric is just ignored."},{"labels":["enhancement",null,null],"text":" I request that the global precision, recall and f1 metrics (for binary classification) be added to callbacks.py. It would be useful for users if they could see their model's precision, recall or f1 score on the validation data (after each epoch). Many times, these metrics are more useful than simple accuracy.\r\nThank you!\r\n\r\n- [y] Check that you are up-to-date with the master branch of Keras. You can update with:\r\n`pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps`\r\n\r\n- [y] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).\r\n\r\n- [] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\r\n"},{"labels":["enhancement",null],"text":"Currently tensorboard cannot find embeddings if they are in a wrapper layer or in a submodel.\r\n\r\nHere is the original issue: #8564\r\n A pull request was made for this, but is now stale. #9509\r\n\r\nSince the PR is stale, the torch must be passed on to someone else (see what I did there? :p ). Who would like to take a stab at it?"},{"labels":["enhancement",null],"text":"Set up versioning of the docs. We should be able to autogenerate docs for various Keras version using MkDocs, then upload that to keras.io. Have a look at http://deeplearning.net/software/theano/ to see what we would like to see.\r\n\r\nNote created by @fchollet in the \"Requests for contributions\".\r\n"},{"labels":["enhancement",null],"text":"\r\nNote created by @fchollet in the \"Requests for contributions\".\r\n\r\n"},{"labels":["enhancement",null],"text":"Write a test to check for absence of API changes, and enforce validation by API owners in case of changes.\r\n\r\nNote created by @fchollet in the \"Requests for contributions\".\r\n"},{"labels":["enhancement",null],"text":"Put this example in the `examples` directory with the others.\r\n\r\nNote created by @fchollet in the \"Requests for contributions\".\r\n"},{"labels":["enhancement",null],"text":"\r\nNote created by @fchollet in the \"Requests for contributions\".\r\n"},{"labels":["enhancement",null],"text":"Everything is in the title.\r\n\r\nNote created by @fchollet in the \"Requests for contributions\".\r\n"},{"labels":["enhancement",null],"text":"Write a cross-backend way (backend method) to check whether a certain tensor depends on a certain other tensor. See for instance [this TF implementation I wrote](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/tf_utils.py#L88).\r\n\r\nNote created by @fchollet in the \"Requests for contributions\".\r\n\r\n"},{"labels":["enhancement",null],"text":"Improve performance (GPU utilization) of `multi_gpu_model` for small models (see [this analysis](https://github.com/rossumai/keras-multi-gpu/blob/master/blog/docs/measurements.md)).\r\n\r\nNote created by @fchollet in the \"Requests for contributions\".\r\n\r\n"},{"labels":["enhancement",null],"text":"The current Identity initializer will handle non-square matrices by repeating an identity matrix. It would be better to instead pad with zeros (same behavior as `tf.eye`).\r\n\r\nAs we do this we also need a way to make sure passing an Identity initializer to a RNN (with fused kernels) results in the correct behavior. This has to changed at the level of the `build` method of the RNNs.\r\n\r\nNote created by @fchollet in the \"Requests for contributions\".\r\n\r\n"},{"labels":["enhancement",null],"text":"Existing integration tests are fairly outdated and limited.\r\n\r\nNote created by @fchollet in the \"Requests for contributions\"."},{"labels":["enhancement",null],"text":"I'm training a model using fit_generator with multiprocessing. I've been working on optimizing training performance, and I've found via some profiling that roughly half of the time training is spent in various multiprocessing _recv methods (the other half waiting on tensorflow for GPU callbacks). It appears that significant time is spent transferring bytes and pickling/unpickling (I just benchmarked, and the pickling/unpickling is sufficient to explain the time).\r\n\r\nMy workload is moderately extreme in terms of batch size and number of GPUs, but I would imagine many people are unknowingly getting limited by this. I was curious if anyone had suggestions or if there was interest in contributions here - I'm considering implementing something to improve this situation, though I'm not sure what yet."},{"labels":["enhancement",null],"text":"Other datasets under the package `keras.datasets` has a load_data function which has a ·path· parameter. The fashion mnist dataset is the only special case . This is inconsistent.\r\nSo I think the better way is to give freedom to users to config where to put the downloaded data .\r\n\r\n\r\nthis is the reuter datasets use case:\r\n```python\r\n(x_train, y_train), (x_test, y_test) = reuters.load_data(\"/data/reuters.npz\",num_words=max_words,test_split=0.2)\r\n```\r\n\r\n\r\nfashion mnist is different from other dataset, it's very ugly.\r\nAnd I cannot think up why don't give freedom to users.\r\n\r\nthis is the fashion mnist code:\r\n```python\r\n\r\ndef load_data():\r\n    \"\"\"Loads the Fashion-MNIST dataset.\r\n\r\n    # Returns\r\n        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\r\n    \"\"\"\r\n    dirname = os.path.join('datasets', 'fashion-mnist')\r\n    base = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'\r\n    files = ['train-labels-idx1-ubyte.gz', 'train-images-idx3-ubyte.gz',\r\n             't10k-labels-idx1-ubyte.gz', 't10k-images-idx3-ubyte.gz']\r\n\r\n```\r\n"},{"labels":["enhancement"],"text":"Please provide a code example to predict newsgroup classification from the model given in\r\nhttps://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\r\n"},{"labels":["enhancement"],"text":"Using the [MNIST example](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py) model, when multi_gpu_model is set to 2 GPU, the speed is up to 50% **LOWER** than with a single GPU on the same system. Nothing else changes. I'm using the example model as-is with the exception that there is just one Dense layer and no Dropout. I'm timing just `model.fit()`. \r\n\r\nThe [exact code](https://pastebin.com/wVAUbjEX) to try and reproduce.\r\n\r\nIncreasing the epochs to 100, the difference is now 100 seconds with multi_gpu_support on vs. 63 seconds without. \r\n\r\nI've confirmed that both GPU are activated with:\r\n\r\n`\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))`\r\n\r\n...which produces in my console: \r\n\r\n```\r\n2018-10-09 17:18:47.610895: I tensorflow/core/common_runtime/direct_session.cc:291] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\r\n/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: GeForce GTX 1070 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\r\n```\r\n\r\nI'm on Ubuntu 18.04, CUDA 9, cudNN 7, Keras 2.2.4, and Tensorflow 1.11.0. \r\n\r\nMy system has two GPUs.\r\n\r\n\r\n"},{"labels":["enhancement"],"text":"Keras offers a function\r\n`\r\nfrom keras.utils.vis_utils import plot_model\r\n`\r\n\r\nHowever, this function only draws the recurrent form of the RNN. I find that the unrolled form of RNN is much simpler to understand. I have tried to google for it but I haven’t found anything so far.\r\n\r\nAny suggestions?"},{"labels":["enhancement"],"text":"Unable to load a model from Blob URL or file share URL. The load_model API accepts only disk paths. "},{"labels":["enhancement"],"text":"Currently keras and keras-preprocessing depends on each other.\r\nSince it's already a separate module with it's own pip package, shouldn't we be able to use keras-preprocessing as an independent tool that does not depends on keras?\r\nHaving these two modules mutually depends on each other is causing conflicts at keras-mxnet (a keras fork) awslabs/keras-apache-mxnet#129\r\n\r\nsame applies to keras-applications"},{"labels":["enhancement",null],"text":"For following python script,\r\n\r\n```\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense,Conv2D, Flatten\r\n\r\nmodel=Sequential()\r\nmodel.add(Conv2D(10,(3,3)))\r\nmodel.add(Conv2D(10,(3,3)))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(20))\r\nmodel.add(Dense(10))\r\n\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer='sgd',\r\n              metrics=['accuracy'])\r\n\r\nmodel.summary()\r\n\r\n```\r\n\r\nIn the keras version [2.2.2](https://github.com/keras-team/keras/releases/tag/2.2.2), the error message is like following\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-83-30ace2d20b7d> in <module>()\r\n     13               metrics=['accuracy'])\r\n     14 \r\n---> 15 model.summary()\r\n\r\n/opt/conda/lib/python3.6/site-packages/keras/engine/network.py in summary(self, line_length, positions, print_fn)\r\n   1245         if not self.built:\r\n   1246             raise ValueError(\r\n-> 1247                 'This model has never been called, thus its weights '\r\n   1248                 'have not yet been created, so no summary can be displayed. '\r\n   1249                 'Build the model first '\r\n\r\nValueError: This model has never been called, thus its weights have not yet been created, so no summary can be displayed. Build the model first (e.g. by calling it on some test data).\r\n```\r\n\r\nBut in the keras version [2.1.6](https://github.com/keras-team/keras/releases/tag/2.1.6), it is like following\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-30ace2d20b7d> in <module>()\r\n      3 \r\n      4 model=Sequential()\r\n----> 5 model.add(Conv2D(10,(3,3)))\r\n      6 model.add(Conv2D(10,(3,3)))\r\n      7 model.add(Flatten())\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras/models.py in add(self, layer)\r\n    482                     # know about its input shape. Otherwise, that's an error.\r\n    483                     if not hasattr(layer, 'batch_input_shape'):\r\n--> 484                         raise ValueError('The first layer in a '\r\n    485                                          'Sequential model must '\r\n    486                                          'get an `input_shape` or '\r\n\r\nValueError: The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument.\r\n```\r\n\r\n**The difference here is that version 2.1.6 doesn't allow to add the first layer if input_shape is missing and version 2.2.2 allows to build the model even if input shape is missing**. I am new to this repository so, I am not sure about it but I think that keras 2.2.2 will figure out input_shape when fit() is called for the first time, and that is being suggested by error msg in version 2.2.2. From looking at the code I can say that it is checking whether or not the model is built or not before calling summary utility.\r\n\r\nBut error message seems misleading, it asks to fit the model before calling summary() but many times we need to see the number of parameters and output shapes (especially CNN) before actually training. And it is very likely that such error will occur due to **not specifying \"input_shape\" by mistake. And in such scenario debugging the error will be very difficult.**\r\n\r\nSo i am suggesting following error message\r\n\r\n```\r\nValueError: This model has not yet been built, so no summary can be displayed. Build the model first (e.g. by calling it on some test data) or specify the 'input_shape' in the first layer\r\n```\r\n\r\n**Above suggested error message would help in debugging if \"input_shape\" is not specified due to a mistake and creating a model which computes input shape on the go is not the intention**\r\n\r\n **I will be happy to send PR to fix this issue, but I want to discuss it with the community first.**\r\n\r\n\r\nIssue Guidelines List : \r\n- [X] Check that you are up-to-date with the master branch of Keras. You can update with:\r\npip install git+git://github.com/keras-team/keras.git --upgrade --no-deps\r\n\r\n- [X] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).\r\n   \r\n    _Used keras 2.2.2 and 2.1.6_\r\n\r\n- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:\r\npip install git+git://github.com/Theano/Theano.git --upgrade --no-deps\r\n\r\n     _I am using tensorflow backend_\r\n\r\n- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\r\n     \r\n   _Provided above_\r\n"},{"labels":["enhancement"],"text":"Like what is described in [#6900](https://github.com/keras-team/keras/issues/6900), \r\nThe code  `load_model()` for a single custom layer works well:\r\n`model = load_model('Network.hdf5', custom_objects= {'Myclass': Myclass})`\r\n\r\nbut not works for the Model which contains multipe same Custom layers, like AlexNet, which contains\r\n3 LRN, and LRN is my custom layer.\r\n\r\n Since I will get something like that:\r\n\r\n ```\r\nFile \"D:/AlexNet.py\", line 114, in <module>\r\n    model = load_model('AlexNet.h5', custom_objects= {'LRN': LRN})\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\", line 260, in load_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\", line 334, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\keras\\layers\\__init__.py\", line 55, in deserialize\r\n    printable_module_name='layer')\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 145, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\", line 293, in from_config\r\n    model.add(layer)\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\", line 193, in add\r\n    self.build()\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\", line 233, in build\r\n    name=self.name)\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\", line 237, in _init_graph_network\r\n    self.inputs, self.outputs)\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\", line 1442, in _map_graph_network\r\n    ' times in the model. \r\nValueError: The name \"LRN2D\" is used 2 times in the model. All layer names should be unique.\r\n```\r\n\r\nWhich is caused by multipe same Custom layers\r\nSince I am working on network prune experiment, it is a little complicated  to use `model.load_weights()`,\r\nbecause the model architecture is always changing. \r\nAny idea about this? Appreciation in advance!"},{"labels":["enhancement"],"text":"when I retrain the ResNet50 network model for imagenet, I want to train all the weights.\r\nso I set all the layers.trainable= True, but when it started training, suddenly killed the training process, and has no error reported.\r\n\r\n    model = keras.applications.resnet50.ResNet50()\r\n    print(\"len layers={}\".format(len(model.layers)))\r\n    model.layers.pop()\r\n    for layer in model.layers:\r\n        layer.trainable=True\r\n\r\nwhen run this training code, the log as below:\r\nInstructions for updating:\r\n\r\nFuture major versions of TensorFlow will allow gradients to flow\r\ninto the labels input on backprop by default.\r\n\r\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\r\n\r\nEpoch 1/100\r\n已杀死\r\n\r\nbut when I changed code  layer.trainable=False, it ran successfully.\r\nconfused that is there any layers in ResNet50 can't be set layer.trainable=True?"},{"labels":["enhancement"],"text":"I'm using Keras 2.1.4 with tensorflow 1.5.0 with CPU and GPU versions.\r\n\r\nI've noticed this several times. But every once and a while, when sending a Ctrl + C or SIGINT, not all processes under the process tree of a Keras process terminates. We are left with a \"child process leak\". This can be quite annoying, I later have to find the Process Group ID, and then explicitly send `kill -TERM -PGID`, and I think even once I had to send `kill -KILL -PGID`.\r\n\r\nMy suggestion is to incorporate tests into Keras that tests multiprocessing shutdown behaviour. That is run multiprocessing and test \"chaotically\" sending SIGINT, SIGTERM, SIGQUIT... etc to the process tree at random points in time and observe the behaviour. "},{"labels":["enhancement",null],"text":"TensorBoard in Keras crashes deep in the callbacks for the end-of-epoch. Here's a simplified version of the code:\r\n\r\n# The Code\r\n    import pandas as pd\r\n    import keras\r\n    from keras.models import Sequential\r\n    from keras.layers import Dense\r\n\r\n    class ToyNet ():\r\n        def __init__(self, run=1, layer_1_nodes=50, layer_2_nodes=100, layer_3_nodes=50):\r\n            self.run_seq = run\r\n            self.layer_1_nodes = layer_1_nodes\r\n            self.layer_2_nodes = layer_2_nodes\r\n            self.layer_3_nodes = layer_3_nodes\r\n\r\n            # Define the model\r\n            self.model = Sequential()\r\n            self.model.add(Dense(self.layer_1_nodes, input_dim=9, activation='relu', name='layer_1'))\r\n            self.model.add(Dense(self.layer_2_nodes, activation='relu', name='layer_2'))\r\n            self.model.add(Dense(self.layer_3_nodes, activation='relu', name='layer_3'))\r\n            self.model.add(Dense(1, activation='linear', name='output_layer'))\r\n            self.model.compile(loss='mean_squared_error', optimizer='adam')\r\n\r\n            # Create a TensorBoard logger\r\n            log_dir = \"logs_{}\".format(self.run_seq)\r\n            self.logger = keras.callbacks.TensorBoard(\r\n                log_dir=log_dir,\r\n                histogram_freq=5\r\n            )\r\n\r\n        def train(self, X, Y, epochs=50):\r\n            # Train the model\r\n            self.model.fit(\r\n                X,\r\n                Y,\r\n                epochs=epochs,\r\n                shuffle=True,\r\n                verbose=2,\r\n                validation_split=0.05,\r\n                callbacks=[self.logger]\r\n            )\r\n\r\n        def test(self, Xt, Yt):\r\n            test_error_rate = self.model.evaluate(Xt, Yt, verbose=0)\r\n            print(\"The mean squared error (MSE) for the test data set is: {}\".format(test_error_rate))\r\n\r\n    if __name__ == '__main__':\r\n\r\n        training_data_df = pd.read_csv(\"sales_data_training_scaled.csv\")\r\n\r\n        X = training_data_df.drop('total_earnings', axis=1).values\r\n        Y = training_data_df[['total_earnings']].values\r\n\r\n        # Load the test data set\r\n        test_data_df = pd.read_csv(\"sales_data_test_scaled.csv\")\r\n\r\n        X_test = test_data_df.drop('total_earnings', axis=1).values\r\n        Y_test = test_data_df[['total_earnings']].values\r\n\r\n        print(\"Run #1\")\r\n        toy1 = ToyNet(1, 50, 100, 50)\r\n        toy1.train(X, Y)\r\n        toy1.test(X_test, Y_test)\r\n\r\n        print(\"Run #2\")\r\n        toy2 = ToyNet(2, 5, 100, 50)\r\n        toy2.train(X,Y)              ### <--- Crashes here at the end of first epoch while doing callbacks\r\n        toy2.test(X_test, Y_test)\r\n\r\n# The Output\r\n\r\n/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nUsing TensorFlow backend.\r\nRun #1\r\nTrain on 950 samples, validate on 50 samples\r\nEpoch 1/50\r\n - 1s - loss: 0.0314 - val_loss: 0.0043\r\nEpoch 2/50\r\n - 0s - loss: 0.0048 - val_loss: 0.0011\r\n\r\n> ... output snipped for brevity\r\n\r\nEpoch 50/50\r\n - 0s - loss: 2.4237e-05 - val_loss: 5.0361e-05\r\nThe mean squared error (MSE) for the test data set is: 7.575784547952935e-05\r\nRun #2\r\nTrain on 950 samples, validate on 50 samples\r\nEpoch 1/50\r\n - 1s - loss: 0.0333 - val_loss: 0.0172\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1321     try:\r\n-> 1322       return fn(*args)\r\n   1323     except errors.OpError as e:\r\n\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1306       return self._call_tf_sessionrun(\r\n-> 1307           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1308 \r\n\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1408           self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1409           run_metadata)\r\n   1410     else:\r\n\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'layer_1_input' with dtype float and shape [?,9]\r\n\t [[Node: layer_1_input = Placeholder[dtype=DT_FLOAT, shape=[?,9], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-bb0bb05e17b3> in <module>()\r\n     64 \r\n     65     toy2 = ToyNet(2, 5, 100, 50)\r\n---> 66     toy2.train(X,Y)              ### <--- Crashes here at the end of first epoch while doing callbacks\r\n     67     toy2.test(X_test, Y_test)\r\n\r\n<ipython-input-1-bb0bb05e17b3> in train(self, X, Y, epochs)\r\n     39             verbose=2,\r\n     40             validation_split=0.05,\r\n---> 41             callbacks=[self.logger]\r\n     42         )\r\n     43 \r\n\r\n/anaconda3/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1043                                         initial_epoch=initial_epoch,\r\n   1044                                         steps_per_epoch=steps_per_epoch,\r\n-> 1045                                         validation_steps=validation_steps)\r\n   1046 \r\n   1047     def evaluate(self, x=None, y=None,\r\n\r\n/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n    215                         for l, o in zip(out_labels, val_outs):\r\n    216                             epoch_logs['val_' + l] = o\r\n--> 217         callbacks.on_epoch_end(epoch, epoch_logs)\r\n    218         if callback_model.stop_training:\r\n    219             break\r\n\r\n/anaconda3/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)\r\n     75         logs = logs or {}\r\n     76         for callback in self.callbacks:\r\n---> 77             callback.on_epoch_end(epoch, logs)\r\n     78 \r\n     79     def on_batch_begin(self, batch, logs=None):\r\n\r\n/anaconda3/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)\r\n    915                     assert len(batch_val) == len(tensors)\r\n    916                     feed_dict = dict(zip(tensors, batch_val))\r\n--> 917                     result = self.sess.run([self.merged], feed_dict=feed_dict)\r\n    918                     summary_str = result[0]\r\n    919                     self.writer.add_summary(summary_str, epoch)\r\n\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    898     try:\r\n    899       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 900                          run_metadata_ptr)\r\n    901       if run_metadata:\r\n    902         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1133     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1134       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1135                              feed_dict_tensor, options, run_metadata)\r\n   1136     else:\r\n   1137       results = []\r\n\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1314     if handle is None:\r\n   1315       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1316                            run_metadata)\r\n   1317     else:\r\n   1318       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1333         except KeyError:\r\n   1334           pass\r\n-> 1335       raise type(e)(node_def, op, message)\r\n   1336 \r\n   1337   def _extend_graph(self):\r\n\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'layer_1_input' with dtype float and shape [?,9]\r\n\t [[Node: layer_1_input = Placeholder[dtype=DT_FLOAT, shape=[?,9], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'layer_1_input', defined at:\r\n  File \"/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\r\n    self.io_loop.start()\r\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\r\n    self._run_once()\r\n  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\r\n    handle._run()\r\n  File \"/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\r\n    handler_func(fileobj, events)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\r\n    self._handle_recv()\r\n  File \"/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-bb0bb05e17b3>\", line 61, in <module>\r\n    toy1 = ToyNet(1, 50, 100, 50)\r\n  File \"<ipython-input-1-bb0bb05e17b3>\", line 19, in __init__\r\n    self.model.add(Dense(self.layer_1_nodes, input_dim=9, activation='relu', name='layer_1'))\r\n  File \"/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py\", line 160, in add\r\n    name=layer.name + '_input')\r\n  File \"/anaconda3/lib/python3.6/site-packages/keras/engine/input_layer.py\", line 178, in Input\r\n    input_tensor=tensor)\r\n  File \"/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/anaconda3/lib/python3.6/site-packages/keras/engine/input_layer.py\", line 87, in __init__\r\n    name=self.name)\r\n  File \"/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 517, in placeholder\r\n    x = tf.placeholder(dtype, shape=shape, name=name)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1734, in placeholder\r\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4924, in placeholder\r\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\r\n    op_def=op_def)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'layer_1_input' with dtype float and shape [?,9]\r\n\t [[Node: layer_1_input = Placeholder[dtype=DT_FLOAT, shape=[?,9], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\n# The Data\r\n\r\nFor sanity's sake I've only included a short sample of each training and testing data below.\r\n\r\n## Training data (\"sales_data_training_scaled.csv\")\r\n    critic_rating,is_action,is_exclusive_to_us,is_portable,is_role_playing,is_sequel,is_sports,suitable_for_kids,total_earnings,unit_price\r\n    0.4999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,1.0,0.7991793127668619,1.0\r\n    0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.15750170976506905,1.0\r\n    0.4999999999999999,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.18970444169239015,1.0\r\n    0.6666666666666666,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.39223304559989647,0.0\r\n    0.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.21546366980277626,1.0\r\n    0.4999999999999999,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.2675699155283636,1.0\r\n    0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.2418106874179775,1.0\r\n    0.4999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.7090183175911721,1.0\r\n    0.6666666666666666,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.4385279384854254,1.0\r\n    0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.3957893569434946,1.0\r\n    0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.24197334614886973,0.0\r\n    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.33607142197001905,1.0\r\n    0.4999999999999999,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.26054601578529046,1.0\r\n    0.6666666666666666,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.3501229182455038,1.0\r\n    0.8333333333333334,0.0,1.0,1.0,0.0,1.0,1.0,0.0,0.39457681004047984,0.0\r\n    0.6666666666666666,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2248313339864328,1.0\r\n    0.4999999999999999,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.28864900833625995,1.0\r\n    0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.05268664165172547,0.0\r\n    0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.24431711058945305,0.0\r\n    0.16666666666666663,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.2740097225559601,1.0\r\n    0.33333333333333337,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.20141217352729157,1.0\r\n    0.4999999999999999,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.2802240254339107,0.0\r\n    0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.4069129960629193,1.0\r\n    0.6666666666666666,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.21487957708729966,1.0\r\n    0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.4250642317147557,1.0\r\n    0.4999999999999999,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.28513336167538494,1.0\r\n    0.33333333333333337,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.40075044823570727,0.5\r\n    0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.18811482227685256,0.0\r\n    0.33333333333333337,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1428661207741077,1.0\r\n    0.8333333333333334,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.27292286649045305,0.5\r\n    0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.16160144914142066,1.0\r\n    0.4999999999999999,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.5831426406166245,1.0\r\n    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.18850668194672926,0.0\r\n    0.8333333333333334,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.6118297258830706,1.0\r\n    0.9999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.18928670449714424,0.0\r\n    0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.13759449917746436,1.0\r\n    0.9999999999999999,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.34539842147095245,0.0\r\n    0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.13583113066301916,0.5\r\n    0.9999999999999999,0.0,1.0,1.0,0.0,1.0,1.0,1.0,0.48629415352766125,0.0\r\n    0.9999999999999999,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.7500009241973347,1.0\r\n    0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.04253895491765401,0.0\r\n    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.11630468937727585,0.0\r\n    0.16666666666666663,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.2775253692168352,1.0\r\n    0.4999999999999999,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.10216816694700652,0.5\r\n    0.4999999999999999,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.23650949150662648,0.0\r\n    0.33333333333333337,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.18031089998336447,0.0\r\n    0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.05621337868061589,1.0\r\n    0.9999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.7949538825530027,0.5\r\n    0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.18538289495573096,0.0\r\n    0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2587900408495222,1.0\r\n    0.16666666666666663,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.08373227851610877,1.0\r\n    0.9999999999999999,1.0,1.0,1.0,0.0,0.0,0.0,1.0,0.7447329993900298,1.0\r\n    0.16666666666666663,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.20902386277517976,1.0\r\n    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.18850668194672926,0.0\r\n    0.16666666666666663,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.12509565442413267,0.5\r\n    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.3313875898781908,1.0\r\n    0.33333333333333337,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.3717861037688767,1.0\r\n    0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.1434502134895843,1.0\r\n    0.33333333333333337,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.6329051219016284,1.0\r\n    0.8333333333333334,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.727753645958485,1.0\r\n    0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15144267203933381,0.5\r\n    0.8333333333333334,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.8846601726400621,1.0\r\n    0.8333333333333334,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.30738433670357296,1.0\r\n\r\n## Testing data (\"sales_data_test_scaled.csv\")\r\n    critic_rating,is_action,is_exclusive_to_us,is_portable,is_role_playing,is_sequel,is_sports,suitable_for_kids,total_earnings,unit_price\r\n    0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.3747139609249367,1.0\r\n    0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.19242527864549636,0.5\r\n    0.33333333333333337,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.11485185116726125,0.5\r\n    0.8333333333333334,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.14245208036820023,0.0\r\n    0.6666666666666666,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.4806824273118796,1.0\r\n    0.6666666666666666,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.13972015304707863,0.0\r\n    0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.11338792258923126,0.5\r\n    0.8333333333333334,0.0,0.0,1.0,1.0,1.0,0.0,1.0,0.44906748488937354,1.0\r\n    0.4999999999999999,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.06127428328496702,0.0\r\n    0.16666666666666663,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.20668009833459638,1.0\r\n    0.8333333333333334,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.4777545701558197,1.0\r\n    0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.13232657437015954,1.0\r\n    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.17925361823256503,0.5\r\n    0.6666666666666666,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.16335742407718895,1.0\r\n    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.23946692297739414,1.0\r\n    0.6666666666666666,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.31206816879540117,1.0\r\n    0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.2285281233248923,0.5\r\n    0.33333333333333337,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.29274505092327313,1.0\r\n    0.16666666666666663,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.3480564130053049,0.5\r\n    0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.42799208887081575,1.0\r\n    0.6666666666666666,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2248313339864328,1.0\r\n    0.8333333333333334,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.1280235115801926,0.5\r\n    0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.40164507125561455,1.0\r\n    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.2963420269495943,0.5\r\n    0.8333333333333334,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.2486090830114046,0.0\r\n    0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.28923310105173655,1.0\r\n    0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.06283432838579693,0.0\r\n    0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.29536607456424097,0.5\r\n    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.2693258904641319,1.0\r\n    0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.10460804791038984,0.5\r\n    0.4999999999999999,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.4651485185116726,0.5\r\n"},{"labels":["enhancement"],"text":"Keras has this awesome flow_from_directory function to read images that are stored in different directories named with their class names for image classification tasks, but it doesn't provide anything for regression tasks and also many datasets available on the internet simply has all the images stored in a single directory, with a csv file which maps the filename to the class names. So my suggestion is If someone can implement a flow_from_csv function that takes a CSV file as input. Something that looks like the below code will look great!\r\n`datagen.flow_from_csv(csv=\"train.csv\",filename_col=\"filenames\",\r\noutput_cols=[\"value1\",\"value2\"],\r\nsep=\",\",\r\ntarget_size=(224, 224),\r\nbatch_size=32)`\r\nThe filename_col specifies the column that contains the filenames,\r\nThe output_cols (is a list) specifies the columns that will be treated as Y values (specifying a column that contains filenames here should also read the images and return them as Y).\r\nThe sep tells what seperator is used to seperate different columns(default \",\")\r\ntarget_size and batch_size has the same functions as in flow_from_directory."},{"labels":["enhancement"],"text":"As far as I can tell, it's not possible to write formulas in LaTeX in the documentation. In my opinion, it would be useful to add formulas to documentations of e.g. optimizers and activations. At the moment, some parts of the documentation write formulas as code, which is a bit ugly and would be even uglier for more complicated formulas.\r\n\r\nThere exists a MathJax extension to MkDocs that could be used, see: https://stackoverflow.com/questions/27882261/mkdocs-and-mathjax .\r\n\r\nEDIT: Apparently the same issue has already been posted here, but it was closed due to inactivity, see https://github.com/keras-team/keras/issues/2911"},{"labels":["enhancement"],"text":"when using fit_generator with a validation generator, the progress gets to the penultimate training batch & stops, and the time freezes at that point for the period needed to process the validation batches.\r\n\r\nThis is in jupyter notebook (IPython = 6.2.1), keras 2.1.5 & tensorflow 1.4.1.\r\n\r\nIt would be helpful if the batch number got to the final batch :-) and then something indicated that validation is happening.\r\nAlso, the estimated time to complete should include an allowance for validation batches.  It would be reasonable to allow half the time of a training batch until the first epoch, and then update the multiplier.\r\n\r\nThank you!\r\n\r\n- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:\r\npip install git+git://github.com/keras-team/keras.git --upgrade --no-deps\r\n\r\n- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).\r\n\r\n- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short). - I'd also need to supply all the training data etc.\r\n\r\nHere is the output I've been looking at for the past 3 minutes:\r\n```\r\nEpoch 1/10\r\n74/74 [==============================] - 1308s 18s/step - loss: 9.3432 - acc: 0.5959 - val_loss: 8.2431 - val_acc: 0.6018\r\n\r\nEpoch 00001: val_loss improved from inf to 8.24310, saving model to checkpoints/dermatology_v1_I-20180324_230205/best.ckpt\r\nEpoch 2/10\r\n74/74 [==============================] - 1254s 17s/step - loss: 7.5390 - acc: 0.6018 - val_loss: 7.0590 - val_acc: 0.6018\r\n\r\nEpoch 00002: val_loss improved from 8.24310 to 7.05902, saving model to checkpoints/dermatology_v1_I-20180324_230205/best.ckpt\r\nEpoch 3/10\r\n73/74 [============================>.] - ETA: 7s - loss: 6.8114 - acc: 0.6036\r\n``` \r\n\r\nI cannot update the software versions easily as I'm part way through a course, and I need to stick with the versions they're using. :-(\r\n"},{"labels":["enhancement"],"text":"Here are the method of `fit` and `fit_generator`:\r\n\r\n```sh\r\n    def fit(self,\r\n            x=None,\r\n            y=None,\r\n            batch_size=None,\r\n            epochs=1,\r\n            verbose=1,\r\n            callbacks=None,\r\n            validation_split=0.,\r\n            validation_data=None,\r\n            shuffle=True,\r\n            class_weight=None,\r\n            sample_weight=None,\r\n            initial_epoch=0,\r\n            steps_per_epoch=None,\r\n            validation_steps=None,\r\n            **kwargs):\r\n```\r\n\r\n```sh\r\n    def fit_generator(self,\r\n                      generator,\r\n                      steps_per_epoch=None,\r\n                      epochs=1,\r\n                      verbose=1,\r\n                      callbacks=None,\r\n                      validation_data=None,\r\n                      validation_steps=None,\r\n                      class_weight=None,\r\n                      max_queue_size=10,\r\n                      workers=1,\r\n                      use_multiprocessing=False,\r\n                      shuffle=True,\r\n                      initial_epoch=0):\r\n```\r\n\r\nThe interface is almost the same while they are totally implemented separately, why not wrap `fit` with `fit_generator + IndexArrayGenerator`? Similarly, there are also `evaluate_generator`, etc."},{"labels":["enhancement"],"text":"```\r\nMAXLEN = 50\r\nMAX_DIGIT = 7\r\nfrom keras.models import Model\r\nfrom keras.layers import Input, LSTM, Dense\r\nlatent_dim = 150\r\n# Define an input sequence and process it.\r\nencoder_inputs = Input(shape=(None, MAX_DIGIT))\r\nencoder = LSTM(latent_dim, return_state=True)\r\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\r\n# We discard `encoder_outputs` and only keep the states.\r\nencoder_states = [state_h, state_c]\r\n\r\n# Set up the decoder, using `encoder_states` as initial state.\r\ndecoder_inputs = Input(shape=(None, MAXLEN))\r\n# We set up our decoder to return full output sequences,\r\n# and to return internal states as well. We don't use the \r\n# return states in the training model, but we will use them in inference.\r\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\r\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs,\r\n                                     initial_state=encoder_states)\r\ndecoder_dense = Dense(MAXLEN, activation='softmax')\r\ndecoder_outputs = decoder_dense(decoder_outputs)\r\n\r\n# Define the model that will turn\r\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\r\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\r\nmodel.summary()\r\n```\r\n![alt text](https://i.imgur.com/yZgXZwx.png)\r\n\r\nPlease look at lstm_3 (LSTM)    and lstm_4 (LSTM)    \r\n\r\n\r\n\r\n"},{"labels":[null,"enhancement"],"text":"We should consider moving the modules `preprocessing` and `applications` to separate repositories under `keras-team`: `keras-team/preprocessing` and `keras-team/applications`.\r\n\r\nThey would be listed as a dependency of `keras`, and would still be importable from e.g. `keras.preprocessing`.\r\n\r\nWhy?\r\n\r\n- They're not part of \"the core Keras API\" (which is about model building and training)\r\n- They're not really coupled to the rest of the codebase, so unbundling is safe\r\n- Faster CI runs, better test coverage\r\n- Unbundling makes it easier to develop in an open-source setting\r\n- Easier to sync across `keras-team/keras` and `tf.keras` (shared dependencies instead of code duplication)\r\n- Other projects might have need of Numpy preprocessing utilities\r\n\r\nAny comments or concerns? Who would be interested in taking greater ownership of these repos, should they materialize?"},{"labels":["enhancement",null],"text":"# Dense Prediction API Design, Including Segmentation and Fully Convolutional Networks\r\n\r\nThis issue is to develop an API design for dense prediction tasks such as Segmentation, which includes Fully Convolutional Networks (FCN), and was based on the discussion at https://github.com/fchollet/keras/pull/5228#issuecomment-299611150. The goal is to ensure Keras incorporates best practices by default for this sort of problem. Community input, volunteers, and implementations will be very welcome.  #6655 is where preprocessing layers can be discussed.\r\n\r\n\r\n## Motivating Tasks and Datasets\r\n\r\n  - [Pascal VOC 2012 Single Label Segmentation](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6)\r\n  - [MSCOCO Multi Label Segmentation](http://mscoco.org/explore/?id=180169)\r\n  - Unambiguously refer to a particular person or object in image with [refcoco](https://github.com/lichengunc/refer)\r\n  - Reinforcement Learning with [OpenAI Gym](https://gym.openai.com/)\r\n  - [mscoco.org/external](http://mscoco.org/external/) has additional examples\r\n     \r\n## Reference Materials\r\n\r\n  - [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1605.06211)\r\n  - [U-Net](https://arxiv.org/abs/1505.04597)\r\n  - [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122)\r\n  - [ResNet](http://arxiv.org/abs/1512.03385) and [Resnetv2](https://arxiv.org/abs/1603.05027)\r\n  - [Wider or Deeper: Revisiting the ResNet Model for Visual Recognition](https://arxiv.org/abs/1611.10080)\r\n  - [Fully Convolutional DenseNets](https://arxiv.org/abs/1611.09326)\r\n  - Daniil's Blog (highly detailed, but for tensorflow)\r\n      - [FCN for Image Segmentation](http://warmspringwinds.github.io/tensorflow/tf-slim/2017/01/23/fully-convolutional-networks-(fcns)-for-image-segmentation/)\r\n      - [Image Segmentation with Tensorflow using CNNs and Conditional Random Fields](http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/18/image-segmentation-with-tensorflow-using-cnns-and-conditional-random-fields/)\r\n      - [Upsampling and Image Segmentation with Tensorflow and TF-Slim](http://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/)\r\n      - [tf-image-segmentation](https://github.com/warmspringwinds/tf-image-segmentation) companion repository (tf only)\r\n      \r\n\r\n## Feature Requests\r\n\r\nThese are ideas rather than a finalized proposal so input is welcome!\r\n\r\n - Input data: Support one or more Images as input + Supplemental data (ex: image + vector)\r\n - Augmentation of Input Data and Dense Labels\r\n    - Example: Both image and label must be zoomed & translated equally in Pascal VOC\r\n - Input image dimensions should be able to vary\r\n    - Ideally by height, width & number of channels\r\n - Loss function \"2D\" support, such as single and multi label results for each pixel in an image\r\n - [class_weight](https://keras.io/models/sequential/) support for dense labels\r\n    - Example: Single class weight value for each class in an image segmentation task such as in Pascal VOC 2012.\r\n - Sparse to Dense Prediction weight transfer\r\n    - [Conversion of ImageNet weights from pre-trained models](https://github.com/tensorflow/models/tree/master/slim#pre-trained-models) for segmentation tasks\r\n    - [Keras-FCN example](https://github.com/aurora95/Keras-FCN/blob/master/utils/transfer_FCN.py)\r\n    - [Locking of batch normalization layers](https://github.com/tensorflow/tensorflow/issues/1122), often used during transfer process\r\n - Automatic Sparse to Dense Model conversion (advanced)\r\n   - configuration at each downsampling stage\r\n   - remove pooling layers and apply an equivalent atrous dilation in the next convolution layer\r\n   - add an upsampling layer for each downsampling stage\r\n - SegmentationTop Layer?\r\n   - Sigmoid single class predictions\r\n   - Spatial Softmax argmax multi class predictions\r\n   - Multi Label Predictions (sigmoid?)\r\n - \"Upsample\" Layer?\r\n   - like \"Activation\" layer, where reasonable upsampling approaches can be defined with a simple string parameter\r\n - Example implementation training & testing on [MSCOCO](https://github.com/farizrahman4u/keras-contrib/pull/81) & [Pascal VOC 2012 + extended berkeley labels](https://github.com/farizrahman4u/keras-contrib/pull/80)\r\n    - (advanced) pretrain pascal voc on coco then VOC\r\n - COCO [pycocotools](https://github.com/pdollar/coco/tree/master/PythonAPI/pycocotools) json format dataset support [used by several datasets](mscoco.org/external/)\r\n    - supports multi-label segmentation, keypoint data, image descriptions, and more\r\n - TFRecord dataset support (probably TensorFlow only, maybe only in tensorflow implementation of keras)\r\n - flow_from_directory & Segmentation Data Generator\r\n   - [Keras-FCN](https://github.com/ahundt/Keras-FCN/blob/master/utils/SegDataGenerator.py), \r\n   - Single class label support\r\n   - Multi class label support\r\n - mean Intesection Over Union (mIOU) utility [Keras-FCN](https://github.com/ahundt/Keras-FCN/blob/master/evaluate.py) \r\n - Image and label masks\r\n - Proper [palette handling for png based labels](https://github.com/nicolov/segmentation_keras/issues/14)\r\n - sparse label format for multi-label data?\r\n - debugging utilities\r\n     - save predictions to file\r\n - Iterative training of partial networks at varying strides, as described in the FCN paper (advanced, may not be necessary as per Keras-FCN performance)\r\n \r\n## Existing Keras Utilities with compatible license\r\n\r\n - [keras-contrib](https://github.com/farizrahman4u/keras-contrib) has:\r\n     - [DensenetFCN](https://github.com/farizrahman4u/keras-contrib/blob/master/keras_contrib/applications/densenet.py) implementation\r\n     - [MSCOCO](https://github.com/farizrahman4u/keras-contrib/pull/81)\r\n     - [Pascal VOC 2012 + extended berkeley labels](https://github.com/farizrahman4u/keras-contrib/pull/80)\r\n     - a couple of upsampling approaches\r\n     - https://github.com/farizrahman4u/keras-contrib/issues/47 incorporating coco + voc 2012\r\n - [Keras-FCN](https://github.com/aurora95/Keras-FCN)\r\n    - I've been working on this one, current basis for design suggestions\r\n - [segmentation_keras](https://github.com/nicolov/segmentation_keras)\r\n     - includes example using caffe weight conversion utilties\r\n     - fairly clean\r\n - [enet-keras](https://github.com/PavlosMelissinos/enet-keras)\r\n     - includes work towards mscoco support\r\n - https://github.com/azavea/raster-vision/ \r\n    - is apache v2 compatible? I think so if keras is in tf now\r\n -  https://github.com/JihongJu/keras-fcn\r\n\r\n## Questions\r\n\r\n - Is something as clear as [30 seconds to keras segmentation possible]((https://keras.io/#getting-started-30-seconds-to-keras))?\r\n - Is anything above missing, redundant, or out of date compared to the state of the art?\r\n - Should the current ImageDataGenerator be extended or is a separate class like [Keras-FCN's SegDataGenerator](https://github.com/ahundt/Keras-FCN/blob/master/utils/SegDataGenerator.py) clearer?\r\n - Should there be a guide of some sort?\r\n - What will make for useful training progress and debugging data? (sparse mIOU?, something else?)\r\n - What is needed to handle large datasets quickly and efficiently? (should this be out of scope?)"},{"labels":["enhancement",null],"text":"This place is for discussing how to add Recurrentshop's features to Keras.\r\n\r\nRecurrentshop is a framework for building complex RNNs using Keras.\r\n\r\nhttps://github.com/datalogai/recurrentshop\r\n\r\nSee readme and docs/ for more info on the functionalities that recurrentshop provides.\r\n\r\nA couple of Caveats : \r\n\r\n* When `unroll=False` we can not have a `Dropout` layer in the RNN on theano backend. This is some issue on theano's end. `RandomStreams` op is not supported inside `scan` it seems. I tried feeding the updates from `scan` back to the model's updates, didn't help. Perhaps @nouiz  can help?\r\n\r\n* Teacherforcing not working in tensorflow. Something about the ground truth tensor being created in a different frame. I don't know what that means. @fchollet?\r\n\r\nInterested folks please go through the documentation and post your thoughts here. Once we have a well defined roadmap, we can get to the actual work.\r\n\r\n\r\nCheers!\r\n\r\n@fchollet @EderSantana @abhaikollara @malaikannan @Joshua-Chin \r\n\r\n"},{"labels":["enhancement",null],"text":"A few items that are up for grabs right now! If you're looking to contribute to Keras, working on one of these would be a great place to start.\n- An example script of feedforward neural doodle (e.g. [this](https://github.com/DmitryUlyanov/online-neural-doodle)).\n- An example script to generate reasonable-sounding MIDI tracks using a LSTM trained on an open MIDI dataset.\n- Create versions of the weights files of VGG16, VGG19, ResNet50 and Inception V3 fine-tuned on a face dataset.\n- [DONE] Test improvements: stricter PEP8 checks in our PEP8 test. That's an easy one. In particular, we need to check for missing spaces after commas, trailing spaces, this kind of thing.\n"},{"labels":["enhancement",null],"text":"Currently Keras implements its own batchnorm system \"manually\", i.e. by creating update ops to maintain exponential averages of relevant statistics. However batchnorm ops are available natively in TF, and now Theano, which wrap the more efficient cuDNN implementation when available.\n\nWe should consider creating a batchnorm op in the Keras backend, which would wrap the TF and Theano implementations, and thus leverage cuDNN when available.\n\nAnyone interested in making a valuable contribution to Keras is welcome to look into this.\n"},{"labels":["enhancement",null],"text":"Here are the key features that we will have in Keras by the time it hits v1.0. \n- Visualization tools, possibly built on top of Bokeh. You should be able to see everything that's going on in your experiment, and maybe even manage your experiments from a GUI. Total visualization is key to doing good research.\n- Easy support for Spearmint for hyperparameter search.\n- Complete unit tests. The recent regularizers/constraints debacle that left Kaggle users confused highlights once more that reliable unit tests should be an absolute priority. We want to be able to develop quality code safely and with confidence. \n- Better convnet features, including FFT convolutions, and maybe new padding options. In the medium term we should look into incorporating support for Nervana Systems' fast convolution kernels. Let's stay state of the art ;-)\n- Support for non-sequential models, by the way of:\n  - [DONE] a Merge container that takes a list of Sequential models and turns them into a single output\n  - a Fork container that replicates the output of a Sequential model to a list of Sequential models\n    (both would be trainable end-to-end, of course).\n\nIf you see anything you would like to work on, please post here with your thoughts on how you would incorporate it into the current architecture / data structures. This is our opportunity to make a big contribution to the deep learning ecosystem! : )\n"},{"labels":["enhancement",null],"text":"One of our goals for the v1 release is to have full unit test coverage. Let's discuss tests!\n\nWe want tests to be: \n- modular (for maintainability); essentially each module should have an independent test file, with independent test functions for each feature of the module. \n- fast. It should take a few seconds to test the entirety of the library. Otherwise tests would probably not be run often enough, or would result in a significant waste of time, which is very contrary to the Keras philosophy. \n\nWhat are some best practices that you know of for unit-testing a ML library? I am not a big fan of the way tests are handled in Torch7 (one large file concatenating all test functions).  \n"}]